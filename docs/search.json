[
  {
    "objectID": "content/course-outline.html",
    "href": "content/course-outline.html",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Introduce course objectives and outline\nR setup\n\n\n\n\n\nGetting started with R/R-studio: installation and package management\n\n\n\n\nNo Readings\n\n\n\n\n\n\n\n\n\nUnderstanding causal diagrams: definitions and applications\nIntroduction to five elementary structures and four rules in causal inference\nIntroduction to R interface and data simulation\n\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html\n\n\n\n\n\nPractical exercises in R: Using the interface and simulating data\n\n\n\n\n\n\n\n\nConfounding bias using causal diagrams\nApplication of regression and simulation in R\n\n\n\n\n\nPractical exercises in R: regression and ggdag\n\n\n\n\n\n(Bulbulia 2024b) link\nsee simplified reading\n\n\n\n\n[Bulbulia (2024d)][link](https://www.cambridge.org/core/journals/evolutionary-human-sciences/article/methods-in-causal-inference-part-3-measurement-error-and-external-validity-threats/4D35FFDECF32B2EFF7557EC26075175F)\n(Suzuki, Shinozaki, and Yamamoto 2020) link\n(Neal 2020) Chapter 3 link\n(Hernan and Robins 2020) Chapter 6 link\n\n\n\n\n\n\n\n\n\nKey concepts of interaction, measurement bias, and selection bias understood through causal diagrams\nBoth External and Internal Validity clarified by Causal Graphs\nAdvanced regression and simulation exercises in R\n\n\n\n\n\nContinuation of regression and simulation exercises in R\n\n\n\n\n\n(Bulbulia 2024b) link\n\n\n\n\n(Miguel A. Hernán, Hernández-Díaz, and Robins 2004) link\n(M. A. Hernán 2017) link\n(Miguel A. Hernán and Cole 2009) link\n(Tyler J. VanderWeele and Hernán 2012) link\n(Hernan and Robins 2020) Chapter 6-9 link\n\n\n\n\n\n\n\n\n\nKey concepts of Average Treatment Effect (ATE)\nApplication of regression and simulation in R to obtain ATE estimation\n\n\n\n\n\n(Hernan and Robins 2020) Chapters 1-3 link\n\n\n\n\n(Neal 2020) Chapter 1-2 link\n\n\n\n\n\n\nRegression and simulation exercises in R focussed on estimating the ATE\n\n\n\n\n\n\n(Hernan and Robins 2020) Chapters 4-5 link\n\n\n\n\n[Bulbulia (2024c)][link](https://www.cambridge.org/core/journals/evolutionary-human-sciences/article/methods-in-causal-inference-part-2-interaction-mediation-and-timevarying-treatments/D7FD95D3ED64FE0FBBEC37AC6CEAFBC1)\n(Tyler J. VanderWeele and Robins 2007) link\n(Tyler J. VanderWeele 2009) link\n\n\n\n\n\nEffect Modification: Definine your Causal Estimand\nDistinguishing Cultural Effect-Modification from the confused and conflated concepts of “Moderation”, “Mediation”, “Interaction.”\nDetour into Causal Mediation\n\n\n\n\n\nAnalysis step 1: data wrangling and descriptive tables/graphs\n\n\n\n\n\n\n\n\nAssessment covering key terms and concepts taught so far.\n\n\n\n\n\n\n\n\nWorkflow for causal question formulation, population statement, and causal diagram creation\nMarginal Structural Models: propensity scores and Inverse Probability of Treatment Weighting (IPTW)\nIPTW when estimating conditional causal effects\nEstimation techniques evaluating evidence for group-wise effect modification using R.\n\n\n\n\n\n(Greifer 2023) link\n(Tyler J. VanderWeele, Mathur, and Chen 2020) link\n\n\n\n\n(Bulbulia 2024a) link\n(Hoffman et al. 2023) link\n\n\n\n\n\n\nEstimation ATE; CATE\n\n\n\n\n\n\n\n\nSecond assessment covering advanced topics in causal inference\nTopics include ATE, Effect-Modification, fundamental assumptions of causal inference, experiments, and real-world confounding\n\n\n\n\n\nPreparing your analysis: Hands On Study!\n\n\n\n\n\n\nNo readings, do your take-home assignment (see course details).\n\n\n\n\n\nCreating and managing Quarto documents for publication quality research workflows\n\n\n\n\n\n\n\n\n\nFactor analysis, confirmatory factor analysis (CFA), multigroup CFA, partial invariance\nWorked example on configural, metric, and scalar equivalence\n\n\n\n\n\n(Fischer and Karl 2019) link\n\n\n\n\n(Vijver et al. 2021) link\n(He and Vijver 2012) link\n(J. [et. al]. Harkness 2003) link\n\n\n\n\n\n\n\n\n\nUnderstanding causal assumptions of measurement theory\nGuidance on your final assessment.\n\n\n\n\n\n[Tyler J. VanderWeele (2022)][link](https://www.dropbox.com/scl/fi/mmyguc0hrci8wtyyfkv6w/tyler-vanderweele-contruct-measures.pdf?rlkey=o18fiyajdqqpyjgssyh6mz6qm&dl=0)\n[Bulbulia (2024d)][link](https://www.cambridge.org/core/journals/evolutionary-human-sciences/article/methods-in-causal-inference-part-3-measurement-error-and-external-validity-threats/4D35FFDECF32B2EFF7557EC26075175F)\n\n\n\n\n(J. A. Harkness, Van de Vijver, and Johnson 2003) link\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract. Psychosocial constructs can only be assessed indirectly, and measures are typically formed by a combination of indicators that are thought to relate to the construct. Reflective and formative measurement models offer different conceptualizations of the relation between the indicators and what is sometimes conceived of as a univariate latent variable supposedly corresponding to the construct. I argue that the empirical implications of these models will often be violated by data since the causally relevant constituents will generally be multivariate, not univariate. In fact, the assumption of an underlying univariate structural latent variable is so strong that it has empirically testable implications, even though the latent is unobserved. Formal statistical tests can be developed to reject this assumption, but factor analysis, as typically practiced, is not adequate to do so. Factor analysis also suffers from the inability to distinguish associations arising from causal versus conceptual relations. I put forward an outline for a new model of the process of measure construction and propose a causal interpretation of associations between constructed measures and subsequent outcomes that is applicable even if the usual assumptions of reflective and formative models fail. I discuss the practical implications of these observations and proposals for the provision of definitions, the selection of items, item-by-item analyses, the construction of measures, and the causal interpretation of regression analyses.\n\n\n\n\n\n\n\n\n\n\nAbstract: Causal inference research has shifted from being primarily descriptive (describing the data-generating mechanism using statistical models) to being primarily prescriptive (evaluating the effects of specific interventions). The focus has thereby moved from being centered on statistical models to being centered on causal estimands. This evolution has been driven by the increasing need for practical solutions to real-world problems, such as designing effective interventions, making policy decisions, and identifying effective treatment strategies. It has brought enormous progress, not solely in terms of delivering more useful answers to the scientific questions at stake, but also in providing a more hygienic inference that targets a well-understood causal estimand. However, many causal questions are not readily translated into the effects of specific interventions, and even if they can, scientists may be reliant on help from an expert statistician to make that translation, may not find the considered interventions feasible or of immediate interest, or may find too little information in the data about the considered estimand. In this talk, I will reflect on this and argue that hygienic causal inference thinking therefore comes with a price. I will next propose a compromise solution at the intersection of descriptive and prescriptive causal inference. It borrows the flexibility of statistical modeling, while tying model parameters to causal estimands in order to ensure that we understand what is being estimated and obtain valid (data-adaptive) inference for it, even when the model is wrong. Examples on structural (nested) mean models, instrumental variables estimation, target trials, … will be used to provide insight."
  },
  {
    "objectID": "content/course-outline.html#week-1---course-introduction---introduction-to-r",
    "href": "content/course-outline.html#week-1---course-introduction---introduction-to-r",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Introduce course objectives and outline\nR setup\n\n\n\n\n\nGetting started with R/R-studio: installation and package management\n\n\n\n\nNo Readings"
  },
  {
    "objectID": "content/course-outline.html#week-2---causal-diagrams-five-elementary-causal-structures",
    "href": "content/course-outline.html#week-2---causal-diagrams-five-elementary-causal-structures",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Understanding causal diagrams: definitions and applications\nIntroduction to five elementary structures and four rules in causal inference\nIntroduction to R interface and data simulation\n\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html\n\n\n\n\n\nPractical exercises in R: Using the interface and simulating data"
  },
  {
    "objectID": "content/course-outline.html#week-3-march-11---causal-diagrams-the-structures-of-confounding-bias",
    "href": "content/course-outline.html#week-3-march-11---causal-diagrams-the-structures-of-confounding-bias",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Confounding bias using causal diagrams\nApplication of regression and simulation in R\n\n\n\n\n\nPractical exercises in R: regression and ggdag\n\n\n\n\n\n(Bulbulia 2024b) link\nsee simplified reading\n\n\n\n\n(Suzuki, Shinozaki, and Yamamoto 2020) link\n(Neal 2020) Chapter 3 link\n(Hernan and Robins 2024) Chapter 6 link"
  },
  {
    "objectID": "content/course-outline.html#week-4---causal-diagrams-the-structures-of-interactioneffect-modification-measurement-bias-selection-bias",
    "href": "content/course-outline.html#week-4---causal-diagrams-the-structures-of-interactioneffect-modification-measurement-bias-selection-bias",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Key concepts of interaction, measurement bias, and selection bias understood through causal diagrams\nBoth External and Internal Validity clarified by Causal Graphs\nAdvanced regression and simulation exercises in R\n\n\n\n\n\nContinuation of regression and simulation exercises in R\n\n\n\n\n\n(Bulbulia 2024b) link\n\n\n\n\n(Miguel A. Hernán, Hernández-Díaz, and Robins 2004) link\n(M. A. Hernán 2017) link\n(Miguel A. Hernán and Cole 2009) link\n(Tyler J. VanderWeele and Hernán 2012) link\n(Hernan and Robins 2020) Chapter 6-9 link"
  },
  {
    "objectID": "content/course-outline.html#week-5---quiztest-in-class-25",
    "href": "content/course-outline.html#week-5---quiztest-in-class-25",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Assessment covering key terms and concepts taught so far"
  },
  {
    "objectID": "content/course-outline.html#week-6---causal-inference-average-treatment-marginal-effects",
    "href": "content/course-outline.html#week-6---causal-inference-average-treatment-marginal-effects",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Key concepts of Average Treatment Effect (ATE)\nApplication of regression and simulation in R to obtain ATE estimation\n\n\n\n\n\n(Hernan and Robins 2024) Chapters 1-3 link\n\n\n\n\n(Neal 2020) Chapter 1-2 link\n\n\n\n\n\n\nRegression and simulation exercises in R focussed on estimating the ATE"
  },
  {
    "objectID": "content/course-outline.html#week-7---causal-inference-and-effect-modification",
    "href": "content/course-outline.html#week-7---causal-inference-and-effect-modification",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "(Hernan and Robins 2024) Chapters 4-5 link\n\n\n\n\n[Bulbulia (2024c)][link](https://www.cambridge.org/core/journals/evolutionary-human-sciences/article/methods-in-causal-inference-part-2-interaction-mediation-and-timevarying-treatments/D7FD95D3ED64FE0FBBEC37AC6CEAFBC1)\n(Tyler J. VanderWeele and Robins 2007) link\n(Tyler J. VanderWeele 2009) link\n\n\n\n\n\nEffect Modification: Definine your Causal Estimand\nDistinguishing Cultural Effect-Modification from the confused and conflated concepts of “Moderation”, “Mediation”, “Interaction.”\nDetour into Causal Mediation\n\n\n\n\n\nAnalysis step 1: data wrangling and descriptive tables/graphs"
  },
  {
    "objectID": "content/course-outline.html#week-8---causal-inference-marginal-structural-models-inverse-probability-of-treatment-weighting-conditional-average-treatment-effects-iptw-when-groups-are-compared.",
    "href": "content/course-outline.html#week-8---causal-inference-marginal-structural-models-inverse-probability-of-treatment-weighting-conditional-average-treatment-effects-iptw-when-groups-are-compared.",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Workflow for causal question formulation, population statement, and causal diagram creation\nMarginal Structural Models: propensity scores and Inverse Probability of Treatment Weighting (IPTW)\nIPTW when estimating conditional causal effects\nEstimation techniques evaluating evidence for group-wise effect modification using R.\n\n\n\n\n\n(Greifer 2023) link\n(Tyler J. VanderWeele, Mathur, and Chen 2020) link\n\n\n\n\n(Bulbulia 2024a) link\n(Hoffman et al. 2023) link\n\n\n\n\n\n\nEstimation ATE; CATE"
  },
  {
    "objectID": "content/course-outline.html#week-9---hands-on-analysis-and-take-home-assessment-25",
    "href": "content/course-outline.html#week-9---hands-on-analysis-and-take-home-assessment-25",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Second assessment covering advanced topics in causal inference\nTopics include ATE, Effect-Modification, fundamental assumptions of causal inference, experiments, and real-world confounding\n\n\n\n\n\nPreparing your analysis: Hands On Study!"
  },
  {
    "objectID": "content/course-outline.html#week-10-may-13---hands-on-working-with-quarto-manuscript",
    "href": "content/course-outline.html#week-10-may-13---hands-on-working-with-quarto-manuscript",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "No readings, do your take-home assignment (see course details)."
  },
  {
    "objectID": "content/course-outline.html#labs",
    "href": "content/course-outline.html#labs",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Creating and managing Quarto documents for publication quality research workflows"
  },
  {
    "objectID": "content/course-outline.html#week-11-may-20---measurement-matters",
    "href": "content/course-outline.html#week-11-may-20---measurement-matters",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Factor analysis, confirmatory factor analysis (CFA), multigroup CFA, partial invariance\nWorked example on configural, metric, and scalar equivalence\n\n\n\n\n\n(Fischer and Karl 2019) link\n\n\n\n\n(Vijver et al. 2021) link\n(He and Vijver 2012) link\n(J. [et. al]. Harkness 2003) link\n\n\n\n\n\n\nR exercises focusing on measurement theory applications and graphing"
  },
  {
    "objectID": "content/course-outline.html#week-12-may-27---measurement-external-validity-in-causal-inference",
    "href": "content/course-outline.html#week-12-may-27---measurement-external-validity-in-causal-inference",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Understanding causal assumptions of measurement theory\nGuidance on your final assessment.\n\n\n\n\n\n[Tyler J. VanderWeele (2022)][link](https://www.dropbox.com/scl/fi/mmyguc0hrci8wtyyfkv6w/tyler-vanderweele-contruct-measures.pdf?rlkey=o18fiyajdqqpyjgssyh6mz6qm&dl=0)\n[Bulbulia (2024d)][link](https://www.cambridge.org/core/journals/evolutionary-human-sciences/article/methods-in-causal-inference-part-3-measurement-error-and-external-validity-threats/4D35FFDECF32B2EFF7557EC26075175F)\n\n\n\n\n(J. A. Harkness, Van de Vijver, and Johnson 2003) link\n\n\n\n\n\n\nR programming using causal inference to examine failure modes in measurement models"
  },
  {
    "objectID": "content/course-outline.html#week-13-june-3-no-seminar-comparative-report-due-40",
    "href": "content/course-outline.html#week-13-june-3-no-seminar-comparative-report-due-40",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "You assess a questions: do cultural groups vary in response to interventions on well-beings?"
  },
  {
    "objectID": "content/course-outline.html#appendix-optional-videos",
    "href": "content/course-outline.html#appendix-optional-videos",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Abstract. Psychosocial constructs can only be assessed indirectly, and measures are typically formed by a combination of indicators that are thought to relate to the construct. Reflective and formative measurement models offer different conceptualizations of the relation between the indicators and what is sometimes conceived of as a univariate latent variable supposedly corresponding to the construct. I argue that the empirical implications of these models will often be violated by data since the causally relevant constituents will generally be multivariate, not univariate. In fact, the assumption of an underlying univariate structural latent variable is so strong that it has empirically testable implications, even though the latent is unobserved. Formal statistical tests can be developed to reject this assumption, but factor analysis, as typically practiced, is not adequate to do so. Factor analysis also suffers from the inability to distinguish associations arising from causal versus conceptual relations. I put forward an outline for a new model of the process of measure construction and propose a causal interpretation of associations between constructed measures and subsequent outcomes that is applicable even if the usual assumptions of reflective and formative models fail. I discuss the practical implications of these observations and proposals for the provision of definitions, the selection of items, item-by-item analyses, the construction of measures, and the causal interpretation of regression analyses.\n\n\n\n\n\n\n\n\n\n\nAbstract: Causal inference research has shifted from being primarily descriptive (describing the data-generating mechanism using statistical models) to being primarily prescriptive (evaluating the effects of specific interventions). The focus has thereby moved from being centered on statistical models to being centered on causal estimands. This evolution has been driven by the increasing need for practical solutions to real-world problems, such as designing effective interventions, making policy decisions, and identifying effective treatment strategies. It has brought enormous progress, not solely in terms of delivering more useful answers to the scientific questions at stake, but also in providing a more hygienic inference that targets a well-understood causal estimand. However, many causal questions are not readily translated into the effects of specific interventions, and even if they can, scientists may be reliant on help from an expert statistician to make that translation, may not find the considered interventions feasible or of immediate interest, or may find too little information in the data about the considered estimand. In this talk, I will reflect on this and argue that hygienic causal inference thinking therefore comes with a price. I will next propose a compromise solution at the intersection of descriptive and prescriptive causal inference. It borrows the flexibility of statistical modeling, while tying model parameters to causal estimands in order to ensure that we understand what is being estimated and obtain valid (data-adaptive) inference for it, even when the model is wrong. Examples on structural (nested) mean models, instrumental variables estimation, target trials, … will be used to provide insight."
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Causal inference: a step by step guide",
    "section": "",
    "text": "Note\n\n\n\nRequired - grf package readings https://grf-labs.github.io/grf/ - Do “Homework” form Lecture 8\nOptional - (Bulbulia 2024a) link - (Hoffman et al. 2023) link - (Tyler J. VanderWeele, Mathur, and Chen 2020) link"
  },
  {
    "objectID": "content/09-content.html#lab",
    "href": "content/09-content.html#lab",
    "title": "Causal inference: a step by step guide",
    "section": "LAB",
    "text": "LAB\n\nScript 0 Setup\n\n# for students: reproducibility is like following a recipe; each step ensures the same result\n# restart fresh session if needed\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# load packages ----------------------------------------------------------\n# install and load 'margot' from GitHub if missing\nif (!requireNamespace(\"margot\", quietly = TRUE)) {\n  message(\"installing 'margot' from GitHub\")\n  devtools::install_github(\"go-bayes/margot\", upgrade = \"never\")\n}\nlibrary(margot)\n\n# install and load other packages from CRAN if missing\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\nif (!requireNamespace(\"qs\", quietly = TRUE)) {\n  install.packages(\"qs\")\n}\nlibrary(qs)\n\nif (!requireNamespace(\"here\", quietly = TRUE)) {\n  install.packages(\"here\")\n}\nlibrary(here)\n\n# create data directory if it doesn't exist -----------------------------\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\n# define file paths ------------------------------------------------------\n# use here() to build paths relative to your project root\ndata_dir &lt;- here::here(\"data\")\n\n# download synthetic data ------------------------------------------------\n# specify the url for the data file\nurl &lt;- \"https://www.dropbox.com/scl/fi/ru0ecayju04ja8ky1mhel/df_nz_long.qs?rlkey=prpk9a5v4vcg1ilhkgf357dhd&dl=1\"\n\n# download to a temporary file for safety\ntmp_file &lt;- tempfile(fileext = \".qs\")\ndownload.file(url, tmp_file, mode = \"wb\")\n\n# read the data into R using qread\ndf_nz_long &lt;- qread(tmp_file)\n\n# inspect the data -------------------------------------------------------\n# view the first few rows to check it loaded correctly\nprint(head(df_nz_long))\n# list column names so you know what variables are available\nprint(colnames(df_nz_long))\n\n# save a copy of the data ------------------------------------------------\n# save the dataset to your data directory for future use\nhere_save_qs(df_nz_long, \"df_nz_long\", data_dir)\n\n# takeaway: clear setup saves time later.\n\n\n\nScript 01 Initial Data Wrangling\n\n# script 1 causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  margot,          # off-cran causal workflow tools\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf, ranger,     # machine learning forests\n  doParallel      # parallel processing\n)\n\n# check margot version ------------------------------------------------------\nif (packageVersion(\"margot\") &lt; \"1.0.33\") {\n  stop(\"please install margot &gt;= 1.0.33 for this workflow\")\n}\n\n# create directories --------------------------------------------------------\n# create data directory if it doesn't exist\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\nif (!dir.exists(\"models_example_2\")) {\n  dir.create(\"models_example_2\")  # first time only: make a folder named 'data'\n}\n\ndata_dir    &lt;- here::here(\"data\")\npush_mods &lt;- here::here(\"models_example_2\") # implicit directory `margot::here_save()` uses\n\n# load data -----------------------------------------------------------------\ndf_nz_long &lt;- margot::here_read_qs(\"df_nz_long\", data_dir)\n\n# initial data prep ---------------------------------------------------------\n# prepare intial data\n\n# define labels for rural classification\nrural_labels &lt;- c(\n  \"High Urban Accessibility\", \n  \"Medium Urban Accessibility\",\n  \"Low Urban Accessibility\", \n  \"Remote\", \n  \"Very Remote\"\n)\n\ndat_prep &lt;- df_nz_long |&gt;\n  arrange(id, wave) |&gt;\n  margot::remove_numeric_attributes() |&gt;\n  mutate(\n    # cap extreme values\n    alcohol_intensity = pmin(alcohol_intensity, 15),\n    # flag heavy drinkers: freq ≥3 → 1, ≤2 → 0, else NA\n    heavy_drinker = case_when(\n      alcohol_frequency &gt;= 3 ~ 1,\n      alcohol_frequency &lt;= 2 ~ 0,\n      TRUE                  ~ NA_real_\n    ),\n    # map freq categories to weekly counts\n    alcohol_frequency_weekly = recode(\n      alcohol_frequency,\n      `0` = 0, `1` = 0.25,\n      `2` = 1, `3` = 2.5,\n      `4` = 4.5,\n      .default = NA_real_\n    ),\n    # relabel rural factor\n    rural_gch_2018_l = factor(\n      rural_gch_2018_l,\n      levels = 1:5,\n      labels = rural_labels,\n      ordered = TRUE\n    )\n  ) |&gt;\n  droplevels()\n\n# define study variables ----------------------------------------------------\n# ** key decision 1: define your exposure variable **\nname_exposure &lt;- \"extraversion\"\nexposure_var_binary = paste0(name_exposure, \"_binary\")\nexposure_var  &lt;- c(name_exposure, paste0(name_exposure, \"_binary\"))\n\n# ** key decision 2: define your study waves **\nbaseline_wave      &lt;- \"2018\"        # baseline measurement\nexposure_waves     &lt;- c(\"2019\")     # when exposure is measured\noutcome_wave       &lt;- \"2020\"        # when outcomes are measured\nall_waves          &lt;- c(baseline_wave, exposure_waves, outcome_wave)\n\n# save key variables --------------------------------------------------------\nmargot::here_save(name_exposure, \"name_exposure\",push_mods)\nmargot::here_save(exposure_var, \"exposure_var\", push_mods)\nmargot::here_save(exposure_var_binary, \"exposure_var_binary\", push_mods)\nmargot::here_save(all_waves,\"all_waves\", push_mods)\n\n# ** key decision 3: define baseline covariates **\n# these are demographics, traits, etc. measured at baseline\nbaseline_vars &lt;- c(\n  # demographics\n  \"age\", \"born_nz_binary\", \"education_level_coarsen\",\n  \"employed_binary\", \"eth_cat\", \"male_binary\",\n  \"not_heterosexual_binary\", \"parent_binary\", \"partner_binary\",\n  \"rural_gch_2018_l\", \"sample_frame_opt_in_binary\",\n  \n  # personality traits (excluding exposure)\n  \"agreeableness\", \"conscientiousness\", \"neuroticism\", \"openness\",\n  \n  # health and lifestyle\n  \"alcohol_frequency\", \"alcohol_intensity\", \"hlth_disability_binary\",\n  \"log_hours_children\", \"log_hours_commute\", \"log_hours_exercise\",\n  \"log_hours_housework\", \"log_household_inc\",\n  \"short_form_health\", \"smoker_binary\",\n  \n  # social and psychological\n  \"belong\", \"nz_dep2018\", \"nzsei_13_l\",\n  \"political_conservative\", \"religion_identification_level\"\n)\n\n# sort for easier reference\nbaseline_vars &lt;- sort(baseline_vars)\n\nbaseline_vars_no_log_init &lt;- c(\n  baseline_vars,\n  c(\n    \"hours_children\",\n    \"hours_commute\",\n    \"hours_exercise\",\n    \"hours_housework\",\n    \"hours_community\",\n    \"household_inc\"\n  )\n)\nbaseline_vars_no_log = setdiff(baseline_vars_no_log_init, c(\n  \"log_hours_children\",\n  \"log_hours_commute\",\n  \"log_hours_exercise\",\n  \"log_hours_housework\",\n  \"log_hours_community\",\n  \"log_household_inc\"\n))\n\n# sort\nbaseline_vars_no_log &lt;- sort(baseline_vars_no_log)\n\n# save\nhere_save(baseline_vars_no_log, \"baseline_vars_no_log\")\n\n# ** key decision 4: define outcome variables **\noutcome_vars &lt;- c(\n  # health outcomes\n  \"alcohol_frequency_weekly\", \"alcohol_intensity\",\n  \"hlth_bmi\", \"log_hours_exercise\", \"hlth_sleep_hours\", \n  \"short_form_health\",\n  \n  # psychological outcomes\n  \"hlth_fatigue\", \"kessler_latent_anxiety\", \n  \"kessler_latent_depression\", \"rumination\",\n  \n  # wellbeing outcomes\n  \"bodysat\", \"forgiveness\", \"gratitude\", \"lifesat\", \n  \"meaning_purpose\", \"meaning_sense\", \"perfectionism\", \n  \"pwi\", \"self_control\", \"self_esteem\", \n  \"sexual_satisfaction\",\n  \n  # social outcomes\n  \"belong\", \"neighbourhood_community\", \"support\"\n)\n\n# sort for easier reference\noutcome_vars &lt;- sort(outcome_vars)\n\n# outcome vars no log\noutcome_vars_no_log_init &lt;- c(outcome_vars,\"hours_exercise\")\n\noutcome_vars_no_log = setdiff(outcome_vars_no_log_init, c(\n  \"log_hours_exercise\"\n))\n\n\n# sort\noutcome_vars_no_log &lt;- sort(outcome_vars_no_log_init)\n\n# save\nhere_save(baseline_vars_no_log, \"baseline_vars_no_log\")\n\n\n# outcome variables by domain ---------------------------------------------\nraw_outcomes_health &lt;- c(\n  \"alcohol_frequency_weekly\", \n  \"alcohol_intensity\",\n  \"hlth_bmi\", \n  \"log_hours_exercise\", \n  \"hlth_sleep_hours\", \n  \"short_form_health\"\n)\n# sort\nraw_outcomes_health &lt;- sort(raw_outcomes_health)\n\n# save \nhere_save(raw_outcomes_health, \"raw_outcomes_health\")\n\n# with no log\nraw_outcomes_health_no_log &lt;- c(raw_outcomes_health, \"hours_exercise\")\n\n# sort\nraw_outcomes_health_no_log &lt;- sort(raw_outcomes_health_no_log)\n\n# save \nhere_save(raw_outcomes_health_no_log, \"raw_outcomes_health_no_log\")\n\n# define psych outcomes labels\nraw_outcomes_psych &lt;- c( \n  \"hlth_fatigue\", \n  \"kessler_latent_anxiety\", \n  \"kessler_latent_depression\",  \n  \"rumination\"\n)\n\n# sort\nraw_outcomes_psych &lt;- sort(raw_outcomes_psych)\n\n# save\nhere_save(raw_outcomes_psych, \"raw_outcomes_psych\")\n\n# define present outcomes labels\nraw_outcomes_present &lt;- c(\n  \"bodysat\",\n  \"forgiveness\",\n  \"perfectionism\", \n  \"self_control\" , \n  \"self_esteem\", \n  \"sexual_satisfaction\" )\n\n# sort\nraw_outcomes_present &lt;- sort(raw_outcomes_present)\n\n# save\nhere_save(raw_outcomes_present, \"raw_outcomes_present\")\n\n# define life outcomes labels\nraw_outcomes_life &lt;- c( \n  \"gratitude\", \n  \"lifesat\", \n  \"meaning_purpose\", \n  \"meaning_sense\",\n  \"pwi\"  # move personal well-being here if not using individual facents\n)\n\n# sort\nraw_outcomes_life &lt;- sort(raw_outcomes_life)\n\n# save\nhere_save(raw_outcomes_life, \"raw_outcomes_life\")\n\n# define social outcomes labels\nraw_outcomes_social &lt;- c(\n  \"belong\",\n  \"neighbourhood_community\", \n  \"support\" \n)\n\n# sort\nraw_outcomes_social &lt;- sort(raw_outcomes_social)\n\n# save\nhere_save(raw_outcomes_social, \"raw_outcomes_social\")\n\n# create all outcome variable names ---------------------------------------\n# for tables\nraw_outcomes_all = c(\n  baseline_vars_no_log,\n  raw_outcomes_health_no_log,\n  raw_outcomes_psych,\n  raw_outcomes_present,\n  raw_outcomes_life,\n  raw_outcomes_social\n)\n\n# select only unique measures\nraw_outcomes_all &lt;- unique(raw_outcomes_all)\n\n# save\nhere_save(raw_outcomes_all, \"raw_outcomes_all\")\n\n\n\n# time-varying confounder if needed ---------------------------------------\n\n# ** key decision 5: define time-varying confounders **\n# these are variables that could affect the outcome but cannot be affected by exposure\nconfounder_vars &lt;- c(\n  \"hlth_disability_binary\"  # consider carefully if this could be affected by the exposure\n)\n\n# save variables for reproducibility ----------------------------------------\nmargot::here_save(baseline_vars,    \"baseline_vars\", push_mods)\nmargot::here_save(outcome_vars,     \"outcome_vars\", push_mods)\nmargot::here_save(confounder_vars,  \"confounder_vars\", push_mods)\n\n# select eligible participants ----------------------------------------------\n# only include participants who have exposure data at baseline\nids_baseline &lt;- dat_prep |&gt; \n  filter(wave == baseline_wave, !is.na(!!sym(name_exposure))) |&gt; \n  pull(id)\n\n# filter data to include only eligible participants and relevant waves\ndat_long_1 &lt;- dat_prep |&gt; \n  filter(id %in% ids_baseline, wave %in% all_waves) |&gt; \n  droplevels()\n\n# ** key decision 6: determine binary cutpoint for exposure **\n# visualise distribution to decide on appropriate cutpoint\ndat_long_exposure &lt;- dat_long_1 |&gt; filter(wave %in% exposure_waves)\n\n# plot distribution to help with cutpoint decision\ngraph_cut &lt;- margot::margot_plot_categorical(\n  dat_long_exposure,\n  col_name         = name_exposure,\n  sd_multipliers = c(-1, 1), # select to suit\n  # either use n_divisions for equal-sized groups:\n  # n_divisions      = 2,\n  # or use custom_breaks for specific values:\n  custom_breaks    = c(1, 4),  # ** adjust as needed **\n  cutpoint_inclusive = \"upper\",\n  show_mean        = TRUE,\n  show_sd          = TRUE\n)\nprint(graph_cut)\nmargot::here_save(graph_cut, \"graph_cut\", push_mods)\n\n# create binary exposure variable based on chosen cutpoint\ndat_long_2 &lt;- margot::create_ordered_variable(\n  dat_long_1,\n  var_name           = name_exposure,\n  custom_breaks      = c(1, 4),  # ** adjust based on your decision **\n  cutpoint_inclusive = \"upper\"\n)\n\n# process binary variables and log-transform --------------------------------\n# convert binary factors to 0/1 format\ndat_long_3 &lt;- margot::margot_process_binary_vars(dat_long_2)\n\n# log-transform hours and income variables: tables for manuscript\ndat_long_tables &lt;- margot::margot_log_transform_vars(\n  dat_long_3,\n  vars            = c(starts_with(\"hours_\"), \"household_inc\"),\n  prefix          = \"log_\",\n  keep_original   = TRUE  # keep both original and transformed variables\n) |&gt; \n  # select only variables needed for analysis\n  select(all_of(c(baseline_vars_no_log, exposure_var, outcome_vars_no_log, \"id\", \"wave\"))) |&gt; \n  droplevels()\n\n# log-transform hours and income variables: tables for analysis (only logged versions of vars)\ndat_long_final &lt;- margot::margot_log_transform_vars(\n  dat_long_3,\n  vars            = c(starts_with(\"hours_\"), \"household_inc\"),\n  prefix          = \"log_\",\n  keep_original   = FALSE  # omit original variables\n) |&gt; \n  # select only variables needed for analysis\n  select(all_of(c(baseline_vars, exposure_var, outcome_vars, \"id\", \"wave\"))) |&gt; \n  droplevels()\n\n\n# check missing data --------------------------------------------------------\n# this is crucial to understand potential biases\nmissing_summary &lt;- naniar::miss_var_summary(dat_long_final)\nprint(missing_summary)\nmargot::here_save(missing_summary, \"missing_summary\", push_mods)\n\n# visualise missing data pattern\n# ** -- takes a while to render ** \nvis_miss &lt;- naniar::vis_miss(dat_long_final, warn_large_data = FALSE)\nprint(vis_miss)\nmargot::here_save(vis_miss, \"vis_miss\", push_mods)\n\n# calculate percentage of missing data at baseline\ndat_baseline_pct &lt;- dat_long_final |&gt; filter(wave == baseline_wave)\npercent_missing_baseline &lt;- naniar::pct_miss(dat_baseline_pct)\nmargot::here_save(percent_missing_baseline, \"percent_missing_baseline\", push_mods)\n\n# save prepared dataset for next stage --------------------------------------\nmargot::here_save(dat_long_final, \"dat_long_prepare\", push_mods)\n\n# visualise individual changes in exposure over time ------------------------\n# useful for understanding exposure dynamics\nindividual_plot &lt;- margot_plot_individual_responses(\n  dat_long_1,\n  y_vars = name_exposure,\n  id_col = \"id\",\n  waves = c(2018:2019),\n  random_draws = 56,  # number of randomly selected individuals to show\n  theme = theme_classic(),\n  scale_range = c(1, 7),  # range of the exposure variable\n  full_response_scale = TRUE,\n  seed = 123\n)\nprint(individual_plot)\nmargot::here_save(individual_plot, \"individual_plot_exposure\", push_mods)\n\n# create transition matrices to check positivity ----------------------------\n# this helps assess whether there are sufficient observations in all exposure states\ndt_positivity &lt;- dat_long_final |&gt;\n  filter(wave %in% c(baseline_wave, exposure_waves)) |&gt;\n  select(!!sym(name_exposure), id, wave) |&gt;\n  mutate(exposure = round(as.numeric(!!sym(name_exposure)), 0)) |&gt;\n  # create binary exposure based on cutpoint\n  mutate(exposure_binary = ifelse(exposure &gt;= 4, 1, 0)) |&gt;\n  mutate(wave = as.numeric(wave) -1 )\n\n# create transition tables\ntransition_tables &lt;- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"wave\",\n  table_name = \"transition_table\"\n)\nprint(transition_tables$tables[[1]])\nmargot::here_save(transition_tables, \"transition_tables\", push_mods)\n\n# create binary transition tables\ntransition_tables_binary &lt;- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure_binary\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"wave\",\n  table_name = \"transition_table_binary\"\n)\nprint(transition_tables_binary$tables[[1]])\nmargot::here_save(transition_tables_binary, \"transition_tables_binary\", push_mods)\n\n# create descriptive tables -------------------------------------------------\n# define variable labels for tables and plots -------------------------------\n# ** key decision 7: define clear labels for all variables **\n# create labels by category for better organisation\n\n# exposure variable labels\nvar_labels_exposure &lt;- list(\n  \"extraversion\" = \"Extraversion\",\n  \"extraversion_binary\" = \"Extraversion (binary)\"\n)\n\n# baseline variable labels\nvar_labels_baseline &lt;- list(\n  # demographics\n  \"age\" = \"Age\",\n  \"born_nz_binary\" = \"Born in NZ\",\n  \"education_level_coarsen\" = \"Education Level\",\n  \"employed_binary\" = \"Employed\",\n  \"eth_cat\" = \"Ethnicity\",\n  \"male_binary\" = \"Male\",\n  \"not_heterosexual_binary\" = \"Non-heterosexual\",\n  \"parent_binary\" = \"Parent\",\n  \"partner_binary\" = \"Has Partner\",\n  \"rural_gch_2018_l\" = \"Rural Classification\",\n  \"sample_frame_opt_in_binary\" = \"Sample Frame Opt-In\",\n  \n  # economic & social status\n  \"household_inc\" = \"Household Income\",\n  \"log_household_inc\" = \"Log Household Income\",\n  \"nz_dep2018\" = \"NZ Deprivation Index\",\n  \"nzsei_13_l\" = \"Occupational Prestige Index\",\n  \"household_inc\" = \"Household Income\",\n\n  \n  # personality traits\n  \"agreeableness\" = \"Agreeableness\",\n  \"conscientiousness\" = \"Conscientiousness\",\n  \"neuroticism\" = \"Neuroticism\",\n  \"openness\" = \"Openness\",\n  \n  # beliefs & attitudes\n  \"political_conservative\" = \"Political Conservatism\",\n  \"religion_identification_level\" = \"Religious Identification\",\n  \n  # health behaviors\n  \"alcohol_frequency\" = \"Alcohol Frequency\",\n  \"alcohol_intensity\" = \"Alcohol Intensity\",\n  \"hlth_disability_binary\" = \"Disability Status\",\n  \"smoker_binary\" = \"Smoker\",\n  \"hours_exercise\" = \"Hours of Exercise\",\n  \n  \n  # time use\n  \"hours_children\" = \"Hours with Children\",\n  \"hours_commute\" = \"Hours Commuting\",\n  \"hours_exercise\" = \"Hours Exercising\",\n  \"hours_housework\" = \"Hours on Housework\",\n  \"log_hours_children\" = \"Log Hours with Children\",\n  \"log_hours_commute\" = \"Log Hours Commuting\",\n  \"log_hours_exercise\" = \"Log Hours Exercising\",\n  \"log_hours_housework\" = \"Log Hours on Housework\"\n)\n\n# outcome variable labels, organized by domain\n\nvar_labels_health &lt;- list(\n  \"alcohol_frequency_weekly\" = \"Alcohol Frequency (weekly)\",\n  \"alcohol_intensity\" = \"Alcohol Intensity\",\n  \"hlth_bmi\" = \"Body Mass Index\",\n  \"hlth_sleep_hours\" = \"Sleep\",\n  \"hours_exercise\" = \"Hours of Exercise\",\n  \"short_form_health\" = \"Short Form Health\"\n)\n\n# define psych outcomes\nvar_labels_psych &lt;- list(\n  \"hlth_fatigue\" = \"Fatigue\",\n  \"kessler_latent_anxiety\" = \"Anxiety\",\n  \"kessler_latent_depression\" = \"Depression\",\n  \"rumination\" = \"Rumination\"\n)\n\n# define present outcomes\nvar_labels_present &lt;- list(\n  \"bodysat\" = \"Body Satisfaction\",\n  \"foregiveness\" = \"Forgiveness\",\n  \"perfectionism\" = \"Perfectionism\",\n  \"self_control\" = \"Self Control\",\n  \"self_esteem\" = \"Self Esteem\",\n  \"sexual_satisfaction\" = \"Sexual Satisfaction\"\n)\n\n# define life outcomes\nvar_labels_life &lt;- list(\n  \"gratitude\" = \"Gratitude\",\n  \"lifesat\" = \"Life Satisfaction\",\n  \"meaning_purpose\" = \"Meaning: Purpose\",\n  # exposure variable\n  \"meaning_sense\" = \"Meaning: Sense\",\n  \"pwi = Personal Well-being Index\"\n)\n\n# define social outcome names\nvar_labels_social &lt;- list(\n  \"belong\" = \"Social Belonging\",\n  \"neighbourhood_community\" = \"Neighbourhood Community\",\n  \"support\" = \"Social Support\"\n)\n# combine all label lists\nvar_labels_all = c(\n  var_labels_baseline,\n  var_labels_exposure,\n  var_labels_health,\n  var_labels_psych,\n  var_labels_present,\n  var_labels_life,\n  var_labels_social\n)\n\n# save for manuscript\nhere_save(var_labels_all, \"var_labels_all\", push_mods)\n\n\n# tables ------------------------------------------------------------------\n# create baseline characteristics table\ndat_baseline = dat_long_tables |&gt;\n  filter(wave %in% c(baseline_wave)) |&gt;\n  mutate(\n    male_binary = factor(male_binary),\n    parent_binary = factor(parent_binary),\n    smoker_binary = factor(smoker_binary),\n    born_nz_binary = factor(born_nz_binary),\n    employed_binary = factor(employed_binary),\n    not_heterosexual_binary = factor(not_heterosexual_binary),\n    sample_frame_opt_in_binary = factor(sample_frame_opt_in_binary)\n  )\n\nbaseline_table &lt;- margot::margot_make_tables(\n  data = dat_baseline,\n  vars = baseline_vars,\n  by = \"wave\",\n  labels = var_labels_all,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(baseline_table)\nmargot::here_save(baseline_table, \"baseline_table\", push_mods)\n\n# create exposure table by wave\nexposure_table &lt;- margot::margot_make_tables(\n  data = dat_long_tables |&gt; filter(wave %in% c(baseline_wave, exposure_waves)),\n  vars = exposure_var,\n  by = \"wave\",\n  labels = var_labels_all,\n  factor_vars = exposure_var_binary,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(exposure_table)\nmargot::here_save(exposure_table, \"exposure_table\", push_mods)\n\n# create outcomes table by wave\noutcomes_table &lt;- margot::margot_make_tables(\n  data = dat_long_tables |&gt; filter(wave %in% c(baseline_wave, outcome_wave)),\n  vars = outcome_vars_no_log,\n  by = \"wave\",\n  labels = var_labels,\n  format = \"markdown\"\n)\nprint(outcomes_table)\nmargot::here_save(outcomes_table, \"outcomes_table\", push_mods)\n\n# note: completed data preparation step -------------------------------------\n# you're now ready for the next steps:\n# 1. creating wide-format dataset for analysis \n# 2. applying causal inference methods\n# 3. conducting sensitivity analyses\n\n# key decisions summary:\n# 1. exposure variable: extraversion\n# 2. study waves: baseline (2018), exposure (2019), outcome (2020)\n# 3. baseline covariates: demographics, traits, health measures (excluding exposure)\n# 4. outcomes: health, psychological, wellbeing, and social variables\n# 5. time-varying confounders: disability status??\n# 6. binary cutpoint for exposure: 4 on the extraversion scale\n# 7. label names for tables\n\n\n\nScript 2 Secondary Data Wrangling\n\n# script 1: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n\n# save paths -------------------------------------------------------------------\npush_mods &lt;- here::here(\"models_example_2\")\n\n\n# packages needed ---------------------------------------------------------\n\n# load libraries ----------------------------------------------------------\npacman::p_load(\n  DiagrammeR,   # graph and network visualization\n  doParallel,   # parallel processing with foreach\n  fastDummies,  # fast creation of dummy variables\n  fs,           # cross-platform file system operations\n  ggplot2,      # data visualisation\n  grf,          # generalized random forests\n  here,         # simple and robust file referencing\n  janitor,      # data cleaning and validation\n  kableExtra,   # advanced table formatting\n  # margot,       # functions for casual inference\n  naniar,       # handling and visualization of missing data\n  parameters,   # parameters and performance metrics\n  policytree,   # causal inference with policy trees\n  progressr,    # progress reporting for R\n  tidyverse,    # collection of R packages for data science\n  EValue,       # compute Evalues\n  data.table,   # fast data wrangling\n  maq,          # qini curves\n  purrr,        # data wrangling\n  patchwork,     # multiple plots\n  labelled,\n  cli,\n  rlang\n)\n\n# read variables\nbaseline_vars &lt;- margot::here_read(\"baseline_vars\")\nexposure_var &lt;- margot::here_read(\"exposure_var\")\noutcome_vars &lt;- margot::here_read(\"outcome_vars\")\nt0_sample_weights &lt;- margot::here_read(\"t0_sample_weights\")\nbaseline_wave &lt;- margot::here_read(\"baseline_wave\")\nexposure_waves &lt;- margot::here_read(\"exposure_waves\")\noutcome_wave &lt;- margot::here_read(\"outcome_wave\")\n\n# define continuous columns to keep\ncontinuous_columns_keep &lt;- c(\"t0_sample_weights\")\n\n# define ordinal columns that we will expand into binary variables\nordinal_columns &lt;- c(\"t0_education_level_coarsen\",\n                     \"t0_eth_cat\",\n                     \"t0_rural_gch_2018_l\")\n\n# read data\ndat_long_prepare &lt;- margot::here_read(\"dat_long_prepare\")\n\n# read exposure\nname_exposure &lt;- margot::here_read(\"name_exposure\")\nname_exposure_binary = paste0(name_exposure, \"_binary\")\nname_exposure_continuous = name_exposure\n\n#check\nname_exposure_binary\nname_exposure_continuous\n\n# define wide variable names\nt0_name_exposure_binary &lt;- paste0(\"t0_\", name_exposure_binary)\nt0_name_exposure_binary\n\n\n# make exposure names (continuous not genreally used)\nt1_name_exposure_binary &lt;- paste0(\"t1_\", name_exposure_binary)\nt1_name_exposure_binary\n\n# treatments (continuous verion)\nt0_name_exposure &lt;- paste0(\"t0_\", name_exposure_continuous)\nt1_name_exposure &lt;- paste0(\"t1_\", name_exposure_continuous)\nt0_name_exposure_continuous &lt;- paste0(\"t0_\", name_exposure)\nt1_name_exposure_continuous &lt;- paste0(\"t1_\", name_exposure)\n\n# raw outcomes\n# read health outcomes\nraw_outcomes_health &lt;- here_read(\"raw_outcomes_health\")\nt2_outcome_health_z &lt;- paste0(\"t2_\", raw_outcomes_health, \"_z\")\nt2_outcome_health_z &lt;- sort(t2_outcome_health_z)\nt2_outcome_health_z\n\n# read raw outcomes\nraw_outcomes_psych &lt;- here_read(\"raw_outcomes_psych\")\nt2_outcome_psych_z &lt;- paste0(\"t2_\", raw_outcomes_psych, \"_z\")\nt2_outcome_psych_z &lt;- sort(t2_outcome_psych_z)\nt2_outcome_psych_z\n\n# read raw outcomes\nraw_outcomes_present &lt;- here_read(\"raw_outcomes_present\")\nt2_outcome_present_z &lt;- paste0(\"t2_\", raw_outcomes_present, \"_z\")\nt2_outcome_present &lt;- sort(t2_outcome_present_z)\nt2_outcome_present_z\n\n# read raw outcomes\nraw_outcomes_life &lt;- here_read(\"raw_outcomes_life\")\nt2_outcome_life_z &lt;- paste0(\"t2_\", raw_outcomes_life, \"_z\")\nt2_outcome_life_z &lt;- sort(t2_outcome_life_z)\nt2_outcome_life_z\n\n# read raw outcomes\nraw_outcomes_social &lt;- here_read(\"raw_outcomes_social\")\nt2_outcome_social_z &lt;- paste0(\"t2_\", raw_outcomes_social, \"_z\")\nt2_outcome_social_z &lt;- sort(t2_outcome_social_z)\nt2_outcome_social_z\n\n\n# time-varying confounders * ONLY IF USED **\nconfounder_vars &lt;- here_read(\"confounder_vars\")\n# ensure unique\nconfounder_vars &lt;- unique(confounder_vars)\n# check\nconfounder_vars\n\n# check\nstr(dat_long_prepare)\n\n# check\nnaniar::gg_miss_var(dat_long_prepare)\n\n# impute data --------------------------------------------------------------\n\n# get vars\nbaseline_vars &lt;- margot::here_read(\"baseline_vars\")\noutcome_vars &lt;- margot::here_read(\"outcome_vars\")\n\n# make both\nname_exposure_both &lt;- c(name_exposure_binary, name_exposure_continuous)\n\n#check\nname_exposure_both\n\n# for predictive models for censoring/ use continuous variable for better\n\n\n# ordinal use\nordinal_columns &lt;- c(\n  \"t0_education_level_coarsen\",\n  \"t0_eth_cat\",\n  \"t0_rural_gch_2018_l\",\n  \"t0_gen_cohort\"\n)\n\n# for\ncontinuous_columns_keep &lt;- c(\"t0_sample_weights\")\n\n# prepare data for analysis ----------------------\ndat_long_prepare &lt;- margot::remove_numeric_attributes(dat_long_prepare)\nname_exposure_both\n\n# wide data\ndf_wide &lt;- margot_wide_machine(\n  dat_long_prepare,\n  id = \"id\",\n  wave = \"wave\",\n  baseline_vars,\n  exposure_var = name_exposure_both,\n  outcome_vars,\n  confounder_vars = NULL,\n  imputation_method = \"none\",\n  include_exposure_var_baseline = TRUE,\n  include_outcome_vars_baseline = TRUE,\n  extend_baseline = FALSE,\n  include_na_indicators = FALSE\n)\n\n# check\ncolnames(df_wide)\n\n#\nhead(df_wide)\n\n# add weights back to data\ndf_wide$t0_sample_weights &lt;- t0_sample_weights\n\n# make sure that rural is a factor\ndf_wide$t0_rural_gch_2018_l &lt;- as.factor(df_wide$t0_rural_gch_2018_l)\n\n# save the wide data\nmargot::here_save(df_wide, \"df_wide\")\n\n#df_wide &lt;- margot::here_read(\"df_wide\")\nnaniar::vis_miss(df_wide, warn_large_data = FALSE)\n\n# order data with missingness assigned to work with grf and lmtp\n# if any outcome is censored all are censored\n# create version for model reports\n\n# check\ncolnames(df_wide)\n\n\n# made data wide in correct format\n# ignore warning\ndf_wide_encoded  &lt;- margot::margot_process_longitudinal_data_wider(\n  df_wide,\n  ordinal_columns = ordinal_columns,\n  continuous_columns_keep = continuous_columns_keep,\n  not_lost_in_following_wave = \"not_lost_following_wave\",\n  lost_in_following_wave = \"lost_following_wave\",\n  remove_selected_columns = TRUE,\n  exposure_var = name_exposure_both,\n  scale_continuous = TRUE,\n  censored_if_any_lost = FALSE\n)\n\n# check\ncolnames(df_wide_encoded)\n\n# check\ntable(df_wide_encoded$t0_not_lost_following_wave)\n\n# make the binary variable numeric\ndf_wide_encoded[[t0_name_exposure_binary]] &lt;-\n  as.numeric(df_wide_encoded[[t0_name_exposure_binary]]) - 1\ndf_wide_encoded[[t1_name_exposure_binary]] &lt;-\n  as.numeric(df_wide_encoded[[t1_name_exposure_binary]]) - 1\n\n# view\ndf_wide_encoded[[t0_name_exposure_binary]]\ndf_wide_encoded[[t1_name_exposure_binary]]\n\n# 1. ensure both binaries only take values 0 or 1 (ignore NA)\nstopifnot(all(df_wide_encoded[[t0_name_exposure_binary]][!is.na(df_wide_encoded[[t0_name_exposure_binary]])] %in% 0:1),\n          all(df_wide_encoded[[t1_name_exposure_binary]][!is.na(df_wide_encoded[[t1_name_exposure_binary]])] %in% 0:1))\n\n# 2. ensure NA‐patterns match between t1_exposure and t0_lost flag\n# count n-as in t1 exposure\nn_na_t1 &lt;- sum(is.na(df_wide_encoded[[t1_name_exposure_binary]]))\n\n# count how many were lost at t0\nn_lost_t0 &lt;- sum(df_wide_encoded$t0_lost_following_wave == 1, na.rm = TRUE)\n\n# print them for inspection\nmessage(\"NAs in \", t1_name_exposure_binary, \": \", n_na_t1)\nmessage(\"t0_lost_following_wave == 1: \", n_lost_t0)\n\n# stop if they don’t match\nstopifnot(n_na_t1 == n_lost_t0)\n\n# 3. ensure if t1 is non‐NA then subject was not lost at t0\nstopifnot(all(is.na(df_wide_encoded[[t1_name_exposure_binary]]) |\n                df_wide_encoded[[\"t0_not_lost_following_wave\"]] == 1))\n\n# now it’s safe to save\nhere_save_qs(df_wide_encoded, \"df_wide_encoded\", push_mods)\ndf_wide_encoded &lt;- here_read_qs(\"df_wide_encoded\", push_mods)\n\n# view\nhead(df_wide_encoded)\n\n#naniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\nnaniar::gg_miss_var(df_wide_encoded)\n\n\n# predict attrition and create censoring weights --------------------------\n# step 1: prepare baseline covariates\n# select all t0_ variables except the exposure binary and any _lost indicators, then sort their names\nt0_var_names &lt;- df_wide_encoded |&gt;\n  select(-all_of(t0_name_exposure_binary)) |&gt;\n  select(starts_with(\"t0_\"),-ends_with(\"_lost\"),-ends_with(\"lost_following_wave\")) |&gt;\n  colnames() |&gt;\n  sort()\n\n# get unique values (to be safe)\nE &lt;- unique(t0_var_names)\n\n# view\nprint(E)\n\n# save baseline covariates\nmargot::here_save(E, \"E\")\n\n# view\nprint(E)\n\n# step 2: calculate weights for t0\nD_0 &lt;- as.factor(df_wide_encoded$t0_lost_following_wave)\n\n# get co-variates\ncen_0 &lt;- df_wide_encoded[, E]\n\n# probability forest for censoring\n# this will take time\ncen_forest_0 &lt;- probability_forest(cen_0, D_0)\n\n# get predictions\npredictions_grf_0 &lt;- predict(cen_forest_0, newdata = cen_0, type = \"response\")\n\n# get propensity scores\npscore_0 &lt;- predictions_grf_0$pred[, 2]\n\n# use margot_adjust_weights for t0\nt0_weights &lt;- margot_adjust_weights(\n  pscore = pscore_0,\n  trim = TRUE,\n  normalize = TRUE,\n  # lower trimming\n  lower_percentile = 0.00,\n  # upper trimming\n  upper_percentile = 0.99,\n  censoring_indicator = df_wide_encoded$t0_lost_following_wave,\n  sample_weights = df_wide_encoded$t0_sample_weights\n)\n\n# view\nhist(t0_weights$adjusted_weights)\n\n# give weights\ndf_wide_encoded$t0_adjusted_weights &lt;- t0_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\n\n# remove lost next wave (censored)\ndf_wide_encoded_1 &lt;- df_wide_encoded %&gt;%\n  filter(t0_lost_following_wave == 0) %&gt;%\n  droplevels()\n\n# step 4: calculate weights for t1\nE_and_exposure &lt;- c(E, t1_name_exposure_continuous)\nD_1 &lt;- as.factor(df_wide_encoded_1$t1_lost_following_wave)\ncen_1 &lt;- df_wide_encoded_1[, E_and_exposure]\n\n# probability forest for censoring\n#  *** this will take time ***\ncen_forest_1 &lt;- probability_forest(cen_1, D_1, sample.weights = df_wide_encoded_1$t0_adjusted_weights)\n\n# predict forest\npredictions_grf_1 &lt;- predict(cen_forest_1, newdata = cen_1, type = \"response\")\n\n# get propensity score\npscore_1 &lt;- predictions_grf_1$pred[, 2]\n\n# check\nhist(pscore_1)\n\n# use margot_adjust_weights for t1\nt1_weights &lt;- margot_adjust_weights(\n  pscore = pscore_1,\n  trim = TRUE,\n  normalize = TRUE,\n  lower_percentile = 0.00,\n  # upper trimming\n  upper_percentile = 0.99,\n  censoring_indicator = df_wide_encoded_1$t1_lost_following_wave,\n  sample_weights = df_wide_encoded_1$t0_adjusted_weights # combine with weights\n)\n\n# add weights -- these will be the weights we use\ndf_wide_encoded_1$t1_adjusted_weights &lt;- t1_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded_1, warn_large_data = FALSE)\n\n# save\nhere_save(df_wide_encoded_1, \"df_wide_encoded_1\")\n\n# check names\ncolnames(df_wide_encoded_1)\n\n# check\ndf_wide_encoded_1[[t1_name_exposure_binary]]\n\n# step 5: prepare final dataset\nnrow(df_wide_encoded_1)\ntable(df_wide_encoded_1$t1_lost_following_wave)\n\n# arrange\ndf_grf &lt;- df_wide_encoded_1 |&gt;\n  filter(t1_lost_following_wave == 0) |&gt;\n  select(\n    where(is.factor),\n    ends_with(\"_binary\"),\n    ends_with(\"_lost_following_wave\"),\n    ends_with(\"_z\"),\n    ends_with(\"_weights\"),\n    starts_with(\"t0_\"),\n    starts_with(\"t1_\"),\n    starts_with(\"t2_\"),\n  ) |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\")) |&gt;\n  relocate(starts_with(\"t1_\"), .before = starts_with(\"t2_\")) |&gt;\n  relocate(\"t0_not_lost_following_wave\", .before = starts_with(\"t1_\")) |&gt;\n  relocate(all_of(t1_name_exposure_binary), .before = starts_with(\"t2_\")) |&gt;\n  droplevels()\n\n# save final data\nmargot::here_save(df_grf, \"df_grf\")\ndf_grf &lt;- margot::here_read(\"df_grf\")\n\n# check final dataset\ncolnames(df_grf)\n\n# visualise missing\n# should have no missing in t1 and t2 variables\n# handled by IPCW\nnaniar::vis_miss(df_grf, warn_large_data = FALSE)\n\n#checks\ncolnames(df_grf)\nstr(df_grf)\n\n# check exposures\ntable(df_grf[[t1_name_exposure_binary]])\n\n# check\nhist(df_grf$t1_adjusted_weights)\n\n# calculate summary statistics\nt0_weight_summary &lt;- summary(df_wide_encoded)\n\n# check\nglimpse(df_grf$t1_adjusted_weights)\n\n# visualise weight distributions\nhist(df_grf$t1_adjusted_weights, main = \"t0_stabalised weights\", xlab = \"Weight\")\n\n# visualise and check missing values\nnaniar::gg_miss_var(df_grf)\n\n# check n\nn_observed_grf &lt;- nrow(df_grf)\n\n# view\nn_observed_grf\n\n# save\nmargot::here_save(n_observed_grf, \"n_observed_grf\")\n\n\n\n# this is just for your interest ------------------------------------------\n# not used in final manuscript\n\n\n# inspect propensity scores -----------------------------------------------\n# get data\ndf_grf &lt;- here_read('df_grf')\n\n# assign weights var name\nweights_var_name = \"t0_adjusted_weights\"\n\n# baseline covariates  # E already exists and is defined\nE\n\n# must be a data frame, no NA in exposure\n\n# df_grf is a data frame - we must process this data frame in several steps\n# user to specify which columns are outcomes, default to 'starts_with(\"t2_\")'\ndf_propensity_org &lt;- df_grf |&gt; select(!starts_with(\"t2_\"))\n\n# Remove NAs and print message that this has been done\ndf_propensity &lt;- df_propensity_org |&gt; drop_na() |&gt; droplevels()\n\n# E_propensity_names\n# first run model for baseline propensity if this is selected.  The default should be to not select it.\npropensity_model_and_plots &lt;- margot_propensity_model_and_plots(\n  df_propensity = df_propensity,\n  exposure_variable = t1_name_exposure_binary,\n  baseline_vars = E,\n  weights_var_name = weights_var_name,\n  estimand = \"ATE\",\n  method = \"ebal\",\n  focal = NULL\n)\n\n# visualise\nsummary(propensity_model_and_plots$match_propensity)\n\n# key plot\npropensity_model_and_plots$love_plot\n\n# other plots\npropensity_model_and_plots$summary_plot\npropensity_model_and_plots$balance_table\npropensity_model_and_plots$diagnostics\n\n\n# check size\nsize_bytes &lt;- object.size(propensity_model_and_plots)\nprint(size_bytes, units = \"auto\") # Mb\n\n# use qs to save only if you have space\nhere_save_qs(propensity_model_and_plots,\n             \"propensity_model_and_plots\",\n             push_mods)\n\n\n\nScript 3 Analysis\n\n# script 3: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\ndevtools::install_github(\"go-bayes/margot\")\n\n# restart fresh session\nrstudioapi::restartSession()\n\n\n# reproducibility ---------------------------------------------------------\nset.seed(123)\n\n\n# essential library ---------------------------------------------------------\n\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n# check package version\npackageVersion(pkg = \"margot\")\n\n# load libraries ----------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  margot,          # off-cran causal workflow tools\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf, ranger,     # machine learning forests\n  doParallel      # parallel processing,\n  ggplot2         # graphs\n  rlang           # functions for base types/Core R/ 'Tidyverse'\n  purrr           # functional programming tools.\n  patchwork.      # nice graph placement\n  janitor         # nice labels\n  glue            # format/ interpolate a string\n)\n\n\n\n# directory path configuration -----------------------------------------------\n# save path (customise for your own computer) ----------------------------\npush_mods &lt;- here::here(\"models_example_2\")\n\n# read original data (for plots) ------------------------------------------\noriginal_df &lt;- margot::here_read(\"df_wide\", push_mods)\n\n# plot title --------------------------------------------------------------\ntitle_binary = \"Extraversion (binary)\"\nfilename_prefix = \"grf_extraversion_wb\"\n\n# for manuscript later\nmargot::here_save(title_binary,\"title_binary\")\n\n# import names ------------------------------------------------------------\nname_exposure &lt;- margot::here_read(\"name_exposure\")\nname_exposure\n\n# make exposure names\nt1_name_exposure_binary &lt;- paste0(\"t1_\", name_exposure, \"_binary\")\n\n# check exposure name\nt1_name_exposure_binary\n\n\n# read and sort outcome variables -----------------------------------------\n# we do this by domain: health, psych, present, life, social\nread_and_sort &lt;- function(key) {\n  raw  &lt;- margot::here_read(key, push_mods)\n  vars &lt;- paste0(\"t2_\", raw, \"_z\")\n  sort(vars)\n}\nt2_outcome_health_z  &lt;- read_and_sort(\"raw_outcomes_health\")\nt2_outcome_psych_z   &lt;- read_and_sort(\"raw_outcomes_psych\")\nt2_outcome_present_z &lt;- read_and_sort(\"raw_outcomes_present\")\nt2_outcome_life_z    &lt;- read_and_sort(\"raw_outcomes_life\")\nt2_outcome_social_z  &lt;- read_and_sort(\"raw_outcomes_social\")\n\n# combine outcomes ---------------------------------------------------------\nraw_outcomes_all   &lt;- c(\n  margot::here_read(\"raw_outcomes_health\", push_mods),\n  margot::here_read(\"raw_outcomes_psych\",  push_mods),\n  margot::here_read(\"raw_outcomes_present\", push_mods),\n  margot::here_read(\"raw_outcomes_life\",    push_mods),\n  margot::here_read(\"raw_outcomes_social\",  push_mods)\n)\nt2_outcomes_all &lt;- c(\n  t2_outcome_health_z, t2_outcome_psych_z,\n  t2_outcome_present_z, t2_outcome_life_z,\n  t2_outcome_social_z\n)\n# save combined lists\nhere_save(t2_outcomes_all, \"t2_outcomes_all\", push_mods)\nhere_save(raw_outcomes_all, \"raw_outcomes_all\",   push_mods)\n\n# save for pub\nhere_save(t2_outcomes_all, \"t2_outcomes_all\")\n\n# label mappings for health outcomes\nlabel_mapping_health &lt;- list(\n  \"t2_alcohol_frequency_weekly_z\" = \"Alcohol Frequency\",\n  \"t2_alcohol_intensity_weekly_z\" = \"Alcohol Intensity\",\n  \"t2_hlth_bmi_z\" = \"BMI\",\n  \"t2_hlth_sleep_hours_z\" = \"Sleep\",\n  \"t2_log_hours_exercise_z\" = \"Hours of Exercise (log)\",\n  \"t2_short_form_health_z\" = \"Short Form Health\"\n)\n\n# label mappings for psychological well-being outcomes\nlabel_mapping_psych &lt;- list(\n  \"t2_hlth_fatigue_z\" = \"Fatigue\",\n  \"t2_kessler_latent_anxiety_z\" = \"Anxiety\",\n  \"t2_kessler_latent_depression_z\" = \"Depression\",\n  \"t2_rumination_z\" = \"Rumination\"\n)\n\n# label mappings for present reflective outcomes\nlabel_mapping_present &lt;- list(\n  \"t2_bodysat_z\" = \"Body Satisfaction\",\n  \"t2_foregiveness_z\" = \"Forgiveness\",\n  \"t2_perfectionism_z\" = \"Perfectionism\",  \n  \"t2_self_control_z\" = \"Self Control\",\n  \"t2_sexual_satisfaction_z\" = \"Sexual Satisfaction\"\n)\n\n# label mappings for life reflective outcomes\nlabel_mapping_life &lt;- list(\n  \"t2_gratitude_z\" = \"Gratitude\",\n  \"t2_lifesat_z\" = \"Life Satisfaction\",\n  \"t2_meaning_purpose_z\" = \"Meaning: Purpose\",\n  \"t2_meaning_sense_z\" = \"Meaning: Sense\",\n  \"t2_pwi_z\" = \"Personal Well-being Index\"\n)\n\n# label mappings for social outcomes\nlabel_mapping_social &lt;- list(\n  \"t2_belong_z\" = \"Social Belonging\",\n  \"t2_neighbourhood_community_z\" = \"Neighbourhood Community\",\n  \"t2_support_z\" = \"Social Support\"\n)\n\n# label mapping all -------------------------------------------------------\nlabel_mapping_all &lt;- c(\n  label_mapping_health,\n  label_mapping_psych,\n  label_mapping_present,\n  label_mapping_life,\n  label_mapping_social\n)\n\n# save\nhere_save(label_mapping_all, \"label_mapping_all\")\n\n# check\nlabel_mapping_all\n\n\n# load GRF data and prepare inputs ----------------------------------------\ndf_grf &lt;- margot::here_read('df_grf', push_mods)\nE      &lt;- margot::here_read('E',      push_mods)\n# check exposure binary\nstopifnot(all(df_grf[[t1_name_exposure_binary]][!is.na(df_grf[[t1_name_exposure_binary]])] %in% 0:1))\n# set exposure and weights\nW       &lt;- as.vector(df_grf[[t1_name_exposure_binary]])\nweights &lt;- df_grf$t1_adjusted_weights\nhist(weights) # quick check for extreme weights\n# select covariates and drop numeric attributes\nX &lt;- margot::remove_numeric_attributes(df_grf[E])\n\n\n# set model defaults -----------------------------------------------------\ngrf_defaults &lt;- list(seed = 123, stabilize.splits = TRUE, num.trees = 2000)\n\ndecision_tree_defaults &lt;- list(\n  span_ratio       = .3,\n  text_size        = 3.8,\n  y_padding        = 0.25,\n  edge_label_offset = .002,\n  border_size      = .05\n)\npolicy_tree_defaults &lt;- list(\n  point_alpha       = .5,\n  title_size        = 12,\n  subtitle_size     = 14,\n  axis_title_size   = 14,\n  legend_title_size = 14,\n  split_line_color  = \"red\",\n  split_line_alpha  = .8,\n  split_label_color = \"red\",\n  list(split_label_nudge_factor = 0.007)\n)\n\npolicy_tree_args &lt;- modifyList(\n  policy_tree_defaults,\n  list(split_label_nudge_factor = 0.007)\n)\n\n\n# example: fit causal forest on a toy subset ------------------------------\n# first, create a smaller test sample\nn   &lt;- nrow(X)\ntoy &lt;- sample(seq_len(n), floor(n / 4))\n# define toy data\ntoy_data     &lt;- df_grf[toy, ]\nX_toy        &lt;- X[toy, ]\nW_toy        &lt;- W[toy]\nweights_toy  &lt;- weights[toy]\n\n# fit the model\ncf_out &lt;- margot_causal_forest(\n  data         = toy_data,\n  outcome_vars = \"t2_kessler_latent_depression_z\",\n  covariates   = X_toy,\n  W            = W_toy,\n  weights      = weights_toy,\n  save_data    = TRUE,\n  save_models  = TRUE\n)\n\n# inspect Qini curve ------------------------------------------------------\nqini_tbl &lt;- margot::margot_inspect_qini(cf_out, propensity_bounds = c(0.01, 0.97))\nprint(qini_tbl)\n\n# plot policy-combo trees --------------------------------------------------\ncombo1 &lt;- margot_plot_policy_combo(\n  result_object    = cf_out,\n  model_name       = \"model_t2_kessler_latent_depression_z\",\n  max_depth        = 1L,          # depth-1 tree\n  decision_tree_args = list(text_size = 4),\n  policy_tree_args   = list(point_alpha = 0.7)\n)\ncombo1$combined_plot\n\n# you can repeat for depth-2 ----------------------------------------------\ncombo2 &lt;- margot_plot_policy_combo(\n  result_object    = cf_out,\n  model_name       = \"model_t2_kessler_latent_depression_z\",\n  max_depth        = 2L,\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  original_df        = original_df,\n  label_mapping      = label_mapping_all\n)\ncombo2$combined_plot\n\n# batch plotting ----------------------------------------------------------\nmodels_batch_1L &lt;- margot_policy(\n  cf_out,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  model_names        = \"model_t2_kessler_latent_depression_z\",\n  original_df        = original_df,\n  label_mapping      = label_mapping_psych,\n  max_depth          = 1L\n)\n# view first model's plots\nmodels_batch_1L[[1]][[3]]  # combo plot\nmodels_batch_1L[[1]][[4]]  # qini plot\n\nmodels_batch_2L &lt;- margot_policy(\n  cf_out,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_args,\n  model_names        = \"model_t2_kessler_latent_depression_z\",\n  original_df        = original_df,\n  label_mapping      = label_mapping_psych,\n  max_depth          = 2L)\n# view first model's plots\nmodels_batch_2L[[1]][[3]]  # combo plot\nmodels_batch_2L[[1]][[4]]  # qini plot\n\n# 2. flip the selected outcomes (and regen trees)\n# use -- when the outcome is undesirable and we want to minimise it \n# (assuming the exposure is something we'd prescribe)\ncf_out_f &lt;- margot_flip_forests(\n  model_results = cf_out,\n  flip_outcomes = c(\"t2_kessler_latent_depression_z\"),\n  recalc_policy = TRUE\n)\n\n# where there are very low or high propensity scores (prob of exposure) we might consider trimming\nmargot::margot_inspect_qini(cf_out_f, propensity_bounds = c(0.01, 0.97)) # can be varied\n\n# if we had extreme scores (not used here)\ncf_out_flipped_trimmed &lt;- margot_rescue_qini(model_results      = cf_out_f,\n                                             propensity_bounds  = c(0.05, 0.95))\n# flipped batch model\nmodels_batch_flipped_2L &lt;- margot_policy(\n  cf_out_f,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = c(\"model_t2_kessler_latent_depression_z\"),\n  original_df = original_df,\n  label_mapping = label_mapping_psych,\n  max_depth     = 2L\n)\n\n# flipped\n# interpretation: exposure minimising depression\nmodels_batch_flipped_2L[[1]][[3]]\n\n# not flipped: exposure as maximizing depression\nmodels_batch_2L[[1]][[3]]\n\n\n# full models -------------------------------------------------------------\n\n# ** uncomment to run full models**\n\n# # health models -----------------------------------------------------------\nmodels_binary_health &lt;- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_health_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n# check size if needed\nmargot::margot_size(models_binary_health)\n\n# save model\nmargot::here_save_qs(models_binary_health, \"models_binary_health\", push_mods)\n\n\n# psych models ------------------------------------------------------------\nmodels_binary_psych &lt;- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_psych_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n# save model\nmargot::here_save_qs(models_binary_psych, \"models_binary_psych\", push_mods)\n\n\n# present models ----------------------------------------------------------\nmodels_binary_present &lt;- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_present_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n\n# save model\nmargot::here_save_qs(models_binary_present, \"models_binary_present\", push_mods)\n\n\n# life models -------------------------------------------------------------\nmodels_binary_life &lt;- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_life_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n# save model\nmargot::here_save_qs(models_binary_life, \"models_binary_life\", push_mods)\n\n\n# social models -----------------------------------------------------------\nmodels_binary_social &lt;- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_social_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n\n# save model\nmargot::here_save_qs(models_binary_social, \"models_binary_social\", push_mods)\n\n\n\n# read results ------------------------------------------------------------\n\n# if you save models you do not need to re-run them\nmodels_binary_health &lt;- margot::here_read_qs(\"models_binary_health\", push_mods)\nmodels_binary_psych &lt;- margot::here_read_qs(\"models_binary_psych\", push_mods)\nmodels_binary_present &lt;- margot::here_read_qs(\"models_binary_present\", push_mods)\nmodels_binary_life &lt;- margot::here_read_qs(\"models_binary_life\", push_mods)\nmodels_binary_social &lt;- margot::here_read_qs(\"models_binary_social\", push_mods)\n\n\n# make graphs -------------------------------------------------------------\n# titles\nsubtitle_health = \"Health\"\nsubtitle_psych = \"Psychological Well-being\"\nsubtitle_present = \"Present-Focussed Well-being\"\nsubtitle_life = \"Life-Focussed Well-being\"\nsubtitle_social = \"Social Well-being\"\n\n\n# settings\nx_offset = -.5\nx_lim_lo = -.5\nx_lim_hi = .5\n\n\n# defaults for ate plots\nbase_defaults_binary &lt;- list(\n  type = \"RD\",\n  title = title_binary,\n  e_val_bound_threshold = 1.2,\n  colors = c(\n    \"positive\" = \"#E69F00\",\n    \"not reliable\" = \"grey50\",\n    \"negative\" = \"#56B4E9\"\n  ),\n  x_offset = x_offset,\n  # will be set based on type\n  x_lim_lo = x_lim_lo,\n  # will be set based on type\n  x_lim_hi = x_lim_hi,\n  text_size = 4,\n  linewidth = 0.5,\n  estimate_scale = 1,\n  base_size = 18,\n  point_size = 2,\n  title_size = 19,\n  subtitle_size = 16,\n  legend_text_size = 10,\n  legend_title_size = 10,\n  include_coefficients = FALSE\n)\n\n# health graph options\noutcomes_options_health &lt;- margot_plot_create_options(\n  title = subtitle_health,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix\n)\n\n# psych graph options\noutcomes_options_psych &lt;- margot_plot_create_options(\n  title = subtitle_psych,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix\n)\n\n# present graph options ---------------------------------------------------\noutcomes_options_present &lt;- margot_plot_create_options(\n  title = subtitle_present,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix\n)\n\n\n# life graph options ------------------------------------------------------\noutcomes_options_life &lt;- margot_plot_create_options(\n  title = subtitle_life,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix\n)\n\n\n# social graph options ----------------------------------------------------\noutcomes_options_social &lt;- margot_plot_create_options(\n  title = subtitle_social,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix\n)\n\n# all graph options ------------------------------------------------------\noptions_all_models &lt;- margot_plot_create_options(\n  title = \"Outcomewide Wellbeing\",\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix\n)\n\n\n# make graphs -------------------------------------------------------------\n\n# make ate plots ----------------------------------------------------------\n# health plots ------------------------------------------------------------\nbinary_results_health &lt;- margot_plot(\n  models_binary_health$combined_table,\n  options = outcomes_options_health,\n  label_mapping = label_mapping_health,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  order = \"evaluebound_asc\",\n  original_df = original_df,\n  e_val_bound_threshold = 1.2\n)\n\n# view\nbinary_results_health$transformed_table |&gt; rename(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\") |&gt;\n  kbl(format = 'markdown')\n\n# check\ncat(binary_results_health$interpretation)\n\n# interpretation\ncat(binary_results_health$interpretation)\n\n# plot psych\nbinary_results_psych_asc &lt;- margot_plot(\n  models_binary_psych$combined_table,\n  options = outcomes_options_psych,\n  label_mapping = label_mapping_psych,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  original_df = original_df,\n  e_val_bound_threshold = 1.2,\n  order = \"evaluebound_asc\"\n)\n\n\n# order\nbinary_results_psych_asc$plot\n\n# reorder for descriptions\nbinary_results_psych &lt;- margot_plot(\n  models_binary_psych$combined_table,\n  options = outcomes_options_psych,\n  label_mapping = label_mapping_psych,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  e_val_bound_threshold = 1.2,\n  original_df = original_df,\n  order = \"evaluebound_asc\"\n)\n\n# table\nbinary_results_psych$transformed_table |&gt; rename(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\") |&gt;\n  kbl(format = 'markdown')\n\n# interpretation\ncat(binary_results_psych$interpretation)\n\n# plot present\n# order\nbinary_results_present &lt;- margot_plot(\n  models_binary_present$combined_table,\n  options = outcomes_options_present,\n  label_mapping = label_mapping_present,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  original_df = original_df,\n  order = \"evaluebound_asc\"\n)\n\n# plot\nbinary_results_present$plot\n\n# interpretation\ncat(binary_results_present$interpretation)\n(binary_results_present$transformed_table)\n\n# plot life\nbinary_results_life &lt;- margot_plot(\n  models_binary_life$combined_table,\n  options = outcomes_options_life,\n  label_mapping = label_mapping_life,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  order = \"evaluebound_asc\",\n  original_df = original_df\n)\n\n# table\nbinary_results_life$transformed_table |&gt; rename(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\") |&gt;\n  kbl(format = 'markdown')\n\n# plot\nbinary_results_life$transformed_table\n\n# interpretation\ncat(binary_results_life$interpretation)\n\n# plot social\nbinary_results_social &lt;- margot_plot(\n  models_binary_social$combined_table,\n  options = outcomes_options_social,\n  label_mapping = label_mapping_social,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  original_df = original_df,\n  order = \"evaluebound_asc\"\n)\n\n# table\nbinary_results_social$transformed_table |&gt; rename(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\") |&gt;\n  kbl(format = 'markdown')\n\n# interpretation\ncat(binary_results_social$interpretation)\n\n\n# combine ate plots ------------------------------------------------------\n# plot_ate_health &lt;- binary_results_health_asc$plot\n# plot_ate_psych &lt;- binary_results_psych_asc$plot\n# plot_ate_present &lt;- binary_results_present_asc$plot\n# plot_ate_life &lt;- binary_results_life_asc$plot\n# plot_ate_social &lt;- binary_results_social_asc$plot\n#\n#\n#\n# # create combined plot with annotations\n# ate_plots_combined &lt;- plot_ate_health +\n#   plot_ate_psych +\n#   plot_ate_present +\n#   plot_ate_life +\n#   plot_ate_social +\n#   plot_annotation(\n#     title = title_binary,\n#     tag_levels = \"A\",\n#     theme = theme(\n#       plot.title = element_text(size = 20),\n#       legend.position = \"top\"\n#     )\n#   ) +\n#   plot_layout(guides = \"collect\")\n#\n# # view combined plot\n# ate_plots_combined\n\n# combine all models -----------------------------------------------------\n# merge all domain models into single object\n\n\nall_models &lt;- margot_bind_models(\n  models_binary_health,\n  models_binary_psych,\n  models_binary_present,\n  models_binary_life,\n  models_binary_social\n)\n# graph\nplot_all_models &lt;- margot_plot(\n  all_models$combined_table,\n  options = options_all_models,\n  save_output = FALSE,\n  e_val_bound_threshold = 1.2,\n  # &lt;- set this value\n  label_mapping = label_mapping_all,\n  save_path = here::here(push_mods),\n  original_df = original_df,\n  include_coefficients = FALSE,\n  order = \"evaluebound_asc\"\n)\n\n# view plot\nplot_all_models$plot\n\n# interpretation\ncat(plot_all_models$interpretation)\n\n# table\nplot_all_models$transformed_table\n\n# nice table\ntables_list &lt;- list(\n  Health = binary_results_health$transformed_table,\n  Psych = binary_results_psych$transformed_table,\n  Present = binary_results_present$transformed_table,\n  Life = binary_results_life$transformed_table,\n  Social = binary_results_social$transformed_table\n)\n\n# make markdown tables (to be imported into the manuscript)\nmargot_bind_tables_markdown &lt;- margot_bind_tables(\n  tables_list = tables_list,\n  #list(all_models$combined_table),\n  sort_E_val_bound = \"desc\",\n  e_val_bound_threshold = 1.2,\n  # ← choose threshold\n  highlight_color = NULL,\n  bold = TRUE,\n  rename_cols = TRUE,\n  col_renames = list(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\"),\n  rename_ate = TRUE,\n  threshold_col = \"E_Val_bound\",\n  output_format = \"markdown\",\n  kbl_args = list(\n    booktabs = TRUE,\n    caption = NULL,\n    align = NULL\n  )\n)\n\n# view markdown table\nmargot_bind_tables_markdown\n\n# save for publication\nhere_save(margot_bind_tables_markdown, \"margot_bind_tables_markdown\")\n\n\n# count models by category\ncat(\"Number of original models:\\n\")\ncat(\"Social models:\", length(models_binary_social$results), \"\\n\")\ncat(\"Psych models:\", length(models_binary_psych$results), \"\\n\")\ncat(\"Health models:\", length(models_binary_health$results), \"\\n\")\ncat(\"Present models:\",\n    length(models_binary_present$results),\n    \"\\n\")\ncat(\"Life models:\", length(models_binary_life$results), \"\\n\")\ncat(\"\\nTotal models in combined object:\",\n    length(all_models$results),\n    \"\\n\")\n\n\n# evaluate models ---------------------------------------------------------\n# trim models if extreme propensity scores dominate\n# diag_tbl_health_trim_98 &lt;- margot_inspect_qini(models_binary_health,\n#                                        propensity_bounds = c(0.01, 0.99))\n# diag_tbl_health_trim_98\n#\n# diag_tbl_health_trim_95 &lt;- margot_inspect_qini(models_binary_health,\n#                                       propensity_bounds = c(0.03, 0.97))\n# diag_tbl_health_trim_95\n\n# rescue qini if needed ---------------------------------------------------\n# test\n# diag_tbl_trim_all &lt;- margot_inspect_qini(all_models,\n#                                        propensity_bounds = c(0.03, 0.97))\n# diag_tbl_trim_all\n\n\n\n\n# flipping models: outcomes we want to minimise given the exposure --------\n# standard negative outcomes/  not used in this study\nflip_outcomes_standard = c(\n  \"t2_alcohol_frequency_weekly_z\",\n  \"t2_alcohol_intensity_z\",\n  \"t2_hlth_bmi_z\",\n  \"t2_hlth_fatigue_z\",\n  \"t2_kessler_latent_anxiety_z\",\n  \"t2_kessler_latent_depression_z\",\n  \"t2_rumination_z\",\n  \"t2_perfectionism_z\" # the exposure variable was not investigated\n)\n\n# we will investigate losses to these outcomes\n# usual flipped names for positive interventions\n# commented out for this study\nflipped_names &lt;- c(\n  \"Alcohol Frequency\",\n  \"Alcohol Intensity\",\n  \"BMI\",\n  \"Fatigue\",\n  \"Anxiety\",\n  \"Depression\",\n  \"Rumination\",\n  \"Perfectionism\"\n)\n\n# set diff for all outcomes to obtain vector of postive outcomes to reverse\nflip_outcomes &lt;- flip_outcomes_standard #c( setdiff(t2_outcomes_all, flip_outcomes_standard) )\n\n# check\nflip_outcomes\n\n# checks\n# neg_check &lt;- vapply(all_models$results[ paste0(\"model_\", flip_outcomes) ],\n#                     \\(x) mean(x$tau_hat, na.rm = TRUE) &lt; 0, logical(1))\n# stopifnot(all(neg_check))   # every chosen outcome has a negative mean cate\n\n# get labels\nflipped_names &lt;- margot_get_labels(flip_outcomes, label_mapping_all)\n\n# check\nflipped_names\n\n# save for publication\nhere_save(flipped_names, \"flipped_names\")\n\n\n\n# flip negatively oriented outcomes --------------------------------------\n\n# flip models using margot's function\n\n#  *** this will take some time ***\n\n# ** give it time **\n# ** once run/ comment out **\nmodels_binary_flipped_all &lt;- margot_flip_forests(all_models,\n                                                 flip_outcomes = flip_outcomes,\n                                                 #  ← select\n                                                 recalc_policy = TRUE)\n\n# save\nhere_save_qs(models_binary_flipped_all,\n             \"models_binary_flipped_all\",\n             push_mods)\n\n# read back if needed\nmodels_binary_flipped_all &lt;- here_read_qs(\"models_binary_flipped_all\", push_mods)\n\n# test\nmodels_binary_flipped_all$results$model_t2_kessler_latent_depression_z$policy_tree_depth_1\nmodels_binary_flipped_all$results$model_t2_kessler_latent_depression_z$policy_tree_depth_2\n\n\n\n# optional ----------------------------------------------------------------\n\n\n\n# omnibus heterogeneity tests --------------------------------------------\n# test for treatment effect heterogeneity across all outcomes\nresult_ominbus_hetero_all &lt;- margot::margot_omnibus_hetero_test(models_binary_flipped_all, label_mapping = label_mapping_all)\n\n# view results table\nresult_ominbus_hetero_all$summary_table |&gt; kbl(\"markdown\")\n\n# view test interpretation\ncat(result_ominbus_hetero_all$brief_interpretation)\n\n# rate test analysis -----------------------------------------------------\n\n# create rate analysis table\nrate_table_all &lt;- margot_rate(\n  models = models_binary_flipped_all,\n  policy = \"treat_best\",  # or \"withold_best\" but don't attempt fitting curves or policytrees\n  label_mapping = label_mapping_all\n)\n\n# view rate tables\nrate_table_all$rate_autoc |&gt; kbl(\"markdown\")\nrate_table_all$rate_qini |&gt; kbl(\"markdown\")\n\n# generate interpretation\nrate_interpretation_all &lt;- margot_interpret_rate(\n  rate_table_all, \n  flipped_outcomes = flipped_names\n)\n\n# view interpretations\ncat(rate_interpretation_all$autoc_results)\ncat(rate_interpretation_all$qini_results)\n\n# compare rate and qini -- see grf documentation\ncat(rate_interpretation_all$comparison)\n\n# check out model names for different ways of thinking about heterogeneity\nrate_interpretation_all$either_model_names\nrate_interpretation_all$qini_model_names\nrate_interpretation_all$both_model_names\nrate_interpretation_all$autoc_model_names\n\n# autoc plots ------------------------------------------------------------\n# generate batch rate plots for models with significant heterogeneity\nbatch_rate_autoc_plots &lt;- margot_plot_rate_batch(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  # just use rate autoc for rate plots\n  model_names = rate_interpretation_all$autoc_model_names\n)\n\n# extract individual plots from the batch result\nautoc_plots &lt;- batch_rate_autoc_plots\n\n# determine number of columns based on number of plots\nnum_cols &lt;- ifelse(length(autoc_plots) &gt; 3, 2, 1)\n\n# combine plots using patchwork\nlibrary(patchwork)\n\n# only proceed if there are plots to combine\nif (length(autoc_plots) &gt; 0) {\n  # initialize with first plot\n  combined_autoc_plot &lt;- autoc_plots[[1]]\n  \n  # add remaining plots if any\n  if (length(autoc_plots) &gt; 1) {\n    for (i in 2:length(autoc_plots)) {\n      combined_autoc_plot &lt;- combined_autoc_plot + autoc_plots[[i]]\n    }\n  }\n  \n  # apply the dynamic layout\n  combined_autoc_plot &lt;- combined_autoc_plot +\n    plot_layout(ncol = num_cols) &\n    plot_annotation(\n      title = \"AUTOC Model Plots\",\n      subtitle = paste0(length(autoc_plots), \" models with significant heterogeneity\"),\n      tag_levels = \"A\"\n    )\n  \n  # view the combined plot\n  print(combined_autoc_plot)\n  \n  # save the combined plot if needed\n  width &lt;- ifelse(num_cols == 1, 8, 12)\n  height &lt;- 6 * ceiling(length(autoc_plots) / num_cols)\n  \n  ggsave(\n    here::here(push_mods, \"combined_autoc_plots.pdf\"),\n    combined_autoc_plot,\n    width = width,\n    height = height\n  )\n} else {\n  # handle case with no plots\n  message(\"No AUTOC plots available\")\n}\n\n\n# qini --------------------------------------------------------------------\nmodels_batch_qini_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$qini_model_names,\n  max_depth  = 2L,\n  # ← new argument\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\n\nplots &lt;- lapply(seq_along(models_batch_qini_2L), function(i) {\n  models_batch_qini_2L[[i]][[4]]  # extract the 4th element (plot) from each model\n})\n\n\nrate_interpretation_all$qini_model_names\n\n# give the plots meaningful names\nnames(plots) &lt;- rate_interpretation_all$qini_model_names\n\n# determine number of columns based on number of plots# determine number of columns based on number of plots\nnum_cols &lt;- ifelse(length(plots) &gt; 3, 2, 1)\n\n# combine plots using patchwork\nlibrary(patchwork)\n\n\n# create combined plot\ncombined_plot &lt;- plots[[1]]\nfor (i in 2:length(plots)) {\n  combined_plot &lt;- combined_plot + plots[[i]]\n}\n\n# apply the dynamic layout\ncombined_plot &lt;- combined_plot + plot_layout(ncol = num_cols)\n\n# add titles and annotations\ncombined_plot &lt;- combined_plot &\n  plot_annotation(\n    title = \"Qini Model Plots\",\n    subtitle = paste0(length(plots), \" models arranged in \", num_cols, \" column(s)\"),\n    tag_levels = \"A\"  # adds a, b, c, etc. to the plots\n  )\n\n# view\ncombined_plot\n\n# save (optional)\nwidth &lt;- ifelse(num_cols == 1, 8, 12)\nheight &lt;- 6 * ceiling(length(plots)/num_cols)  # height per row * number of rows\n\n# save\nggsave(here::here(push_mods, \"combined_qini_plots.pdf\"),\n       combined_plot,\n       width = width, height = height)\n\n\n# interpretation ----------------------------------------------------------\n# interpret qini curves\ninterpretation_qini_curves_2L &lt;- margot_interpret_qini(\n  models_batch_qini_2L,\n  model_names = rate_interpretation_all$qini_model_names,\n  label_mapping = label_mapping_all\n)\ninterpretation_qini_curves_2L\n\n# view qini interpretation\ncat(interpretation_qini_curves_2L$qini_explanation)\n\n# view summary table\ninterpretation_qini_curves_2L$summary_table |&gt; kbl(\"markdown\")\n\n\n\n# policy tree analysis ---------------------------------------------------\n# make policy trees\n# 1 l decision trees are generally very bad\nplots_policy_trees_1L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names,\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 1L\n)\n\n# model 1\nplots_policy_trees_1L[[1]][[3]]\n\n# model 2\nplots_policy_trees_1L[[2]][[3]]\n\n# model 3\nplots_policy_trees_1L[[2]][[3]]\n\ninterpret_plots_policy_trees_1L &lt;- margot_interpret_policy_batch(models_binary_flipped_all, model_names = rate_interpretation_all$either_model_names)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_1L)\n\n# policy tree analysis ---------------------------------------------------\n# make policy trees\n# *** 2l is much more persuasive ***\nplots_policy_trees_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names,\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 2L\n)\n\n\n# model 1\nplots_policy_trees_2L[[1]][[3]]\n\n# model 2\nplots_policy_trees_2L[[2]][[3]]\n\n# model 3\nplots_policy_trees_2L[[2]][[3]]\n\ninterpret_plots_policy_trees_2L &lt;- margot_interpret_policy_batch(models_binary_flipped_all, model_names = rate_interpretation_all$either_model_names)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_2L)\n\n\n\n#############################################################################\n# theoretical comparisons ---------------------------------------------------\n# individual theoretical comparisons (if relevant)\n# need to get values for wealth if wealth is compared\n\n# step 1 get information for wealth for conditonal comparisons\nhead(df_grf$t0_log_household_inc_z)\n\n# get mean on original data scale\nlog_mean_inc &lt;- mean(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# get sd on original data scale\nlog_sd_inc &lt;- sd(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# function to get back to data scale\nmargot_back_transform_log_z(\n  log_mean = log_mean_inc,\n  log_sd = log_sd_inc,\n  z_scores = c(-1, 0, 1),\n  label = \"data_scale\"\n)\n\n# define complex conditions for subsetting\ncomplex_condition_political &lt;- X[, \"t0_political_conservative_z\"] &gt; -1 &\n  X[, \"t0_political_conservative_z\"] &lt; 1\n\ncomplex_condition_wealth &lt;- X[, \"t0_log_household_inc_z\"] &gt; -1 &\n  X[, \"t0_log_household_inc_z\"] &lt; 1\n\ncomplex_condition_age &lt;- X[, \"t0_age_z\"] &gt; -1 &\n  X[, \"t0_age_z\"] &lt; 1\n\n# # if we have specific groups to compare\n# complex_condition_age_under_neg_1_sd  &lt;- X[, \"t0_age_z\"] &lt; -1\n# complex_condition_age_gr_eq_neg_1_sd  &lt;- X[, \"t0_age_z\"] &gt; -1\n\n# check ages to get number\nmean(original_df$t0_age) - sd(original_df$t0_age)\nmean(original_df$t0_age) + sd(original_df$t0_age)\n\n\n# wealth subsets\nsubsets_standard_wealth &lt;- list(\n  Poor = list(\n    var = \"t0_log_household_inc_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those HShold income &lt; -1 SD (NZD ~41k)\",\n    label = \"Poor\"  # label remains as is, but could be changed if desired\n  ),\n  MiddleIncome = list(subset_condition = complex_condition_wealth, description = \"Effects among those HS_hold income within +/-1SD (&gt; NZD 41k &lt; NZD 191k)\"),\n  Rich = list(\n    var = \"t0_log_household_inc_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those HS_hold income &gt; +1 SD (NZD 191k)\",\n    label = \"Rich\"\n  )\n)\n\n# political subsets\nsubsets_standard_political &lt;- list(\n  Liberal = list(\n    var = \"t0_political_conservative_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those &lt; -1 SD in political conservativism\",\n    label = \"Liberal\"\n  ),\n  Centrist = list(\n    var = \"t0_political_conservative_z\",\n    # operator = \"&lt;\",\n    subset_condition = complex_condition_political,\n    description = \"Effects among those &gt; -1 SD and &lt; +1 in political conservativism\",\n    label = \"Centrist\"\n  ),\n  Conservative = list(\n    var = \"t0_political_conservative_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those &gt; +1 SD in political conservativism\",\n    label = \"Conservative\"\n  )\n)\n\n\n# political subsets\nsubsets_standard_age &lt;- list(\n  Younger = list(\n    var = \"t0_age_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those &lt; under 35 years old\",\n    label = \"Age &lt; 35\"\n  ),\n  Middle = list(\n    var = \"t0_age_z\",\n    # operator = \"&lt;\",\n    subset_condition = complex_condition_age,\n    description = \"Effects among those 35-62\",\n    label = \"Age 35-62\"\n  ),\n  Older = list(\n    var = \"t0_age_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those &gt; 62\",\n    label = \"Age &gt; 62\"\n  )\n)\n\n\n# gender subsets\nsubsets_standard_gender &lt;- list(\n  Female = list(\n    var = \"t0_male_binary\",\n    value = 0,\n    description = \"Females\"\n  ),\n  Male = list(\n    var = \"t0_male_binary\",\n    value = 1,\n    description = \"Males\"\n  )\n)\n\n# ethnicity subsets\nsubsets_standard_ethnicity &lt;- list(\n  Asian = list(\n    var = \"t0_eth_cat_asian_binary\",\n    value = 1,\n    description = \"Asians\"\n  ),\n  Euro = list(\n    var = \"t0_eth_cat_euro_binary\",\n    value = 1,\n    description = \"Europeans (Pakeha)\"\n  ),\n  Pacific = list(\n    var = \"t0_eth_cat_pacific_binary\",\n    value = 1,\n    description = \"Pacific Peoples\"\n  ),\n  Maori = list(\n    var = \"t0_eth_cat_maori_binary\",\n    value = 1,\n    description = \"Māori\"\n  )\n)\n\n\n# batch planned subgroup analysis -----------------------------------------\n# set up lists of models, names, and subtitles\ndomain_models &lt;- list(\n  models_binary_health,\n  models_binary_psych,\n  models_binary_present,\n  models_binary_life,\n  models_binary_social\n)\n\n\n# set up domain names\ndomain_names &lt;- c(\"health\", \"psych\", \"present\", \"life\", \"social\")\n\n# set up subtitles\nsubtitles &lt;- c(subtitle_health,\n               subtitle_psych,\n               subtitle_present,\n               subtitle_life,\n               subtitle_social)\n\n# set up subset types in a list\nsubset_types &lt;- list(\n  wealth = subsets_standard_wealth,\n  ethnicity = subsets_standard_ethnicity,\n  political = subsets_standard_political,\n  gender = subsets_standard_gender,\n  cohort = subsets_standard_age\n)\n\n\n# run model\nplanned_subset_results &lt;- margot_planned_subgroups_batch(\n  domain_models = domain_models,\n  X = X,\n  base_defaults = base_defaults_binary,\n  subset_types = subset_types,\n  original_df = original_df,\n  domain_names = domain_names,\n  subtitles = subtitles\n)\n\n\n# results\n# health subgroup\ncat(planned_subset_results$health$wealth$explanation)\ncat(planned_subset_results$health$ethnicity$explanation)\ncat(planned_subset_results$health$political$explanation)\ncat(planned_subset_results$health$gender$explanation)\ncat(planned_subset_results$health$cohort$explanation)\n\n\ncat(planned_subset_results$psych$wealth$explanation)\ncat(planned_subset_results$psych$ethnicity$explanation)\ncat(planned_subset_results$psych$political$explanation)\ncat(planned_subset_results$psych$gender$explanation)\ncat(planned_subset_results$psych$cohort$explanation)\n\n\ncat(planned_subset_results$present$wealth$explanation)\ncat(planned_subset_results$present$ethnicity$explanation)\ncat(planned_subset_results$present$political$explanation)\ncat(planned_subset_results$present$gender$explanation)\ncat(planned_subset_results$present$cohort$explanation)\n\n\ncat(planned_subset_results$life$wealth$explanation)\ncat(planned_subset_results$life$ethnicity$explanation)\ncat(planned_subset_results$life$political$explanation)\ncat(planned_subset_results$life$gender$explanation)\ncat(planned_subset_results$life$cohort$explanation)\n\ncat(planned_subset_results$social$wealth$explanation)\ncat(planned_subset_results$social$ethnicity$explanation)\ncat(planned_subset_results$social$political$explanation)\ncat(planned_subset_results$social$gender$explanation)\ncat(planned_subset_results$social$cohort$explanation)\n\nplanned_subset_results$health$wealth$results$Poor$transformed_table\n# combine tables ----------------------------------------------------------\n# wrap each domain's table in a list:\n# wealth subgroups --------------------------------------------------------\ntables_list_poor &lt;- list(\n  Health = planned_subset_results$health$wealth$results$Poor$transformed_table,\n  Psych  = planned_subset_results$psych$wealth$results$Poor$transformed_table,\n  Life   = planned_subset_results$life$wealth$results$Poor$transformed_table,\n  Social = planned_subset_results$social$wealth$results$Poor$transformed_table\n)\n\n# function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_poor,\n  bold = TRUE,\n  kbl_args = list(booktabs = TRUE, caption = \"Wealth Subgroup Analysis: Poor\"),\n  highlight_color = NULL,\n  output_format = \"html\" # could be \"markdown\"\n)\n\n# create table list for middle income subgroup\ntables_list_middleincome &lt;- list(\n  Health = planned_subset_results$health$wealth$results$MiddleIncome$transformed_table,\n  Psych  = planned_subset_results$psych$wealth$results$MiddleIncome$transformed_table,\n  Life   = planned_subset_results$life$wealth$results$MiddleIncome$transformed_table,\n  Social = planned_subset_results$social$wealth$results$MiddleIncome$transformed_table\n)\n\n# bind tables and display results\nmargot::margot_bind_tables(\n  tables_list = tables_list_middleincome,\n  bold = TRUE,\n  kbl_args = list(booktabs = TRUE, caption = \"Wealth Subgroup Analysis: Middle Income\"),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\ntables_list_rich &lt;- list(\n  Health = planned_subset_results$health$wealth$results$Rich$transformed_table,\n  Psych  = planned_subset_results$psych$wealth$results$Rich$transformed_table,\n  Life   = planned_subset_results$life$wealth$results$Rich$transformed_table,\n  Social = planned_subset_results$social$wealth$results$Rich$transformed_table\n)\n\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_rich,\n  bold = TRUE,\n  kbl_args = list(booktabs = TRUE, caption = \"Wealth Subgroup Analysis: Rich\"),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\n# cohort subgroups --------------------------------------------------------\n\n# wealth subgroups --------------------------------------------------------\n\ntables_list_younger &lt;- list(\n  Health = planned_subset_results$health$cohort$results$`Age &lt; 35`$transformed_table,\n  Psych  = planned_subset_results$psych$cohort$results$`Age &lt; 35`$transformed_table,\n  Life   = planned_subset_results$psych$cohort$results$`Age &lt; 35`$transformed_table,\n  Social = planned_subset_results$psych$cohort$results$`Age &lt; 35`$transformed_table\n)\n\n\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_younger,\n  bold = TRUE,\n  kbl_args = list(booktabs = TRUE, caption = \"Cohort Subgroup Analysis: Age under 35\"),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\n\ntables_list_middle &lt;- list(\n  Health = planned_subset_results$health$cohort$results$`Age 35-62`$transformed_table,\n  Psych  = planned_subset_results$psych$cohort$results$`Age 35-62`$transformed_table,\n  Life   = planned_subset_results$psych$cohort$results$`Age 35-62`$transformed_table,\n  Social = planned_subset_results$psych$cohort$results$`Age 35-62`$transformed_table\n)\n\n\n#  function bind tables\n#  function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_middle,\n  bold = TRUE,\n  kbl_args = list(booktabs = TRUE, caption = \"Cohort Subgroup Analysis: Ages 35-62\"),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\ntables_list_older &lt;- list(\n  Health = planned_subset_results$health$cohort$results$`Age &gt; 62`$transformed_table,\n  Psych  = planned_subset_results$psych$cohort$results$`Age &gt; 62`$transformed_table,\n  Life   = planned_subset_results$psych$cohort$results$`Age &gt; 62`$transformed_table,\n  Social = planned_subset_results$psych$cohort$results$`Age &gt; 62`$transformed_table\n)\n\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_older,\n  bold = TRUE,\n  kbl_args = list(booktabs = TRUE, caption = \"Cohort Subgroup Analysis: Age &gt; 62\"),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\n\n# plots -------------------------------------------------------------------\n# results plots\n# health\nplots_subgroup_wealth_health &lt;- wrap_plots(\n  list(\n    planned_subset_results$health$wealth$results$Poor$plot,\n    planned_subset_results$health$wealth$results$MiddleIncome$plot,\n    planned_subset_results$health$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_health,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_wealth_health)\n\n# plots\nplots_subgroup_ethnicity_health &lt;- wrap_plots(\n  list(\n    planned_subset_results$health$ethnicity$results$Asian$plot,\n    planned_subset_results$health$ethnicity$results$Euro$plot,\n    planned_subset_results$health$ethnicity$results$Maori$plot,\n    planned_subset_results$health$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n) +\n  patchwork::plot_annotation(title = subtitle_health,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_ethnicity_health)\n\n# plots\nplots_subgroup_political_health &lt;- wrap_plots(\n  list(\n    planned_subset_results$health$political$results$Conservative$plot,\n    planned_subset_results$health$political$results$Centrist$plot,\n    planned_subset_results$health$political$results$Conservative$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_health,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_political_health)\n\n# plots\nplots_subgroup_gender_health &lt;- wrap_plots(\n  list(\n    planned_subset_results$health$gender$results$Female$plot,\n    planned_subset_results$health$gender$results$Male$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_health,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_gender_health)\n\n# plots\nplots_subgroup_cohort_health &lt;- wrap_plots(\n  list(\n    planned_subset_results$health$cohort$results$`Age &lt; 35`$plot,\n    planned_subset_results$health$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$health$cohort$results$`Age &gt; 62`$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_health,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_cohort_health)\n\n# psychological well-being\nplots_subgroup_wealth_psych &lt;- wrap_plots(\n  list(\n    planned_subset_results$psych$wealth$results$Poor$plot,\n    planned_subset_results$psych$wealth$results$MiddleIncome$plot,\n    planned_subset_results$psych$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_psych,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_wealth_psych)\n\n# plots\nplots_subgroup_ethnicity_psych &lt;- wrap_plots(\n  list(\n    planned_subset_results$psych$ethnicity$results$Asian$plot,\n    planned_subset_results$psych$ethnicity$results$Euro$plot,\n    planned_subset_results$psych$ethnicity$results$Maori$plot,\n    planned_subset_results$psych$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n) +\n  patchwork::plot_annotation(title = subtitle_psych,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_ethnicity_psych)\n\n# plots\nplots_subgroup_political_psych &lt;- wrap_plots(\n  list(\n    planned_subset_results$psych$political$results$Conservative$plot,\n    planned_subset_results$psych$political$results$Centrist$plot,\n    planned_subset_results$psych$political$results$Conservative$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_psych,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_political_psych)\n\n# plots\nplots_subgroup_gender_psych &lt;- wrap_plots(\n  list(\n    planned_subset_results$psych$gender$results$Female$plot,\n    planned_subset_results$psych$gender$results$Male$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_psych,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_gender_psych)\n\n# plots\nplots_subgroup_cohort_psych &lt;- wrap_plots(\n  list(\n    planned_subset_results$psych$cohort$results$`Age &lt; 35`$plot,\n    planned_subset_results$psych$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$psych$cohort$results$`Age &gt; 62`$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_psych,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_cohort_psych)\n\n# present focussed well-being\nplots_subgroup_wealth_present &lt;- wrap_plots(\n  list(\n    planned_subset_results$present$wealth$results$Poor$plot,\n    planned_subset_results$present$wealth$results$MiddleIncome$plot,\n    planned_subset_results$present$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_present,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_wealth_present)\n\n# plots\nplots_subgroup_ethnicity_present &lt;- wrap_plots(\n  list(\n    planned_subset_results$present$ethnicity$results$Asian$plot,\n    planned_subset_results$present$ethnicity$results$Euro$plot,\n    planned_subset_results$present$ethnicity$results$Maori$plot,\n    planned_subset_results$present$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n) +\n  patchwork::plot_annotation(title = subtitle_present,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_ethnicity_present)\n\n# plots\nplots_subgroup_political_present &lt;- wrap_plots(\n  list(\n    planned_subset_results$present$political$results$Conservative$plot,\n    planned_subset_results$present$political$results$Centrist$plot,\n    planned_subset_results$present$political$results$Conservative$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_present,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_political_present)\n\n# plots\nplots_subgroup_gender_present &lt;- wrap_plots(\n  list(\n    planned_subset_results$present$gender$results$Female$plot,\n    planned_subset_results$present$gender$results$Male$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_present,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_gender_present)\n\n# plots\nplots_subgroup_cohort_present &lt;- wrap_plots(\n  list(\n    planned_subset_results$present$cohort$results$`Age &lt; 35`$plot,\n    planned_subset_results$present$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$present$cohort$results$`Age &gt; 62`$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_present,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_cohort_present)\n\n\n## life focussed well-being\nplots_subgroup_wealth_life &lt;- wrap_plots(\n  list(\n    planned_subset_results$life$wealth$results$Poor$plot,\n    planned_subset_results$life$wealth$results$MiddleIncome$plot,\n    planned_subset_results$life$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_life,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_wealth_life)\n\n# plots\nplots_subgroup_ethnicity_life &lt;- wrap_plots(\n  list(\n    planned_subset_results$life$ethnicity$results$Asian$plot,\n    planned_subset_results$life$ethnicity$results$Euro$plot,\n    planned_subset_results$life$ethnicity$results$Maori$plot,\n    planned_subset_results$life$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n) +\n  patchwork::plot_annotation(title = subtitle_life,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_ethnicity_life)\n\n# plots\nplots_subgroup_political_life &lt;- wrap_plots(\n  list(\n    planned_subset_results$life$political$results$Conservative$plot,\n    planned_subset_results$life$political$results$Centrist$plot,\n    planned_subset_results$life$political$results$Conservative$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_life,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_political_life)\n\n# plots\nplots_subgroup_gender_life &lt;- wrap_plots(\n  list(\n    planned_subset_results$life$gender$results$Female$plot,\n    planned_subset_results$life$gender$results$Male$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_life,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_gender_life)\n\n# plots\nplots_subgroup_cohort_life &lt;- wrap_plots(\n  list(\n    planned_subset_results$life$cohort$results$`Age &lt; 35`$plot,\n    planned_subset_results$life$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$life$cohort$results$`Age &gt; 62`$plot\n    \n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_life,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\nprint(plots_subgroup_cohort_life)\n\n# social well-being\nplots_subgroup_wealth_social &lt;- wrap_plots(\n  list(\n    planned_subset_results$social$wealth$results$Poor$plot,\n    planned_subset_results$social$wealth$results$MiddleIncome$plot,\n    planned_subset_results$social$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_social,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_wealth_social)\n\nplots_subgroup_ethnicity_social &lt;- wrap_plots(\n  list(\n    planned_subset_results$social$ethnicity$results$Asian$plot,\n    planned_subset_results$social$ethnicity$results$Euro$plot,\n    planned_subset_results$social$ethnicity$results$Maori$plot,\n    planned_subset_results$social$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n) +\n  patchwork::plot_annotation(title = subtitle_social,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_ethnicity_social)\n\n\nplots_subgroup_political_social &lt;- wrap_plots(\n  list(\n    planned_subset_results$social$political$results$Conservative$plot,\n    planned_subset_results$social$political$results$Centrist$plot,\n    planned_subset_results$social$political$results$Conservative$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_social,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_political_social)\n\n# plots\nplots_subgroup_gender_social &lt;- wrap_plots(\n  list(\n    planned_subset_results$social$gender$results$Female$plot,\n    planned_subset_results$social$gender$results$Male$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = subtitle_social,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_gender_social)\n\n# plots\nplots_subgroup_cohort_social &lt;- wrap_plots(\n  list(\n    planned_subset_results$social$cohort$results$Boomers$plot,\n    planned_subset_results$social$cohort$results$Generation_X$plot,\n    planned_subset_results$social$cohort$results$Generation_Y$plot,\n    planned_subset_results$social$cohort$results$Generation_Z$plot\n    \n  ),\n  ncol = 2\n) +\n  patchwork::plot_annotation(title = subtitle_social,\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\nprint(plots_subgroup_cohort_social)\n\n\n\n# plot options: showcased ---------------------------------------------\n# default\nmargot_plot_decision_tree(models_binary_social, \"model_t2_support_z\", )\n# tighten branches for easier viewing in single graphs\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  span_ratio = .30,\n  text_size = 3.8,\n  border_size = .1,\n  #  title = \"none\",\n  original_df = original_df\n)\n# colour decision node\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  span_ratio = .3,\n  text_size = 4,\n  title = \"New Title\",\n  non_leaf_fill =  \"violet\",\n  original_df = original_df\n)\n# make new title\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  span_ratio = .2,\n  text_size = 3,\n  title = \"New Title\",\n  non_leaf_fill =  \"white\",\n  original_df = original_df\n)\n\n# remove title\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  text_size = 5,\n  title = 'none',\n  # set title to none\n  original_df = original_df\n)\n\n# policy tree options\n# select only plot 1 change alpha\nmargot::margot_plot_policy_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  point_alpha = .25,\n  plot_selection = \"p1\"\n)\n# select only plot 2 change size of axis_text\n# change colours, modify etc...\nmargot::margot_plot_policy_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  plot_selection = \"p2\",\n  axis_title_size = 30,\n  split_label_size = 20,\n  split_label_color = \"red\",\n  split_line_color = \"red\",\n)\n\n# adjust only the alpha\nmargot::margot_plot_policy_tree(models_binary_social, \"model_t2_support_z\", point_alpha = .1)"
  },
  {
    "objectID": "content/09-content.html#what-you-will-learn",
    "href": "content/09-content.html#what-you-will-learn",
    "title": "Causal inference: a step by step guide",
    "section": "What You Will Learn",
    "text": "What You Will Learn\n\nSensitivity Analysis: E-values\nWorkflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity Analysis using E-values\n\nThe minimum strength of association on the risk ratio scale that an unmeasured confounder would need to have with both the exposure and the outcome, conditional on the measured covariates, to fully explain away a specific exposure-outcome association\n\nSee: Mathur et al. (2018); Linden, Mathur, and VanderWeele (2020); Tyler J. VanderWeele and Ding (2017).\n\nMathur, Maya B, Peng Ding, Corinne A Riddell, and Tyler J VanderWeele. 2018. “Website and r Package for Computing E-Values.” Epidemiology (Cambridge, Mass.) 29 (5): e45.\nFor example, suppose that the lower bound of the the E-value was 1.3 with the lower bound of the confidence interval = 1.12, we might then write:\n\nWith an observed risk ratio of RR=1.3, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 1.3-fold each (or 30%), above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 1.12-fold (or 12%) each could do so, but weaker joint confounder associations could not.\n\nThe equations are as follows (for risk ratios)\n\nE-value_{RR} = RR + \\sqrt{RR \\times (RR - 1)}\n\n\nE-value_{LCL} = LCL + \\sqrt{LCL \\times (LCL - 1)}\n\nHere is an R function that will calculate E-values\n\n\nCode\n# evalue for risk ratio\ncalculate_e_value &lt;- function(rr, lcl) {\n  e_value_rr = rr + sqrt(rr*(rr - 1))\n  e_value_lcl = lcl + sqrt(lcl*(lcl - 1))\n  \n  list(e_value_rr = e_value_rr, e_value_lcl = e_value_lcl)\n}\n\n# e.g. smoking causes cancer\n# finding   RR = 10.73 (95% CI: 8.02, 14.36)\nevalue_computed &lt;- calculate_e_value(10.73, 8.02)\n\n#print\nevalue_computed\n\n\n$e_value_rr\n[1] 20.94777\n\n$e_value_lcl\n[1] 15.52336\n\n\nWe write:\n\nWith an observed risk ratio of RR=10.7, an unmeasured confounder that was associated 20.9477737-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of e_value_rr$e_value_lcl-fold each could do so, but weaker joint confounder associations could not.\n\nNote that in this class, most of the outcomes will be (standardised) continuous outcomes. Here’s a function and LaTeX code to describe the approximation.\nThis function takes a linear regression coefficient estimate (est), its standard error (se), the standard deviation of the outcome (sd), a contrast of interest in the exposure (delta, which defaults to 1), and a “true” standardized mean difference (true, which defaults to 0). It calculates the odds ratio using the formula from Chinn (2000) and VanderWeele (2017), and then uses this to calculate the E-value.\n\n\nCode\n#evalue for ols\ncompute_evalue_ols &lt;- function(est, se, delta = 1, true = 0) {\n  # rescale estimate and SE to get a contrast of size delta\n  est &lt;- est / delta\n  se &lt;- se / delta\n\n  # compute transformed odds ratio and ci's\n  odds_ratio &lt;- exp(0.91 * est)\n  lo &lt;- exp(0.91 * est - 1.78 * se)\n  hi &lt;- exp(0.91 * est + 1.78 * se)\n\n  # compute E-Values based on the RR values\n  evalue_point_estimate &lt;- odds_ratio * sqrt(odds_ratio + 1)\n  evalue_lower_ci &lt;- lo * sqrt(lo + 1)\n\n  # return the e-values\n  return(list(EValue_PointEstimate = evalue_point_estimate,\n              EValue_LowerCI = evalue_lower_ci))\n}\n\n\n# example:\n# suppose we have an estimate of 0.5, a standard error of 0.1, and a standard deviation of 1.\n# this would correspond to a half a standard deviation increase in the outcome per unit increase in the exposure.\nresults &lt;- compute_evalue_ols(est = 0.5, se = 0.1, delta = 1)\npoint_round &lt;- round(results$EValue_PointEstimate, 3)\nci_round &lt;- round(results$EValue_LowerCI, 3)\n\n# print results\nprint(point_round)\n\n\n[1] 2.53\n\n\nCode\nprint(ci_round)\n\n\n[1] 2.009\n\n\nWe write:\n\nWith an observed risk ratio of 2.53, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 2.53-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 2.009-fold each could do so, but weaker joint confounder associations could not.\n\nNote the E-values package will do the computational work for us and this is what we use in the margot package to obtain E-values for sensitivity analysis."
  },
  {
    "objectID": "content/09-content.html#part-1-new-content",
    "href": "content/09-content.html#part-1-new-content",
    "title": "Causal inference: a step by step guide",
    "section": "Part 1: New Content",
    "text": "Part 1: New Content\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity Analysis using E-values\n\nThe minimum strength of association on the risk ratio scale that an unmeasured confounder would need to have with both the exposure and the outcome, conditional on the measured covariates, to fully explain away a specific exposure-outcome association\n\nSee: Mathur et al. (2018); Linden, Mathur, and VanderWeele (2020); Tyler J. VanderWeele and Ding (2017).\n\nMathur, Maya B, Peng Ding, Corinne A Riddell, and Tyler J VanderWeele. 2018. “Website and r Package for Computing E-Values.” Epidemiology (Cambridge, Mass.) 29 (5): e45.\nFor example, suppose that the lower bound of the the E-value was 1.3 with the lower bound of the confidence interval = 1.12, we might then write:\n\nWith an observed risk ratio of RR=1.3, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 1.3-fold each (or 30%), above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 1.12-fold (or 12%) each could do so, but weaker joint confounder associations could not.\n\nThe equations are as follows (for risk ratios)\n\nE-value_{RR} = RR + \\sqrt{RR \\times (RR - 1)}\n\n\nE-value_{LCL} = LCL + \\sqrt{LCL \\times (LCL - 1)}\n\nHere is an R function that will calculate E-values\n\n# evalue for risk ratio\ncalculate_e_value &lt;- function(rr, lcl) {\n  e_value_rr = rr + sqrt(rr*(rr - 1))\n  e_value_lcl = lcl + sqrt(lcl*(lcl - 1))\n  \n  list(e_value_rr = e_value_rr, e_value_lcl = e_value_lcl)\n}\n\n# e.g. smoking causes cancer\n# finding   RR = 10.73 (95% CI: 8.02, 14.36)\nevalue_computed &lt;- calculate_e_value(10.73, 8.02)\n\n#print\nevalue_computed\n\n$e_value_rr\n[1] 20.94777\n\n$e_value_lcl\n[1] 15.52336\n\n\nWe write:\n\nWith an observed risk ratio of RR=10.7, an unmeasured confounder that was associated 20.9477737-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of e_value_rr$e_value_lcl-fold each could do so, but weaker joint confounder associations could not.\n\nNote that in this class, most of the outcomes will be (standardised) continuous outcomes. Here’s a function and LaTeX code to describe the approximation.\nThis function takes a linear regression coefficient estimate (est), its standard error (se), the standard deviation of the outcome (sd), a contrast of interest in the exposure (delta, which defaults to 1), and a “true” standardized mean difference (true, which defaults to 0). It calculates the odds ratio using the formula from Chinn (2000) and VanderWeele (2017), and then uses this to calculate the E-value.\n\n#evalue for ols\ncompute_evalue_ols &lt;- function(est, se, delta = 1, true = 0) {\n  # rescale estimate and SE to get a contrast of size delta\n  est &lt;- est / delta\n  se &lt;- se / delta\n\n  # compute transformed odds ratio and ci's\n  odds_ratio &lt;- exp(0.91 * est)\n  lo &lt;- exp(0.91 * est - 1.78 * se)\n  hi &lt;- exp(0.91 * est + 1.78 * se)\n\n  # compute E-Values based on the RR values\n  evalue_point_estimate &lt;- odds_ratio * sqrt(odds_ratio + 1)\n  evalue_lower_ci &lt;- lo * sqrt(lo + 1)\n\n  # return the e-values\n  return(list(EValue_PointEstimate = evalue_point_estimate,\n              EValue_LowerCI = evalue_lower_ci))\n}\n\n\n# example:\n# suppose we have an estimate of 0.5, a standard error of 0.1, and a standard deviation of 1.\n# this would correspond to a half a standard deviation increase in the outcome per unit increase in the exposure.\nresults &lt;- compute_evalue_ols(est = 0.5, se = 0.1, delta = 1)\npoint_round &lt;- round(results$EValue_PointEstimate, 3)\nci_round &lt;- round(results$EValue_LowerCI, 3)\n\n# print results\nprint(point_round)\n\n[1] 2.53\n\nprint(ci_round)\n\n[1] 2.009\n\n\nWe write:\n\nWith an observed risk ratio of 2.53, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 2.53-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 2.009-fold each could do so, but weaker joint confounder associations could not.\n\nNote the E-values package will do the computational work for us and this is what we use in the margot package to obtain E-values for sensitivity analysis."
  },
  {
    "objectID": "content/09-content.html#part-2-guide-for-preparing-your-study",
    "href": "content/09-content.html#part-2-guide-for-preparing-your-study",
    "title": "Causal inference: a step by step guide",
    "section": "Part 2 Guide For Preparing Your Study",
    "text": "Part 2 Guide For Preparing Your Study\nRecall that psychology begins with two questions.\n\nWhat do I want to know about thought and behaviour? What is the target population?\n\nIn cross-cultural psychology, these questions relate to differences, and similarities, between groups.\nSuppose we have asked a question. How can we address it using observational data?\nToo fast.\nOur question must be made precise.\nToday we will consider how to make psychological questions precise, and how to answer them, using 3-wave panel designs (Tyler J. VanderWeele, Mathur, and Chen 2020)."
  },
  {
    "objectID": "content/09-content.html#motivations-for-a-three-wave-longitudinal-design-for-observational-causal-inference.",
    "href": "content/09-content.html#motivations-for-a-three-wave-longitudinal-design-for-observational-causal-inference.",
    "title": "Causal inference: a step by step guide",
    "section": "Motivations for a Three-Wave Longitudinal Design for Observational Causal Inference.",
    "text": "Motivations for a Three-Wave Longitudinal Design for Observational Causal Inference.\nREVIEW: Causal Diagrammes (DAGS) are a remarkably powerful and simple tool for understanding confounding See here\n\nCommon cause of exposure and outcome.\nOur question: does visiting a clinical psychologist reduce the 10 year incidence of heart attacks?\n\n\n\n\n\n\n\n\nFigure 1: Common cause of exposure and outcome: example\n\n\n\n\n\n\n\nSolution: Adjust for Confounder\n\n\n\n\n\n\n\n\nFigure 2: Solution to this problem.\n\n\n\n\n\n\n\nBias: exposure at baseline is a common cause of the exposure at t1 and outcome at t2\n\n\n\n\n\n\n\n\nFigure 3: Causal graph reveals bias from pre-exosure indicator\n\n\n\n\n\n\n\nA more thorough confounding control\n\n\n\n\n\n\n\n\nFigure 4: Causal graph:more general panel design\n\n\n\n\n\n\n\nGeneric 3-wave panel design (VanderWeeele 2020)\n\n\n\n\n\n\n\n\nFigure 5: Causal graph: three-wave panel design"
  },
  {
    "objectID": "content/09-content.html#sensitivity-analysis",
    "href": "content/09-content.html#sensitivity-analysis",
    "title": "Causal inference: a step by step guide",
    "section": "Sensitivity analysis",
    "text": "Sensitivity analysis"
  },
  {
    "objectID": "content/09-content.html#comprehensive-checklist-for-detailed-reporting-of-a-causal-inferenctial-study.",
    "href": "content/09-content.html#comprehensive-checklist-for-detailed-reporting-of-a-causal-inferenctial-study.",
    "title": "Causal inference: a step by step guide",
    "section": "Comprehensive Checklist for Detailed Reporting of a Causal Inferenctial Study.",
    "text": "Comprehensive Checklist for Detailed Reporting of a Causal Inferenctial Study.\n\nStep 1: Formulate the Research Question\n\nState your question: is my question clearly stated? If not, state it.\nRelevance: have I explained its importance? If not, explain.\nEthics how might this question affect people? How might not investigating this question affect people?\nCausality: Is my question causal? If not, refine your question.\nHeterogeneous Treatment Effects: Do I want to examine who responds differently (CATE)?\nSubgroup analysis: does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nExplain the Framework: can I explain the causal inference framework and convey the gist to non-specialists? If not, review course materials.\n\n\n\nDetermine Data Requirements\n\nData types: are my data experimental? If yes, your project may not fit this course.\nTime-series data: are my data time-series? If not, reconsider your causal question.\nData waves: do I have at least three waves of data? If not, beware of confounding control issues.\nData source: are my data from the NZAVS simulated data set? If not, consult with me.\n\n\n\nDetermine the Outcome\n\nOutcome variable: is the outcome variable Y defined? If not, define it.\nMultiple outcomes: are there multiple outcomes? If yes, explain and define them.\nOutcome relevance: can I explain how the outcome variable/s relate to my question? If not, clarify.\nOutcome type: is my outcome binary and rare? If yes, consider logistic regression. If my outcome is continuous, consider z-transforming it or categorising it (consult an expert).\nOutcome timing: does the outcome appear after the exposure? It should.\n\n\n\nDetermine the Exposure\n\nExposure variable: is the exposure variable A defined? If not, define it.\nMultiple exposures: are there multiple exposures? If yes, reassess; if only one exposure, proceed.\nExposure relevance: can I explain how the exposure variable relates to my question? If not, clarify.\nPositivity: can we intervene on the exposure at all levels of the covariates? We should be able to.\nConsistency: can I interpret what it means to intervene on the exposure? I should be able to.\nExchangeability: are different versions of the exposure conditionally exchangeable given measured baseline confounders? They should be.\nExposure type: is the exposure binary or continuous?\nShift intervention: Am I contrasting static interventions or modified treatment policies? (Not relevant for your final report)\nExposure timing: Does the exposure appear before the outcome? It should.\n\n\n\nAccount for Confounders\n\nBaseline confounders: Have I defined my baseline confounders L? I should have.\nJustification: Can I explain how the baseline confounders could affect both A and Y? I should be able to.\nTiming: Are the baseline confounders measured before the exposure? They should be.\nInclusion: Is the baseline measure of the exposure and the baseline outcome included in the set of baseline confounders? They should be.\nSufficiency: Are the baseline confounders sufficient to ensure balance on the exposure, such that A is independent of Y given L? If not, plan a sensitivity analysis.\nConfounder type: Are the confounders continuous or binary? If so, consider converting them to z-scores. If they are categorical with three or more levels, do not convert them to z-scores.\n\n\n\nDraw a Causal Diagram with Unmeasured Confounders\n\nUnmeasured confounders: Does previous science suggest the presence of unmeasured confounders? If not, expand your understanding.\nCausal diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding? I should have.\nMeasurement error: Have I described potential biases from measurement errors? If not, we’ll discuss later.\nTemporal order: Does my DAG have time indicators to ensure correct temporal order? It should.\nTime consistency: Is my DAG organized so that time follows in a consistent direction? It should.\n\n\n\nIdentify the Estimand\n\nATE or CATE or both? (For you – it will be both)\n\n\n\nUnderstanding Source and Target Populations\n\nPopulations identified: Have I explained how my sample relates to my target populations? I should have.\nGeneralisability and transportability: Have I considered whether my results generalise different populations? I should have.\n\n\n\nSet Eligibility Criteria\n\nCriteria stated: Have I stated the eligibility criteria for the study? I should have.\n\n\n\nDescribe Sample Characteristics\n\nDescriptive statistics: have I provided descriptive statistics for demographic information taken at baseline? I should have.\nExposure change: Have I demonstrated the magnitudes of change in the exposure from baseline to the exposure interval? I should have.\nReferences: Have I included references for more information about the sample? I should have.\n\n\n\nAddressing Missing Data\n\nMissing data checks: Have I checked for missing data? I should have.\nMissing data plan: If there is missing data, have I described how I will address it? I should have.\n\n\n\nSelecting the Model Approach: If Not Using Machine Learning (Lecture 7)\n\nApproach decision: Have I decided on using G-computation, IPTW, or Doubly-Robust Estimation? I should have.\nInteractions: If not using machine learning, have I included the interaction of the exposure and baseline covariates? I should have.\nBig data: If I have a large data set, should I include the interaction of the exposure, group, and baseline confounders? I should consider it.\nModel specification: have I double-checked the model specification? I should.\nOutcome assessment: If the outcome is rare and binary, have I specified logistic regression? If it’s continuous, have I considered converting it to z-scores?\nSensitivity analysis: am I planning a sensitivity analysis using simulation? If yes, describe it (e.g. E-values.)\n\n\n\nMachine Learing\nHave I explained how causal forests work (next week’s lecture).\n\n\nClarify unmeasured pre-treatment covariates\nLet U denoted unmeasured pre-treatment covariates that may potentially bias the statistical association between A and Y independently of the measured covariates.\n\nConsider:\n\nTo affect Y and A, U must occur before A.\nIt is useful to draw a causal diagramme to illustrate all potential sources of bias.\nCausal diagrammes are qualitative tools that require specialist expertise. We cannot typically obtain a causal graph from the data.\nA causal diagramme should include only as much information as is required to assess confounding. See Figure 1 for an example.\nBecause we cannot ensure the absence of unmeasured confounders in observational settings, it is vital to conduct sensitivity analyses for the results. For sensitivity analyeses, we use E-values.\n\n\n\n\n\n\n\n\n\nFigure 1: Causal graph: three-wave panel design.\n\n\n\n\n\n\n\n\nChoose the scale for a causal contrast\nDifference/ Risk ratio?\n\nConsider:\n\nIn this course, we are interested in stratum specific comparisons\nIn the causal inference literature, the concept we use to make sense of stratum specific comparisons is called “effect modification.”\nBy inferring effects within strata, we may evaluate whether the effects of different exposures or treatments on some well-defined outcome (measured in some well-defined time-period after the exposure) differ depending on group measurement.\nThe logic of effect modification differs slightly from that of interaction.\n\n\n\nAside: extensions\nFor continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):\nATE_{A,A'} = E[Y(A) - Y(A')| L]\nThis essentially denotes an average treatment effect comparing the outcome under treatment level A to the outcome under treatment level A'.\nLikewise:\nATE_{A/A'} = \\frac{E[Y(A)| L]}{E[Y(A')| L]}\nThis defines the contrast of A and A' on a ratio scale.\n\n\nDescribe the population(s) for whom the intended study is meant to generalise by distinguishing between source and target populations.\nConsider the following concepts:\n\nSource population: a source population is where we gather our data for a study. We pull our specific sample from this group. It needs to mirror the broader group for our conclusions to be valid and widely applicable.\nTarget population: the target population is the larger group we aim to apply our study’s results to. It could be defined by location, demographics, or specific conditions. The closer the source matches the target in ways that are relevant to our causal questions, the stronger our causal inferences about the target population will be.\n\nGeneralisability refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called “external validity”.\n\n\n\\text{Generalisability} = PATE \\approx ATE_{\\text{sample}}\n\nTransportability refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.\n\n\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, T)\nwhere f is a function and T is a function that maps the results from our source population to another population. To achieve transportability, we need information about the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates differ between the populations. Assessing transportability requires scientific knowledge.\n\n\n\nSummary Step 1: Consider how much we need to do when asking a causal question!\nWe discover that asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: “How may we answer this causal question?”"
  },
  {
    "objectID": "content/09-content.html#step-1-formulate-the-research-question",
    "href": "content/09-content.html#step-1-formulate-the-research-question",
    "title": "Causal inference: a step by step guide",
    "section": "STEP 1 Formulate the Research Question",
    "text": "STEP 1 Formulate the Research Question\n\nState your question: is my question clearly stated? If not, state it.\nRelevance: have I explained its importance? If not, explain.\nEthics how might this question affect people? How might not investigating this question affect people?\nCausality: Is my question causal? If not, refine your question.\nHeterogeneous Treatment Effects: Do I want to examine who responds differently (CATE)?\nSubgroup analysis: does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nExplain the Framework: can I explain the causal inference framework and convey the gist to non-specialists? If not, review course materials.\n\n\nDetermine Data Requirements\n\nData types: are my data experimental? If yes, your project may not fit this course.\nTime-series data: are my data time-series? If not, reconsider your causal question.\nData waves: do I have at least three waves of data? If not, beware of confounding control issues.\nData source: are my data from the NZAVS simulated data set? If not, consult with me.\n\n\n\nDetermine the Outcome\n\nOutcome variable: is the outcome variable Y defined? If not, define it.\nMultiple outcomes: are there multiple outcomes? If yes, explain and define them.\nOutcome relevance: can I explain how the outcome variable/s relate to my question? If not, clarify.\nOutcome type: is my outcome binary and rare? If yes, consider logistic regression. If my outcome is continuous, consider z-transforming it or categorising it (consult an expert).\nOutcome timing: does the outcome appear after the exposure? It should.\n\n\n\nDetermine the Exposure\n\nExposure variable: is the exposure variable A defined? If not, define it.\nMultiple exposures: are there multiple exposures? If yes, reassess; if only one exposure, proceed.\nExposure relevance: can I explain how the exposure variable relates to my question? If not, clarify.\nPositivity: can we intervene on the exposure at all levels of the covariates? We should be able to.\nConsistency: can I interpret what it means to intervene on the exposure? I should be able to.\nExchangeability: are different versions of the exposure conditionally exchangeable given measured baseline confounders? They should be.\nExposure type: is the exposure binary or continuous?\nShift intervention: Am I contrasting static interventions or modified treatment policies?\nExposure timing: Does the exposure appear before the outcome? It should.\n\n\n\nAccount for Confounders\n\nBaseline confounders: Have I defined my baseline confounders L? I should have.\nJustification: Can I explain how the baseline confounders could affect both A and Y? I should be able to.\nTiming: Are the baseline confounders measured before the exposure? They should be.\nInclusion: Is the baseline measure of the exposure and the baseline outcome included in the set of baseline confounders? They should be.\nSufficiency: Are the baseline confounders sufficient to ensure balance on the exposure, such that A is independent of Y given L? If not, plan a sensitivity analysis.\nConfounder type: Are the confounders continuous or binary? If so, consider converting them to z-scores. If they are categorical with three or more levels, do not convert them to z-scores.\n\n\n\nDraw a Causal Diagram with Unmeasured Confounders\n\nUnmeasured confounders: Does previous science suggest the presence of unmeasured confounders? If not, expand your understanding.\nCausal diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding? I should have.\nMeasurement error: Have I described potential biases from measurement errors? If not, we’ll discuss later.\nTemporal order: Does my DAG have time indicators to ensure correct temporal order? It should.\nTime consistency: Is my DAG organized so that time follows in a consistent direction? It should.\n\n\n\nIdentify the Estimand\n\nATE or CATE or both? (For you – it will be both)\n\n\n\nUnderstanding Source and Target Populations\n\nPopulations identified: Have I explained how my sample relates to my target populations? I should have.\nGeneralisability and transportability: Have I considered whether my results generalise different populations? I should have.\n\n\n\nSet Eligibility Criteria\n\nCriteria stated: Have I stated the eligibility criteria for the study? I should have.\n\n\n\nDescribe Sample Characteristics\n\nDescriptive statistics: have I provided descriptive statistics for demographic information taken at baseline? I should have.\nExposure change: Have I demonstrated the magnitudes of change in the exposure from baseline to the exposure interval? I should have.\nReferences: Have I included references for more information about the sample? I should have.\n\n\n\nAddressing Missing Data\n\nMissing data checks: Have I checked for missing data? I should have.\nMissing data plan: If there is missing data, have I described how I will address it? I should have.\n\n\n\nSelecting the Model Approach: If Not Using Machine Learning (Lecture 7)\n\nApproach decision: Have I decided on using G-computation, IPTW, or Doubly-Robust Estimation? I should have.\nInteractions: If not using machine learning, have I included the interaction of the exposure and baseline covariates? I should have.\nBig data: If I have a large data set, should I include the interaction of the exposure, group, and baseline confounders? I should consider it.\nModel specification: have I double-checked the model specification? I should.\nOutcome assessment: If the outcome is rare and binary, have I specified logistic regression? If it’s continuous, have I considered converting it to z-scores?\nSensitivity analysis: am I planning a sensitivity analysis using simulation? If yes, describe it (e.g. E-values.)\n\n\n\nIf Machine Learing\n\nMachine Learning: have I explained how causal forests work (next week’s lecture).\n\n\n\nd. Highlight unmeasured pre-treatment covariates\nLet U denoted unmeasured pre-treatment covariates that may potentially bias the statistical association between A and Y independently of the measured covariates.\n\nConsider:\n\nTo affect Y and A, U must occur before A.\nIt is useful to draw a causal diagramme to illustrate all potential sources of bias.\nCausal diagrammes are qualitative tools that require specialist expertise. We cannot typically obtain a causal graph from the data.\nA causal diagramme should include only as much information as is required to assess confounding. See Figure 1 for an example.\nBecause we cannot ensure the absence of unmeasured confounders in observational settings, it is vital to conduct sensitivity analyses for the results. For sensitivity analyeses, we use E-values.\n\n\n\n\n\n\n\n\n\nFigure 1: Causal graph: three-wave panel design.\n\n\n\n\n\n\n\n\ne. Choose the scale for a causal contrast\nDifference/ Risk ratio?\n\nConsider:\n\nIn this course, we are interested in stratum specific comparisons\nIn the causal inference literature, the concept we use to make sense of stratum specific comparisons is called “effect modification.”\nBy inferring effects within strata, we may evaluate whether the effects of different exposures or treatments on some well-defined outcome (measured in some well-defined time-period after the exposure) differ depending on group measurement.\nThe logic of effect modification differs slightly from that of interaction.\n\n\n\nAside: extensions\nFor continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):\nATE_{A,A'} = E[Y(A) - Y(A')| L]\nThis essentially denotes an average treatment effect comparing the outcome under treatment level A to the outcome under treatment level A'.\nLikewise:\nATE_{A/A'} = \\frac{E[Y(A)| L]}{E[Y(A')| L]}\nThis defines the contrast of A and A' on a ratio scale.\n\n\nf. Describe the population(s) for whom the intended study is meant to generalise by distinguishing between source and target populations.\nConsider the following concepts:\n\nSource population: a source population is where we gather our data for a study. We pull our specific sample from this group. It needs to mirror the broader group for our conclusions to be valid and widely applicable.\nTarget population: the target population is the larger group we aim to apply our study’s results to. It could be defined by location, demographics, or specific conditions. The closer the source matches the target in ways that are relevant to our causal questions, the stronger our causal inferences about the target population will be.\n\nGeneralisability refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called “external validity”.\n\n\n\\text{Generalisability} = PATE \\approx ATE_{\\text{sample}}\n\nTransportability refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.\n\n\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, T)\nwhere f is a function and T is a function that maps the results from our source population to another population. To achieve transportability, we need information about the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates differ between the populations. Assessing transportability requires scientific knowledge.\n\n\n\nSummary Step 1: Consider how much we need to do when asking a causal question!\nWe discover that asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: “How may we answer this causal question?”"
  },
  {
    "objectID": "content/09-content.html#step-2-answer-a-causal-question",
    "href": "content/09-content.html#step-2-answer-a-causal-question",
    "title": "Causal inference: a step by step guide",
    "section": "STEP 2: ANSWER A CAUSAL QUESTION",
    "text": "STEP 2: ANSWER A CAUSAL QUESTION\n\nObtain longitudinal data\nNote that causal inference from observational data turns on the appropriate temporal ordering of the key variables involved in the study.\nRecall we have defined.\n\nA: Our exposure or treatment variable, denoted as A. Here we consider the example of ‘Church attendance’.\nY: The outcome variable we are interested in, represented by Y, is psychological distress. We operationalise this variable through the ‘Kessler-6’ distress scale.\nL: The confounding variables, collectively referred to as L, represent factors that can independently influence both A and Y. For example, socio-economic status could be a confounder that impacts both the likelihood of church attendance and the levels of psychological distress.\n\nGiven the importance of temporal ordering, we must now define time:\n\nt \\in T: Let t denote within a multiwave panel study with T measurement intervals.\n\nWhere t/\\text{{exposure}} denotes the measurement interval for the exposure. Longitudinal data collection provides us the ability to establish a causal model such that:\nt_{confounders} &lt; t_{exposure}&lt; t_{outcome}\nTo minimise the posibility of time-varying confounding and obtain the clearest effect estimates, we should acquire the most recent values of \\mathbf{L} preceding A and the latest values of A before Y.\nNote in Figure 1, We use the prefixes “t0, t1, and t2” to denote temporal ordering. We include in the set of baseline confounders the pre-exposure measurement of A and Y. This allows for more substantial confounding control. For unmeasured confounder to affect both the exposure and the outcome, it would need to do so independently of the pre-exposure confounders. Additionally, including the baseline exposure gives us an effect estimate for the incidence exposure, rather than the prevelance of the exposure. This helps us to assess the expected change in the outcome were we to initate a change in the exposure.\n\n\nInclude the measured exposure with baseline covariates\nControlling for prior exposure enables the interpretation of the effect estimate as a change in the exposure in a manner akin to a randomised trial. We propose that the effect estimate with prior control for the exposure estimates the “incidence exposure” rather than the “prevalence exposure” (Danaei, Tavakkoli, and Hernán 2012). It is crucial to estimate the incidence exposure because if the effects of an exposure are harmful in the short term such that these effects are not subsequently measured, a failure to adjust for prior exposure will yield the illusion that the exposure is beneficial. Furthermore, this approach aids in controlling for unmeasured confounding. For such a confounder to explain away the observed exposure-outcome association, it would need to do so independently of the prior level of the exposure and outcome.\n\nDanaei, Goodarz, Mohammad Tavakkoli, and Miguel A. Hernán. 2012. “Bias in observational studies of prevalent users: lessons for comparative effectiveness research from a meta-analysis of statins.” American Journal of Epidemiology 175 (4): 250–62. https://doi.org/10.1093/aje/kwr301.\n\n\nState the eligibility criteria for participation\nThis step is invaluable for assessing whether we are answering the causal question that we have asked.\n\nConsider:\n\nGeneralisability: we cannot evaluate inferences to a target group from the source population if we do not describe the source population\nEligibility criteria will help us to ensure whether we have correctly evaluated potential measurement bias/error in our instruments.\n\nFor example, the New Zealand Attitudes and Values Study is a National Probability study of New Zealanders. The details provided in the supplementary materials describe how individuals were randomly selected from the country’s electoral roll. From these invitations there was typically less than 15% response rate. How might this process of recruitment affect generalisability and transportability of our results?\n\nAside: discuss per protocol effects/ intention to treat effects\n\n\n\n\nDetermine how missing data will be handled\n\nAs we will consider in the upcoming weeks, loss to follow up and non-response opens sources for bias. We must develop a strategy for handling missing data.\n\n\n\nState a statistical model\nThe models we have considered in this course are G-computation, Inverse Probability of Treatement Weighting, and Doubly-Robust estimation.\n\n\nReporting\nConsider the following ideas about how to report one’s model:\n\nEstimator: Doubly robust where possible.\nPropensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations.\nWeightIt Package: Explicitly mention the use of the ‘WeightIt’ package in R, including any specific options or parameters used in the propensity score estimation process.\nMethod Variations: Report if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as ‘ebal’, ‘energy’, and ‘ps’.\nContinuous Exposures: Highlight that for continuous exposures, only the ‘energy’ option was used for propensity score estimation.\nSubgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\nCovariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting.\nWeighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit.\nOutcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation.\nSubgroup Interaction: Address whether the subgroup was included separately as an interaction in the outcome model, and if the model successfully converged.\nMachine Learning Using lmtp If using the lmtp package, do a stratified analysis. (see today’s lab)\nModel coefficients: note that the model coefficients should not be interpreted, as they are not meaningful in this context.\nConfidence intervals and standard errors: Describe the methods used to derive confidence intervals and standard errors, noting the use of the ‘clarify’ package in R for simulation based inference.\n\n\n\nExample of how to report a doubly robust method in your report\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\nStep 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\nStep 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\nStep 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\nStep 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\nStep 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\n\nInference\nConsider the following ideas about what to discuss in one’s findings: Consider the following ideas about what to discuss in one’s findings. The order of exposition might be different.\n\nSummary of results: What did you find?\nInterpretation of E-values: Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\nCausal Effect Interpretation: What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\nComparison of Subgroups: Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\nUncertainty and Confidence Intervals: Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\nGeneralisability and Transportability: Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\nAssumptions and Limitations: Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\nTheoretical Relevance: How are these findings relevant to existing theories.\nReplication and Future Research: Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\nReal-world Implications: Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research."
  },
  {
    "objectID": "content/09-content.html#appendix-a-details-of-estimation-approaches",
    "href": "content/09-content.html#appendix-a-details-of-estimation-approaches",
    "title": "Causal inference: a step by step guide",
    "section": "Appendix A: Details of Estimation Approaches",
    "text": "Appendix A: Details of Estimation Approaches\n\nG-computation for Subgroup Analysis Estimator\nStep 1: Estimate the outcome model. Fit a model for the outcome Y, conditional on the exposure A, the covariates L, and subgroup indicator G. This model can be a linear regression, logistic regression, or another statistical model. The goal is to capture the relationship between the outcome, exposure, confounders, and subgroups.\n \\hat{E}(Y|A,L,G) = f_Y(A,L,G; \\theta_Y) \nThis equation represents the expected value of the outcome Y given the exposure A, covariates L, and subgroup G, as modelled by the function f_Y with parameters \\theta_Y. This formulation allows for the prediction of the average outcome Y given certain values of A, L, and G.\nStep 2: Simulate potential outcomes. For each individual in each subgroup, predict their potential outcome under the intervention A=a using the estimated outcome model:\n\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,L,G=g; \\hat{\\theta}_Y]\nWe also predict the potential outcome for everyone in each subgroup under the causal contrast, setting the intervention for everyone in that group to A=a':\n\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',L,G=g; \\hat{\\theta}_Y]\nIn these equations, Y represents the potential outcome, A is the intervention, L are the covariates, G=g represents the subgroup, and \\theta_Y are the parameters of the outcome model.\nStep 3: Calculate the estimated difference for each subgroup g:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThis difference \\hat{\\delta}_g represents the average causal effect of changing the exposure from level a' to level a within each subgroup g.\nWe use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\nStep 4: Compare differences in causal effects by subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a^{\\prime})|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a^{\\prime})|G=g^{\\prime}]- \\hat{E}[Y(a)|G=g^{\\prime}]\\big)}^{\\hat{\\delta_{g^{\\prime}}}}\nThis difference \\hat{\\gamma} represents the difference in the average causal effects between the subgroups g and g'. It measures the difference in effect of the exposure A within subgroup G on the outcome Y.\n1\n1 A and G on Y might not be additive. We assume that the potential confounders L are sufficient to control for confounding. See AppendixWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\n\n\nInverse Probability of Treatment Weighting (IPTW) for Subgroup Analysis Estimator\nStep 1: Estimate the propensity score. The propensity score e(L, G) is the conditional probability of the exposure A = 1, given the covariates L and subgroup indicator G. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\n\\hat{e} = P(A = 1 | L, G) = f_A(L, G; \\theta_A)\nHere, f_A(L, G; \\theta_A) is a function (statistical model) that estimates the probability of the exposure A = 1 given covariates L and subgroup G. Then, we calculate the weights for each individual, denoted as v, using the estimated propensity score:\n\nv =\n\\begin{cases}\n\\frac{1}{\\hat{e}} & \\text{if } A = 1 \\\\\n\\frac{1}{1-\\hat{e}} & \\text{if } A = 0\n\\end{cases}\n\nStep 2: Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome Y, conditional on the exposure A and subgroup G. This can be represented as:\n \\hat{E}(Y|A, G; V) = f_Y(A, G ; \\theta_Y, V) \nIn this model, f_Y is a function (such as a weighted regression model) with parameters θ_Y.\nStep 3: Simulate potential outcomes. For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention A=a regardless of their actual exposure level:\n\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,G=g; \\hat{\\theta}_Y, v]\nAnd also under the hypothetical scenario where everyone is exposed to intervention A=a':\n\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',G=g; \\hat{\\theta}_Y, v]\nStep 4: Estimate the average causal effect for each subgroup as the difference in the predicted outcomes:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThe estimated difference \\hat{\\delta}_g represents the average causal effect within group g.\nStep 5: Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a')|G=g']- \\hat{E}[Y(a)|G=g']\\big)}^{\\hat{\\delta_{g'}}}\nThis \\hat{\\gamma} represents the difference in the average causal effects between the subgroups g and g'.\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\n\n\nDoubly Robust Estimation for Subgroup Analysis Estimator\nIt appears that the Doubly Robust Estimation explanation for subgroup analysis is already clear and correct, covering all the necessary steps in the process. Nevertheless, there’s a slight confusion in step 4. The difference \\delta_g is not defined within the document. I assume that you intended to write \\hat{\\delta}_g. Here’s the corrected version:\n\n\nDoubly Robust Estimation for Subgroup Analysis Estimator\nDoubly Robust Estimation is a powerful technique that combines the strengths of both the IPTW and G-computation methods. It uses both the propensity score model and the outcome model, which makes it doubly robust: it produces unbiased estimates if either one of the models is correctly specified.\nStep 1 Estimate the propensity score. The propensity score \\hat{e}(L, G) is the conditional probability of the exposure A = 1, given the covariates L and subgroup indicator G. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\n\\hat{e} = P(A = 1 | L, G) = f_A(L, G; \\theta_A)\nHere, f_A(L, G; \\theta_A) is a function (statistical model) that estimates the probability of the exposure A = 1 given covariates L and subgroup G. Then, we calculate the weights for each individual, denoted as v, using the estimated propensity score:\n\nv =\n\\begin{cases}\n\\frac{1}{\\hat{e}} & \\text{if } A = 1 \\\\\n\\frac{1}{1-\\hat{e}} & \\text{if } A = 0\n\\end{cases}\n\nStep 2 Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome Y, conditional on the exposure A, covariates L, and subgroup G.\n \\hat{E}(Y|A, L, G; V) = f_Y(A, L, G ; \\theta_Y, V) \nStep 3 For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention A=a regardless of their actual exposure level:\n\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,G=g; L,\\hat{\\theta}_Y, v]\nAnd also under the hypothetical scenario where everyone in each subgroup is exposed to intervention A=a':\n\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',G=g; L; \\hat{\\theta}_Y, v]\nStep 4 Estimate the average causal effect for each subgroup. Compute the estimated expected value of the potential outcomes under each intervention level for each subgroup:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThe estimated difference \\hat{\\delta}_g represents the average causal effect of changing the exposure from level a' to level a within each subgroup.\nStep 5 Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a')|G=g']- \\hat{E}[Y(a)|G=g']\\big)}^{\\hat{\\delta_{g'}}}\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023)."
  },
  {
    "objectID": "content/09-content.html#appendix-b-g-computation-for-subgroup-analysis-estimator-with-non-additive-effects",
    "href": "content/09-content.html#appendix-b-g-computation-for-subgroup-analysis-estimator-with-non-additive-effects",
    "title": "Causal inference: a step by step guide",
    "section": "Appendix B: G-computation for Subgroup Analysis Estimator with Non-Additive Effects",
    "text": "Appendix B: G-computation for Subgroup Analysis Estimator with Non-Additive Effects\nStep 1: Estimate the outcome model. Fit a model for the outcome Y, conditional on the exposure A, the covariates L, subgroup indicator G, and interactions between A and G. This model can be a linear regression, logistic regression, or another statistical model. The goal is to capture the relationship between the outcome, exposure, confounders, subgroups, and their interactions.\n \\hat{E}(Y|A,L,G,AG) = f_Y(A,L,G,AG; \\theta_Y) \nThis equation represents the expected value of the outcome Y given the exposure A, covariates L, subgroup G, and interaction term AG, as modeled by the function f_Y with parameters \\theta_Y.\nStep 2: Simulate potential outcomes. For each individual in each subgroup, predict their potential outcome under the intervention A=a using the estimated outcome model:\n\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,L,G=g,AG=ag; \\hat{\\theta}_Y]\nWe also predict the potential outcome for everyone in each subgroup under the causal contrast, setting the intervention for everyone in that group to A=a':\n\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',L,G=g,AG=a'g; \\hat{\\theta}_Y]\nStep 3: Calculate the estimated difference for each subgroup g:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nStep 4: Compare differences in causal effects by subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a^{\\prime})|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a^{\\prime})|G=g^{\\prime}]- \\hat{E}[Y(a)|G=g^{\\prime}]\\big)}^{\\hat{\\delta_{g^{\\prime}}}}\nThis difference \\hat{\\gamma} represents the difference in the average causal effects between the subgroups g and g', taking into account the interaction effect of the exposure A and the subgroup G on the outcome Y.\nNote that the interaction term AG (or ag and a'g in the potential outcomes) stands for the interaction between the exposure level and the subgroup. This term is necessary to accommodate the non-additive effects in the model. As before, we must ensure that potential confounders L are sufficient to control for confounding."
  },
  {
    "objectID": "content/09-content.html#appendix-c-doubly-robust-estimation-for-subgroup-analysis-estimator-with-interaction",
    "href": "content/09-content.html#appendix-c-doubly-robust-estimation-for-subgroup-analysis-estimator-with-interaction",
    "title": "Causal inference: a step by step guide",
    "section": "Appendix C: Doubly Robust Estimation for Subgroup Analysis Estimator with Interaction",
    "text": "Appendix C: Doubly Robust Estimation for Subgroup Analysis Estimator with Interaction\nAgain, Doubly Robust Estimation combines the strengths of both the IPTW and G-computation methods. It uses both the propensity score model and the outcome model, which makes it doubly robust: it produces unbiased estimates if either one of the models is correctly specified.\nStep 1 Estimate the propensity score. The propensity score e(L, G) is the conditional probability of the exposure A = 1, given the covariates L and subgroup indicator G. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\ne = P(A = 1 | L, G) = f_A(L, G; \\theta_A)\nHere, f_A(L, G; \\theta_A) is a function (statistical model) that estimates the probability of the exposure A = 1 given covariates L and subgroup G. Then, we calculate the weights for each individual, denoted as v, using the estimated propensity score:\n\nv =\n\\begin{cases}\n\\frac{1}{\\hat{e}} & \\text{if } A = 1 \\\\\n\\frac{1}{1-\\hat{e}} & \\text{if } A = 0\n\\end{cases}\n\nStep 2 Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome Y, conditional on the exposure A, covariates L, subgroup G and the interaction between A and G.\n \\hat{E}(Y|A, L, G, AG; V) = f_Y(A, L, G, AG ; \\theta_Y, V) \nStep 3 For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention A=a regardless of their actual exposure level:\n\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,G=g, AG=ag; L,\\hat{\\theta}_Y, v]\nAnd also under the hypothetical scenario where everyone in each subgroup is exposed to intervention A=a':\n\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',G=g, AG=a'g; L; \\hat{\\theta}_Y, v]\nStep 4 Estimate the average causal effect for each subgroup. Compute the estimated expected value of the potential outcomes under each intervention level for each subgroup:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThe estimated difference \\hat{\\delta}_g represents the average causal effect of changing the exposure from level a' to level a within each subgroup.\nStep 5 Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a')|G=g']- \\hat{E}[Y(a)|G=g']\\big)}^{\\hat{\\delta_{g'}}}\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023)."
  },
  {
    "objectID": "content/09-content.html#appendix-d-marginal-structural-models-for-estimating-population-average-treatment-effect-with-interaction-doubly-robust",
    "href": "content/09-content.html#appendix-d-marginal-structural-models-for-estimating-population-average-treatment-effect-with-interaction-doubly-robust",
    "title": "Causal inference: a step by step guide",
    "section": "Appendix D: Marginal Structural Models for Estimating Population Average Treatment Effect with Interaction (Doubly Robust)",
    "text": "Appendix D: Marginal Structural Models for Estimating Population Average Treatment Effect with Interaction (Doubly Robust)\nSometimes we will only wish to estimate a marginal effect. In that case.\nStep 1 Estimate the propensity score. The propensity score e(L) is the conditional probability of the exposure A = 1, given the covariates L which contains the subgroup G. This can be modelled using logistic regression or other functions as described in Greifer et al. (2023)\n\\hat{e} = P(A = 1 | L) = f_A(L; \\theta_A)\nHere, f_A(L; \\theta_A) is a function (a statistical model) that estimates the probability of the exposure A = 1 given covariates L. Then, we calculate the weights for each individual, denoted as v, using the estimated propensity score:\n\nv =\n\\begin{cases}\n\\frac{1}{e} & \\text{if } A = 1 \\\\\n\\frac{1}{1-e} & \\text{if } A = 0\n\\end{cases}\n\nStep 2 Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome Y, conditional on the exposure A and covariates L.\n \\hat{E}(Y|A, L; V) = f_Y(A, L; \\theta_Y, V) \nThis model should include terms for both the main effects of A and L and their interaction AL.\nStep 3 For the entire population, simulate the potential outcome under the hypothetical scenario where everyone is exposed to the intervention A=a regardless of their actual exposure level:\n\\hat{E}(Y(a)) = \\hat{E}[Y|A=a; L,\\hat{\\theta}_Y, v]\nAnd also under the hypothetical scenario where everyone is exposed to intervention A=a':\n\\hat{E}(Y(a')) = \\hat{E}[Y|A=a'; L; \\hat{\\theta}_Y, v]\nStep 4 Estimate the average causal effect for the entire population. Compute the estimated expected value of the potential outcomes under each intervention level for the entire population:\n\\hat{\\delta} = \\hat{E}[Y(a)] - \\hat{E}[Y(a')]\nThe estimated difference \\hat{\\delta} represents the average causal effect of changing the exposure from level a' to level a in the entire population.\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\n\nGreifer, Noah, Steven Worthington, Stefano Iacus, and Gary King. 2023. Clarify: Simulation-Based Inference for Regression Models. https://iqss.github.io/clarify/.\n\nMachine Learning\nExample from https://osf.io/cnphs\n\nWe perform statistical estimation using semi-parametric Targeted Learning, specifically a Targeted Minimum Loss-based Estimation (TMLE) estimator. TMLE is a robust method that combines machine learning techniques with traditional statistical models to estimate causal effects while providing valid statistical uncertainty measures for these estimates (Laan and Gruber 2012; Van der Laan 2014).\n\nLaan, Mark J van der, and Susan Gruber. 2012. “Targeted Minimum Loss Based Estimation of Causal Effects of Multiple Time Point Interventions.” The International Journal of Biostatistics 8 (1).\n\nVan der Laan, Mark J. 2014. “Targeted Estimation of Nuisance Parameters to Obtain Valid Statistical Inference.” The International Journal of Biostatistics 10 (1): 29–57.\n\n\nTMLE operates through a two-step process that involves modelling both the outcome and treatment (exposure). Initially, TMLE employs machine learning algorithms to flexibly model the relationship between treatments, covariates, and outcomes. This flexibility allows TMLE to account for complex, high-dimensional covariate spaces without imposing restrictive model assumptions; (Van Der Laan and Rose 2011, 2018). The outcome of this step is a set of initial estimates for these relationships.\n\nVan Der Laan, Mark J., and Sherri Rose. 2011. Targeted Learning: Causal Inference for Observational and Experimental Data. Springer Series in Statistics. New York, NY: Springer. https://link.springer.com/10.1007/978-1-4419-9782-1.\n\n———. 2018. Targeted Learning in Data Science: Causal Inference for Complex Longitudinal Studies. Springer Series in Statistics. Cham: Springer International Publishing. http://link.springer.com/10.1007/978-3-319-65304-4.\n\n\nThe second step of TMLE involves ``targeting’’ these initial estimates by incorporating information about the observed data distribution to improve the accuracy of the causal effect estimate. TMLE achieves this precision through an iterative updating process, which adjusts the initial estimates towards the true causal effect. This updating process is guided by the efficient influence function, ensuring that the final TMLE estimate is as close as possible, given the measures and data, to the targeted causal effect while still being robust to model-misspecification in either the outcome or the treatment model (Laan, Luedtke, and Dı́az 2014).\n\nLaan, Mark J van der, Alexander R Luedtke, and Iván Dı́az. 2014. “Discussion of Identification, Estimation and Approximation of Risk Under Interventions That Depend on the Natural Value of Treatment Using Observational Data, by Jessica Young, Miguel Hernán, and James Robins.” Epidemiologic Methods 3 (1): 21–31.\n\n\nAgain, a central feature of TMLE is its double-robustness property. If either the treatment model or the outcome model is correctly specified, the TMLE estimator will consistently estimate the causal effect. Additionally, we used cross-validation to avoid over-fitting, following the pre-stated protocols in Bulbulia (Bulbulia 2024c). The integration of TMLE and machine learning technologies reduces the dependence on restrictive modelling assumptions and introduces an additional layer of robustness. For further details of the specific targeted learning strategy we favour, see Hoffman et al. (2023). We perform estimation using the package (Williams and Díaz 2021). We used the library for semi-parametric estimation with the predefined libraries , , and (Chen et al. 2023; Polley et al. 2023; Wright and Ziegler 2017). We created graphs, tables and output reports using the package (Bulbulia 2024b).\n\n———. 2024c. “A Practical Guide to Causal Inference in Three-Wave Panel Studies.” PsyArXiv Preprints, February. https://doi.org/10.31234/osf.io/uyg3d.\n\nHoffman, Katherine L., Diego Salazar-Barreto, Kara E. Rudolph, and Iván Díaz. 2023. “Introducing Longitudinal Modified Treatment Policies: A Unified Framework for Studying Complex Exposures,” April. https://doi.org/10.48550/arXiv.2304.09460.\n\nWilliams, Nicholas T., and Iván Díaz. 2021. lmtp: Non-Parametric Causal Effects of Feasible Interventions Based on Modified Treatment Policies. https://doi.org/10.5281/zenodo.3874931.\n\nChen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2023. Xgboost: Extreme Gradient Boosting. https://CRAN.R-project.org/package=xgboost.\n\nPolley, Eric, Erin LeDell, Chris Kennedy, and Mark van der Laan Laan. 2023. SuperLearner: Super Learner Prediction. https://CRAN.R-project.org/package=SuperLearner.\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01.\n\n———. 2024b. Margot: MARGinal Observational Treatment-Effects. https://doi.org/10.5281/zenodo.10907724.\n\n\nSensitivity Analysis Using the E-value\n\nTo assess the sensitivity of results to unmeasured confounding, we report VanderWeele and Ding’s ``E-value’’ in all analyses (Tyler J. VanderWeele and Ding 2017). The E-value quantifies the minimum strength of association (on the risk ratio scale) that an unmeasured confounder would need to have with both the exposure and the outcome (after considering the measured covariates) to explain away the observed exposure-outcome association (Linden, Mathur, and VanderWeele 2020; Tyler J. VanderWeele, Mathur, and Chen 2020). To evaluate the strength of evidence, we use the bound of the E-value 95% confidence interval closest to 1.\n\nVanderWeele, Tyler J., and Peng Ding. 2017. “Sensitivity Analysis in Observational Research: Introducing the E-Value.” Annals of Internal Medicine 167 (4): 268–74. https://doi.org/10.7326/M16-2607.\n\nLinden, Ariel, Maya B Mathur, and Tyler J VanderWeele. 2020. “Conducting Sensitivity Analysis for Unmeasured Confounding in Observational Studies Using e-Values: The Evalue Package.” The Stata Journal 20 (1): 162–75.\n\nVanderWeele, Tyler J, Maya B Mathur, and Ying Chen. 2020. “Outcome-Wide Longitudinal Designs for Causal Inference: A New Template for Empirical Studies.” Statistical Science 35 (3): 437–66.\n\n\n\n\nPackages\n\n\nCode\nreport::cite_packages()\n\n\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 1.0.37 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont &lt;https://doi.org/10.32614/CRAN.package.extrafont&gt;, R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. doi:10.32614/CRAN.package.tibble &lt;https://doi.org/10.32614/CRAN.package.tibble&gt;, R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. doi:10.32614/CRAN.package.forcats &lt;https://doi.org/10.32614/CRAN.package.forcats&gt;, R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. doi:10.32614/CRAN.package.stringr &lt;https://doi.org/10.32614/CRAN.package.stringr&gt;, R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. doi:10.32614/CRAN.package.dplyr &lt;https://doi.org/10.32614/CRAN.package.dplyr&gt;, R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. doi:10.32614/CRAN.package.purrr &lt;https://doi.org/10.32614/CRAN.package.purrr&gt;, R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. doi:10.32614/CRAN.package.readr &lt;https://doi.org/10.32614/CRAN.package.readr&gt;, R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. doi:10.32614/CRAN.package.tidyr &lt;https://doi.org/10.32614/CRAN.package.tidyr&gt;, R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Asking questions in cross-cultural psychology",
    "section": "",
    "text": "Figure 1: Causal graph: we will refer to this image in the lecture and begin reviewing causal graphs in Week 2"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Slides",
    "text": "Slides\nPREVIEW\n\n\n\nOpen in browser here"
  },
  {
    "objectID": "content/01-content.html#slide-deck",
    "href": "content/01-content.html#slide-deck",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Slide deck:",
    "text": "Slide deck:"
  },
  {
    "objectID": "content/01-content.html#slides-1",
    "href": "content/01-content.html#slides-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Slides",
    "text": "Slides\nPREVIEW\n\n\n\nOpen in browser here"
  },
  {
    "objectID": "content/01-content.html#why-learn-r",
    "href": "content/01-content.html#why-learn-r",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Why learn R?",
    "text": "Why learn R?\n\nYou’ll need it for your final report.\nSupports your psychology coursework.\nEnhances your coding skills."
  },
  {
    "objectID": "content/01-content.html#installing-r",
    "href": "content/01-content.html#installing-r",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Installing R",
    "text": "Installing R\n\nVisit the Comprehensive R Archive Network (CRAN) at https://cran.r-project.org/.\nSelect the version of R suitable for your operating system (Windows, Mac, or Linux).\nDownload and install it by following the on-screen instructions."
  },
  {
    "objectID": "content/01-content.html#installing-rstudio",
    "href": "content/01-content.html#installing-rstudio",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nAfter downloading R…\nSee Johannes Karl’s Video\n\nStep 1: Install R-Studio\nIf you have already downloaded and installed R from the Comprehensive R Archive Network (CRAN):\n\nGo to the RStudio download page at https://www.rstudio.com/products/rstudio/download/\nChoose the free version of RStudio Desktop, and download it for your operating system.\nDownload and install RStudio Desktop.\nOpen RStudio to begin setting up your project environment.\n\n\n\nStep 2: Create a new project\n\nIn RStudio, go to File &gt; New Project.\nChoose New Directory for a new project or Existing Directory if you have a folder where you want to initialise an RStudio project.\nFor a new project, select New Project, then provide a directory name. This name will also be the name of your project.\nSpecify the location where the project folder will be created.\nClick Create Project.\n\n\n\n\n\n\n\nOrder your R-studio/R workflow\n\n\n\n\nClear folder structure\nIf you are using GitHub (or similar) create a location on your machine (i.d. not dropbox)\nIf you are not using GitHub choose the cloud (Dropbox or similar).\nWhen creating new files and scripts, use clear labels that anyone could understand.\nThat “anyone” will be your future self, trying to make sense.\n\n\n\n\n\nStep 3: Give project structure\n\nOrganising Files and Folders:\n\nWithin your project, create folders to organise your scripts and data.\nCommon folder names include R/ for R scripts, data/ for datasets, and doc/ for documentation.\nYou can create these folders using RStudio’s Files pane or through your operating system’s file explorer.\n\nCreating and Managing R Scripts:\n\nTo create a new R script, go to File &gt; New File &gt; R Script.\nSave the script in your project directory’s R/ folder to keep your work organised. Use meaningful file names that describe the script’s purpose.\n\nVersion Control:\n\nIf you are familiar with version control, you can initialise a Git repository within your project by selecting the Version Control option when creating a new project.\nThis allows for better tracking of changes and collaboration if working with others.\nIf you are not familiar with version control (or have not installed git on your machine), do not worry about initialising a Git repository.\n\n\n\n\nStep 4: Working with R-scripts\n\nWriting and executing Code:\n\nWrite your R code in the script editor.\nExecute code by selecting lines and pressing Ctrl + Enter (Windows/Linux) or Cmd + Enter (Mac).\n\nCommenting and documentation:\n\nUse comments (preceded by #) to document your code for clarity and future reference.\n\nSaving and organising scripts:\n\nRegularly save your scripts (Ctrl + S or Cmd + S).\nOrganise scripts into folders within your project for different analyses or data processing tasks.\n\n\n\n\nStep 5: When you exit R-studio\n\nBefore concluding your work, save your workspace or clear it to start fresh in the next session (Session &gt; Restart R).\n\n\n\n\n\n\n\nOrder your R-studio/R workflow\n\n\n\n\nAgain, use clearly defined script names\nAnnotate your code\nSave your scripts often (Ctrl + S or Cmd + S).\n\n\n\n\n\n\n\n\n\nExercise 1: Install the tidyverse package\n\n\n\nFollow these instructions to install the tidyverse package in RStudio:\n\nOpen RStudio: launch RStudio on your computer.\nAccess package installation:\n\nNavigate to the menu at the top of RStudio and click on Tools &gt; Install Packages.... This opens the Install Packages dialogue box.\n\nInstall tidyverse:\n\nIn the Install Packages dialogue box, you will see a field labelled “Packages (separate multiple with space or comma):”. Click in this field and type tidyverse.\nBelow the packages field, ensure the checkbox for Install dependencies is checked. This ensures all packages that tidyverse depends on are also installed.\n\nBegin installation:\n\nClick on the Install button to start the installation process.\n\n\nThe installation might take a few minutes. Monitor the progress in the “Console” pane. Once the installation is complete, you will see a message in the console indicating that the process has finished.\n\nLoad tidyverse: After successful installation, you can load the tidyverse package into your R session by typing library(tidyverse) in the console and pressing Enter."
  },
  {
    "objectID": "content/01-content.html#basic-r-commands",
    "href": "content/01-content.html#basic-r-commands",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Basic R Commands",
    "text": "Basic R Commands\n\n\n\n\n\n\nHow to copy the code on this page\n\n\n\n\nR Script &gt; New File &gt; R\nName your new R script and save it in a folder.\nHover your cursor over the top right of the code panel, and click the copy tab.\nCopy the text into your script.\nSave: Ctrl + S or Cmd + S.\n\n\n\n\nAssignment (&lt;-)\nAssignment in R is done using the ‘&lt;-’ operator, which on my machine renders &lt;-. This operator assigns values to variables:\n\nx &lt;- 10 # assigns the value 10 to x\ny &lt;- 5 # assigns the value 5 to y\n\n# this does the same\nx &lt;- 10\ny &lt;- 5\n\n# note what happens when we do this\n# 10 = 5 # not run\n\n# but we can do this\n# 10 == 5 # considered below\n\n\n\n\n\n\n\nRStudio Assignment Operator Shortcut\n\n\n\n\nFor macOS: Option + - (minus key) inserts &lt;-.\nFor Windows and Linux: Alt + - (minus key) inserts &lt;-.\n\nConsult the latest RStudio documentation or access the Keyboard Shortcuts Help (Tools -&gt; Keyboard Shortcuts Help) for up-to-date shortcuts.\n\n\n\n\nConcatenation (c())\nThe c() function combines multiple elements into a vector.\n\nnumbers &lt;- c(1, 2, 3, 4, 5) # a vector of numbers\nprint(numbers)\n\n[1] 1 2 3 4 5\n\n\n\n\nOperations (+, -)\nBasic arithmetic operations include addition (+) and subtraction (-).\n\n# this does the same\nx &lt;- 10\ny &lt;- 5\n\nsum &lt;- x + y # adds x and y\n\nprint(sum)\n\n[1] 15\n\ndifference &lt;- x - y # subtracts y from x\n\n# note we did not need to use the `print()` function\ndifference\n\n[1] 5\n\n\n\n\n\n\n\n\nExecuting code\n\n\n\n\nCtrl + Enter (Windows/Linux) or Cmd + Enter (Mac).\n\n\n\nIn addition to assignment, multiplication and division are fundamental arithmetic operations in R that allow you to manipulate numeric data. Here is how you can incorporate these operations into your basic R commands documentation:\n\n\nMultiplication (*) and Division (/)\nMultiplication and division in R are performed using the * and / operators, respectively. These operators allow for element-wise operations on vectors, as well as operations on individual numeric values.\n\n# multiplication\nproduct &lt;- x * y # multiplies x by y\nproduct\n\n[1] 50\n\n# division\nquotient &lt;- x / y # divides x by y\nquotient\n\n[1] 2\n\n# element-wise multiplication on vectors\nvector1 &lt;- c(1, 2, 3)\nvector2 &lt;- c(4, 5, 6)\n# multiplies each element of vector1 by the corresponding element of vector2\nvector_product &lt;- vector1 * vector2\nvector_product\n\n[1]  4 10 18\n\n# element-wise division on vectors\n# divides each element of vector1 by the corresponding element of vector2\nvector_division &lt;- vector1 / vector2\nvector_division\n\n[1] 0.25 0.40 0.50\n\n\n\nMultiplication and division can be used for scalar (single values) and vector (multiple values) operations. When applied to vectors, these operations are performed element-wise.\nBe mindful of division by zero, as this will result in Inf (infinity) or NaN (not a number) depending on the context.\n\n\n# example of division by zero\nresult &lt;- 10 / 0 # results in Inf\nzero_division &lt;- 0 / 0 # results in NaN\n\n\nR also supports integer division using the %/% operator and modulo operation using %% to find the remainder.\n\n\n# integer division\ninteger_division &lt;- 10 %/% 3 # results in 3\n\n# modulo operation\nremainder &lt;- 10 %% 3 # results in 1\n\n\n\nrm() Remove Object\n\n# `rm()` remove object ----------------------------------------------------\ndevil_number &lt;- 666 # results in 1\n\n# view\ndevil_number\n\n[1] 666\n\n# remove the devil number\nrm(devil_number)\n\n# check\n# devil_number\n\n\n\nLogic (!, !=, ==)\nLogical operations include NOT (!), NOT EQUAL (!=), and EQUAL (==).\n\nx_not_y &lt;- x != y # checks if x is not equal to y\nx_not_y\n\n[1] TRUE\n\nx_equal_10 &lt;- x == 10 # checks if x is equal to 10\nx_equal_10\n\n[1] TRUE\n\n\nLogical operations are fundamental in R for controlling the flow of execution and making decisions based on conditions. In addition to NOT (!), NOT EQUAL (!=), and EQUAL (==), there are several other logical operators you should know:\n\n\nOR (| and ||)\n\nThe | operator performs element-wise logical OR operation. It evaluates each pair of elements in two logical vectors to see if at least one is TRUE.\nThe || operator performs a logical OR operation but only evaluates the first element of each vector – mainly used in if statements and not for vectorised operations.\n\n\n# element-wise OR\nvector_or &lt;- c(TRUE, FALSE) | c(FALSE, TRUE) # returns c(TRUE, TRUE)\nvector_or\n\n[1] TRUE TRUE\n\n# single OR (only looks at first element)\nsingle_or &lt;- TRUE || FALSE # returns TRUE\nsingle_or\n\n[1] TRUE\n\n\n\n\nAND (& and &&)\n\nThe & operator performs element-wise logical AND operations. It checks if both elements in the corresponding positions of two logical vectors are TRUE.\nThe && operator performs a logical AND operation but only evaluates the first element of each vector. Like ||, used in conditions that do not require vectorised operations.\n\n\n# element-wise AND\nvector_and &lt;- c(TRUE, FALSE) & c(FALSE, TRUE) # returns c(FALSE, FALSE)\n\n# single AND (only looks at first element)\nsingle_and &lt;- TRUE && FALSE # returns FALSE\n\n\n\n\n\n\n\nRStudio Workflow Shortcuts\n\n\n\nShortcuts bring order and boost creativity\n\nExecute Code Line: Cmd + Return (Mac) or Ctrl + Enter (Windows/Linux)\nInsert Section Heading: Cmd + Shift + R (Mac) or Ctrl + Shift + R (Windows/Linux)\nAlign Code: Cmd + Shift + A (Mac) or Ctrl + Shift + A (Windows/Linux)\nComment/Uncomment: Cmd/Ctrl + Shift + C\nSave All: Cmd/Ctrl + Shift + S\nFind/Replace: Cmd/Ctrl + F, Cmd/Ctrl + Shift + F\nNew File: Cmd/Ctrl + Shift + N\nAuto-complete: Tab\n\nFor more commands, explore the Command Palette available under Tools -&gt; Command Palette or Shift + Cmd + P (Mac) or Shift + Ctrl + P (Windows/Linux)."
  },
  {
    "objectID": "content/01-content.html#data-types-in-r",
    "href": "content/01-content.html#data-types-in-r",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Data Types in R",
    "text": "Data Types in R\nUnderstanding data types in R is essential. R supports several fundamental data types, including integers, characters, factors, and ordered factors. Each type has its specific use case and functions associated with it.\n\nIntegers\nIntegers are whole numbers without decimal points. In R, integers can be explicitly defined by adding an L suffix to the number.\n\n# define an integer\nx &lt;- 42L\n\nx\n\n[1] 42\n\n# check\nstr(x) # is integer\n\n int 42\n\n# convert to numeric\ny &lt;- as.numeric(x)\n\nstr(y)\n\n num 42\n\n\nIntegers are particularly useful when dealing with counts or indices that do not require fractional values.\n\n\nCharacters\nCharacter data types are used to represent text. In R, text strings are enclosed in quotes, either single (') or double (\").\n\n# define a character string\nname &lt;- \"Alice\"\n\nCharacters are essential for categorical data that does not fit into numerical categories, such as names, labels, and descriptions.\n\n\nFactors\nFactors are used to represent categorical data that can take on a limited number of values, known as levels. Factors are useful for statistical modeling as they explicitly define categories in the data.\n\n# Define a factor\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\"))\n\nFactors can improve efficiency and memory usage when dealing with categorical data, especially in large datasets. And they are useful when dealing with categorical variables (naturally).\n\nOrdered Factors\nOrdered factors are a special type of factor where the levels have an inherent order. They are defined similarly to factors but with an additional argument to denote the order.\n\n# ordered factors ---------------------------------------------------------\n# factors ordinary\neducation_levels &lt;- c(\"high school\", \"bachelor\", \"master\", \"ph.d.\")\neducation_factor_no_order &lt;- factor(education_levels, ordered = FALSE)\nstr(education_factor_no_order)\n\n Factor w/ 4 levels \"bachelor\",\"high school\",..: 2 1 3 4\n\n# factors with inherent order\neducation_factor &lt;- factor(education_levels, ordered = TRUE)\neducation_factor\n\n[1] high school bachelor    master      ph.d.      \nLevels: bachelor &lt; high school &lt; master &lt; ph.d.\n\n# another way to do the same\neducation_ordered_explicit &lt;- factor(education_levels, levels = education_levels, ordered = TRUE)\n\nOrdered factors allow for logical comparisons based on their order, which is particularly useful in analyses where the order of categories matters, such as ordinal regression.\n\n\nOperations with Ordered Factors\nOrdered factors support logical comparisons that consider the order of the levels.\n\n# comparison of ordered factors\nedu1 &lt;- ordered(\"bachelor\", levels = education_levels)\nedu2 &lt;- ordered(\"master\", levels = education_levels)\nedu2 &gt; edu1 # logical comparison\n\n[1] TRUE\n\n# modifying ordered factors\nnew_levels &lt;- c(\"primary school\", \"high school\", \"bachelor\", \"master\", \"ph.d.\")\neducation_updated &lt;- factor(education_levels, levels = new_levels, ordered = TRUE)\nstr(education_updated)\n\n Ord.factor w/ 5 levels \"primary school\"&lt;..: 2 3 4 5\n\n\n\n\nChecking Data with Ordered Factors\nYou can view the structure and summary of ordered factors just as with regular factors, but the output will indicate the order.\n\n# view the structure\nstr(education_ordered_explicit)\n\n Ord.factor w/ 4 levels \"high school\"&lt;..: 1 2 3 4\n\n# summary to see the distribution\nsummary(education_ordered_explicit)\n\nhigh school    bachelor      master       ph.d. \n          1           1           1           1 \n\n\n\n\nModifying Ordered Factors\nIf you need to change the order of levels or add new levels, you can re-factor the variable using factor() or ordered() and specify the new levels.\n\n# modifying ordered factors\nnew_levels &lt;- c(\"primary school\", \"high school\", \"bachelor\", \"master\", \"ph.d.\")\neducation_updated &lt;- factor(education_levels, levels = new_levels, ordered = TRUE)\nstr(education_updated)\n\n Ord.factor w/ 5 levels \"primary school\"&lt;..: 2 3 4 5\n\nstr(education_updated)\n\n Ord.factor w/ 5 levels \"primary school\"&lt;..: 2 3 4 5"
  },
  {
    "objectID": "content/01-content.html#strings",
    "href": "content/01-content.html#strings",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Strings",
    "text": "Strings\nStrings are sequences of characters.\n\n# sequences of characters\nyou &lt;- \"world!\"\ngreeting &lt;- paste(\"hello,\", you)\n# hello world\ngreeting\n\n[1] \"hello, world!\""
  },
  {
    "objectID": "content/01-content.html#vectors",
    "href": "content/01-content.html#vectors",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Vectors",
    "text": "Vectors\nVectors are one of R’s most fundamental data structures, essential for storing and manipulating a sequence of data elements.\nVectors are homogenous, meaning all elements in a vector must be of the same type (e.g., all numeric, all character, etc.).\nVectors in R can be created using the c() function, which stands for concatenate or combine:\n\n# numeric vector\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)\n\n# character vector\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"cherry\")\n\n# logical vector\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, FALSE)\n\n\nManipulating Vectors\nR allows you to perform operations on vectors in a very intuitive way. Operations are vectorised, meaning they are applied element-wise:\n\n# arithmetic operations\nvector_sum &lt;- numeric_vector + 10 # Adds 10 to each element\n# show\nvector_sum\n\n[1] 11 12 13 14 15\n\n# vector mutliplication\nvector_multiplication &lt;- numeric_vector * 2 # Multiplies each element by 2\n# show\nvector_multiplication\n\n[1]  2  4  6  8 10\n\n# logical operations\nvector_greater_than_three &lt;- numeric_vector &gt; 3 # Returns a logical vector\n\n# show\nvector_greater_than_three\n\n[1] FALSE FALSE FALSE  TRUE  TRUE\n\n\nYou can access elements of a vector by using square brackets [ ] with an index or a vector of indices:\n\n#  the first element of numeric_vector\nfirst_element &lt;- numeric_vector[1]\n\n# show\nfirst_element\n\n[1] 1\n\n# multiple elements\nsome_elements &lt;- numeric_vector[c(2, 4)] # Gets the 2nd and 4th elements\n\n# show\nfirst_element\n\n[1] 1\n\n\n\n\nFunctions with vectors\nR provides a rich set of functions for statistical computations and manipulations that work with vectors:\n\n# statistical summary\nvector_mean &lt;- mean(numeric_vector)\nvector_mean\n\n[1] 3\n\nvector_sum &lt;- sum(numeric_vector)\nvector_sum\n\n[1] 15\n\n# sorting\nsorted_vector &lt;- sort(numeric_vector)\nsorted_vector\n\n[1] 1 2 3 4 5\n\n# unique values\nunique_vector &lt;- unique(character_vector)\nunique_vector\n\n[1] \"apple\"  \"banana\" \"cherry\""
  },
  {
    "objectID": "content/01-content.html#data-frames",
    "href": "content/01-content.html#data-frames",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Data Frames",
    "text": "Data Frames\n\nCreate Data Frames\nData frames can be created using the data.frame() function, specifying each column and its values. Here’s a simple example:\n\n# clear any previous `df` object\nrm(df)\ndf &lt;- data.frame(\n    name = c(\"alice\", \"bob\", \"charlie\"),\n    age = c(25, 30, 35),\n    gender = c(\"female\", \"male\", \"male\")\n)\n# check structure\nhead(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\nstr(df)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"alice\" \"bob\" \"charlie\"\n $ age   : num  25 30 35\n $ gender: chr  \"female\" \"male\" \"male\"\n\ntable(df$gender)\n\n\nfemale   male \n     1      2 \n\ntable(df$age)\n\n\n25 30 35 \n 1  1  1 \n\ntable(df$name)\n\n\n  alice     bob charlie \n      1       1       1 \n\n\nIn this example, df is a data frame with three columns (name, age, gender) and three rows, each representing a different individual.\n\n\nAccess Data Frame Elements\nThere are often several ways to do the same thing in R. You can access the elements of a data frame in several ways:\n\nBy column name: use the $ operator followed by the column name.\n\n\nnames &lt;- df$name # extracts the `name` column\nnames\n\n[1] \"alice\"   \"bob\"     \"charlie\"\n\n\n\nBy row and column: Use the [row, column] indexing. Rows or columns can be specified by number or name.\n\n\n# access data frame elements\nnames &lt;- df$name\nnames\n\n[1] \"alice\"   \"bob\"     \"charlie\"\n\nsecond_person &lt;- df[2, ]\nsecond_person\n\n  name age gender\n2  bob  30   male\n\nage_column &lt;- df[, \"age\"]\nage_column\n\n[1] 25 30 35\n\n\n\nUsing subset() Function: To extract subsets of the data frame based on conditions.\n\n\n# use subset()\nvery_old_people &lt;- subset(df, age &gt; 25) # extracts rows where `age` is greater than 18\nvery_old_people\n\n     name age gender\n2     bob  30   male\n3 charlie  35   male\n\nsummary(very_old_people$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.00   31.25   32.50   32.50   33.75   35.00 \n\nmean(very_old_people$age)\n\n[1] 32.5\n\nmin(very_old_people$age)\n\n[1] 30\n\n\n\n\nExplore Your Data Frames\nFunctions such as head(), tail(), and str() help you explore the first few rows, last few rows, and the structure of the data frame, respectively.\n\nhead(df) # first six rows\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\ntail(df) # last six rows\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\nstr(df) # structure of the data frame\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"alice\" \"bob\" \"charlie\"\n $ age   : num  25 30 35\n $ gender: chr  \"female\" \"male\" \"male\"\n\n\n\n\nManipulating Data Frames\nData frames can be manipulated in various ways:\n\nAdding Columns: You can add new columns using the $ operator.\n\n\n# adds a new column \"employed\"\ndf$employed &lt;- c(TRUE, TRUE, FALSE)\n\n# show\nhead(df)\n\n     name age gender employed\n1   alice  25 female     TRUE\n2     bob  30   male     TRUE\n3 charlie  35   male    FALSE\n\n\n\nAdding rows: Use the rbind() function to add new rows.\n\n\nnew_person &lt;- data.frame(name = \"diana\", age = 28, gender = \"female\", employed = TRUE)\ndf &lt;- rbind(df, new_person)\n\n# show\nhead(df)\n\n     name age gender employed\n1   alice  25 female     TRUE\n2     bob  30   male     TRUE\n3 charlie  35   male    FALSE\n4   diana  28 female     TRUE\n\n\n\nModifying values: Access the element or column and assign it a new value.\n\n\n# note brackets\n# changes diana's age to 26\ndf[4, \"age\"] &lt;- 26\n\n# view row\ndf[4, ]\n\n   name age gender employed\n4 diana  26 female     TRUE\n\n\n\nRemoving columns or rows: set columns to NULL to remove them, or use - with row or column indices.\n\n\nhead(df)\n\n     name age gender employed\n1   alice  25 female     TRUE\n2     bob  30   male     TRUE\n3 charlie  35   male    FALSE\n4   diana  26 female     TRUE\n\n# remove employed column\ndf$employed &lt;- NULL\n\n# check\ndf\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n4   diana  26 female\n\n# remove fourth row (Diana)\ndf &lt;- df[-4, ] # removes the fourth row\n\n# show\ndf\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\n\n\n\nAdd Rows with rbind()\nThe rbind() function in R stands for “row bind”. It is used to combine data frames or matrices by rows. This function is particularly useful when you want to add new observations or records to an existing data frame.\n\n#  adding a new row\nnew_person &lt;- data.frame(name = \"eve\", age = 32, gender = \"female\")\ndf &lt;- rbind(df, new_person)\n\n# verify the row addition\nhead(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n4     eve  32 female\n\n\nWhen using rbind(), ensure that the columns in the data frames being combined match in both name and order. If they do not match, you may encounter errors or unexpected results.\n\n\nAdding Columns with cbind()\nConversely, the cbind() function in R stands for “column bind”. It is used to combine data frames or matrices by columns. This function allows you to add new variables to an existing data frame.\n\n# example of adding a new column\ndf$occupation &lt;- c(\"engineer\", \"doctor\", \"artist\", \"doctor\") # direct assignment\n\n# or using cbind for a separate vector\noccupation_vector &lt;- c(\"engineer\", \"doctor\", \"artist\", \"doctor\")\ndf &lt;- cbind(df, occupation_vector)\n\n# verify the column addition\nhead(df)\n\n     name age gender occupation occupation_vector\n1   alice  25 female   engineer          engineer\n2     bob  30   male     doctor            doctor\n3 charlie  35   male     artist            artist\n4     eve  32 female     doctor            doctor\n\n\nAs with rbind(), when using cbind(), it is crucial that the data frames or vectors being combined have compatible dimensions. For cbind(), the number of rows must match.\n\n\nConsiderations for rbind() and cbind()\nWhile rbind() and cbind() are straightforward and powerful functions for combining data, they have some limitations:\n\nMatching Column or Row Names: for rbind(), the column names between the data frames need to match exactly. For cbind(), the row numbers must be equal.\nFactor Levels: when binding factors with different levels, R will unify the levels, which can sometimes lead to unexpected results. Be mindful of factor levels when using these functions.\nPerformance: we will use a different approach next week (and following), draing on thedplyr package, which will connect more easily to our workflows.\n\nBy understanding and utilizing rbind() and cbind(), you can efficiently manipulate the structure of your data frames, adding flexibility to your data analysis workflows in R.\n\n\nView Data Structure (summary(), str(), head(), tail())\n\nsummary(): Provides a summary of an object’s structure.\nstr(): Displays the structure of an object.\nhead(): Shows the first few rows of a data frame or the first elements of a vector.\ntail(): Shows the last few rows of a data frame or the last elements of a vector.\n\n\n# iris is a preloaded dataset\nstr(iris) # displays structure of scores_df\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(iris) # summary statistics\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\nhead(iris) # first few rows\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntail(iris) # last few rows\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n\n\n\n\nmean()\n\nCalculates the arithmetic mean of a numerical object.\n\n\nset.seed(12345)\n\n# we will cover R’s powerful simulation functions like `rnorm()`next week\nvector &lt;- rnorm(n = 40, mean = 0, sd = 1)\nmean(vector) # note the sampling error here\n\n[1] 0.2401853\n\n\n\n\nsd()\n\nComputes the standard deviation, which measures the amount of variation or dispersion of a set of values.\n\n\nsd(vector) # replace 'vector' with your numerical vector\n\n[1] 1.038425\n\n\n\n\nmin() and max()\n\nThese functions return a numerical object’s minimum and maximum values, respectively.\n\n\nmin(vector) # minimum value\n\n[1] -1.817956\n\nmax(vector) # maximum value\n\n[1] 2.196834\n\n\n\n\ntable()\n\nGenerates a frequency table of an object, useful for categorical data. It counts the number of occurrences of each unique element.\n\n\n#  seed for reproducibility\nset.seed(12345)\nstudent_data &lt;- data.frame(\n    name = c(\"alice\", \"bob\", \"charlie\", \"diana\", \"ethan\", \"fiona\", \"george\", \"hannah\"),\n    score = sample(80:100, 8, replace = TRUE),\n    stringsasfactors = FALSE\n)\n\n# determine pass/fail\nstudent_data$passed &lt;- ifelse(student_data$score &gt;= 90, \"passed\", \"failed\")\n# convert 'passed' to factor\nstudent_data$passed &lt;- factor(student_data$passed, levels = c(\"failed\", \"passed\"))\n\n# simulate study hours\nstudent_data$study_hours &lt;- sample(5:15, 8, replace = TRUE)\n# table for categorical data analysis\ngender &lt;- sample(c(\"male\", \"female\"), size = 100, replace = TRUE, prob = c(0.5, 0.5))\neducation_level &lt;- sample(c(\"high school\", \"bachelor\", \"master\"), size = 100, replace = TRUE, prob = c(0.4, 0.4, 0.2))\ndf_table_example &lt;- data.frame(gender, education_level)\ntable(df_table_example)\n\n        education_level\ngender   bachelor high school master\n  female       14          18     11\n  male         23          20     14\n\n\n\n\nCross-Tabulation with table()\n\ntable() can also be used for cross-tabulation, providing a way to analyse the relationship between two or more factors.\n\n\ntable(df_table_example$gender, df_table_example$education_level) # crosstab\n\n        \n         bachelor high school master\n  female       14          18     11\n  male         23          20     14\n\n\nThis produces a contingency table showing the counts at each combination of factor1 and factor2 levels.\n\n\nSummary Statistics\nUse summary() to get a summary of each column.\n\n# show\nsummary(df)\n\n     name                age           gender           occupation       \n Length:4           Min.   :25.00   Length:4           Length:4          \n Class :character   1st Qu.:28.75   Class :character   Class :character  \n Mode  :character   Median :31.00   Mode  :character   Mode  :character  \n                    Mean   :30.50                                        \n                    3rd Qu.:32.75                                        \n                    Max.   :35.00                                        \n occupation_vector \n Length:4          \n Class :character  \n Mode  :character"
  },
  {
    "objectID": "content/01-content.html#first-data-visualisation-with-ggplot2",
    "href": "content/01-content.html#first-data-visualisation-with-ggplot2",
    "title": "Asking questions in cross-cultural psychology",
    "section": "First Data Visualisation with ggplot2",
    "text": "First Data Visualisation with ggplot2\nggplot2 is a powerful and flexible R package for creating elegant data visualisations. It is based on the Grammar of Graphics, allowing users to build plots layer by layer, making it versatile for creating a wide range of plots.\n\nInstalling and Loading ggplot2\nBefore using ggplot2, ensure the package is installed and loaded into your R session:\n\n# load ggplot2\nif (!require(ggplot2)) install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# seed for reproducibility\nset.seed(12345)\n\n# simulate student data (more on simulation next week)\nstudent_data &lt;- data.frame(\n    name = c(\"alice\", \"bob\", \"charlie\", \"diana\", \"ethan\", \"fiona\", \"george\", \"hannah\"),\n    score = sample(80:100, 8, replace = TRUE), # random scores between 80 and 100\n    stringsasfactors = FALSE\n)\n\n# determine pass/fail based on score\n# we will cover the ifelse() operator in detail in upcoming weeks\nstudent_data$passed &lt;- ifelse(student_data$score &gt;= 90, \"passed\", \"failed\")\n\n# convert 'passed' to factor for colour coding in ggplot2\nstudent_data$passed &lt;- factor(student_data$passed, levels = c(\"failed\", \"passed\"))\n\n# view the first few rows of the data frame\nhead(student_data)\n\n     name score stringsasfactors passed\n1   alice    93            FALSE passed\n2     bob    98            FALSE passed\n3 charlie    95            FALSE passed\n4   diana    90            FALSE passed\n5   ethan    81            FALSE failed\n6   fiona    90            FALSE passed\n\n# simulate study hours\nstudent_data$study_hours &lt;- sample(5:15, 8, replace = TRUE)\n\n\n\nBasic Components of a ggplot2 Plot\n\nData: the dataset you want to visualise.\nAesthetics (aes): defines how data are mapped to colour, size, shape, and other visual properties.\nGeometries (geom_ functions): the type of plot or layer you want to add (e.g., points, lines, bars).\n\n\nCreate a Basic Plot\nStart by creating a simple bar plot:\n\nggplot(student_data, aes(x = name, y = score)) +\n    geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nThis code plots score for each name in the student_data dataframe. The stat = \"identity\" argument tells ggplot2 to use the score values directly to determine the height of the bars.\n\n\nCustomising the plot\nTo enhance your plot, you can add titles, change axis labels, and modify colours:\n\nggplot(student_data, aes(x = name, y = score, fill = passed)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"true\" = \"blue\", \"FALSE\" = \"red\")) +\n    labs(title = \"student scores\", x = \"student name\", y = \"score\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\naes(fill = passed): maps the passed variable to the colour fill of the bars, allowing for colour differentiation based on whether students passed or failed.\nscale_fill_manual(): customizes the colours used for the true and FALSE values of the passed variable.\nlabs(): adds a main title and axis labels.\ntheme_minimal(): applies a minimalistic theme to the plot for a cleaner appearance.\n\n\n\n\nScatter Plot with ggplot2\nA scatter plot is useful for examining the relationship between two continuous variables.\nWe next simulate a scenario where we compare student scores against study hours.\n\n# create scatter plot\nggplot(student_data, aes(x = study_hours, y = score, color = passed)) +\n    geom_point(size = 4) +\n    labs(title = \"student scores vs. study hours\", x = \"study hours\", y = \"score\") +\n    theme_minimal() +\n    scale_color_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\nBox Plot with ggplot2\nBox plots are excellent for visualising the distribution of scores by pass/fail status, showing medians, quartiles, and potential outliers.\n\n# create box plot\nggplot(student_data, aes(x = passed, y = score, fill = passed)) +\n    geom_boxplot() +\n    labs(title = \"score distribution by pass/fail status\", x = \"status\", y = \"score\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\nHistogram with ggplot2\nHistograms are helpful for understanding the distribution of a single continuous variable, such as scores.\n\n# create a histogram\nggplot(student_data, aes(x = score, fill = passed)) +\n    geom_histogram(binwidth = 5, color = \"black\", alpha = 0.7) +\n    labs(title = \"histogram of scores\", x = \"score\", y = \"count\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\nLine Plot with ggplot2 (Time Series)\nFor demonstrating a line plot, we simulate monthly study hours over a semester for a student.\n\n# simulate monthly study hours\nmonths &lt;- factor(month.abb[1:8], levels = month.abb[1:8])\nstudy_hours &lt;- c(0, 3, 15, 30, 35, 120, 18, 15)\n\n# make data frame\nstudy_data &lt;- data.frame(month = months, study_hours = study_hours)\n\n# create a line plot\nggplot(study_data, aes(x = month, y = study_hours, group = 1)) +\n    geom_line(linewidth = 1, color = \"blue\") +\n    geom_point(color = \"red\", size = 1) +\n    labs(title = \"monthly study hours\", x = \"month\", y = \"study hours\") +\n    theme_minimal()"
  },
  {
    "objectID": "content/01-content.html#base-r-graphs",
    "href": "content/01-content.html#base-r-graphs",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Base R Graphs",
    "text": "Base R Graphs\nAlthough ggplot2 is renowned for its flexibility and aesthetic appeal, Base R graphics remain a staple for straightforward and quick visualisations.\nBase R provides a set of plotting functions that are readily available without the need for additional packages, and it is speedy.\n\nBasic Plotting Functions\n\nplot(): the workhorse of Base R for creating scatter plots, line graphs, and more, with extensive customisation options.\nhist(): generates histograms to explore the distribution of a single continuous variable.\nboxplot(): useful for comparing distributions across groups, showing medians, quartiles, and outliers.\nbarplot(): Creates bar graphs for visualising categorical data.\n\n\n\nCreating a Basic Scatter Plot with Base R\nConsider simple scatter plot using the data we have just simulated\n\n# basic scatter plot with Base R\nplot(student_data$study_hours, student_data$score,\n    main = \"Scatter Plot of Scores vs. Study Hours\",\n    xlab = \"Study Hours\", ylab = \"Score\",\n    pch = 19, col = ifelse(student_data$passed == \"passed\", \"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\nThis plot uses the plot function to create a scatter plot, with study hours on the x-axis and scores on the y-axis. The pch parameter specifies the symbol type, and col changes the colour based on whether the student passed or failed.\n\n\nGenerating a Histogram with Base R\nTo visualise the distribution of student scores:\n\n# histogram with Base R\nhist(student_data$score,\n    breaks = 5,\n    col = \"skyblue\",\n    main = \"Histogram of Student Scores\",\n    xlab = \"Scores\",\n    border = \"white\"\n)\n\n\n\n\n\n\n\n\nThis histogram provides a quick overview of the scores’ distribution, using the hist function with specified breaks, col for colour, and border for the colour of the histogram borders.\n\n\nGenerate Line Plot with Base R\n\n# must be numeric\nmonths_num &lt;- 1:length(study_data$month) # Simple numeric sequence\n\n# plot points with suppressed x-axis\nplot(months_num, study_data$study_hours,\n    type = \"p\", # Points\n    pch = 19, # Type of point\n    col = \"red\",\n    xlab = \"Month\",\n    ylab = \"Study Hours\",\n    main = \"Monthly Study Hours\",\n    xaxt = \"n\"\n) # Suppress the x-axis\n\n# add lines between points\nlines(months_num, study_data$study_hours,\n    col = \"blue\",\n    lwd = 1\n) # Line width\n\n# add custom month labels to the x-axis at appropriate positions\naxis(1, at = months_num, labels = study_data$month, las = 2) # `las=2` makes labels perpendicular to axis\n\n# Optional: adding a box around the plot for a minimalistic look\nbox()\n\n\n\n\n\n\n\n\n\n\nComparing Distributions with Box Plots\nBox plots in Base R can compare the score distributions across the pass/fail status:\n\n# Box plot with Base R\nboxplot(score ~ passed,\n    data = student_data,\n    main = \"Score Distribution by Pass/Fail Status\",\n    xlab = \"Status\", ylab = \"Scores\",\n    col = c(\"red\", \"blue\")\n)\n\n\n\n\n\n\n\n\nThis code uses the boxplot function to create box plots for scores, grouped by the pass/fail status, with custom colours for each group."
  },
  {
    "objectID": "content/01-content.html#summary-of-todays-lab",
    "href": "content/01-content.html#summary-of-todays-lab",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Summary of Today’s Lab",
    "text": "Summary of Today’s Lab\nCongratulations on completing Lab 1!\nThis session has laid the groundwork. We have covered a lot, but we’ll have a good deal of practice throughout the course to reinforce the learning.\n\nWhat We Have Learned\n\nHow to install and setup R:\n\nYou’ve successfully installed R and RStudio, setting up your workstation for statistical analysis.\n\nHow to install and use R-Studio:\n\nYou’ve familiarised yourself with the RStudio interface, including the console, source editor, environment tab, and other utilities for effective data analysis.\n\nBasic R operations:\n\nYou’ve practided using R for basic arithmetic operations, understanding how to execute simple commands in the console.\n\nBasic R Data Structures such as:\n\nVectors and Matrices: You have learned to create and manipulate vectors and matrices, the simplest forms of data storage in R, which are crucial for handling numeric, character, and logical data types in a structured manner.\nData Frames: You’ve been introduced to data frames, a key data structure in R for storing tabular data. Data frames accommodate columns of different data types, making them highly versatile for data analysis and manipulation.\nFactors and Ordered Factors: Understanding factors and ordered factors has provided you with the tools to handle categorical data effectively, including the ability to manage and analyse data involving categorical variables with both unordered and ordered levels.\n\nBasics of ggplot2:\n\nYou’ve been equipped with the fundamentals of data visualisation using ggplot2, including how to create basic plots like bar charts, scatter plots, and line graphs. You’ve learned about the importance of aesthetics (aes) and geometries (geom_ functions) in creating visually appealing and informative graphics.\n\nCustomizing Plots:\n\nTechniques for enhancing plots with titles, axis labels, and custom colour schemes have been covered. You’ve practised making your visualisations more informative and engaging by customising plot aesthetics.\n\n\nHow to Build Skills?\n\nPractical Application:\n\nDo the hands-on exercises at home. They’ll help you apply what you have learned here.\n\n\nWhere to Get Help\nAs sure as night follows day, you will need help coding. Good resources:\n\nLarge Language Models (LLMs): LLMs are trained on extensive datasets. They are extremely good coding tutors. Open AI’s GPT-4 considerably outperforms GPT-3.5. However GPT 3.5 should be good enough. Gemini has a two-month free trial. LLM’s are rapidly evolving. However, presently, to use these tools, and to spot their errors, you will need to know how to code. Which is fortunate because coding makes you smarter!\n\nNote: you will not be assessed for R-code. Help from LLM’s for coding does not consitute a breach of academic integrity in this course. Your tests are in-class; no LLM’s allowed. For your final report, you will need to cite all sources, and how you used them, including LLMs.\n\nStack Overflow:: an outstanding resource for most problems. Great community.\nCross-validated the best place to go for stats advice. (LLM’s are only safe for standard statistics. They do not perform well for causal inference.)\nDeveloper Websites and GitHub Pages: Tidyverse\nYour tutors and course coordinator. We care. We’re here to help you!\n\n\n\nRecommended Reading\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. O’Reilly Media. [Available online](https://r4ds.had.co.nz\nA helpful resource for learning R is Megan Hall’s lecture available at: https://meghan.rbind.io/talk/neair/.\nRStudio has compiled numerous accessible materials for learning R, which can be found here: https://education.rstudio.com/learn/beginner/.\nMaterials from a previous course on learning R can be accessed here. https://go-bayes.github.io/psych-447/\nJohannes Karl’s Video\n\n\n\nJohannas Karl on Getting Started In R\n\n\n\nPackages\n\nreport::cite_packages()\n\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Xie Y (2024). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.54, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/01-content.html#appendix-a-at-home-exercises",
    "href": "content/01-content.html#appendix-a-at-home-exercises",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix A: At Home Exercises",
    "text": "Appendix A: At Home Exercises\n\n\n\n\n\n\nExercise 1: Install the tidyverse package\n\n\n\nFollow these instructions to install the tidyverse package in RStudio:\n\nOpen RStudio: launch RStudio on your computer.\nAccess package installation:\n\nNavigate to the menu at the top of RStudio and click on Tools &gt; Install Packages.... This opens the Install Packages dialogue box.\n\nInstall tidyverse:\n\nIn the Install Packages dialogue box, you will see a field labelled “Packages (separate multiple with space or comma):”. Click in this field and type tidyverse.\nBelow the packages field, ensure the checkbox for Install dependencies is checked. This ensures all packages that tidyverse depends on are also installed.\n\nBegin installation:\n\nClick on the Install button to start the installation process.\n\n\nThe installation might take a few minutes. Monitor the progress in the “Console” pane. Once the installation is complete, you will see a message in the console indicating that the process has finished.\n\nLoad tidyverse: After successful installation, you can load the tidyverse package into your R session by typing library(tidyverse) in the console and pressing Enter.\n\n\n\n\n\n\n\n\n\nExercise 2: Install the parameters and report packages\n\n\n\nTo install the parameters and report packages in RStudio, follow these instructions:\n\nOpen RStudio: start by launching the RStudio application on your computer.\nAccess Package Installation:\n\nGo to the RStudio menu bar at the top of the screen and click on Tools &gt; Install Packages.... This action opens the Install Packages dialogue box.\n\nInstall parameters and report:\n\nIn the Install Packages dialogue box, locate the field labelled “Packages (separate multiple with space or comma):”. Click in this field and type parameters, report, separating the package names with a comma.\nMake sure the checkbox for Install dependencies is selected. This ensures that any additional packages needed by parameters and report are also installed.\nClick the Install button to initiate the installation of both packages and their dependencies.\n\n\n\n\n\n\n\n\n\n\nExercise 3: Basic Operations and Data Structure Manipulation\n\n\n\nObjective: Practice creating vectors and performing basic arithmetic operations.\n\nCreate two numeric vectors, vector_a and vector_b, with the following values:\n\nvector_a: 2, 4, 6, 8\nvector_b: 1, 3, 5, 7\n\nPerform the following operations and store the results in new variables:\n\nAdd vector_a and vector_b.\nSubtract vector_b from vector_a.\nMultiply vector_a by 2.\nDivide vector_b by 2.\n\nCalculate the mean and standard deviation of both vector_a and vector_b.\n\n\n\n\n\n\n\n\n\nExercise 4: Working with Data Frames\n\n\n\nObjective: Gain familiarity with data frame creation, manipulation, and basic data exploration functions.\n\nCreate a data frame student_data with the following columns:\n\nid: 1, 2, 3, 4\nname: alice, bob, charlie, diana\nscore: 88, 92, 85, 95\nEnsure you set stringsAsFactors = FALSE.\n\nAdd a new column passed to student_data indicating whether the student passed. Assume a pass mark of 90.\nExtract the name and score of students who passed into a new data frame.\nUse summary(), head(), and str() functions to explore student_data.\n\n\n\n\n\n\n\n\n\nExercise 5 Logical Operations and Subsetting\n\n\n\nObjective: Practice using logical operations to subset data frames.\n\nUsing the student_data data frame from Exercise 2, subset the data to find students who scored above the mean score of the class.\nCreate a vector attendance with values (present, absent, present, present) corresponding to each student’s attendance.\nAdd attendance as a new column to student_data and then subset the data frame to select only the rows where students were present.\n\n\n\n\n\n\n\n\n\nExercise 6: Cross-Tabulation and Analysis\n\n\n\nObjective: Understand the use of table() function for cross-tabulation and analysis.\n\nCreate two-factor variables:\n\nfruit: apple, banana, apple, orange, banana\ncolour: red, yellow, green, orange, green\n\nConvert fruit and colour into factors and then into a data frame named fruit_data.\nUse the table() function to perform a cross-tabulation of fruit by colour.\nInterpret the results. Which fruit has the most colour variety?\n\n\n\n\n\n\n\n\n\nExercise 7: Visualization with ggplot2\n\n\n\nObjective: (If ggplot2 was introduced) Create a simple plot to visualise the data.\n\nInstall and load the ggplot2 package if not already done.\nUsing student_data, create a bar plot showing the scores of students. Use name for the x-axis and score for the y-axis.\nEnhance the plot by adding a title, x and y-axis labels, and use different colours for passed and failed students.\n\nThese exercises are designed to be progressively challenging, ensuring that students apply what they’ve learned about basic operations, data frame manipulation, logical operations, and simple data analysis and visualisation in R."
  },
  {
    "objectID": "content/01-content.html#appendix-b-solutions",
    "href": "content/01-content.html#appendix-b-solutions",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix B: Solutions",
    "text": "Appendix B: Solutions\n\nSolutions 1 and 2\n\nExercises 1 and 2 have no solutions. Installation worked or it did not! If you have trouble, please see your tutor or instructor.\n\n\n\nSolution Exercise 3: Basic Operations and Data Structure Manipulation\n\n# e.g. create vectors\nvector_a &lt;- c(2, 4, 6, 8)\nvector_b &lt;- c(1, 3, 5, 7)\n\n# operations\nsum_vector &lt;- vector_a + vector_b\ndiff_vector &lt;- vector_a - vector_b\ndouble_vector_a &lt;- vector_a * 2\nhalf_vector_b &lt;- vector_b / 2\n\n# view\nsum_vector\n\n[1]  3  7 11 15\n\ndiff_vector\n\n[1] 1 1 1 1\n\ndouble_vector_a\n\n[1]  4  8 12 16\n\nhalf_vector_b\n\n[1] 0.5 1.5 2.5 3.5\n\n# Mean and Standard Deviation\nmean_a &lt;- mean(vector_a)\nsd_a &lt;- sd(vector_a)\nmean_b &lt;- mean(vector_b)\nsd_b &lt;- sd(vector_b)\n\n# view\nmean_a\n\n[1] 5\n\nsd_a\n\n[1] 2.581989\n\nmean_b\n\n[1] 4\n\nsd_b\n\n[1] 2.581989\n\n\n\n\nSolution 4: Working with Data Frames\n\n# create data frame\nstudent_data &lt;- data.frame(\n    id = 1:4,\n    name = c(\"alice\", \"bob\", \"charlie\", \"diana\"),\n    score = c(88, 92, 85, 95),\n    stringsAsFactors = FALSE\n)\n\n# add `passed` column\nstudent_data$passed &lt;- student_data$score &gt;= 90\n\n# subset students who passed\npassed_students &lt;- student_data[student_data$passed == TRUE, ]\n\n# explore data frame\nsummary(student_data)\n\n       id           name               score         passed       \n Min.   :1.00   Length:4           Min.   :85.00   Mode :logical  \n 1st Qu.:1.75   Class :character   1st Qu.:87.25   FALSE:2        \n Median :2.50   Mode  :character   Median :90.00   TRUE :2        \n Mean   :2.50                      Mean   :90.00                  \n 3rd Qu.:3.25                      3rd Qu.:92.75                  \n Max.   :4.00                      Max.   :95.00                  \n\nhead(student_data)\n\n  id    name score passed\n1  1   alice    88  FALSE\n2  2     bob    92   TRUE\n3  3 charlie    85  FALSE\n4  4   diana    95   TRUE\n\nstr(student_data)\n\n'data.frame':   4 obs. of  4 variables:\n $ id    : int  1 2 3 4\n $ name  : chr  \"alice\" \"bob\" \"charlie\" \"diana\"\n $ score : num  88 92 85 95\n $ passed: logi  FALSE TRUE FALSE TRUE\n\n\n\n\nSolution 5: Logical Operations and Subsetting\n\n# subset data based on score\nmean_score &lt;- mean(student_data$score)\nstudents_above_mean &lt;- student_data[student_data$Score &gt; mean_score, ]\n\n# add attendance and subset\nattendance &lt;- c(\"present\", \"absent\", \"present\", \"present\")\nstudent_data$Attendance &lt;- attendance\npresent_students &lt;- student_data[student_data$Attendance == \"present\", ]\n\n\n\nSolution 6: Cross-Tabulation and Analysis\n\n# create factor variables\nfruit &lt;- factor(c(\"apple\", \"banana\", \"apple\", \"orange\", \"banana\"))\ncolour &lt;- factor(c(\"red\", \"yellow\", \"green\", \"orange\", \"green\"))\n\n# create data frame\nfruit_data &lt;- data.frame(fruit, colour)\n\n# cross-tabulation\nfruit_color_table &lt;- table(fruit_data$fruit, fruit_data$colour)\nprint(fruit_color_table)\n\n        \n         green orange red yellow\n  apple      1      0   1      0\n  banana     1      0   0      1\n  orange     0      1   0      0\n\n# interpretation: Apple has the most colour variety with 2 colours (Red, Green).\n\n\n\nSolution 7: Visualization with ggplot2\n\n# install and load ggplot2\nif (!require(ggplot2)) install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# create bar plot\nggplot(student_data, aes(x = name, y = score, fill = passed)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\")) +\n    labs(title = \"Student Scores\", x = \"Name\", y = \"Score\") +\n    theme_minimal()"
  },
  {
    "objectID": "content/01-content.html#appendix-b-other-data-types-you-may-encounter",
    "href": "content/01-content.html#appendix-b-other-data-types-you-may-encounter",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix B: Other Data Types You May Encounter",
    "text": "Appendix B: Other Data Types You May Encounter\n\nArrays and Matrices\nArrays are multi-dimensional data structures, while matrices are two-dimensional.\n\nmatrix_1 &lt;- matrix(1:9, nrow = 3) # creates a 3x3 matrix\narray_1 &lt;- array(1:12, dim = c(2, 3, 2)) # creates a 2x3x2 array\n\n\n\nConvert Matrix to Data Frame\nA data.frame is used for storing tabular data.\n\n# change matrix to array:\ndf_matrix_1 &lt;- data.frame(matrix_1)\n\nstr(df_matrix_1)\n\n'data.frame':   3 obs. of  3 variables:\n $ X1: int  1 2 3\n $ X2: int  4 5 6\n $ X3: int  7 8 9\n\nhead(df_matrix_1)\n\n  X1 X2 X3\n1  1  4  7\n2  2  5  8\n3  3  6  9\n\n# change colnames\nnew_colnames &lt;- c(\"col_1\", \"col_2\", \"col_3\")\n\ncolnames(df_matrix_1) &lt;- new_colnames\n\n# check\nstr(df_matrix_1)\n\n'data.frame':   3 obs. of  3 variables:\n $ col_1: int  1 2 3\n $ col_2: int  4 5 6\n $ col_3: int  7 8 9\n\nhead(df_matrix_1)\n\n  col_1 col_2 col_3\n1     1     4     7\n2     2     5     8\n3     3     6     9\n\n\n\n\nWorking with Lists in R\n\nCreating lists\nTo create a list, you use the list() function.\n\n# Creating a simple list\nmy_list &lt;- list(name = \"John Doe\", age = 30, scores = c(90, 80, 70))\n\n# A list containing various types of elements, including another list\ncomplex_list &lt;- list(id = 1, name = \"Jane Doe\", preferences = list(color = \"blue\", hobby = \"reading\"))\n\n\n\nAccessing list elements\nList elements can be accessed using the [[ ]] notation for single elements, or the $ notation if you’re accessing named elements:\n\n# Accessing elements\nname &lt;- my_list$name # or my_list[[\"name\"]]\n\npreference_color &lt;- complex_list$preferences$color\n\n\n\nModifying lists\nLists can be modified by adding new elements, changing existing elements, or removing elements:\n\n# Adding a new element\nmy_list$gender &lt;- \"Male\"\n\n# Changing an existing element\nmy_list$age &lt;- 31\n\n# Removing an element\nmy_list$scores &lt;- NULL\n\n\n\nLists in Functions\nLists are often used as return values for functions that need to provide multiple pieces of data:\n\n# Function returning a list\ncalculate_stats &lt;- function(numbers) {\n    mean_val &lt;- mean(numbers)\n    sum_val &lt;- sum(numbers)\n    return(list(mean = mean_val, sum = sum_val))\n}\n\n# Using the function\nresults &lt;- calculate_stats(c(1, 2, 3, 4, 5))"
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "",
    "text": "Note\n\n\n\nRequired - (Hernan and Robins 2020) Chapter 6 link\nOptional\n\n(Suzuki, Shinozaki, and Yamamoto 2020) link\n(J. A. Bulbulia 2024) link\n(Neal 2020) Chapter 3 link"
  },
  {
    "objectID": "content/03-content.html#learning-outcomes",
    "href": "content/03-content.html#learning-outcomes",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou will learn how to use causal diagrams to evaluate the “no unmeasured confounding” assumption of causal inference.\nYou will understand how time-series data-collection may address common confounding problems.\nYou will understand why time-series data-collection are insufficient for addressing other common confounding problems."
  },
  {
    "objectID": "content/03-content.html#confounding-problems-resolved-by-time-series-data",
    "href": "content/03-content.html#confounding-problems-resolved-by-time-series-data",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Confounding problems resolved by time-series data",
    "text": "Confounding problems resolved by time-series data\nFigure 3 presents the structural features of seven confounding problems. We shall discuss examples of each, and how longitudinal data collection resolves each problem.\n\n\n\n\n\n\nFigure 1: This figure is adapted from (J. A. Bulbulia 2024)\n\nBulbulia, J. A. 2024. “Methods in Causal Inference Part 1: Causal Diagrams and Confounding.” Evolutionary Human Sciences 6: e40. https://doi.org/10.1017/ehs.2024.35."
  },
  {
    "objectID": "content/03-content.html#confounding-problems-not-resolved-by-time-series-data-alone",
    "href": "content/03-content.html#confounding-problems-not-resolved-by-time-series-data-alone",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Confounding problems not resolved by time-series data alone",
    "text": "Confounding problems not resolved by time-series data alone\nFigure 2 presents six examples of time-series data that are not resolved by longitudinal data collection. Before seminar, consider why time series data are insufficient to address confounding in each of the six scenarios described in this figure.\n\n\n\n\n\n\nFigure 2: This figure is adapted from (J. A. Bulbulia 2024)\n\nBulbulia, J. A. 2024. “Methods in Causal Inference Part 1: Causal Diagrams and Confounding.” Evolutionary Human Sciences 6: e40. https://doi.org/10.1017/ehs.2024.35."
  },
  {
    "objectID": "content/03-content.html#worked-example-the-assumptions-in-causal-mediation",
    "href": "content/03-content.html#worked-example-the-assumptions-in-causal-mediation",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Worked Example: The Assumptions in Causal Mediation",
    "text": "Worked Example: The Assumptions in Causal Mediation\n\n\n\n\n\n\nFigure 3: This figure is adapted from (J. A. Bulbulia 2024)\n\nBulbulia, J. A. 2024. “Methods in Causal Inference Part 1: Causal Diagrams and Confounding.” Evolutionary Human Sciences 6: e40. https://doi.org/10.1017/ehs.2024.35."
  },
  {
    "objectID": "content/03-content.html#learning-outcomes-1",
    "href": "content/03-content.html#learning-outcomes-1",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy learning regression, you will be better equipped to do psychological science and to evaluate psychological research."
  },
  {
    "objectID": "content/03-content.html#what-is-regression",
    "href": "content/03-content.html#what-is-regression",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "What is Regression?",
    "text": "What is Regression?\nBroadly speaking, a regression model is a method for inferring the expected average features of a population and its variance conditional on other features of the population as measured in a sample.\nWe’ll see that regression encompasses more than this definition; however, this definition makes a start.\nTo understand regression, then, we need to understand the following jargon words: population, sample, measurement, and inference.\n\nWhat is a population?\nIn science, a population is a hypothetical construct. It is the set of all potential members of a set of things. In psychological science, that set is typically a collection of individuals. We want to understand “The population of all human beings?” or “The New Zealand adult population”; or “The population of undergraduates who may be recruited for IPRP in New Zealand.”\n\n\nWhat is a sample?\nA sample is a randomly realised sub-population from the larger abstract population that a scientific community hopes to generalise about.\nThink of selecting balls randomly from an urn. When pulled at random, the balls may inform us about the urn’s contents. For example, if we select one white ball and one black ball, we may infer that the balls in the urn are not all white or all black.\n\n\nWhat is “measurement”?\nA measure is a tool or method for obtaining numerical descriptions of a sample. We often call measures “scales.”\nA measurement is the numerical description we obtain from sensors such as statistical surveys, census data, twitter feeds, & etc.\nIn the course, we have encountered numerical scales, ordinal scales, and factors. The topic of measurement in psychology is very broad. As we shall see, the trail of the serpent of measurement runs across comparative psychological research.\nIt is essential to remember that measures can be prone to error.\nError-prone scales may nevertheless be helpful. However, we need to investigate their utility against the backdrop of specific interests and purposes.\n\n\nWhat is a parameter?\nIn regression, we combine measurements on samples with probability theory to guess about the properties of a population we will never observe. We call these properties “parameters.”\n\n\nWhat is statistical inference?\nThe bulk of statistical inference consists of educated guessing about population parameters.\n\n\nProbability distributions and statistical guessing\nInference is possible because the parameters of naturally occurring populations are structured by data-generating processes that are approximated by probability distributions. A probability distribution is a mathematical function describing a random event’s probability. Today we will be focusing on height.1\n1 The relationship of probability distributions and data-generating processes is complex, intriguing, and both historically and philosophically rich \\dots. Because our interests are applied, we will hardly touch up this richness in this course.Today we will discuss the “normal” or “Gaussian distribution.” A large number of data-generating processes in nature conform the normal distribution.\nLet’s consider some examples of randomly generated samples, which we will obtain using R’s rnorm function.\n\n\n10-person sample of heights\n\n# seed\nset.seed(123)\n\n# generate 10 samples, average 170, sd = 20\ndraws_10 &lt;- rnorm(10, mean = 170, sd = 20)\n\n# ggplot quick-histogram\nggplot2::qplot(draws_10, binwidth = 2)\n\n\n\n\n\n\n\n\n\n\n100-person sample of heights\n\n# reproducibility \nset.seed(123)\n\n# generate 100 samples, average 170, sd = 20\ndraws_100 &lt;-rnorm(100, mean = 170, sd = 20)\n\n# graph\nggplot2::qplot(\n  draws_100, binwidth = 2\n  )\n\n\n\n\n\n\n\n\n\n\n10000-person sample of heights\n\n# reproducibility\nset.seed(123)\n\n# N = 10,000\ndraws_10000 &lt;- rnorm(1e5, mean = 170, sd = 20)\n\n# plot\nggplot2::qplot(draws_10000, binwidth = 2)\n\n\n\n\n\n\n\n\n\n\nHow can I use regression to infer a population parameter?\nWe can use R to investigate the average height of our imaginary population from which the preceding samples were randomly drawn. We do this in R by writing an “intercept-only” model as follows:\n\n# syntax for an intercept-only model\nmodel &lt;- lm(outcome ~ 1, data = data)\n\n# base R summary\nsummary(model)\n\nUsing the previous simulations:\nN = 10 random draws\n\n#|code-fold: false\n\n#write the model and get a nice table for it\nsjPlot::tab_model(lm(draws_10 ~ 1))\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n171.49\n157.85 – 185.14\n&lt;0.001\n\n\nObservations\n10\n\n\nR2 / R2 adjusted\n0.000 / 0.000\n\n\n\n\n\n\n\nN = 100 random draws\n\nsjPlot::tab_model(lm(draws_100 ~ 1))\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n171.81\n168.19 – 175.43\n&lt;0.001\n\n\nObservations\n100\n\n\nR2 / R2 adjusted\n0.000 / 0.000\n\n\n\n\n\n\n\nN = 10,000 random draws\n\nsjPlot::tab_model(lm(draws_10000 ~ 1))\n\n\n\n\n \nDependent variable\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n170.02\n169.90 – 170.14\n&lt;0.001\n\n\nObservations\n100000\n\n\nR2 / R2 adjusted\n0.000 / 0.000\n\n\n\n\n\n\n\nWhat do we notice about the relationship between sample size the estimated population average?\n\nsjPlot::tab_model(lm(draws_10 ~ 1),\n                  lm(draws_100 ~ 1),\n                  lm(draws_10000 ~ 1))\n\n\n\n\n \nDependent variable\nDependent variable\nDependent variable\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n171.49\n157.85 – 185.14\n&lt;0.001\n171.81\n168.19 – 175.43\n&lt;0.001\n170.02\n169.90 – 170.14\n&lt;0.001\n\n\nObservations\n10\n100\n100000\n\n\nR2 / R2 adjusted\n0.000 / 0.000\n0.000 / 0.000\n0.000 / 0.000\n\n\n\n\n\n\n\n\n\nRegression with a single co-variate\nDo mothers’s heights predict daughter height? If so, what is the magnitude of the relationship?\nFrancis Galton is credited with inventing regression analysis. Galton observed that offspring’s heights tend to fall between parental height and the population average, which Galton termed “regression to the mean.” Galton sought a method for educated guessing about heights, and this led to fitting a line of regression by a method called “least squares” (For a history, see: here).\nThe following dataset is from “The heredity of height” by Karl Pearson and Alice Lee (1903)(Pearson and Lee 1903). I obtained it from (Gelman, Hill, and Vehtari 2020). Let’s use this dataset to investigate the relationship between mothers’ and daughters’ heights.\n\nPearson, Karl, and Alice Lee. 1903. “On the Laws of Inheritance in Man: I. Inheritance of Physical Characters.” Biometrika 2 (4): 357–462.\n\n# import data\ndf_pearson_lee &lt;-\n  data.frame(read.table(\n    url(\n      \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt\"\n    ),\n    header = TRUE\n  ))\n\n# save\n# saveRDS(df_pearson_lee, here::here(\"data\", \"df_pearson_lee\"))\n\n# Center mother's height for later example\ndf_pearson_lee_centered &lt;- df_pearson_lee |&gt;\n  dplyr::mutate(mother_height_c = as.numeric(scale(\n    mother_height, center = TRUE, scale = FALSE\n  )))\n\nskimr::skim(df_pearson_lee_centered)\n\n\n\n\n\nName\ndf_pearson_lee_centered\n\n\nNumber of rows\n5524\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndaughter_height\n0\n1\n63.86\n2.62\n52.5\n62.5\n63.5\n65.5\n73.5\n▁▂▇▅▁\n\n\nmother_height\n0\n1\n62.50\n2.41\n52.5\n60.5\n62.5\n64.5\n70.5\n▁▂▇▇▁\n\n\nmother_height_c\n0\n1\n0.00\n2.41\n-10.0\n-2.0\n0.0\n2.0\n8.0\n▁▂▇▇▁\n\n\n\n\n\nPearson and Lee collected 5,524 observations from mother/daughter height pairs. Let’s examine the data, first by plotting the relationship.\nWhat is happening here?\n\n# explore pearson lee data\nexplore_md &lt;-\n  ggplot2::ggplot(data = df_pearson_lee_centered, aes(y = daughter_height, x = mother_height)) +\n  geom_jitter(alpha = .2) +\n  labs(title = \"The relationship between mothers height and daughter's height\") +\n  ylab(\"Daughter's height\") +\n  xlab(\"Mother's height\") + theme_classic()\n\n# print\nprint( explore_md )\n\n\n\n\n\n\n\n\nIs there a linear predictive relationship between these two parameters? In regression we examine the line of best fit.\n\n# regression \nm1 &lt;-\n  lm(daughter_height ~ mother_height, data = df_pearson_lee_centered)\n\n# graph\nsjPlot::tab_model(m1)\n\n\n\n\n \ndaughter height\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n29.80\n28.25 – 31.35\n&lt;0.001\n\n\nmother height\n0.54\n0.52 – 0.57\n&lt;0.001\n\n\nObservations\n5524\n\n\nR2 / R2 adjusted\n0.252 / 0.252\n\n\n\n\n\n\n\nWe can plot the coefficient, but in a model with one predictor, this isn’t very informative. However, as we continue in the course, we’ll see that plotting coefficients can be easier than deciphering the numbers in tables. Here are two methods for plotting.\n\n# get model parameters\nt_m1&lt;-parameters::model_parameters(m1,  \n                                   ci = 0.95)\n# plot \nplot(t_m1) +\n  labs(title = \"The relationship between mothers height and daughter's height\") + \n  ylab(\"Daughter's height\")"
  },
  {
    "objectID": "content/03-content.html#how-do-we-interpret-the-regression-model",
    "href": "content/03-content.html#how-do-we-interpret-the-regression-model",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "How do we interpret the regression model?",
    "text": "How do we interpret the regression model?\nLet’s write the equation out in mathematics. How do we read this? [^2] [^2]: Later, we’ll prefer a different way of writing regression equations in math. (Note: writing math isn’t math - it’s just encoding the model that we’ve written).\n\nlibrary(\"equatiomatic\")\n# extract equation\n#extract_eq(m1, use_coefs = FALSE)\n\n\n\\operatorname{daughter\\_height} = \\alpha + \\beta_{1}(\\operatorname{mother\\_height}) + \\epsilon\n\nThe math says that the expected daughter’s height in a population is predicted by the average height of the population when mothers’ heights are set to zero units (note, this is impossible - we’ll come back to this) plus \\beta ~\\times units of daughter’s height (inches) for each additional unit of mother’s height (inches)\nWe can plug the output of the model directly into the equation as follows:\n\n# extract equation\n#extract_eq(m1, use_coefs = TRUE)\n\n\n\\operatorname{\\widehat{daughter\\_height}} = 29.8 + 0.54(\\operatorname{mother\\_height})\n\n\nGraph the relationship between mother’s and daughter’s heights\n\nlibrary(ggeffects)\npredictions &lt;- ggeffects::ggpredict(m1, terms = \"mother_height\",    \n    add.data = TRUE,\n    dot.alpha = .1,\n    jitter = TRUE)\n\n\nplot_predictions &lt;-\n  plot(predictions) +   theme_classic() + labs(title = \"Predicted values of daughter's height from the Pearson/Fox 1903 dataset\")\nplot_predictions\n\n\n\n\n\n\n\n\n\n\nRegression to predict beyond the range of a dataset\nJoyte Amge is the world’s shortest woman at 25 inches. Sandy Allen was the world’s tallest woman at 91 inches. What are the expected heights of their daughter and of every intermediary woman in between?\n\n# use the `expand.grid` command to create a sequence of points for mother's height\ndf_expand_grid &lt;- expand.grid(mother_height = c(25:91))\n\n# use the `predict` function to create a new response\ndf_predict &lt;-\n  predict(m1,\n          type = \"response\",\n          interval = \"confidence\",\n          newdata = df_expand_grid)\n\n# have a look at the object\n#dplyr::glimpse(df_predict)\n\n# create a new dataframe for the new sequence of points for mother's height and the predicted data\nnewdata &lt;- data.frame(df_expand_grid, df_predict)\nhead(newdata)\n\n  mother_height      fit      lwr      upr\n1            25 43.42183 42.49099 44.35266\n2            26 43.96676 43.06065 44.87288\n3            27 44.51170 43.63030 45.39310\n4            28 45.05664 44.19995 45.91332\n5            29 45.60157 44.76960 46.43355\n6            30 46.14651 45.33924 46.95378\n\n\nGraph the predicted results\n\n# graph the expected results\npredplot &lt;- ggplot(data = newdata,\n                   aes(x = mother_height, y = fit))  +\n  geom_point() +  geom_errorbar(aes(ymin = lwr, ymax = upr), width = .1) +\n  expand_limits(x = c(20, 91), y = c(0, 81))  + theme_classic() +\n  labs(title = \"Predicted values for a broader population\")\n\n# plot the two graphs together (making the x and y axis at the same scale \nlibrary(\"patchwork\")\n# rescale heightplot\n\n# old plot with the new axis and y-axis scales, and remove points\n\nplot_height &lt;- plot(predplot, add.data = FALSE) +   theme_classic()\n\n\n\n\n\n\n\nplot_height_title &lt;-\n  plot_height +  expand_limits(x = c(20, 91), y = c(0, 81)) +  labs(title = \"Predicted values of daughter's height from the Pearson/Fox 1903 dataset\")\n\n# double graph\nplot_height_title / predplot  + plot_annotation(title = \"What do you notice about these relationships?\", tag_levels = \"a\")\n\n\n\n\n\n\n\n\nA simple method for obtaining the predicted values from your fitted model is to obtain the effects output without producing a graph.\n\n# prediction plots\nlibrary(ggeffects)\n# predicted values of mother height on daughter height\nggeffects::ggpredict(m1, terms = \"mother_height\")\n\n# Predicted values of daughter_height\n\nmother_height | Predicted |       95% CI\n----------------------------------------\n        52.50 |     58.41 | 58.15, 58.66\n        54.50 |     59.50 | 59.29, 59.70\n        57.50 |     61.13 | 60.99, 61.27\n        59.50 |     62.22 | 62.13, 62.32\n        61.50 |     63.31 | 63.25, 63.38\n        63.50 |     64.40 | 64.34, 64.47\n        65.50 |     65.49 | 65.40, 65.59\n        70.50 |     68.22 | 68.01, 68.42\n\n\n\n\nNon-linear relationships\nLinear regression assumes linearity conditional on a model. Often your data will not be linear!\nConsider the following example:\n\n# simulate nonlinear relationship between x and y\nb &lt;- c(2, 0.75)\nset.seed(12)\nx &lt;- rnorm(100)\nset.seed(12)\ny &lt;- rnorm(100, mean = b[1] * exp(b[2] * x))\ndat1 &lt;- data.frame(x, y)\n\not1 &lt;- lm(y ~ x, data  = dat1)\n# performance::check_model(ot1)\n\n# plot linear effect\nplot(ggeffects::ggpredict(ot1, terms = \"x\",\n     add.data = TRUE,\n     dot.alpha = .4))\n\n\n\n\n\n\n\n\nNon-linear relationship as modelled by a polynomial regression:\n\n# model: quadratic\nfit_non_linear &lt;- lm(y ~ x + I(x ^ 2), data  = dat1)\n\n# predictive plot\nplot(ggeffects::ggpredict(fit_non_linear, terms = \"x\",\n     add.data = TRUE,\n     dot.alpha = .4))\n\n\n\n\n\n\n\n\nHere is another approach:\n\n# non-linear regression \nlibrary(splines)\n\n# fit model \nfit_non_linear_b &lt;- lm(y ~ x + poly(x, 2), data  = dat1)\n\n# graph model\nplot(\n  ggeffects::ggpredict(fit_non_linear_b, terms = \"x\",\n  add.data = TRUE,\n  dot.alpha = .4\n))\n\n\n\n\n\n\n\n\nNon-linear relationship as modelled by a general additive model (spline)\n\n# fit spline: not specified\nfit_non_linear_c &lt;-lm(y ~ bs(x), data  = dat1)\n\n# model parameters: coefficients are not interpretable\nparameters::model_parameters(\n  fit_non_linear_c\n)\n\nParameter      | Coefficient |   SE |         95% CI |  t(96) |      p\n----------------------------------------------------------------------\n(Intercept)    |       -1.90 | 0.03 | [-1.95, -1.84] | -70.74 | &lt; .001\nx [1st degree] |        2.60 | 0.06 | [ 2.48,  2.72] |  43.47 | &lt; .001\nx [2nd degree] |        3.00 | 0.03 | [ 2.94,  3.06] |  96.12 | &lt; .001\nx [3rd degree] |       13.33 | 0.04 | [13.26, 13.40] | 368.37 | &lt; .001\n\n#performance::check_model(ot2)\nplot(\n  ggeffects::ggpredict(fit_non_linear_c, terms = \"x\",\n  add.data = TRUE,\n  dot.alpha = .4\n))\n\n\n\n\n\n\n\n\n\n\nCentering\nAny linear transformation of a predictor is OK. Often we centre (or centre and scale) all indicators, which gives us an interpretable intercept (the expected population average when the other indicators are set their average).\n\nlibrary(ggeffects)\n\n# fit raw data \nfit_raw &lt;- lm(daughter_height ~ mother_height, data = df_pearson_lee_centered)\n\n# fit centred data\nfit_centered &lt;-\n  lm(daughter_height ~ mother_height_c, data = df_pearson_lee_centered)\n\n# compare the models\nsjPlot::tab_model(fit_raw, fit_centered)\n\n\n\n\n \ndaughter height\ndaughter height\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n29.80\n28.25 – 31.35\n&lt;0.001\n63.86\n63.80 – 63.92\n&lt;0.001\n\n\nmother height\n0.54\n0.52 – 0.57\n&lt;0.001\n\n\n\n\n\nmother height c\n\n\n\n0.54\n0.52 – 0.57\n&lt;0.001\n\n\nObservations\n5524\n5524\n\n\nR2 / R2 adjusted\n0.252 / 0.252\n0.252 / 0.252\n\n\n\n\n\n\n\nGraph model\n\n# graph centred model\nplot(\n  ggeffects::ggpredict(fit_centered, terms = \"mother_height_c\",\n  add.data = TRUE,\n  dot.alpha = .4\n))\n\n\n\n\n\n\n\n\nNote: when fitting a polynomial or any interaction, it is important to center your indicators. We’ll come back to this point in later lectures.\n\n\nModel evaluation\nReviewers will sometime ask you to assess model fit.\nA simple, but flawed way to assess accuracy of your model fit is to compare a model with one covariate with a simple intercept-only model and to assess improvement in either the AIC statistic or the BIC statistic. The BIC is similar to the AIC but adds a penalty for extra predictors. An absolute improvement in either statistic of n &gt; 10 is considered to be a “better” model.\nWe can use the performance package to generate a table that compares model fits.\n\n# load library \nlibrary(performance)\n# intercept only\nfig_intercept_only &lt;- lm(daughter_height ~ 1, data = df_pearson_lee)\n\n# covariate added\nfig_covariate &lt;- lm(daughter_height ~ mother_height, data = df_pearson_lee)\n\n# evaluate\nperformance::compare_performance(fig_intercept_only, fig_covariate)\n\n# Comparison of Model Performance Indices\n\nName               | Model |   AIC (weights) |  AICc (weights)\n--------------------------------------------------------------\nfig_intercept_only |    lm | 26300.0 (&lt;.001) | 26300.0 (&lt;.001)\nfig_covariate      |    lm | 24698.5 (&gt;.999) | 24698.5 (&gt;.999)\n\nName               |   BIC (weights) |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------------\nfig_intercept_only | 26313.2 (&lt;.001) | 0.000 |     0.000 | 2.615 | 2.615\nfig_covariate      | 24718.4 (&gt;.999) | 0.252 |     0.252 | 2.262 | 2.262\n\n\nWhat was the model “improvement?”\n\n# improved fit\nBIC(fig_intercept_only) - BIC(fig_covariate)\n\n[1] 1594.839\n\n\n\n\nGenerate a report\nThis is easy with the report package\nFor example:\n\nreport::report_statistics(fig_covariate)\n\nbeta = 29.80, 95% CI [28.25, 31.35], t(5522) = 37.70, p &lt; .001; Std. beta = 7.21e-15, 95% CI [-0.02, 0.02]\nbeta = 0.54, 95% CI [0.52, 0.57], t(5522) = 43.12, p &lt; .001; Std. beta = 0.50, 95% CI [0.48, 0.52]\n\n\nOr, if you want a longer report:\n\nreport::report(fig_covariate)\n\nUse statistically significant in place of significant. This will avoid misleading your audience into thinking your result is important when what you intend to communicate is that it is reliable."
  },
  {
    "objectID": "content/03-content.html#assumptions-of-regression",
    "href": "content/03-content.html#assumptions-of-regression",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Assumptions of Regression",
    "text": "Assumptions of Regression\nFrom Gelman and Hill (Gelman and Hill 2006)\n\nValidity\n\nThe most important is that the data you are analyzing should map to the research question you are trying to answer (Gelman, Hill, and Vehtari 2020, 152)\n\nRepresentativeness\n\nA regression model is fit to data and is used to make inferences about a larger population, hence the implicit assumption in interpreting regression coefficients is that the sample is representative of the population. (Gelman, Hill, and Vehtari 2020, 153)\n\n\nLinearity\n\nThe most important mathematical assumption of the linear regression model is that its deterministic component is a linear function of the separate predictors: y = β0 + β1x1 + β2x2 +···. If additivity is violated, it might make sense to transform the data (for example, if y = abc, then log y = log a + log b + log c) or to add interactions. If linearity is violated, perhaps a predictor should be put in as 1/x or log(x) instead of simply linearly. Or a more complicated relationship could be expressed using a nonlinear function such as a spline or Gaussian process, (Gelman, Hill, and Vehtari 2020, 153)\n\nIndependence of errors\n\nThe simple regression model assumes that the errors from the prediction line are independent, an assumption that is violated in time series, spatial, and multilevel settings (Gelman, Hill, and Vehtari 2020, 153)\n\nEqual variance of errors\n\n\n…unequal variance does not affect the most important aspect of a regression model, which is the form of the predictors (Gelman, Hill, and Vehtari 2020, 153)\n\n\nNormality of errors (statistical independence)\n\n\nThe regression assumption that is generally least important is that the errors are normally distributed. In fact, for the purpose of estimating the regression line (as compared to predicting individual data points), the assumption of normality is barely important at all. Thus, in contrast to many regression textbooks, we do not recommend diagnostics of the normality of re-gression residuals. (Gelman and Hill 2006, 46)\n\n\nA good way to diagnose violations of some of the assumptions just considered (importantly, linearity) is to plot the residuals versus fitted values or simply individual predictors.(Gelman and Hill 2006, 46)\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge university press.\n\n\nCommon Confusions\n\nRegressions do not automatically give us “effects”\nPeople use the work “effect”, but that is not what regression gives us, by default.\n\n\n“Normality Assumption”\nGelman and Hill note that the “normality” assumption is the least important. The assumption pertains to the normality of residuals.\n\n\nStatistical Independence\nThis is the motivation for doing multi-level modelling: to condition on dependencies in the data. However, multi-level modelling can produce new problems if the error terms in the model are not independent of the outcome.\n\n\nExternal Validity (wrong population)\nWe sample from undergraduates, but infer about the human population.\n\n\n\n\n\n\nImportant\n\n\n\nNOTE: The concept of “better model fit” is relative to our interests and purposes. A better (lower) AIC statistic does not tell us whether a model is better for better causal inference. We must assess whether a model satisfies the assumptions necessary for valid causal inference."
  },
  {
    "objectID": "content/03-content.html#simulation-demonstration-1-how-regression-coefficients-mislead",
    "href": "content/03-content.html#simulation-demonstration-1-how-regression-coefficients-mislead",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Simulation Demonstration 1: How Regression Coefficients Mislead",
    "text": "Simulation Demonstration 1: How Regression Coefficients Mislead\n\nMethodology\n\nData Generation: we simulate a dataset for 1,000 individuals, where religious service attendance (A) affects wealth (L), which in turn affects charitable donations (Y). The simulation is based on predefined parameters that establish L as a mediator between A and Y.\nParameter Definitions:\n\nThe probability of religious service (A) is set at 0.5.\nThe effect of A on L (wealth) is given by \\beta = 2.\nThe effect of L on Y (charity) is given by \\delta = 1.5.\nStandard deviations for L and Y are set at 1 and 1.5, respectively.\n\nModel Specifications:\n\nModel 1 (Correct Assumption): considerss a linear regression model assuming L as a mediator, including A and L as regressors on Y. This model aligns with the data-generating process and correctly identifies L as a mediator. According to the rules of d-separation (last week): to identify the total effect of A on Y we must not include L\nModel 2 (Correct model): This fits a linear regression model that includes only A as a regressor on Y and omits the mediator L. This model assesses the direct effect of A on Y without accounting for mediation.\n\nAnalysis and Comparison: the analysis compares the estimated effects of A on Y under both model specifications. By including L as a predictor in Model 1, we induce mediation bias. Whereas Model 2 correctly excludes L from the model.\nPresentation: the results are displayed in a comparative table formatted for publication. The table contrasts the regression coefficients and significance levels obtained under each model.\n\n\n# simulation seed\nset.seed(123) #  reproducibility\n\n# define the parameters \nn = 1000 # Number of observations\np = 0.5  # Probability of A = 1 \nalpha = 0 # Intercept for L \nbeta = 2  # Effect of A on L \ngamma = 1 # Intercept for Y \ndelta = 1.5 # Effect of L on Y\nsigma_L = 1 # Standard deviation of L\nsigma_Y = 1.5 # Standard deviation of Y\n\n# simulate the data: fully mediated effect by L\nA = rbinom(n, 1, p) # binary exposure variable\nL = alpha + beta*A + rnorm(n, 0, sigma_L) # mediator L affect by A\nY = gamma + delta*L + rnorm(n, 0, sigma_Y) # Y affected only by L,\n\n# make the data frame\ndata = data.frame(A = A, L = L, Y = Y)\n\n# fit regression in which L is assumed to be a mediator\n# (cross-sectional data is consistent with this model)\nexample_fit_1 &lt;- lm( Y ~ A + L, data = data)\n\n# fit regression in which L is assumed to be a mediator\n# (cross-sectional data is also consistent with this model)\nexample_fit_2 &lt;- lm( Y ~ A, data = data)\n\n# create gtsummary tables for each regression model\ntable1 &lt;- gtsummary::tbl_regression(example_fit_1)\ntable2 &lt;- gtsummary::tbl_regression(example_fit_2)\n\n# merge the tables for comparison\ntable_comparison &lt;- gtsummary::tbl_merge(\n  list(table1, table2),\n  tab_spanner = c(\"Model: Wealth assumed confounder\", \n                  \"Model: Wealth assumed to be a mediator\")\n)\n# make latex table (for publication)\nmarkdown_table_0 &lt;- as_kable_extra(table_comparison, \n                                   format = \"markdown\", \n                                   booktabs = TRUE)\n# print markdown table (note, you might prefer \"latex\" or another format)                                \nmarkdown_table_0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel: Wealth assumed confounder\n\n\nModel: Wealth assumed to be a mediator\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\n\n\n\n\nA\n-0.27\n-0.53, -0.01\n0.043\n2.9\n2.6, 3.2\n&lt;0.001\n\n\nL\n1.6\n1.5, 1.7\n&lt;0.001\n\n\n\n\n\n\n Abbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n\nCode for a simulation of a data generating process in which the effect of exercise (L) fully mediates the effect of greenspace (A) on happiness (Y).\n\n\n\nCompare model fits\n\nBy all metrics, model 1 fits better but it is confounded\n\nperformance::compare_performance(example_fit_1, example_fit_2)\n\n# Comparison of Model Performance Indices\n\nName          | Model |  AIC (weights) | AICc (weights) |  BIC (weights)\n------------------------------------------------------------------------\nexample_fit_1 |    lm | 3620.6 (&gt;.999) | 3620.6 (&gt;.999) | 3640.2 (&gt;.999)\nexample_fit_2 |    lm | 4392.2 (&lt;.001) | 4392.2 (&lt;.001) | 4406.9 (&lt;.001)\n\nName          |    R2 | R2 (adj.) |  RMSE | Sigma\n-------------------------------------------------\nexample_fit_1 | 0.682 |     0.682 | 1.473 | 1.475\nexample_fit_2 | 0.312 |     0.311 | 2.169 | 2.171\n\n\nModel 1 exhibits mediator bias, but it has a considerably higher R^2, and a lower BIC\n\nFocussing on the BIC (lower is better) Model 1 fits better, but it is confounded.\n\nBIC(example_fit_1) - BIC(example_fit_2)\n\n[1] -766.643\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this simulation, we discovered that if we assess “effect” by “model fit” we will get the wrong sign for the coefficient of interest. The use of model fit perpetuates the causality crisis in psychological science (Joseph A. Bulbulia 2023). Few are presently aware of this crisis. Causal diagrams show us how we can do better."
  },
  {
    "objectID": "content/03-content.html#simulation-demonstration-2-how-regression-coefficients-mislead",
    "href": "content/03-content.html#simulation-demonstration-2-how-regression-coefficients-mislead",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Simulation Demonstration 2: How Regression Coefficients Mislead",
    "text": "Simulation Demonstration 2: How Regression Coefficients Mislead\n\nMethodology\n\nData Generation: simulate a dataset for 1,000 individuals, where religious service attendance (A) affects wealth (L),and chartitable donations charitable donations (Y) also affects wealth L, but A and Y are not causally related\nParameter Definitions:\n\nThe probability of religious service (A) is set at 0.5.\nY is has a mean 0 and sd = 1, and is independent of A.\nL is a linear function of A and Y.\n\nModel Specifications:\n\nModel 1 (Correct Assumption): do not control for L\nModel 2 (Correct model): control for L\n\nAnalysis and Comparison: compare models.\nPresentation: a table.\n\n\n# simulation seed\nset.seed(123) #  reproducibility\n\n# define parameters \nn = 1000 # Number of observations\np = 0.5  # Probability of A = 1 \nalpha = 0 # Intercept for L\n\n\n# simulate the data\nA_1 = rbinom(n, 1, p) # binary exposure variable\nY_1 = rnorm(n, 0, 1) # Y affected only by L,\nL_2 = rnorm(n, A_1 + Y_1)\n\n# make the data frame\ndata_collider = data.frame(A = A_1, L = L_2, Y = Y_1)\n\n# fit regression in which L is assumed to be a mediator\n# (cross-sectional data is consistent with this model)\ncollider_example_fit_1 &lt;- lm( Y ~ A + L, data = data_collider)\n\n# fit regression in which L is assumed to be a mediator\n# (cross-sectional data is also consistent with this model)\ncollider_example_fit_2 &lt;- lm( Y ~ A, data = data_collider)\nsummary(collider_example_fit_1)\n\n\nCall:\nlm(formula = Y ~ A + L, data = data_collider)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.16111 -0.46220 -0.00342  0.45893  1.98913 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.008157   0.030247   -0.27    0.787    \nA           -0.476938   0.045314  -10.53   &lt;2e-16 ***\nL            0.508127   0.014894   34.12   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.681 on 997 degrees of freedom\nMultiple R-squared:  0.5386,    Adjusted R-squared:  0.5377 \nF-statistic:   582 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n# create gtsummary tables for each regression model\ncollider_table1 &lt;- gtsummary::tbl_regression(collider_example_fit_1)\ncollider_table2 &lt;- gtsummary::tbl_regression(collider_example_fit_2)\n\n# merge the tables for comparison\ncollider_table_comparison &lt;- gtsummary::tbl_merge(\n  list(collider_table1, collider_table2),\n  tab_spanner = c(\"Model: Wealth assumed confounder\", \n                  \"Model: Wealth not assumed to be a mediator\")\n)\n# make latex table (for publication)\nmarkdown_table_1 &lt;- as_kable_extra(collider_table_comparison, \n                                   format = \"markdown\", \n                                   booktabs = TRUE)\n# print markdown table (note, you might prefer \"latex\" or another format)                                \ncollider_table_comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModel: Wealth assumed confounder\n\n\nModel: Wealth not assumed to be a mediator\n\n\n\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\n\n\n\n\nA\n-0.48\n-0.57, -0.39\n&lt;0.001\n0.00\n-0.12, 0.13\n&gt;0.9\n\n\nL\n0.51\n0.48, 0.54\n&lt;0.001\n\n\n\n\n\n\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\nL is a collider of A and Y.\n\n\n\n\nCompare model fits\n\nBy all metrics, model 1 fits better but it is confounded.\nA is not causally associated with Y. We generated the data so we know. However, the confounded model shows a better fit, and in this model the coefficient for A is significant (and negative).\n\nperformance::compare_performance(collider_example_fit_1, collider_example_fit_2)\n\n# Comparison of Model Performance Indices\n\nName                   | Model |  AIC (weights) | AICc (weights)\n----------------------------------------------------------------\ncollider_example_fit_1 |    lm | 2074.4 (&gt;.999) | 2074.4 (&gt;.999)\ncollider_example_fit_2 |    lm | 2845.9 (&lt;.001) | 2845.9 (&lt;.001)\n\nName                   |  BIC (weights) |        R2 |  R2 (adj.) |  RMSE | Sigma\n--------------------------------------------------------------------------------\ncollider_example_fit_1 | 2094.0 (&gt;.999) |     0.539 |      0.538 | 0.680 | 0.681\ncollider_example_fit_2 | 2860.6 (&lt;.001) | 2.784e-06 | -9.992e-04 | 1.001 | 1.002\n\n\n\nAgain, the BIC for (lower is better) Model 1 fits better, but it is entirely confounded.\n\nBIC(collider_example_fit_1) - BIC(collider_example_fit_2)\n\n[1] -766.643\n\n\nHow does collider bias work? If we know that someone is not attending church, if they are charitable then we can predict they are wealthy. Similarly if we know someone is not wealthy but charitable, we can predict they attend religious service.\nHowever, in this simulation, we know the religion and wealth are not casually associated because we have simulated the data.\n\n\n\n\n\n\nImportant\n\n\n\nIn this simulation, we discovered that if we assess “effect” by “model fit”, we get the **wrong scientific inference. The use of model fit perpetuates the causality crisis* in psychological science (Joseph A. Bulbulia 2023). Let’s do better.\n\n\n\nBulbulia, Joseph A. 2023. “A Workflow for Causal Inference in Cross-Cultural Psychology.” Religion, Brain & Behavior 13 (3): 291–306. https://doi.org/10.1080/2153599X.2022.2070245."
  },
  {
    "objectID": "content/03-content.html#resources-on-regression-that-do-not-perpetuate-the-causality-crisis",
    "href": "content/03-content.html#resources-on-regression-that-do-not-perpetuate-the-causality-crisis",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Resources on Regression that Do Not Perpetuate The Causality Crisis",
    "text": "Resources on Regression that Do Not Perpetuate The Causality Crisis\n\nStatistical Rethinking (McElreath 2020)\nRegression and Other Stories (Gelman, Hill, and Vehtari 2020)\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC press.\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press."
  },
  {
    "objectID": "content/03-content.html#setup",
    "href": "content/03-content.html#setup",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Setup",
    "text": "Setup\n\nLibraries\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(lubridate)\nlibrary(kableExtra)\nlibrary(gtsummary)\n\n\n\nGet data\n\n# package with data\nlibrary(margot)\n\n# load data\ndata(\"df_nz\")\n\n\n\nImport Pearson and Lee mother’s and daughters data\n\ndf_pearson_lee &lt;-\n  data.frame(read.table(\n    url(\n      \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt\"\n    ),\n    header = TRUE\n  ))\n# Center mother's height for later example\ndf_pearson_lee &lt;- df_pearson_lee|&gt;\n  dplyr::mutate(mother_height_c = as.numeric(scale(\n    mother_height, center = TRUE, scale = FALSE\n  )))\n\n# dplyr::glimpse(df_pearson_lee)\n\n# In 1903, Pearson and Lee collected 5,524 observations from mother/daughter height pairs. See lecture 5 for details\n\n\n\nNote\nFor all exercises below, use only the 2018 wave of the df_nz dataset.\ncolnames(df_nz) ## Q1. Create a descriptive table and a descriptive graph for the hlth_weight and hlth_height variables in the df_nz dataset\nSelect hlth_height, hlth_weight from the nz dataset.\nFilter only the 2018 wave.\nCreate a descriptive table and graph these two variables\nAnnotate your workflow (at each step, describe what you are doing and why)."
  },
  {
    "objectID": "content/03-content.html#q2.-regression-height-weight-and-report-results",
    "href": "content/03-content.html#q2.-regression-height-weight-and-report-results",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Q2. Regression height ~ weight and report results",
    "text": "Q2. Regression height ~ weight and report results\nUsing the df_nz dataset, write a regression model for height as predicted by weight.\nCreate a table for your results.\nCreate a graphs/graphs to clarify the results of your regression model.\nBriefly report your results."
  },
  {
    "objectID": "content/03-content.html#q3.-regress-height-male-and-report-results",
    "href": "content/03-content.html#q3.-regress-height-male-and-report-results",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Q3. Regress height ~ male and report results",
    "text": "Q3. Regress height ~ male and report results\nUsing the df_nz dataset, write a regression model for height as predicted by male\nCreate a table for your results.\nCreate a graphs/graphs to clarify the results of your regression model.\nBriefly report your results."
  },
  {
    "objectID": "content/03-content.html#q4.-regression-to-predict",
    "href": "content/03-content.html#q4.-regression-to-predict",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Q4. Regression to predict",
    "text": "Q4. Regression to predict\nUsing the regression coefficients from the Pearson and Lee 1903 dataset, predict the heights of daughters of women in the df_nz dataset."
  },
  {
    "objectID": "content/03-content.html#q5.-bonus",
    "href": "content/03-content.html#q5.-bonus",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Q5. Bonus",
    "text": "Q5. Bonus\nOn average, how much taller or shorter are women in New Zealand as sampled in 2019 df_nz dataset compared with women in 1903 as sampled in the Pearson and Lee dataset.\nClarify your inference."
  },
  {
    "objectID": "content/03-content.html#solutions-set-up",
    "href": "content/03-content.html#solutions-set-up",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Solutions: Set Up",
    "text": "Solutions: Set Up\n\n### libraries\nlibrary(\"tidyverse\")\nlibrary(\"patchwork\")\nlibrary(\"lubridate\")\nlibrary(\"kableExtra\")\nlibrary(\"gtsummary\")\n\n\n# get data\nlibrary(\"margot\")\ndata(\"df_nz\")\n\n\n# Import `Pearson and Lee` mother's and daughters data\n# In 1903, Pearson and Lee collected 5,524 observations from mother/daughter height pairs. \n\ndf_pearson_lee &lt;- data.frame(read.table(\n  url(\n    \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt\"\n  ),\n  header = TRUE\n))\n# Center mother's height for later example\ndf_pearson_lee &lt;- df_pearson_lee|&gt;\n  dplyr::mutate(mother_height_c = as.numeric(scale(\n    mother_height, center = TRUE, scale = FALSE\n  )))\n\n# dplyr::glimpse(df_pearson_lee)"
  },
  {
    "objectID": "content/03-content.html#solution-q1-descriptive-table",
    "href": "content/03-content.html#solution-q1-descriptive-table",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Solution Q1 descriptive table",
    "text": "Solution Q1 descriptive table\n\n# required libraries\nlibrary(gtsummary)\nlibrary(dplyr)\nlibrary(margot)\n\n# select focal variables and rename them for clarity\ndf_nzdat &lt;- df_nz  |&gt;\n  dplyr::filter(wave == 2019) |&gt;\n  dplyr::select(hlth_weight, hlth_height, male) |&gt;\n  dplyr::rename(weight = hlth_weight,\n                height = hlth_height) |&gt;\n  dplyr::select(weight, height, male) |&gt;\n  dplyr::mutate(weight_c = as.numeric(scale(\n    weight, scale = F, center = TRUE\n  )))\n\n\n# create table\ndf_nzdat |&gt; \n  dplyr::select(weight,\n                height) |&gt; \n  gtsummary::tbl_summary(\n    #by = wave,\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} / {N} ({p}%)\"\n    ),\n    digits = all_continuous() ~ 2,\n    missing_text = \"(Missing)\"\n  )|&gt;\n  bold_labels() \n\n\n\n\n\n\n\nCharacteristic\nN = 20,0001\n\n\n\n\nweight\n79.24 (18.42)\n\n\n    (Missing)\n5,655\n\n\nheight\n1.70 (0.10)\n\n\n    (Missing)\n5,655\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\nHere’s another approach:\n\n# libs we need\nlibrary(table1)\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(margot)\n\n# filter 2019 wave\ndf_nz_1 &lt;- df_nz|&gt;\n  dplyr::filter(wave == 2019)\n\n# nicer labels\ntable1::label(df_nz_1$hlth_weight)     &lt;- \"Weight\"\ntable1::label(df_nz_1$hlth_height)     &lt;- \"Height\"\n\n# table\ntable1::table1(~ hlth_weight + hlth_height, data = df_nz_1)     \n\n\n\n\n\n\n\n\n\n\nOverall\n(N=20000)\n\n\n\n\nWeight\n\n\n\nMean (SD)\n79.2 (18.4)\n\n\nMedian [Min, Max]\n77.0 [38.0, 200]\n\n\nMissing\n5655 (28.3%)\n\n\nHeight\n\n\n\nMean (SD)\n1.70 (0.100)\n\n\nMedian [Min, Max]\n1.69 [1.26, 2.04]\n\n\nMissing\n5655 (28.3%)\n\n\n\n\n\n\n\nCreate graph\n\nlibrary(\"patchwork\")\n# create data set where filtering na vals\ndf_nzdat1 &lt;- df_nzdat |&gt; \n    dplyr::filter(!is.na(weight),\n                  !is.na(height),\n                  !is.na(male)) # filter na's for density plots\n  \nweight_density &lt;-ggplot2::ggplot(data = df_nzdat1, aes(x = weight)) + geom_density(fill = \"chocolate2\") + \n  labs(title = \"Density plot of weight of NZ sample years 2019/2020\") + theme_classic()\nweight_density\n\n\n\n\n\n\n\nheight_density &lt;-ggplot2::ggplot(data = df_nzdat1, aes(x = height)) + geom_density(fill =\"blue2\") + \n  labs(title = \"Density plot of height of NZ sample years 2019/2020\") + theme_classic()\n\nheight_density\n\n\n\n\n\n\n\nweight_density / height_density + plot_annotation(tag_levels = 'a')\n\n\n\n\n\n\n\n\nHere’s a density plot:\n\n# library\nlibrary(ggplot2)\n\n\nggplot2::ggplot(data = df_nzdat1, aes(x = weight, fill = as.factor(male))) +\n  geom_density() + \n  labs(title = \"Density plot of weight of NZ sample years 2019/2020\") +\n  theme_classic() + \n  scale_fill_viridis_d() # nicer colour"
  },
  {
    "objectID": "content/03-content.html#solution-q2.-regress-height-weight-and-report-results",
    "href": "content/03-content.html#solution-q2.-regress-height-weight-and-report-results",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Solution Q2. Regress height ~ weight and report results",
    "text": "Solution Q2. Regress height ~ weight and report results\nUsing the nz dataset, write a regression model for height as predicted by weight. Create a table for your results. Create a graphs/graphs to clarify the results of your regression model. Briefly report your results.\nModel:\n\n# regression of height ~ weight\nfit_1 &lt;- lm(height ~ weight_c, data = df_nzdat)\n\nTable:\n\nlibrary(parameters)\n#table of model\nparameters::model_parameters( fit_1) |&gt; \n  print_html(  digits = 2,  # options\n               select = \"{coef}{stars}|({ci})\",\n               column_labels = c(\"Estimate\", \"95% CI\")\n) \n\n\n\n\n\n\n\nParameter\nEstimate\n95% CI\n\n\n\n\n(Intercept)\n1.70***\n(1.70, 1.70)\n\n\nweight c\n2.23e-03***\n(2.15e-03, 2.31e-03)\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction:\n\n# Ensure necessary libraries are loaded\nlibrary(ggeffects)\nlibrary(ggplot2)\n\n# generate the plot with adjusted y-scale\nplot(ggeffects::ggpredict(fit_1, terms = \"weight_c\")) \n\n\n\n\n\n\n\n\nBriefly report your results. (note: please replace “significant” with “statistically significant.)\n\nreport::report(fit_1)\n\nWe fitted a linear model (estimated using OLS) to predict height with weight_c\n(formula: height ~ weight_c). The model explains a statistically significant\nand moderate proportion of variance (R2 = 0.17, F(1, 14286) = 2858.73, p &lt;\n.001, adj. R2 = 0.17). The model's intercept, corresponding to weight_c = 0, is\nat 1.70 (95% CI [1.70, 1.70], t(14286) = 2213.51, p &lt; .001). Within this model:\n\n  - The effect of weight c is statistically significant and positive (beta =\n2.23e-03, 95% CI [2.15e-03, 2.31e-03], t(14286) = 53.47, p &lt; .001; Std. beta =\n0.41, 95% CI [0.39, 0.42])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "content/03-content.html#solution-q3.-regress-height-male-and-report-results",
    "href": "content/03-content.html#solution-q3.-regress-height-male-and-report-results",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Solution Q3. Regress height ~ male and report results",
    "text": "Solution Q3. Regress height ~ male and report results\nUsing the df_nz dataset, write a regression model for height as predicted by male Create a table for your results. Create a graph/graphs to clarify the results of your regression model. Briefly report your results.\nModel and table:\n\n# regression of height ~ weight\nfit_2 &lt;- lm(hlth_height ~ male, data = df_nz)\nsjPlot::tab_model(fit_2)\n\n\n\n\n \nhlth height\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n1.65\n1.65 – 1.65\n&lt;0.001\n\n\nmale\n0.13\n0.13 – 0.13\n&lt;0.001\n\n\nObservations\n47205\n\n\nR2 / R2 adjusted\n0.388 / 0.388\n\n\n\n\n\n\n\nGraph:\n\n# plot over the range of the data\nplot_fit_2 &lt;- plot(\n  ggeffects::ggpredict(fit_2, terms = \"male[all]\",\n  jitter = .1,\n  add.data = TRUE,\n  dot.alpha = .2\n) )\n\n# plot\nplot_fit_2 + \n  scale_y_continuous(limits = c(1.2, 2.1))\n\n\n\n\n\n\n\n\nReport\n\n# report\nreport::report(fit_2)\n\nWe fitted a linear model (estimated using OLS) to predict hlth_height with male\n(formula: hlth_height ~ male). The model explains a statistically significant\nand substantial proportion of variance (R2 = 0.39, F(1, 47203) = 29979.00, p &lt;\n.001, adj. R2 = 0.39). The model's intercept, corresponding to male = 0, is at\n1.65 (95% CI [1.65, 1.65], t(47203) = 3603.30, p &lt; .001). Within this model:\n\n  - The effect of male is statistically significant and positive (beta = 0.13,\n95% CI [0.13, 0.13], t(47203) = 173.14, p &lt; .001; Std. beta = 0.62, 95% CI\n[0.62, 0.63])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "content/03-content.html#solution-q4.-regression-to-predict",
    "href": "content/03-content.html#solution-q4.-regression-to-predict",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Solution Q4. Regression to predict",
    "text": "Solution Q4. Regression to predict\nUsing the regression coefficients from the Pearson and Lee 1903 dataset, predict the heights of daughters of women in the nz dataset. ::: {.cell}\n# model for daughter height from mother height\nfit_3 &lt;- lm(daughter_height ~ mother_height, data = df_pearson_lee)\n\n# create data frame of not_male's in 2019\n# notice problem. not_male != woman\n# additionally, woman != mother!\n\ndf_nz_2 &lt;- df_nz|&gt;\n  filter(male == 1)|&gt; # not_males\n  dplyr::select(hlth_height)|&gt; # variable of interest\n  dplyr::mutate(mother_height = hlth_height * 39.36)|&gt; # Convert meters to inches\n  dplyr::select(mother_height)|&gt;\n  dplyr::arrange((mother_height))\n\n# find min and max heights, store as objects\nmin_mother_height &lt;- min(df_nz_2$mother_height, na.rm = TRUE)\nmax_mother_height &lt;- max(df_nz_2$mother_height, na.rm = TRUE)\n\n# expand grid, use stored objects to define boundaries.\ndf_expand_grid_nz_2 &lt;-\n  expand.grid(mother_height = seq(\n    from = min_mother_height,\n    to = max_mother_height,\n    length.out = 200\n  ))\n\n# use the `predict` function to create a new response using the pearson and fox regression model\n\npredict_fit_3 &lt;-\n  predict(fit_3,\n          type = \"response\",\n          interval = \"confidence\",\n          newdata = df_expand_grid_nz_2)\n\n# create a new dataframe for the response variables, following the method in lecture 5\n\n# combine variables into a data frame\ndf_expand_grid_nz_2_predict_fit_3 &lt;-\n  data.frame(df_expand_grid_nz_2, predict_fit_3)\n\n# graph the expected average hypothetical heights of \"daughters\"\npredplot2 &lt;-\n  ggplot(data = df_expand_grid_nz_2_predict_fit_3,\n         aes(x = mother_height,\n             y = fit))  +\n  geom_line(colour = \"cadetblue\")  +  geom_errorbar(aes(ymin = lwr, ymax = upr), width = .1) + scale_x_continuous(limits = c(50, 75)) + scale_y_continuous(limits = c(50, 75)) + theme_classic()  +\n  xlab(\"NZ 2019 female population\") +\n  ylab(\"predicted daughter heights in inches\") +\n  labs(title = \"Regression prediction for hypothetical daughter heights of NZ population in 2019 \")\n\n# plot\npredplot2\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "content/03-content.html#solution-q6.-bonus",
    "href": "content/03-content.html#solution-q6.-bonus",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Solution Q6. Bonus",
    "text": "Solution Q6. Bonus\nOn average, how much taller or shorter are women in New Zealand as sampled in 2019 nz dataset compared with women in 1903 as sampled in the Pearson and Lee dataset.\n\n# create var for 1903 dataset\n# For the historical dataset (assuming year 1903 for df_pearson_lee_2)\ndf_pearson_lee_2 &lt;- df_pearson_lee |&gt;\n  dplyr::select(mother_height, daughter_height) |&gt;\n  tidyr::pivot_longer(everything(), names_to = \"relation\", values_to = \"f_height\") |&gt;\n  dplyr::mutate(year_is = factor(\"1903\"))\n\n# create var the 2019 dataset\ndf_nz_combine_pearson_lee &lt;- df_nz_2 |&gt;\n  dplyr::rename(f_height = mother_height) |&gt;\n  dplyr::mutate(year_is = factor(\"2019\"),\n                relation = factor(\"mother\"))\n\n# both dataframes have the `year_is` column but with different factor levels\n# check\nhead(df_nz_combine_pearson_lee)\n\n  f_height year_is relation\n1 49.59360    2019   mother\n2 51.21146    2019   mother\n3 51.45691    2019   mother\n4 51.95953    2019   mother\n5 52.92635    2019   mother\n6 53.22973    2019   mother\n\n# combine data frames row-wise\ndf_nz_combine_pearson_lee_1 &lt;- rbind(df_pearson_lee_2, df_nz_combine_pearson_lee)\n\n\n# look at data structure\n#dplyr::glimpse(df_nz_combine_pearson_lee_1)\n\nLook at heights in sample\n\n# get min and max heights for graph range\ntable(df_nz_combine_pearson_lee_1$year_is)\n\n\n 1903  2019 \n11048 22278 \n\n# box plot\nggplot2::ggplot(data =\n                  df_nz_combine_pearson_lee_1,\n                aes(x = year_is, y = f_height, fill = year_is)) +\n  geom_boxplot(notch = TRUE) +\n  labs(title = \"Comparison of female height 1903/2019\") +\n  theme_classic() + scale_fill_viridis_d()\n\n\n\n\n\n\n\n\nPredict heights out of sample\n\n# regression model\nfit_4 &lt;- lm(f_height ~ year_is, data = df_nz_combine_pearson_lee_1)\n\n# table\nsjPlot::tab_model(fit_4)\n\n\n\n\n \nf height\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n63.18\n63.12 – 63.23\n&lt;0.001\n\n\nyear is [2019]\n6.97\n6.90 – 7.04\n&lt;0.001\n\n\nObservations\n28422\n\n\nR2 / R2 adjusted\n0.569 / 0.569\n\n\n\n\n\n\n\nWomen in 2019 are taller\n\nreport::report(fit_4)\n\nWe fitted a linear model (estimated using OLS) to predict f_height with year_is\n(formula: f_height ~ year_is). The model explains a statistically significant\nand substantial proportion of variance (R2 = 0.57, F(1, 28420) = 37516.23, p &lt;\n.001, adj. R2 = 0.57). The model's intercept, corresponding to year_is = 1903,\nis at 63.18 (95% CI [63.12, 63.23], t(28420) = 2244.43, p &lt; .001). Within this\nmodel:\n\n  - The effect of year is [2019] is statistically significant and positive (beta\n= 6.97, 95% CI [6.90, 7.04], t(28420) = 193.69, p &lt; .001; Std. beta = 1.55, 95%\nCI [1.53, 1.56])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nGraph:\n\n# get meaningful values for the y access range\n\n# min\nmin_mother_height_for_range &lt;- min(df_nz_combine_pearson_lee_1$f_height, na.rm = TRUE)\n\n# max\nmax_mother_height_for_range &lt;- max(df_nz_combine_pearson_lee_1$f_height, na.rm = TRUE)\n\n# make graph \nplot(\n  ggeffects::ggpredict(fit_4, terms = \"year_is\",\n  add.data = TRUE,\n  dot.alpha = .2\n) )+ labs(title = \"Predicted difference in female heights 1903/2019\") + \n  xlab(\"Study year\") + \n  ylab (\"Predicted femal height (inches)\") +   scale_y_continuous(\n    limits = c(min_mother_height_for_range,max_mother_height_for_range))\n\n\n\n\n\n\n\n\nGraph with predicted points:\n\nggeffects::ggpredict(fit_4, terms = \"year_is\")\n\n# Predicted values of f_height\n\nyear_is | Predicted |       95% CI\n----------------------------------\n1903    |     63.18 | 63.12, 63.23\n2019    |     70.15 | 70.11, 70.19\n\n# graph with predicted values based on the model\nplot(\n  ggeffects::ggpredict(fit_4, terms = \"year_is\",\n  add.data = TRUE,\n  dot.alpha = .01,\n   colors = \"us\", # see colour pallet options \n  jitter =.5)) + labs(title = \"Predicted difference in female heights 1903/2019\") +\n  xlab(\"Study year\") +\n  ylab (\"Predicted femal height (inches)\") +\n  scale_y_continuous(limits = c(min_mother_height_for_range, max_mother_height_for_range))\n\n\n\n\n\n\n\n# show all color palettes\n#show_pals()"
  },
  {
    "objectID": "content/03-content.html#appendix-1-conceptual-background",
    "href": "content/03-content.html#appendix-1-conceptual-background",
    "title": "Causal Diagrams: The Structures of Confounding Bias",
    "section": "Appendix 1 Conceptual Background",
    "text": "Appendix 1 Conceptual Background\nSome preliminaries about science.\n\nScience begins with a question\nScience begins with a question about the world. The first step in science, then, is to clarify what you want to know.\nBecause science is a social practice, you will also need to clarify why your question is interesting: so what?\nIn short, know your question.\n\n\nScientific model (or theory)\nSometimes, scientists are interested in specific features of the world: how did virus x originate? Such a question might have a forensic interest: what constellation of events gave rise to a novel infectious disease?\nScientists typically seek generalisations. How do infectious diseases evolve? How do biological organisms evolve? Such questions have applied interests. How can we better prevent infectious diseases? How did life originate?\nA scientific model proposes how nature is structured (and unstructured). For example, the theory of evolution by natural selection proposes that life emerges from variation, inheritance, and differential reproduction/survival.\nTo evaluate a scientific model, scientists must make generalisations beyond individual cases. This is where statistics shines.\n\n\nWhat is statistics?\nMathematics is a logic of certainty.\nStatistics is a logic of uncertainty.\nA statistical model uses the logic of probability to make better guesses.\n\n\nApplications of statistical models in science\nScientific models seek to explain how nature is structured. Where scientific models conflict, we can combine statistical models with data collection to evaluate the credibility of of one theoretical model over others. To do this, a scientific model must make distinct, non-trivial predictions about the world.\nIf the predictions are not distinct, the observations will not enable a shift in credibility for one theory over another. Consider the theory that predicts any observation. Such a theory would be better classified as a conspiracy theory; it is compatible with any evidence.\n\n\n\nPackages\n\nreport::cite_packages()\n\n  - Anderson D, Heiss A, Sumners J (2024). _equatiomatic: Transform Models into 'LaTeX' Equations_. R package version 0.3.3, &lt;https://CRAN.R-project.org/package=equatiomatic&gt;.\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 0.3.1.6 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Lüdecke D (2018). \"ggeffects: Tidy Data Frames of Marginal Effects from Regression Models.\" _Journal of Open Source Software_, *3*(26), 772. doi:10.21105/joss.00772 &lt;https://doi.org/10.21105/joss.00772&gt;.\n  - Lüdecke D (2024). _sjPlot: Data Visualization for Statistics in Social Science_. R package version 2.8.17, &lt;https://CRAN.R-project.org/package=sjPlot&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 &lt;https://doi.org/10.21105/joss.02445&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Waggoner P, Makowski D (2021). \"performance: An R Package for Assessment, Comparison and Testing of Statistical Models.\" _Journal of Open Source Software_, *6*(60), 3139. doi:10.21105/joss.03139 &lt;https://doi.org/10.21105/joss.03139&gt;.\n  - Makowski D, Lüdecke D, Patil I, Thériault R, Ben-Shachar M, Wiernik B (2023). \"Automated Results Reporting as a Practical Tool to Improve Reproducibility and Methodological Best Practices Adoption.\" _CRAN_. &lt;https://easystats.github.io/report/&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. R package version 1.3.0, &lt;https://CRAN.R-project.org/package=patchwork&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Rich B (2023). _table1: Tables of Descriptive Statistics in HTML_. R package version 1.4.3, &lt;https://CRAN.R-project.org/package=table1&gt;.\n  - Sjoberg D, Whiting K, Curry M, Lavery J, Larmarange J (2021). \"Reproducible Summary Tables with the gtsummary Package.\" _The R Journal_, *13*, 570-580. doi:10.32614/RJ-2021-053 &lt;https://doi.org/10.32614/RJ-2021-053&gt;, &lt;https://doi.org/10.32614/RJ-2021-053&gt;.\n  - Waring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, Ellis S (2022). _skimr: Compact and Flexible Summaries of Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=skimr&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, Bryan J, Barrett M, Teucher A (2024). _usethis: Automate Package and Project Setup_. R package version 3.1.0, &lt;https://CRAN.R-project.org/package=usethis&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Hester J, Chang W, Bryan J (2022). _devtools: Tools to Make Developing R Packages Easier_. R package version 2.4.5, &lt;https://CRAN.R-project.org/package=devtools&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.56, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;.\n  - Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. R package version 1.4.0, &lt;https://CRAN.R-project.org/package=kableExtra&gt;."
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nRequired - https://grf-labs.github.io/grf/\nOptional - (VanderWeele, Mathur, and Chen 2020) link - (Suzuki, Shinozaki, and Yamamoto 2020) link - (Bulbulia 2024) link - (Hoffman et al. 2023) link"
  },
  {
    "objectID": "content/08-content.html#learning-outcomes",
    "href": "content/08-content.html#learning-outcomes",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou will learn how to state a causal question in a three-wave panel\nYou will learn how to identify causal effects in a three-waves of panel data.\nConditional Average Treatment Effect by doubly-robust estimation\n\n\n\nWhy is this lesson important?\nThe methods you will learn today will help you to define and answer comparative questions in psychology."
  },
  {
    "objectID": "content/08-content.html#review-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "href": "content/08-content.html#review-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "Review: The Fundamental Problem of Causal Inference as a Missing Data Problem",
    "text": "Review: The Fundamental Problem of Causal Inference as a Missing Data Problem\nRecall the fundamental problem of causal inference, returning to the question of whether bilingualism improves cognitive abilities:\n\nY_i^{a = 1}: The cognitive ability of child i if they were bilingual. This is the counterfactual outcome when A = 1.\nY_i^{a = 0}:: The cognitive ability of child i if they were monolingual. This is the counterfactual outcome when A = 0.\n\nThe causal effect of bilingualism on cognitive ability for individual i is then defined as the difference between these potential outcomes:\n\n\\text{Causal Effect}_i = Y_i^{a=1} - Y_i^{a=0}\n\nWe say there is a causal effect if:\n\nY_i^{a=1} - Y_i^{a=0}  \\neq 0\n\nHowever, we only observe one of the potential outcomes for each child. The other outcome is not observed because physics prevents a child from both receiving and not receiving bilingual exposure.\nThe fact that causal contrasts are not observed in individuals is called “The fundamental problem of causal inference.”\nAlthough we typically cannot observe individual causal effects, we can obtain average causal effects when certain assumptions are satisfied.\n\\begin{align}\nE(\\delta) = E(Y^{a=1} - Y^{a=0})\\\\\n          ~  = E(Y^{a=1}) - E(Y^{a=0}) \\\\\n          ~  = ATE\n\\end{align}\nWe may identify average causal effects from the data when the following assumptions are met:\n\nCausal Consistency: The exposure values under comparisons correspond to well-defined interventions that, in turn, correspond to the treatment versions in the data.[]\nPositivity: The probability of receiving every value of the exposure within all strata of co-variates is greater than zero []\nExchangeability: The conditional probability of receiving every value of an exposure level, though not decided by the investigators, depends only on the measured covariates []\n\nFurther assumptions:\n\nNo Interference, also known as the Stable Unit Treatment Value Assumption (SUTVA), requires that the treatment given to one unit (e.g., person, group, organization) does not interfere with the potential outcomes of another unit. Put differently, there are no “spillover” effects. Note: this assumption may be thought to be part of causal consistency, namely individual has only one potential outcome under each treatment condition.\nCorrectly specified model: the requirement that the underlying statistical model used to estimate causal effects accurately represents the true relationships between the variables of interest. We say the model should be able to capture “the functional form” of the relationship between the treatment, the outcome, and any covariates. The model’s functional form should be flexible enough to capture the true underlying relationship. The estimated causal effects may be biased if the model’s functional form is incorrect. Additionally, the model must handle omitted variable bias by including all relevant confounders and should correctly handle missing data from non-response or loss-to follow up. We will return to the bias arising from missing data in the weeks ahead. For now, it is important to note that causal inference assumes that our model is correctly specified."
  },
  {
    "objectID": "content/08-content.html#subgroup-analysis",
    "href": "content/08-content.html#subgroup-analysis",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\nRedcall, Effect Modification (also known as “heterogeneity of treatment effects”, and “Effect-measure modification”) occurs when the causal effect of intervention A varies across different levels of another variable R:\nE(Y^{a=1}|G=g_1, L=l) - E(Y^{a=0}|G=g_1, L=l) \\neq E(Y^{a=1}|G=g_2, L=l) - E(Y^{a=0}|G=g_2, L=l)\nEffect modification indicates that the magnitude of the causal effect of intervention A is related to the modifier variable G level. As discussed last week, effect modification can be observed even when there is no direct causal interaction between the treatment and the modifier variable. We noted that interaction in causal inference refers to a situation where the combined effect of two interventions is not equal to the sum of their individual effects. Effect modification, on the other hand, occurs when the causal effect of one intervention varies across different levels of another variable.\nWe also noted that\n\nFor comparative research, we are typically interested in effect-modification, which requires subgroup analysis.\n\n\nCausal Estimand, Statistical Estimand, Statistical Estimator\nLet’s set subgroup analysis to the side for a moment and begin focussing on statistical estimation.\nSuppose a researcher wants to understand the causal effect of marriage on individual happiness. Participants in the study are surveyed for their marital status (“married” or “not married”) and their self-reported happiness on a scale from 1 to 10.\n\nCausal Estimand\n\nDefinition: The causal estimand is the specific quantity or parameter that we aim to estimate to understand the causal effect of an intervention or treatment on an outcome.\nExample: Here, the Causal Estimand would be the Average Treatment Effect (ATE) of being married on happiness. Specifically, we define the ATE as the difference in the potential outcomes of happiness if all individuals were married versus if no individuals were married:\n\n\\text{ATE} = E[Y^{a=1} - Y^{a=0}]\n\nHere, Y^{a=1} represents the potential happiness score if an individual is married, and Y^{a=0} if they are not married.\n\n\n\nNext step: Are Causal Assumptions Met?\n\nIdentification (Exchangeability): balance in the confounders across the treatments to be compared\nConsistency: well-defined interventions\nPositivity: treatments occur within levels of covariates L\n\n\n\nStatistical Estimand (next step)\n\nThe problem: how do we bridge the gap between potential outcomes and data?\nDefinition: the statistical estimand is the parameter or function that summarises the relationship between variables as described by a statistical model applied to data.\nExample: for our study, the Statistical Estimand might be the mean difference in happiness scores between individuals who are married and those who are not, as derived from a linear regression model:\n\n\\text{Happiness} = \\beta_0 + \\beta_1 \\times \\text{Married} + \\epsilon\n\nIn this equation, \\beta_1 represents the estimated difference in happiness scores between the married and non-married groups.\n\n\n\nStatistical Estimator\n\nDefinition: a statistical estimator is a rule or method by which a numerical estimate of a statistical estimand is calculated from the data.\nExample: in our marriage study, the Statistical Estimator for \\beta_1 is the ordinary least squares (OLS) estimator. This estimator is used to calculate \\beta_1 from the sample data provided by the survey. It provides an estimate of the impact of being married on happiness, calculated using: \n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n where X_i is a binary indicator for being married (1 for married, 0 for not married), Y_i is the observed happiness score, and \\bar{X}, \\bar{Y} are the sample means of X and Y, respectively.\n\nThe upshot, we anchor our causal inquiries within a multi-step framework of data analysis. This involves:\n\nclearly defining our causal estimand within a specified target population,\nclarifying assumptions, & especially identification assumptions,\ndescribing a statistical strategy for extracting this estimand from the data, and then\napplying an algorithm that embodies this statistical method."
  },
  {
    "objectID": "content/08-content.html#methods-for-statistical-estimation-in-causal-inference-inverse-probability-of-treatment-weights-using-propensity-scores",
    "href": "content/08-content.html#methods-for-statistical-estimation-in-causal-inference-inverse-probability-of-treatment-weights-using-propensity-scores",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Methods for Statistical Estimation in Causal Inference: Inverse Probability of Treatment Weights Using Propensity Scores",
    "text": "Methods for Statistical Estimation in Causal Inference: Inverse Probability of Treatment Weights Using Propensity Scores\nLast week, we discussed confounding control using regression adjustment. Recall the formula for the average treatment effect (ATE) when conditioning on a set of covariates L:\n\n\\begin{aligned}\n\\text{ATE} = E[Y^{a=1} \\mid L = l] - E[Y^{a=0} \\mid L = l] \\quad \\text{for any value of } l\n\\end{aligned}\n\n\n“We say that a set L of measured non-descendants of L is a sufficient set for confounding adjustment when conditioning on L blocks all backdoor paths—that is, the treated and the untreated are exchangeable within levels of L” (Hernán & Robins, Causal Inference, p. 86).\n\nThis formula calculates the expected outcome difference between treated (a=1) and untreated (a=0) groups, given a specific value of the covariates l.\nInverse Probability of Treatment Weighting (IPTW) takes a different approach. We create a pseudo-population where the treatment assignment is independent of the observed covariates by assigning weights to each individual based on their propensity scores.\nWe do this by modelling the treatment\nDenote the treatment indicator by A, where A = 1 if an individual receives treatment and A = 0 otherwise. L represents the vector of observed covariates, and Y^a the potential outcomes. The propensity score, e(L), is defined as the probability of receiving the treatment given the observed covariates:\n\n\\hat{e}(L) = P(A = 1 \\mid L)\n\nTo obtain IPTW weights, compute the inverse probability of treatment:\n\nv_i = \\frac{A_i}{\\hat{e}(L_i)} + \\frac{1 - A_i}{1 - \\hat{e}(L_i)}\n\nWhich simplifies to\n\nv_i =\n\\begin{cases}\n\\frac{1}{\\hat{e}} & \\text{if } A_i = 1 \\\\\n\\frac{1}{1-\\hat{e}} & \\text{if } A_i = 0\n\\end{cases}\n\nwhere v_i is the IPTW weight for individual i, A_i is the treatment indicator for individual i, and \\hat{e}(L_i) is the estimated propensity score for individual i.\nHow might we use these weights to obtain causal effect estimates?"
  },
  {
    "objectID": "content/08-content.html#marginal-structural-models-msms",
    "href": "content/08-content.html#marginal-structural-models-msms",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Marginal Structural Models (MSMs)",
    "text": "Marginal Structural Models (MSMs)\nMarginal Structural Models (MSMs) estimate causal effects without requiring an “outcome model” that stratifies on covariates. Rather, MSMs employ weights derived from the inverse probability of treatment weighting (IPTW) to create a pseudo-population in which the distribution of covariates is independent of treatment assignment over time.\nThe general form of an MSM can be expressed as follows:\n\nE[Y^a] = \\beta_0 + \\beta_1a\n\nwhere E[Y^a] is the expected outcome under treatment a and \\beta_0 and \\beta_1 are parameters estimated by fitting the weighted model. Again, the weights used in the MSM, typically derived from the IPTW (or another treatment model), adjust for the confounding, allowing the model to estimate the unbiased effect of the treatment on the outcome without requiring covariates in the model.\nWhere do weights fit in? Note, we have E[Y^a] in please of E[Y|A=a]. When applying propensity score weights in the linear regression model E[Y^a] = \\beta_0 + \\beta_1a, each observation is weighted by v_i, such that v_i(\\beta_0 + \\beta_1a). This changes the estimation process to focus on a weighted sum of outcomes, where each individual’s contribution is adjusted to reflect their probability of receiving the treatment, given their covariates."
  },
  {
    "objectID": "content/08-content.html#interpretation-of-beta_0-and-beta_1-in-a-marginal-structural-model",
    "href": "content/08-content.html#interpretation-of-beta_0-and-beta_1-in-a-marginal-structural-model",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Interpretation of \\beta_0 and \\beta_1 in a Marginal Structural Model",
    "text": "Interpretation of \\beta_0 and \\beta_1 in a Marginal Structural Model\n\nBinary Treatment\nIn models where the treatment a is binary (e.g., a = 0 or a = 1), such as in many causal inference studies:\n\n\\beta_0: the expected value of the outcome Y when the treatment is not applied (a = 0). This is the baseline level of the outcome in the absence of treatment.\n\\beta_1: the change in the expected outcome when the treatment status changes from 0 to 1. In logistic regression, \\beta_1 represents the log-odds ratio of the outcome for the treatment group relative to the control group. In linear regression, \\beta_1 quantifies the difference in the average outcome between the treated and untreated groups.\n\n\n\nContinuous Treatment\nWhen the treatment a is continuous, the interpretation of \\beta_0 and \\beta_1 adjusts slightly:\n\n\\beta_0: represents the expected value of the outcome Y when the treatment a is at its reference value (often zero).\n\\beta_1: represents the expected change in the outcome for each unit increase in the treatment. In this case, \\beta_1 measures the gradient or slope of the relationship between the treatment and the outcome. For every one-unit increase in treatment, the outcome changes by \\beta_1 units, assuming all other factors remain constant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can we apply marginal structural models in subgroups?\n\n\nAssumptions\n\nModel assumptions: the treatment model is correctly specified.\nCausal assumptions: all confounders are appropriately controlled, positivity and consistency assumptions hold.\n\n\n\nCalculating Treatment Weights (Propensity Scores) and Confounding Control in Subgroups\nWe may often achieve greater balance when conducting weighted analyses in subgroups by estimating propensity scores within these subgroups. The propensity score $ e(L, G) $ is the conditional probability of receiving the exposure $ A = 1 $, given the covariates $ L $ and subgroup indicator $ G $. This is often modelled using logistic regression or other methods that ensure covariate balance We define the estimated propensity score as follows:\n\n\\hat{e} = P(A = 1 \\mid L, G) = f_A(L, G; \\theta_A)\n\nHere, $ f_A(L, G; _A) $ is the statistical model estimating the probability of exposure A = 1 given covariates L and subgroup G. We then calculate the weights for each individual, denoted v, using the estimated propensity score:\n\\theta_A encapsulates all the coefficients (parameters) in this model, including intercepts, slopes, and potentially other parameters depending on the model complexity (e.g., interaction terms, non-linear effects…etc).\nThese weights v depend on A and are calculated as the inverse of the propensity score for exposed individuals and as the inverse of $ 1- $ for unexposed individuals.\nPropensity scores are estimated separately within strata of the subgroup to control for potential confounding tailored to each subgroup. These weights v are specific to each individual in subgroup G. In the lab, we will clarify how to fit models to estimate contrasts for the causal effects within groups \\hat{\\delta}_{g}, \\hat{\\delta}_{g'}, etc., and how to obtain estimates for group-wise differences:\n\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y^a \\mid G=g] - \\hat{E}[Y^{a'} \\mid G=g] \\big)}^{\\hat{\\delta}_g} - \\overbrace{\\big( \\hat{E}[Y^{a'} \\mid G=g'] - \\hat{E}[Y^a \\mid G=g'] \\big)}^{\\hat{\\delta}_{g'}}\n\n\n\\hat{E}[Y^a \\mid G=g]: Estimated expected outcome when treatment a is applied to subgroup G=g.\n\\hat{E}[Y^{a'} \\mid G=g]: Estimated expected outcome when a different treatment or control a' is applied to the same subgroup G=g.\n\\hat{\\delta}_g: Represents the estimated treatment effect within subgroup G=g, computed as the difference in expected outcomes between treatment a and a' within this subgroup.\n\\hat{E}[Y^{a'} \\mid G=g']: Estimated expected outcome when treatment a' is applied to a different subgroup G=g'.\n\\hat{E}[Y^a \\mid G=g']: Estimated expected outcome when treatment a is applied to subgroup G=g'.\n\\hat{\\delta}_{g'}: Represents the estimated treatment effect within subgroup G=g', computed as the difference in expected outcomes between treatment a' and a within this subgroup.\n\\hat{\\gamma}: The overall measure calculated from your formula represents the difference in treatment effects between two subgroups, G=g and G=g'. It quantifies how the effect of switching between treatments a and a' differs across the two subgroups.\n\n\n\nConsiderations\n\nEstimation: to estimate the expected outcomes \\hat{E}[Y^a \\mid G] and \\hat{E}[Y^{a'} \\mid G], we require statistical models. If we use regression, we include interaction terms between treatment and subgroup indicators to directly estimate subgroup-specific treatment effects. Our use depends on correct model specification.\nConfidence intervals: we may compute confidence intervals for \\hat{\\gamma} using bootstrap, the delta method, or – in our excercises – simulation based methods.\nCausal assumptions: again, a causal interpretation of \\hat{\\gamma} relies on satisfying both causal assumptions and modelling assumptions. Here, we have described estimation using propensity scores."
  },
  {
    "objectID": "content/08-content.html#doubly-robust-estimation",
    "href": "content/08-content.html#doubly-robust-estimation",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Doubly Robust Estimation",
    "text": "Doubly Robust Estimation\nWe can combine regression-based estimation with propensity score estimation to obtain doubly robust estimation. I will walk you through the steps in the lab. The TL;DR is this: doubly robust estimation reduces reliance on correct model specification. If either the PS model or the regression model is correctly specified, the model will be unbiased – if the other causal inference assumptions are met.\nWe cannot know whether these assumptions are met, we will need to do a sensitivity analysis, the topic of next week.\nI’ll show you in lab how to employ simulation-based inference methods to compute standard errors and confidence intervals, following the approaches suggested by Greifer (2023)[]."
  },
  {
    "objectID": "content/08-content.html#readings",
    "href": "content/08-content.html#readings",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Readings:",
    "text": "Readings:\nNoah Griefer’s Software and Blogs: https://ngreifer.github.io/blog/subgroup-analysis-psm/"
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html",
    "href": "content/v_2_tempate_causal_estimation.html",
    "title": "Template: Causal Estimatation",
    "section": "",
    "text": "Answer the following:\n\nState the Question: is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nCausality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\nState how you will use time-series data to address causality.\nDefine your exposure.\nDefine your outcome(s)\nExplain how the the exposure and outcome is relevant to your question.\nDefine your causal estimand (see: lecture 9). Hint: it is ATE_g_risk difference = E[Y(1)-(0)|G,L], where G is your multiple-group indicator and L is your set of baseline confounders."
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#intoduction",
    "href": "content/v_2_tempate_causal_estimation.html#intoduction",
    "title": "Template: Causal Estimatation",
    "section": "",
    "text": "Answer the following:\n\nState the Question: is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nCausality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\nState how you will use time-series data to address causality.\nDefine your exposure.\nDefine your outcome(s)\nExplain how the the exposure and outcome is relevant to your question.\nDefine your causal estimand (see: lecture 9). Hint: it is ATE_g_risk difference = E[Y(1)-(0)|G,L], where G is your multiple-group indicator and L is your set of baseline confounders."
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#methods",
    "href": "content/v_2_tempate_causal_estimation.html#methods",
    "title": "Template: Causal Estimatation",
    "section": "Methods",
    "text": "Methods\n\nConsider any ethical implications.\nExplain the sample. Provide descriptive statistics\nDiscuss inclusion criteria.\nDiscuss how your sample relates to the “source population” (lecture 9.)\nExplain NZAVS measures. State the questions used in the items\nIn your own words describe how the data meet the following assumptions required for causal inference:\nPositivity: Can we intervene on the exposure at all levels of the covariates? Use the code I provided to test whether there is change in the exposure from the baseline in the source population(s)\nConsistency: Can I interpret what it means to intervene on the exposure?\nExchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? This requires stating baseline confounders and explaining how they may be related to both the exposure and outcome. As part of this, you must explain why the baseline measure of your exposure and outcome are included as potential confounders.\nNote: Unmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? (e.g. childhood exposures that are not measured).\nDraw a causal diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\nMeasurement Error: Have I described potential biases from measurement errors? Return to lecture 11.\nState that you do not have missing data in this synthetic dataset, but that ordinarily missing data would need to be handled.\nState what your estimator will be. Note I’ve given you the following text to modify:\n\n\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\n\n\nStep 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\n\n\nStep 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\n\n\nStep 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\n\n\nStep 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\n\n\nStep 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\n\n\nState what E-values are and how you will use them to clarify the risk of unmeasured confounding."
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#results",
    "href": "content/v_2_tempate_causal_estimation.html#results",
    "title": "Template: Causal Estimatation",
    "section": "Results",
    "text": "Results\n\nUse the scripts I have provided as a template for your analysis.\nPropensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations: e.g.: A ~ x1 + x2 + x3 + .... using logistic regression, all continuous predictors were transformed to z-scores\n\nWeightIt Package Utilisation: Explicitly mention the use of the ‘WeightIt’ package in R, including any specific options or parameters used in the propensity score estimation process (Greifer 2023).\nReport if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as ‘ebal’, ‘energy’, and ‘ps’.\nIf your exposure is continuous only the ‘energy’ option was used for propensity score estimation.\nSubgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\nCovariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting. The script will generate these plots.\nWeighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit. The script I gave you will generate this information\n\n\nExample:\n\nWe estimated propensity scores by fitting a model for the exposure A as it is predicted by the set of baseline covariates defined by L. Because we are interested in effect modification by group, we fit different propensity score models for within strata of G using the subgroup command of the WeightIt package. Thus the propensity score is the the probability of receiving a value of a treatment (A=a) conditional on the covariates L, and stratum within G. We compared balance using the following methods of weighting: “ebal” or entropy balancing, “energy” or energy balancing, and “ps” or traditional inverse probability of weighting balancing. Of these methods “ebal” performed the best. Table X and Figure Y present the results of the ebalancing method.\n\n\nInterpretation of Propensity Scores: we interpret the proposensity scores as yeilding good balance across the exposure conditions.\nOutcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation. Example\n\n\nWe fit a linear model using maximum likelihood estimation with the outcome Y predicted by the exposure A. We interacted the exposure with all baseline confounders L. Continuous baseline confounders were converted to z-scores, whereas categorical exposures were not. Also interacted with all baseline confounders was a term for the subgroup interactoin. This allowed uas to flexibily fit non-linearities for the modification of the effect of the exposure within levels of the levels of the cultural group strata of interest. We note that model coefficients have no interpretation in this context so are not reported. The remaining steps of Doubly-Robust estimation were performed as outlined in the Method section. We calculated confidence intervals and standard errors, using the clarify package in R, which relies on simulation based inference for these quantities of interest (Greifer et al. 2023)\n\n\nReport the causal estimates.\n\nATE contrasts for groups in setting the exposure to for each group in setting level A = a and A = a*\ndifferences between groups in the magnitude of the effects. (ATE_group 1 - ATE_group_2)\n\nReport the E-value: how sensitive are your results to unmeasured confounding? Hint I gave you code that will create a table for you: See here\n\ntable_depression &lt;- tab_ate_subgroup_rd(table_estimates_depression, delta = 1, sd = 1)"
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#discussion",
    "href": "content/v_2_tempate_causal_estimation.html#discussion",
    "title": "Template: Causal Estimatation",
    "section": "Discussion",
    "text": "Discussion\nMake sure to hit these points:\nConsider the following ideas about what to discuss in one’s findings. The order of exposition might be different.\n\nSummary of results: What did you find?\nInterpretation of E-values: Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\nCausal Effect Interpretation: What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\nComparison of Subgroups: Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\nUncertainty and Confidence Intervals: Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\nGeneralisability and Transportability: Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\nAssumptions and Limitations: Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\nTheoretical Relevance: How are these findings relevant to existing theories.\nReplication and Future Research: Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\nReal-world Implications: Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;!–  –&gt;\n&lt;!–  –&gt;\n&lt;!–  –&gt;\n&lt;!–  –&gt;"
  },
  {
    "objectID": "content/step-by-step-reporting-guide.html",
    "href": "content/step-by-step-reporting-guide.html",
    "title": "Template: Stratified Causal Estimatation",
    "section": "",
    "text": "Your step-by-step guide to reporting is as follows:"
  },
  {
    "objectID": "content/step-by-step-reporting-guide.html#intoduction",
    "href": "content/step-by-step-reporting-guide.html#intoduction",
    "title": "Template: Stratified Causal Estimatation",
    "section": "Intoduction",
    "text": "Intoduction\nAnswer the following:\n\nState the Question: is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nCausality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\nState how you will use time-series data to address causality.\nDefine your exposure.\nDefine your outcome(s)\nExplain how the the exposure and outcome is relevant to your question.\nDefine your causal estimand (see: lecture 8). Hint: it is ATE_g_risk difference = E[Y(1)-(0)|G,L], where G is your multiple-group indicator and L is your set of baseline confounders."
  },
  {
    "objectID": "content/step-by-step-reporting-guide.html#method",
    "href": "content/step-by-step-reporting-guide.html#method",
    "title": "Template: Stratified Causal Estimatation",
    "section": "Method",
    "text": "Method\n\nConsider any ethical implications.\nExplain the sample. Provide descriptive statistics\nDiscuss inclusion criteria.\nDiscuss how your sample relates to the “source population” (lecture 8.)\nExplain NZAVS measures. State the questions used in the items\nIn your own words describe how the data meet the following assumptions required for causal inference:\nPositivity: Can we intervene on the exposure at all levels of the covariates? Use the code I provided to test whether there is change in the exposure from the baseline in the source population(s)\nConsistency: Can I interpret what it means to intervene on the exposure?\nExchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? This requires stating baseline confounders and explaining how they may be related to both the exposure and outcome. As part of this, you must explain why the baseline measure of your exposure and outcome are included as potential confounders.\nNote: Unmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? (e.g. childhood exposures that are not measured).\nDraw a causal diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\nMeasurement Error: Have I described potential biases from measurement errors? Return to lecture 11.\nState how you will handle missing data\nState what your estimator will be. Note I’ve given you the following text to modify:\n\n\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\n\n\nStep 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\n\n\nStep 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\n\n\nStep 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\n\n\nStep 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\n\n\nStep 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\nOR if you decide to use machine learning:\n\nWe perform statistical estimation using semi-parametric Targeted Learning, specifically a Targeted Minimum Loss-based Estimation (TMLE) estimator. TMLE is a robust method that combines machine learning techniques with traditional statistical models to estimate causal effects while providing valid statistical uncertainty measures for these estimates (Van der Laan 2014; Laan and Gruber 2012).\n\nVan der Laan, Mark J. 2014. “Targeted Estimation of Nuisance Parameters to Obtain Valid Statistical Inference.” The International Journal of Biostatistics 10 (1): 29–57.\n\nLaan, Mark J van der, and Susan Gruber. 2012. “Targeted Minimum Loss Based Estimation of Causal Effects of Multiple Time Point Interventions.” The International Journal of Biostatistics 8 (1).\n\n\nTMLE operates through a two-step process that involves modelling both the outcome and treatment (exposure). Initially, TMLE employs machine learning algorithms to flexibly model the relationship between treatments, covariates, and outcomes. This flexibility allows TMLE to account for complex, high-dimensional covariate spaces efficiently without imposing restrictive model assumptions (Laan, Luedtke, and Dı́az 2014; Van Der Laan and Rose 2011, 2018). The outcome of this step is a set of initial estimates for these relationships.\n\nVan Der Laan, Mark J., and Sherri Rose. 2011. Targeted Learning: Causal Inference for Observational and Experimental Data. Springer Series in Statistics. New York, NY: Springer. https://link.springer.com/10.1007/978-1-4419-9782-1.\n\n———. 2018. Targeted Learning in Data Science: Causal Inference for Complex Longitudinal Studies. Springer Series in Statistics. Cham: Springer International Publishing. http://link.springer.com/10.1007/978-3-319-65304-4.\n\n\nThe second step of TMLE involves “targeting” these initial estimates by incorporating information about the observed data distribution to improve the accuracy of the causal effect estimate. TMLE achieves this precision through an iterative updating process, which adjusts the initial estimates towards the true causal effect. This updating process is guided by the efficient influence function, ensuring that the final TMLE estimate is as close as possible, given the measures and data, to the targeted causal effect while still being robust to model-misspecification in either the outcome or the treatment model (Laan, Luedtke, and Dı́az 2014).\n\nLaan, Mark J van der, Alexander R Luedtke, and Iván Dı́az. 2014. “Discussion of Identification, Estimation and Approximation of Risk Under Interventions That Depend on the Natural Value of Treatment Using Observational Data, by Jessica Young, Miguel Hernán, and James Robins.” Epidemiologic Methods 3 (1): 21–31.\n\n\nAgain, a central feature of TMLE is its double-robustness property. If either the treatment model or the outcome model is correctly specified, the TMLE estimator will consistently estimate the causal effect. Additionally, we used cross-validation to avoid over-fitting, following the pre-stated protocols in Bulbulia (2024b). The integration of TMLE and machine learning technologies reduces the dependence on restrictive modelling assumptions and introduces an additional layer of robustness. For further details of the specific targeted learning strategy we favour, see (Hoffman et al. 2022, 2023; Díaz et al. 2021). We perform estimation using the lmtp package (Williams and Díaz 2021). We used the superlearner library for semi-parametric estimation with the predefined libraries SL.ranger, SL.glmnet, and SL.xgboost (Polley et al. 2023; Chen et al. 2023; Wright and Ziegler 2017). We created graphs, tables and output reports using the margot package (Bulbulia 2024a).\n\n———. 2024b. “A Practical Guide to Causal Inference in Three-Wave Panel Studies.” PsyArXiv Preprints, February. https://doi.org/10.31234/osf.io/uyg3d.\n\nHoffman, Katherine L., Edward J. Schenck, Michael J. Satlin, William Whalen, Di Pan, Nicholas Williams, and Iván Díaz. 2022. “Comparison of a Target Trial Emulation Framework Vs Cox Regression to Estimate the Association of Corticosteroids with COVID-19 Mortality.” JAMA Network Open 5 (10): e2234425. https://doi.org/10.1001/jamanetworkopen.2022.34425.\n\nHoffman, Katherine L., Diego Salazar-Barreto, Kara E. Rudolph, and Iván Díaz. 2023. “Introducing Longitudinal Modified Treatment Policies: A Unified Framework for Studying Complex Exposures,” April. https://doi.org/10.48550/arXiv.2304.09460.\n\nDíaz, Iván, Nicholas Williams, Katherine L. Hoffman, and Edward J. Schenck. 2021. “Non-Parametric Causal Effects Based on Longitudinal Modified Treatment Policies.” Journal of the American Statistical Association. https://doi.org/10.1080/01621459.2021.1955691.\n\nWilliams, Nicholas T., and Iván Díaz. 2021. lmtp: Non-Parametric Causal Effects of Feasible Interventions Based on Modified Treatment Policies. https://doi.org/10.5281/zenodo.3874931.\n\nPolley, Eric, Erin LeDell, Chris Kennedy, and Mark van der Laan. 2023. SuperLearner: Super Learner Prediction. https://CRAN.R-project.org/package=SuperLearner.\n\nChen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2023. Xgboost: Extreme Gradient Boosting. https://CRAN.R-project.org/package=xgboost.\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01.\n\nBulbulia, J. A. 2024a. Margot: MARGinal Observational Treatment-Effects. https://doi.org/10.5281/zenodo.10907724.\n\n\n\nState what E-values are (lecture 9) and how you will use Evalues to clarify the risk of unmeasured confounding."
  },
  {
    "objectID": "content/step-by-step-reporting-guide.html#results",
    "href": "content/step-by-step-reporting-guide.html#results",
    "title": "Template: Stratified Causal Estimatation",
    "section": "Results",
    "text": "Results\n\nUse the scripts I have provided as a template for your analysis.\nPropensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations: e.g.: A ~ x1 + x2 + x3 + .... using logistic regression, all continuous predictors were transformed to z-scores\n\nWeightIt Package Utilisation: Explicitly mention the use of the ‘WeightIt’ package in R, including any specific options or parameters used in the propensity score estimation process (Greifer 2023).\nReport if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as ‘ebal’, ‘energy’, and ‘ps’.\nIf your exposure is continuous only the ‘energy’ option was used for propensity score estimation.\nSubgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\nCovariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting. The script will generate these plots.\nWeighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit. The script I gave you will generate this information\n\n\n\nGreifer, Noah. 2023. WeightIt: Weighting for Covariate Balance in Observational Studies.\nExample:\n\nWe estimated propensity scores by fitting a model for the exposure A as it is predicted by the set of baseline covariates defined by L. Because we are interested in effect modification by group, we fit different propensity score models for within strata of G using the subgroup command of the WeightIt package. Thus the propensity score is the the probability of receiving a value of a treatment (A=a) conditional on the covariates L, and stratum within G. We compared balance using the following methods of weighting: “ebal” or entropy balancing, “energy” or energy balancing, and “ps” or traditional inverse probability of weighting balancing. Of these methods “ebal” performed the best. Table X and Figure Y present the results of the ebalancing method.\n\n\nInterpretation of Propensity Scores: we interpret the proposensity scores as yeilding good balance across the exposure conditions.\nOutcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation. Example\n\n\nWe fit a linear model using maximum likelihood estimation with the outcome Y predicted by the exposure A. We interacted the exposure with all baseline confounders L. Continuous baseline confounders were converted to z-scores, whereas categorical exposures were not. Also interacted with all baseline confounders was a term for the subgroup, which was also interacted with the exposure and baseline covariates. This allowed uas to flexibily fit non-linearities for the modification of the effect of the exposure within levels of the cultural group strata of interest. We note that model coefficients have no interpretation in this context so are not reported. The remaining steps of Doubly-Robust estimation were performed as outlined in the Method section. We calculated confidence intervals and standard errors, using the clarify package in R, which relies on simulation based inference for these quantities of interest (Greifer et al. 2023)\n\nGreifer, Noah, Steven Worthington, Stefano Iacus, and Gary King. 2023. Clarify: Simulation-Based Inference for Regression Models. https://iqss.github.io/clarify/.\n\n\nReport the causal estimates.\n\nATE contrasts for groups in setting the exposure to for each group in setting level A = a and A = a*\ndifferences between groups in the magnitude of the effects. (ATE_group 1 - ATE_group_2)\n\nReport the E-value: how sensitive are your results to unmeasured confounding? Hint: see the code below. I’ve substantially automated this task."
  },
  {
    "objectID": "content/step-by-step-reporting-guide.html#discussion",
    "href": "content/step-by-step-reporting-guide.html#discussion",
    "title": "Template: Stratified Causal Estimatation",
    "section": "Discussion",
    "text": "Discussion\nMake sure to hit these points:\nConsider the following ideas about what to discuss in one’s findings. The order of exposition might be different.\n\nSummary of results: What did you find?\nInterpretation of E-values: Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\nCausal Effect Interpretation: What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\nComparison of Subgroups: Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\nUncertainty and Confidence Intervals: Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\nGeneralisability and Transportability: Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\nAssumptions and Limitations: Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\nTheoretical Relevance: How are these findings relevant to existing theories.\nReplication and Future Research: Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\nReal-world Implications: Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;!–  –&gt;\n&lt;!–  –&gt;\n&lt;!–  –&gt;\n&lt;!–  –&gt;"
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "",
    "text": "Note\n\n\n\nRequired - (Hernan and Robins 2024) Chapters 4-5 link\nOptional - (Tyler J. VanderWeele and Robins 2007a) link - (Tyler J. VanderWeele 2009) link"
  },
  {
    "objectID": "content/07-content.html#if-you-learning-nothing-else-from-this-course",
    "href": "content/07-content.html#if-you-learning-nothing-else-from-this-course",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "If you learning nothing else from this course…",
    "text": "If you learning nothing else from this course…\nTo answer a psychological question we must first ask it."
  },
  {
    "objectID": "content/07-content.html#causal-inference-begins-by-stating-a-counterfactual-contrast",
    "href": "content/07-content.html#causal-inference-begins-by-stating-a-counterfactual-contrast",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "Causal inference begins by stating a counterfactual contrast",
    "text": "Causal inference begins by stating a counterfactual contrast\nIn the simpliest case, we calculate the contrast between two or more “treatments” or equivalently “exposures”:\n\n\\text{Average Treatment Effect} = \\mathbb{E}[Y(a^*) -Y(a)]\n\n\nAfter Stating the Causal Interest, We State the Conditions Needed for Identifying This Effect from Data\nIn observational studies, the next step is to identify this effect by controlling for common causes of both the treatment and outcome (or proxies of such common causes). The regression standardization formula is:\n\n\\widehat{\\text{ATE}} = \\int \\left( \\mathbb{E}[Y(a^*) \\mid L] - \\mathbb{E}[Y(a) \\mid L] \\right) dP(L)\n\nWhere:\n\nStatistical Estimand: \\widehat{\\text{ATE}}, is the estimated average treatment effect.\n\\mathbb{E}[Y(a^*) \\mid L]: is the expected value (mean) of the outcome variable Y when the treatment a^* is applied, given the confounders L.\n\\mathbb{E}[Y(a) \\mid L]: is the expected value of Y when the treatment a is applied, given L.\n\\mathbb{E}[Y(a^*) \\mid L] - \\mathbb{E}[Y(a) \\mid L]: is the difference in the expected treatments, controlling for L.\n\\int (...) dP(L): dP(L) is an integral (remember high school math?). Integrals give us areas under curves. Here the area is a distribution of probabilities. To obtain valid inference, the treatment effect is computed over all the probability distribution of L in the population.\n\n\n\nSo, in the simplest possible terms\nUnderstanding causal inference is fundamentally about comparing what happened with what could have happened an average under different conditions or treatments in some population, while carefully accounting for factors that might confound or distort the comparison that interests us (i.e. that might give a false answer to the question we asked)."
  },
  {
    "objectID": "content/07-content.html#what-do-the-words-moderation-and-interaction-mean",
    "href": "content/07-content.html#what-do-the-words-moderation-and-interaction-mean",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "What do the words “Moderation” and “Interaction” mean?",
    "text": "What do the words “Moderation” and “Interaction” mean?\nWe here these words used freely in psychology. What do they mean? Causal inference allows us to be precise.\nLet’s set the term moderation to the side, and in its place consider “effect-modification.” Our task today is to use counterfactual notation to distinguish between two concepts:\n\nInteraction\nEffect-modification"
  },
  {
    "objectID": "content/07-content.html#interaction",
    "href": "content/07-content.html#interaction",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "Interaction",
    "text": "Interaction\nAn interaction is a joint intervention\nWhen assessing “interaction” we are interested in the combined effect of two interventions (two “treatments” or “exposures”).\nCall these treatments, A and B. Call the outcome Y.\nRecall that in potential outcomes terminology, we defined the potential outcome under treatment A = \\tilde{a}:\n\nY(\\tilde{a})\n\nAnd we identify Y(\\tilde{a}) from data using the consistency assumption: Y_i(\\tilde{a}) = (Y_i |A_i = \\tilde{a}), assuming that Y_i(\\tilde{a})\\coprod A|L, where, again, L donotes a set of measured covariates sufficient to insure no confounding (no common causes), or equivalently, no open backdoor path between A and Y.\nA joint intervention of two treatments, call them A and B, which may act on Y, requires that we extend our notation:\n\nY(\\tilde{a}, \\tilde{b})\n This is the effect on Y when A is set to \\tilde{a} amd B is set to \\tilde{b}.\nIn addition to requiring Y_i(\\tilde{a}) = (Y_i \\mid A_i = \\tilde{a}), assuming that Y_i(\\tilde{a}) \\coprod A \\mid L, we require that for all i \\in 1\\dots N individuals in the target population, both Y_i(\\tilde{b}) = (Y_i \\mid B_i = \\tilde{b}), and that Y_i(\\tilde{b}) \\coprod B \\mid Q, where Q denotes a set of confounders sufficient to ensure no confounding for the effect of B on Y.\nNotably, the sets L and Q may have overlapping variables, which in set notation we write:\n\nL \\cap Q \\not\\equiv \\emptyset\n\nThat is the union of L and Q does not imply the empty set, which implies that identification may be harder when there are interactions. Statistical estimation is more difficult too, but we’ll ignore that.\n\nHow do we define such a contrast between two interventions of equal status in our model?\nConsider the effect of beliefs in Big Gods (treatment, denoted A) on social complexity (outcome, denotedY), potentially influenced by a culture’s monumental architecture (treatment, denoted B). Suppose we wish to assess the individual and combined effects of A and B on the difference scale. Let assume the treatments are binary variables.\nIn the counterfactual framework, we say there is evidence for interaction only if the following inequality were to hold:\n\\bigg(\\underbrace{\\mathbb{E}[Y(1,1)]}_{\\text{joint exposure}} - \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither exposed}}\\bigg) - \\bigg[ \\bigg(\\underbrace{\\mathbb{E}[Y(1,0)]}_{\\text{only A exposed}} - \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither exposed}}\\bigg) + \\bigg(\\underbrace{\\mathbb{E}[Y(0,1)]}_{\\text{only B exposed}} - \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither exposed}} \\bigg)\\bigg] \\neq 0 \nExpanding the terms inside the brackets:\n \\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(1,0)] + \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)] \nAnd cancelling \\mathbb{E}[Y(0,0)], which appears three times, once with a minus sign and twice with a plus sign, simplifies to:\n \\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)] \nAnd we define interaction on the additive scale:\n \\underbrace{\\mathbb{E}[Y(1,1)]}_{\\text{joint exposure}} - \\underbrace{\\mathbb{E}[Y(1,0)]}_{\\text{only A exposed}} - \\underbrace{\\mathbb{E}[Y(0,1)]}_{\\text{only B exposed}} + \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither exposed}} \\neq 0 \nThe estimand is net interaction effect, after accounting for the individual effects of A and B, as well as the baseline where neither treatment A nor treatment B is given.\nA positive value would indicate evidence for additive interaction. A negative value would indicate evidence for sub-additive interaction. A value near zero would imply no reliable evidence for interaction.\nNote that the order of the terms does not matter:\n\nY(\\tilde{a}, \\tilde{b})\\equiv Y(\\tilde{b}, \\tilde{a})\n\n\nHaving defined the causal estimand, we next consider what would be needed to identify it using data?\nFor this we can write our causal diagram. Figure 1 describes the structure of the identification problem:\n\n\n\n\n\n\n\n\nFigure 1: This diagram presents the individual and joint effects of two exposures, A and B, on outcome Y. We assume that A and B are causally independent. If either exposure affects the other, then we may conduct effect-modification analysis or mediation analysis, but we should avoid causal interaction analysis. The diagram includes confounders L and W. Control for these confounders is necessary to close the backdoor paths that relate each exposure, A and B, to the outcome. Each exposure has equal status in our model: Y(a,b) = Y(b,a). The red path denotes paths of confounding.\n\n\n\n\n\nHere, Figure 1 clarifies the need to evaluate two sources of counfounding, one for each intervention (A and B).\nBy adjusting for L_{0} we obtain an unbiased estimate of the A\\to Y path.\nBy adjusting for Q_{0} we obtain an unbiased estimate of the B\\to Y path. As indicated in Figure 2, we must condition on both L_{0} and Q_{0} to identify causal interaction conceived as a joint interaction.\n\n\n\n\n\n\n\n\n\n\nFigure 2: We adjust for confounding in causal interaction analysis by adjusting for all confounders of the A to Y path as well as all for the B to Y path. The box over the confounders indicates the biasing paths are closed.\n\n\n\n\n\nConsider the following set of plausible confounders:\nL: variables in L that might confound the relationship between beliefs in Big Gods and social complexity:\n\nGeneral religiosity: prevalence of religion might influence both the belief in Big Gods and social complexity.\nCultural norms and values: ingrained social norms and values, not any part of religion.\nHistorical path dependent events: e.g. conquest.\nEducation: affects religious beliefs and the complexity.\nEconomic development: economic conditions might whether people prefer religious practices or resource intensive hobbies and also, social complexity.\nPolitical stability: stable governance can influence both the proliferation of religious practices and the development of complex social structures\nPolitical complexity in the past: huge confounder, must be adjusted for.\nBeliefs in big gods in the past: huge confounder, must be adjusted for.\n\nQ: variables in Q that that might confound the relationship between monumental architecture and social complexity:\n\nTechnology: for constructing of monumental architecture and managing social complexity.\nEconomic resources: as above (overlap)\nHistorical path dependent events: e.g. conquestm as above\nLabour: the availability of builders, slaves, etc.\nGeography: natural resources can affect a society’s ability to build monumental structures and its social development.\nCultural norms and values: ingrained social norms and values, not any part of religion.\nPolitical complexity in the past: huge confounder, must be adjusted for.\nBeliefs in big gods in the past: huge confounder, must be adjusted for.\n\nAs we see:\n\nL \\cap Q \\not\\equiv \\emptyset\n\nHowever, the set of control variables are larger, and note, we are assuming that monumental architecture and big God beliefs do not affect each other."
  },
  {
    "objectID": "content/07-content.html#effect-modification",
    "href": "content/07-content.html#effect-modification",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "Effect Modification",
    "text": "Effect Modification\nIt is often scientifically interesting to consider whether treatment effects vary over levels of other variable without imagining a double intervention. Broadly, we will think of “effect-modification” as a set of related and overlapping concepts pertaining to the understanding variability of a single effect within a population.\n\nHeterogeneous Treatment Effects – a broad concept of effect-modification\nHeterogeneous Treatment Effects (HTE) refer to settings where treatment effects vary across individuals or subgroups within a study population. This variability can arise from differences in baseline characteristics, environmental conditions, histories, …. much of which is unmeasured.\nAppendix A discusses the challenges in identifying HTE from data, but we already know that individual causal effects are not typically identifiable, and so heteroeneous treatment effects, at the limit of individual causal effects, are elusive.\n\n\nConditional Average Treatment Effect (CATE) – a narrower concept of effect-modification.\nWe might be interested estimating average treatment effects within specific levels of measured covariates\nWhen our focus is estimation of the average treatment effect conditional on a specific level of a covariate or set of covariates. Our causal estimand is often written:\n\\text{CATE}(x) = E[Y(1) - Y(0) | X = x]\nwhere X = x, where X denotes the level of a measured covariate or covariates of interest.\nThe question: how does a causal effect vary within the population defined by levels of stratum X=x\n\n\nComparing Effect Heterogenity in Groups.\nWe might be interested in the following causal quantity (estimand), which compares two conditional average treatment effects between levels defined by G = \\tilde{g}:\n{\\gamma} = \\overbrace{\\big( \\mathbb{E}[Y(a^*)|G=g] - \\mathbb{E}[Y(a)|G=g] \\big)}^{{\\delta_g}} - \\overbrace{\\big(\\mathbb{E}[Y(a^*)|G=g^{\\prime}]- \\mathbb{E}[Y(a)|G=g']\\big)}^{{\\delta_{g^{\\prime}}}}\nSuppose A is the treatment, G is the effect-modifier, and Y is the outcome. The analysis of effect-modification assesses whether the effect of A on Y is different across levels of G (i.e., whether the effect of A on Y is different when G = g_1 compared to when G = g_2).\nWhat do we need to identify such effects? Note that G here is not an intervention variable. It might not be coherent to imagine that G can be intervened upon.\nFigure 3 is a causal diagram. Imagine we are intereste in whether the effect of A on Y differs across levels of G. Because we are not interested in the causal effect of G as such, but rather, how the effect of A varies across G, we need not adjust by Q. However, which variables shall we include in our model? What will happen if we assess the G by examining a regression coefficient? Suppose L were not a confounder of A to Y. How then should we interpret our model?\n\n\n\n\n\n\n\n\nFigure 3: Imagine A is an experiment. How shall we investigate effect modification of A on Y by Z? Can you see why regression coefficients will not work?\n\n\n\n\n\nOften, it is useful to obtain causal effects by restricting to one level of a population. Where G denotes a society, consider:\n\nCausal effect within North American societies (G=1): \\delta_{g_1} = \\mathbb{E}[Y(1)|G= g_1] - \\mathbb{E}[Y(0)|G = g_1]\n\nHere, \\delta_{g_2} represents the estimated causal effect of changing the exposure from A = 0 to A = 1 within the North American societies.\n\nCausal effect within Continental societies (G=g_2):\n\\delta_{g2} = \\mathbb{E}[Y(1)|G=g_2] - \\mathbb{E}[Y(0)|G=g_2]\nSimilarly, {\\delta}_{g_2} denotes the estimated causal effect for the Continental societies.\nComparing causal effects across groups:\n{\\gamma} = {\\delta}_{g_1} - {\\delta}_{g_2}\n\nThe quantity {\\gamma} defines the difference in the causal estimands between the two groups. A nonzero \\hat{\\gamma} indicates effect-modification, suggesting that the effect of changing the exposure differs between the two groups. If we were to observe that \\hat{\\gamma} \\neq 0, this would provide evidence for variability in the effect of the exposure on the outcome in different groups. Note that the causal effect for one group might be indistinguishable from zero, and yet we might nevertheless find evidence for effect-modification if the comparison group exhibits reliably different responses from the contrast group that is indistinguishable from zero.\nThat’s it, we now know our causal quantity of interest. Next week, we’ll develop our statistical estimands, estimators, and begin evaluating effect-modification by groups.\n\n\nSummary of what we learned\n\nInteraction targets a joint intervention in a population\nEffect modificiation targets how a single intervention varies by groups within a population\n\nNote, to obtain the group-wise contrasts, we must be able to imagine the groups as belonging to a larger population. When thinking of people, we all belong Later we shall consider how measurement complicates this assumption, even if all humans belong to the same species…"
  },
  {
    "objectID": "content/07-content.html#lab-part-1-setting-up-your-data",
    "href": "content/07-content.html#lab-part-1-setting-up-your-data",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "Lab Part 1: Setting up your data",
    "text": "Lab Part 1: Setting up your data\n\nSet up your libraries\n\nlibrary(\"margot\")\nlibrary(\"tidyverse\")\nlibrary(\"ggplot2\")\nlibrary(\"here\")\nlibrary(\"skimr\")\nif (!require(gtsummary)) install.packages(\"gtsummary\")\nif (!require(janitor)) install.packages(\"janitor\")\n\n\n# if you need to update the margot package, uncomment and do this\n# devtools::install_github(\"go-bayes/margot\")\n#devtools::install_github(\"go-bayes/margot\")\n\n\n\nSet up a path to a folder in your directory\nThis will allow you to save the outputs of models and other information, which will be handy when you are producing your manuscript. Call this push_mods\n\n### Set up a path to a folder in your directory \n\n# create a folder called saved and make a path like this\npush_mods &lt;- here::here('/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/saved')\n\n# view it (will be different for you)\npush_mods\n\n[1] \"/Users/joseph/Library/CloudStorage/Dropbox-v-project/data/saved\"\n\n# another option\n# saveRDS(object, here::here(push_mods, \"object\"))\n\n\n\nInitial Data Wrangling to select the study sample\n\nWhere to find variable names information\nFind it the data directory here: https://osf.io/75snb/\nAlso see here: https://github.com/go-bayes/templates/tree/main/method\n\n\nThink, what are my eligibility criteria?\n\nParticipated in at baseline\nParticipated at treatment wave\nMay have been lost to follow up at the end of the study\nFull information on the treatment variable (think about this…)\n\n\nlibrary(margot)\n# eliminate haven labels\ndf_nz &lt;- as.data.frame(df_nz)\ndf_nz &lt;- haven::zap_formats(df_nz)\ndf_nz &lt;- haven::zap_label(df_nz)\ndf_nz &lt;- haven::zap_widths(df_nz)\n\n# name output folder\npush_mods &lt;- here::here(\"outputs\")\n\n# set exposure name\nname_exposure &lt;-  \"perfectionism\"\n\n# obtain ids for individuals who participated in 2018 and have no missing baseline exposure\nids_2018 &lt;- df_nz %&gt;%\n   dplyr::filter(year_measured == 1, wave == 2018) %&gt;%\n   dplyr::filter(!is.na(!!sym(name_exposure))) |&gt; # criteria, no missing\n  pull(id)\n\n# obtain ids for individuals who participated in 2019\nids_2019 &lt;- df_nz %&gt;%\n   dplyr::filter(year_measured == 1, wave == 2019) %&gt;%\n   dplyr::filter(!is.na(!!sym(name_exposure))) |&gt; # criteria, no missing\n  pull(id)\n\n# intersect IDs from 2018 and 2019 to ensure participation in both years\nids_2018_2019 &lt;- intersect(ids_2018, ids_2019)\n\n# data wrangling\ndat_long &lt;- df_nz |&gt;\n  dplyr::filter(id %in% ids_2018_2019 &\n                  wave %in% c(2018, 2019, 2020)) |&gt;\n  arrange(id, wave) |&gt;\n  select(\n    \"id\",\n    \"wave\",\n    \"year_measured\",\n    \"age\",\n    \"male\",\n    \"born_nz\",\n    \"eth_cat\",\n    #factor(EthCat, labels = c(\"Euro\", \"Maori\", \"Pacific\", \"Asian\")),\n    \"employed\",\n    # Are you currently employed? (this includes self-employment or casual work)\n    \"edu\",\n    # \"gen_cohort\",\n    \"household_inc\",\n    \"partner\",\n    # 0 = no, 1 = yes\n    \"parent\",\n    \"alert_level_combined_lead\", # see bibliography\n    # 0 = no, 1 = yes\n    \"political_conservative\", # see nzavs sheet\n    \"hours_exercise\", # see nzavs sheet\n    \"agreeableness\", \n    # Mini-IPIP6 Agreeableness (also modelled as empathy facet)\n    # Sympathize with others' feelings.\n    # Am not interested in other people's problems.\n    # Feel others' emotions.\n    # Am not really interested in others.\n    \"conscientiousness\",\n    # see mini ipip6\n    # Get chores done right away.\n    # Like order.\n    # Make a mess of things.\n    # Often forget to put things back in their proper place.\n    \"extraversion\",\n    # Mini-IPIP6 Extraversion\n    # Am the life of the party.\n    # Don't talk a lot.\n    # Keep in the background.\n    # Talk to a lot of different people at parties.\n    \"honesty_humility\",\n    # see mini ipip6\n    # Would like to be seen driving around in a very expensive car.\n    # Would get a lot of pleasure from owning expensive luxury goods.\n    # Feel entitled to more of everything.\n    # Deserve more things in life.\n    \"openness\",\n    # see mini ipip6\n    # Have a vivid imagination.\n    # Have difficulty understanding abstract ideas.\n    # Do not have a good imagination.\n    # Am not interested in abstract ideas.\n    \"neuroticism\",\n    # see mini ipip6\n    # Have frequent mood swings.\n    # Am relaxed most of the time.\n    # Get upset easily.\n    # Seldom feel blue.\n    \"modesty\",\n    # # see mini ipip6\n    # # I want people to know that I am an important person of high status,\n    # # I am an ordinary person who is no better than others.\n    # # I wouldn’t want people to treat me as though I were superior to them.\n    # # I think that I am entitled to more respect than the average person is\n    #\"w_gend_age_ethnic\",\n    \"sample_weights\", # see nzavs sheet\n    \"neighbourhood_community\",\n    # #I feel a sense of community with others in my local neighbourhood.\n    \"belong\", # see nzavs sheet\n    \"rural_gch_2018_l\",# see nzavs sheet\n    \"support\",\n    # \"support_help\",\n    # # 'There are people I can depend on to help me if I really need it.\n    # \"support_turnto\",\n    # # There is no one I can turn to for guidance in times of stress.\n    # \"support_rnoguidance\",\n    #There is no one I can turn to for guidance in times of stress.\n    \"perfectionism\",\n    \"religion_religious\",\n    \"kessler_latent_depression\",\n    \"kessler_latent_anxiety\"\n  ) |&gt;\n  mutate(\n    #initialize 'censored'\n    censored = ifelse(lead(year_measured) == 1, 1, 0),\n    \n    # modify 'censored' based on the condition; no need to check for NA here as 'censored' is already defined in the previous step\n    censored =  ifelse(is.na(censored) &\n                         year_measured == 1, 1, censored)\n  ) |&gt;\n  select(-year_measured) |&gt;\n  dplyr::mutate(\n    # rescale these variables, to get all variables on a similar scale\n    # otherwise your models can blow up, or become uninterpretable. \n    household_inc_log = log(household_inc + 1),\n    hours_exercise_log = log(hours_exercise + 1)  ) |&gt;\n  dplyr::select(\n    -c(\n      household_inc,\n      hours_exercise)\n  ) |&gt;\n  droplevels() |&gt;\n  # dplyr::rename(sample_weights = w_gend_age_ethnic,\n  #               sample_origin =  sample_origin_names_combined) |&gt;\n  arrange(id, wave) |&gt;\n  mutate(\n  rural_gch_2018_l = as.numeric(as.character(rural_gch_2018_l)),\n  #   parent = as.numeric(as.character(parent)),\n  partner = as.numeric(as.character(partner)),\n  born_nz = as.numeric(as.character(born_nz)),\n  censored = as.numeric(as.character(censored)),\n  employed = as.numeric(as.character(employed))\n  ) |&gt;\n  droplevels() |&gt;\n  data.frame() |&gt;\n  droplevels() |&gt;\n  arrange(id, wave) |&gt;\n  data.frame()\n\n# check n in this sample\nn_participants &lt;- skimr::n_unique(dat_long$id)\n\n\n# make number pretty\nn_participants&lt;- prettyNum(n_participants,big.mark=\",\")\n\n# save this so that you can use it in your manuscript\nmargot::here_save(n_participants, \"n_participants\")\n\nObject saved to: /Users/joseph/GIT/psych-434-2025/outputs/n_participants.rds\nObject size: 0.00 MB\n👍 Save operation completed successfully!\n\n# try reading \nn_participants_did_it_work_question_mark &lt;- margot::here_read(\"n_participants\")\n\nObject read from: /Users/joseph/GIT/psych-434-2025/outputs/n_participants.rds\nObject size: 0.00 MB\n👍 Read operation completed successfully!\n\n# view\n# n_participants_did_it_work_question_mark\n\nThere are N = 14,439 participants.\n\n\n\nDefine your baseline covariates, treatment (exposure), and outcome\n\n# for example \nbaseline_vars = c(\"age\", \"male\", \"edu\", \"eth_cat\", \"partner\", \"employed\", \"born_nz\", \"neighbourhood_community\", \"household_inc_log\",\n\"parent\", \"religion_religious\", \"rural_gch_2018_l\",\"sample_weights\", \"employed\", \"alert_level_combined_lead\")\n\n# treatment\nexposure_var = c(\"perfectionism\", \"censored\") # we will use the censored variable later\n\n# outcome, can be many\noutcome_vars = c(\"kessler_latent_anxiety\", \"kessler_latent_depression\")\n\n\n\nMake your baseline table\n\nlibrary(gtsummary)\n# the setdiff command allows us to remove names from the baseline vars list that we do not want\nbase_var &lt;-\n  setdiff(baseline_vars, c(\"censored\", \"sample_weights\", \"alert_level_combined_lead\", outcome_vars))\n\n\n#  we only want the data at baseline \n\n# select only baseline wave\ndt_18 &lt;- dat_long |&gt; \n  dplyr::filter(wave == 2018)\n\n\n# get only the baseline variables for the table\nselected_base_cols &lt;-\n  dt_18 |&gt; select(all_of(base_var))\n\n#check\ncolnames_use &lt;- colnames(selected_base_cols)\ncolnames_use\n\n [1] \"age\"                     \"male\"                   \n [3] \"edu\"                     \"eth_cat\"                \n [5] \"partner\"                 \"employed\"               \n [7] \"born_nz\"                 \"neighbourhood_community\"\n [9] \"household_inc_log\"       \"parent\"                 \n[11] \"religion_religious\"      \"rural_gch_2018_l\"       \n\n# # aside to get different names in lists\n# everything_better_cols &lt;- c(colnames_use, \"inkuk\")\n# everything_better_cols\n# \n# # get rid of name\n# everything_worse_cols &lt;- setdiff(everything_better_cols, \"inkuk\")\n\n# baseline table\nlibrary(gtsummary)\n\n\n# this is just a demo of another package, which can be useful for quick tables, less for your manuscripts\nno_miss_df &lt;- df_nz |&gt; \n  drop_na()\n\ntable1::table1( ~ perfectionism  | wave * eth_cat ,\n                data = no_miss_df,\n                overall = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018\n\n\n2020\n\n\n\n\neuro\n(N=6165)\nmaori\n(N=717)\npacific\n(N=156)\nasian\n(N=305)\neuro\n(N=4690)\nmaori\n(N=493)\npacific\n(N=95)\nasian\n(N=226)\n\n\n\n\nperfectionism\n\n\n\n\n\n\n\n\n\n\nMean (SD)\n3.06 (1.32)\n3.15 (1.34)\n3.22 (1.31)\n3.47 (1.26)\n2.94 (1.33)\n3.12 (1.44)\n3.20 (1.34)\n3.36 (1.37)\n\n\nMedian [Min, Max]\n2.97 [1.00, 7.00]\n3.00 [1.00, 7.00]\n3.04 [1.00, 6.97]\n3.35 [1.00, 7.00]\n2.69 [1.00, 7.00]\n2.98 [1.00, 7.00]\n3.00 [1.00, 6.69]\n3.34 [1.00, 6.64]\n\n\n\n\n\n\ntable_baseline &lt;- selected_base_cols |&gt; \n  janitor::clean_names(case = \"title\") |&gt; \n  tbl_summary(\n    missing = \"ifany\",\n    percent = \"column\",\n    statistic = list(\n      all_continuous() ~ c(\n        \"{mean} ({sd})\", # Mean and SD\n        \"{min}, {max}\", # Range (Min, Max)\n        \"{p25}, {p75}\" # IQR (25th percentile, 75th percentile)\n      )\n    ),\n    type = all_continuous() ~ \"continuous2\"\n  ) |&gt;\n  modify_header(label = \"**Exposure + Demographic Variables**\") |&gt; # update the column header\n  bold_labels() \n\n\n# baseline\ntable_baseline\n\n\n\n\n\n\n\n\n\n\n\nExposure + Demographic Variables\nN = 14,4391\n\n\n\n\nAge\n\n\n\n\n    Mean (SD)\n50 (14)\n\n\n    Min, Max\n18, 94\n\n\n    Q1, Q3\n41, 61\n\n\nMale\n5,238 (36%)\n\n\nEdu\n\n\n\n\n    Mean (SD)\n5.38 (2.72)\n\n\n    Min, Max\n0.00, 10.00\n\n\n    Q1, Q3\n3.00, 7.00\n\n\n    Unknown\n110\n\n\nEth Cat\n\n\n\n\n    euro\n11,835 (83%)\n\n\n    maori\n1,526 (11%)\n\n\n    pacific\n314 (2.2%)\n\n\n    asian\n655 (4.6%)\n\n\n    Unknown\n109\n\n\nPartner\n10,661 (76%)\n\n\n    Unknown\n399\n\n\nEmployed\n11,466 (80%)\n\n\n    Unknown\n22\n\n\nBorn Nz\n11,398 (79%)\n\n\n    Unknown\n25\n\n\nNeighbourhood Community\n\n\n\n\n    Mean (SD)\n4.23 (1.66)\n\n\n    Min, Max\n1.00, 7.00\n\n\n    Q1, Q3\n2.99, 5.95\n\n\n    Unknown\n73\n\n\nHousehold Inc log\n\n\n\n\n    Mean (SD)\n11.41 (0.74)\n\n\n    Min, Max\n0.71, 14.40\n\n\n    Q1, Q3\n11.00, 11.92\n\n\n    Unknown\n655\n\n\nParent\n10,350 (72%)\n\n\n    Unknown\n8\n\n\nReligion Religious\n5,218 (36%)\n\n\n    Unknown\n19\n\n\nRural Gch 2018 l\n\n\n\n\n    1\n8,822 (62%)\n\n\n    2\n2,784 (19%)\n\n\n    3\n1,740 (12%)\n\n\n    4\n820 (5.7%)\n\n\n    5\n170 (1.2%)\n\n\n    Unknown\n103\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n# save your baseline table\nmargot::here_save(table_baseline, \"table_baseline\")\n\nObject saved to: /Users/joseph/GIT/psych-434-2025/outputs/table_baseline.rds\nObject size: 0.00 MB\n👍 Save operation completed successfully!\n\n\n\n\nMake exposure wave table\n\n# get first and second wave\ndt_18_19 &lt;- dat_long |&gt; \n  dplyr::filter(wave == 2018 | wave == 2019) |&gt; \n  # we need to drop unused levels of the wave\n  droplevels()\n\n# get vars.\nselected_exposure_cols &lt;-\n  dt_18_19 %&gt;% select(\n    c(\n      \"perfectionism\",\n      \"wave\"\n    )\n  )\n\n# check\n#str(selected_exposure_cols)\n\n\nlibrary(gtsummary)\n\ntable_exposures &lt;- selected_exposure_cols %&gt;%\n  janitor::clean_names(case = \"title\") %&gt;% \n  labelled::to_factor() %&gt;%  # ensure consistent use of pipe operator\n  tbl_summary(\n    by = \"Wave\",  #specify the grouping variable. Adjust \"Wave\" to match the cleaned column name\n    missing = \"always\", \n    percent = \"column\",\n    # statistic = list(all_continuous() ~ \"{mean} ({sd})\")  # Uncomment and adjust if needed for continuous variables\n  ) %&gt;%\n  #  add_n() %&gt;%  # Add column with total number of non-missing observations\n  modify_header(label = \"**Exposure Variables by Wave**\") %&gt;%  # Update the column header\n  bold_labels()\n\ntable_exposures\n\n\n\n\n\n\n\n\n\n\n\n\nExposure Variables by Wave\n2018 N = 14,4391\n2019 N = 14,4391\n\n\n\n\nPerfectionism\n3.00 (2.01, 4.02)\n2.99 (2.00, 4.02)\n\n\n    Unknown\n0\n0\n\n\n\n1 Median (Q1, Q3)\n\n\n\n\n\n\n\n# save baseline\nhere_save(table_exposures, \"table_exposures\")\n\nObject saved to: /Users/joseph/GIT/psych-434-2025/outputs/table_exposures.rds\nObject size: 0.00 MB\n👍 Save operation completed successfully!\n\ntable_exposures\n\n\n\n\n\n\n\n\n\n\n\n\nExposure Variables by Wave\n2018 N = 14,4391\n2019 N = 14,4391\n\n\n\n\nPerfectionism\n3.00 (2.01, 4.02)\n2.99 (2.00, 4.02)\n\n\n    Unknown\n0\n0\n\n\n\n1 Median (Q1, Q3)\n\n\n\n\n\n\n\n\n\n\nTable for the outcome\n\n# outcome table -----------------------------------------------------------\ndt_18_20 &lt;- dat_long |&gt; \n  dplyr::filter(wave == 2018 | wave == 2020) |&gt; \n  droplevels()\n\nnames_outcomes_tab &lt;- setdiff(outcome_vars, dt_18_20)\nnames_outcomes_sorted &lt;- sort(names_outcomes_tab)\nnames_outcomes_final &lt;- names_outcomes_sorted # consistent workflow\n\n# better names if desirable\nselected_outcome_cols &lt;-\n  dt_18_20 %&gt;% select(all_of(names_outcomes_final),\n                      wave) \n# |&gt; # example if you want to rename your variables for the table\n#   rename(\n#     Social_belonging = belong,\n#     Annual_charity = charity_donate,\n#     Volunteering_hours = hours_charity,\n#     Community_gives_money_binary = community_money_binary,\n#     Community_gives_time_binary = community_time_binary,\n#     Family_gives_money_binary = family_money_binary,\n#     Family_gives_time_binary = family_time_binary,\n#     Friends_give_money_binary = friends_money_binary,\n#     Friends_give_time = friends_time_binary,\n#     Social_support = support,\n#     Sense_neighbourhood_community = neighbourhood_community\n#   )\n\n# order names correctly\nselected_outcome_cols &lt;- selected_outcome_cols %&gt;%\n  select(sort(names(selected_outcome_cols)))\n\n# checks\n# str(selected_outcome_cols)\n# colnames(selected_outcome_cols)\n\ntable_outcomes &lt;- selected_outcome_cols %&gt;%\n  janitor::clean_names(case = \"title\") %&gt;% \n  labelled::to_factor() %&gt;%  # ensure consistent use of pipe operator\n  tbl_summary(\n    by = \"Wave\",  #specify the grouping variable. Adjust \"Wave\" to match the cleaned column name\n    missing = \"always\", \n    percent = \"column\",\n    # statistic = list(all_continuous() ~ \"{mean} ({sd})\")  # Uncomment and adjust if needed for continuous variables\n  ) %&gt;%\n  #  add_n() %&gt;%  # Add column with total number of non-missing observations\n  modify_header(label = \"**Outcome Variables by Wave**\") %&gt;%  # Update the column header\n  bold_labels()\n\n\n# save\nmargot::here_save(table_outcomes, \"table_outcomes\")\n\nObject saved to: /Users/joseph/GIT/psych-434-2025/outputs/table_outcomes.rds\nObject size: 0.00 MB\n👍 Save operation completed successfully!\n\n# read if needed\ntable_outcomes &lt;- margot::here_read(\"table_outcomes\")\n\nObject read from: /Users/joseph/GIT/psych-434-2025/outputs/table_outcomes.rds\nObject size: 1.44 MB\n👍 Read operation completed successfully!\n\ntable_outcomes\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome Variables by Wave\n2018 N = 14,4391\n2020 N = 14,4391\n\n\n\n\nKessler Latent Anxiety\n1.04 (0.65, 1.67)\n1.03 (0.65, 1.67)\n\n\n    Unknown\n148\n2,893\n\n\nKessler Latent Depression\n0.31 (0.01, 0.96)\n0.31 (0.01, 0.96)\n\n\n    Unknown\n152\n2,890\n\n\n\n1 Median (Q1, Q3)\n\n\n\n\n\n\n\n\n\n\nInspect the distribution of the exposure in treatment wave\n\n# select 2019 wave\ndt_19 &lt;- dat_long |&gt; dplyr::filter(wave == 2019)\n\n# mean\nmean_exposure &lt;-mean(dt_19$perfectionism, na.rm=TRUE)\n\n# median\nmedian_exposure &lt;-median(dt_19$perfectionism, na.rm=TRUE)\n\n# check if you like\n# median_exposure\n# mean_exposure\n\n# # generate bar plot\ngraph_density_of_exposure_up &lt;- margot::coloured_histogram_shift(\n  dt_19,\n  shift = \"up\",\n  col_name = \"perfectionism\",\n  binwidth = .25, \n  range_highlight = c(0,mean_exposure)\n)\n\n# show\ngraph_density_of_exposure_up\n\n\n\n\n\n\n\n# save\n#margot::here_save(graph_density_of_exposure_up, \"graph_density_of_exposure_up\")\n\n\n\nAnother Graph: Shift Interventions Up\n\nmargot::coloured_histogram(\n  dt_19,\n  col_name = \"perfectionism\",\n  binwidth = .25,\n  unit_of_change = 1,\n  scale_min = 1,\n  scale_max = 7,\n  highlight_range = \"both\"\n)\n\n\n\n\n\n\n\n\n\n\nCheck for change in the treatment (Positivity )\n\n#  select data from wave 18 and 19 \ndt_18_19_positivity &lt;- dat_long |&gt;\n  dplyr::filter(wave == 2018 | wave == 2019) |&gt;\n  dplyr::mutate(perfectionism_round = round(perfectionism, digits = 0)) |&gt;\n  dplyr::select(perfectionism_round, id, wave) |&gt;\n  droplevels()\n\nout &lt;-margot::create_transition_matrix(data = dt_18_19_positivity, state_var = \"perfectionism_round\", id_var = \"id\")\n\n\n# t_tab_2_labels &lt;- c(\"&lt; weekly\", \"&gt;= weekly\")\n# transition table\ntransition_table  &lt;- margot::transition_table(out)\n\n# for import later\n# margot::here_save(transition_table, \"transition_table\")\n\n\nprint(transition_table$table)\n\n\n\n|  From   | State 1 | State 2  | State 3  | State 4  | State 5 | State 6 | State 7 |\n|:-------:|:-------:|:--------:|:--------:|:--------:|:-------:|:-------:|:-------:|\n| State 1 | **893** |   484    |   194    |    40    |   16    |    3    |    2    |\n| State 2 |   657   | **1737** |   904    |   283    |   78    |    9    |    1    |\n| State 3 |   237   |   1073   | **1368** |   768    |   245   |   40    |    5    |\n| State 4 |   66    |   335    |   803    | **1076** |   523   |   108   |   10    |\n| State 5 |   24    |    77    |   253    |   531    | **579** |   223   |   26    |\n| State 6 |    7    |    9     |    38    |   106    |   205   | **216** |   53    |\n| State 7 |    2    |    1     |    5     |    8     |   25    |   45    | **48**  |\n\n\n\n\nExplanation for the table\n\ntransition_table$explanation\n\n[1] \"This transition matrix captures shifts in states across across the treatment intervals. Each cell in the matrix represents the count of individuals transitioning from one state to another. The rows correspond to the treatment at baseline (From), and the columns correspond to the state at the following wave (To). **Diagonal entries** (in **bold**) correspond to the number of individuals who remained in their initial state across both waves. **Off-diagonal entries** correspond to the transitions of individuals from their baseline state to a different state in the treatment wave.\\nA higher number on the diagonal relative to the off-diagonal entries in the same row indicates greater stability in a state. Conversely, higher off-diagonal numbers suggest more frequent shifts from the baseline state to other states.\""
  },
  {
    "objectID": "content/07-content.html#lab-part-2-simulation-to-clarify-why-the-distribution-of-effect-modifiers-matter-for-estimating-treatment-effects",
    "href": "content/07-content.html#lab-part-2-simulation-to-clarify-why-the-distribution-of-effect-modifiers-matter-for-estimating-treatment-effects",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "Lab Part 2: Simulation to Clarify Why The Distribution of Effect Modifiers Matter For Estimating Treatment Effects",
    "text": "Lab Part 2: Simulation to Clarify Why The Distribution of Effect Modifiers Matter For Estimating Treatment Effects\nFirst, we load the stdReg library, which obtains marginal effect estimates by simulating counterfactuals under different levels of treatment (Sjölander 2016). If a treatment is continuous, the levels can be specified.\n\nSjölander, Arvid. 2016. “Regression Standardization with the R Package stdReg.” European Journal of Epidemiology 31 (6): 563–74. https://doi.org/10.1007/s10654-016-0157-3.\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, and Dominique Makowski. 2020. “Extracting, Computing and Exploring the Parameters of Statistical Models Using R.” Journal of Open Source Software 5 (53): 2445. https://doi.org/10.21105/joss.02445.\nWe also load the parameters library, which creates nice tables (Lüdecke et al. 2020).\n\n# to obtain marginal effects\nlibrary(stdReg)\n# to create nice tables\nlibrary(parameters)\n\nNext, we write a function to simulate data for the sample and and target populations.\nWe assume the treatment effect is the same in the sample and target population. We will assume that the coefficient for the effect-modifier and the coefficient for interaction are the same. We assume no unmeasured confounding throughout the study. We assume only selective attrition of one effect modifier such that the baseline population differs from the sample population at the end of the study.\nThat is: the distribution of effect modifiers is the only respect in which the sample will differ from the target population.\nThis function will generate data under a range of scenarios.1\n1 See documentation in the margot package: Bulbulia (2024)\nBulbulia, J. A. 2024. Margot: MARGinal Observational Treatment-Effects. https://doi.org/10.5281/zenodo.10907724.\n\n# function to generate data for the sample and population, \n# along with precise sample weights for the population, there are differences \n# in the distribution of the true effect modifier but no differences in the treatment effect \n# or the effect modification.all that differs between the sample and the population is \n# the distribution of effect-modifiers.\n\n\n# reproducability\nset.seed(123)\n\n# simulate the data -- you can use different parameters\ndata &lt;- margot::simulate_ate_data_with_weights(\n  n_sample = 10000,\n  n_population = 100000,\n  p_z_sample = 0.1,\n  p_z_population = 0.5,\n  beta_a = 1,\n  beta_z = 2.5,\n  noise_sd = 0.5\n)\n\nskimr::skim(data)\n\n\n\n\n\nName\ndata\n\n\nNumber of rows\n100000\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nsample_data.y_sample\n0\n1\n0.74\n1.08\n-1.71\n0.01\n0.59\n1.21\n5.21\n▂▇▃▁▁\n\n\nsample_data.a_sample\n0\n1\n0.49\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nsample_data.z_sample\n0\n1\n0.09\n0.29\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nsample_data.weights\n0\n1\n0.98\n1.30\n0.56\n0.56\n0.56\n0.56\n5.00\n▇▁▁▁▁\n\n\npopulation_data.y_population\n0\n1\n1.88\n1.60\n-2.09\n0.51\n1.76\n3.26\n6.01\n▁▇▆▆▁\n\n\npopulation_data.a_population\n0\n1\n0.50\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n▇▁▁▁▇\n\n\npopulation_data.z_population\n0\n1\n0.50\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n▇▁▁▁▇\n\n\n\n\n\nOk, we have generated both sample and population data.\nNext, we verify that the distributions of effect modifiers differ in the sample and in the target population:\n\n# obtain the generated data\nsample_data &lt;- data$sample_data\npopulation_data &lt;- data$population_data\n\n\n# check imbalance\ntable(sample_data$z_sample) # type 1 is rare\n\n\n   0    1 \n9055  945 \n\ntable(population_data$z_population) # type 1 is common\n\n\n    0     1 \n49916 50084 \n\n\nGood, the distributions differ. The simulation is working as intended.\nNext, consider the question: “What are the differences in the coefficients that we obtain from the study population at the end of study, as compared with the target population?”\nFirst, we obtain the coefficients for the sample. They are as follows:\n\n# model coefficients sample\nmodel_sample  &lt;-\n  glm(y_sample ~ a_sample * z_sample, data = sample_data)\n\n# summary\nparameters::model_parameters(model_sample, ci_method = \"wald\")\n\nParameter           | Coefficient |       SE |        95% CI | t(9996) |      p\n-------------------------------------------------------------------------------\n(Intercept)         |   -6.89e-03 | 7.38e-03 | [-0.02, 0.01] |   -0.93 | 0.350 \na sample            |        1.01 |     0.01 | [ 0.99, 1.03] |   95.84 | &lt; .001\nz sample            |        2.47 |     0.02 | [ 2.43, 2.52] |  104.09 | &lt; .001\na sample × z sample |        0.51 |     0.03 | [ 0.44, 0.57] |   14.82 | &lt; .001\n\n\nOk, let’s obtain the coefficients for the weighted regression of the sample. Notice that the coefficients are virtually the same:\n\n# model the sample weighted to the population, again note that these coefficients are similar \nmodel_weighted_sample &lt;-\n  glm(y_sample ~  a_sample  * z_sample,\n      data = sample_data,\n      weights = weights)\n\n# summary\nsummary(parameters::model_parameters(model_weighted_sample, ci_method =\n                                       \"wald\"))\n\nParameter           | Coefficient |        95% CI |      p\n----------------------------------------------------------\n(Intercept)         |   -6.89e-03 | [-0.03, 0.01] | 0.480 \na sample            |        1.01 | [ 0.98, 1.04] | &lt; .001\nz sample            |        2.47 | [ 2.45, 2.50] | &lt; .001\na sample × z sample |        0.51 | [ 0.47, 0.55] | &lt; .001\n\nModel: y_sample ~ a_sample * z_sample (10000 Observations)\nSigma: 0.494 (df = 9996)\n\n\nWe might be tempted to infer that weighting wasn’t relevant to the analysis. However, we’ll see that such an interpretation would be a mistake.\nNext, let us obtain model coefficients for the population. Note again there is no difference – only narrower errors owing to the large sample size.\n\n# model coefficients population -- note that these coefficients are very similar. \nmodel_population &lt;-\n  glm(y_population ~ a_population * z_population, data = population_data)\n\nparameters::model_parameters(model_population, ci_method = \"wald\")\n\nParameter                   | Coefficient |       SE |        95% CI | t(99996) |      p\n----------------------------------------------------------------------------------------\n(Intercept)                 |    2.49e-03 | 3.18e-03 | [ 0.00, 0.01] |     0.78 | 0.434 \na population                |        1.00 | 4.49e-03 | [ 0.99, 1.01] |   222.35 | &lt; .001\nz population                |        2.50 | 4.49e-03 | [ 2.49, 2.51] |   556.80 | &lt; .001\na population × z population |        0.50 | 6.35e-03 | [ 0.49, 0.51] |    78.80 | &lt; .001\n\n\nAgain, there is no difference. That is, we find that all model coefficients are practically equivalent. The different distribution of effect modifiers does not result in different coefficient values for the treatment effect, the effect-modifier “effect,” or the interaction of effect modifier and treatment.\nConsider why this is the case: in a large sample where the causal effects are invariant – as we have simulated them to be – we will have good replication in the effect modifiers within the sample, so our statistical model can recover the coefficients for the population – no problem.\nHowever, in causal inference, we are interested in obtaining the marginal effect of the treatment. That is, we seek an estimate for the counterfactual contrast in which everyone in a pre-specified population was subject to one level of treatment compared with a counterfactual condition in which everyone in a population was subject to another level of the same treatment. When the sample population differs in the distribution of effect modifiers from the target population effect, the marginal effect estimates will typically differ.\nTo see this, we use the stdReg package to recover marginal effect estimates, comparing (1) the sample ATE, (2) the true oracle ATE for the population, and (3) the weighted sample ATE. We will use the outputs of the same models above. The only difference is that we will calculate marginal effects from these outputs. We will contrast a difference from an intervention in which everyone receives treatment = 0 with one in which everyone receives treatment = 1, however, this choice is arbitrary, and the general lessons apply irrespective of the estimand.\nFirst, consider this ATE for the sample population.\n\n# What inference do we draw?  We cannot say the models are unbiased for the marginal effect estimates. \n# regression standardisation \nlibrary(stdReg) # to obtain marginal effects \n\n\n# obtain sample ate\nfit_std_sample &lt;-\n  stdReg::stdGlm(model_sample, data = sample_data, X = \"a_sample\")\n\n# summary\nsummary(fit_std_sample,\n        contrast = \"difference\",\n        reference = 0)\n\n\nFormula: y_sample ~ a_sample * z_sample\nFamily: gaussian \nLink function: identity \nExposure:  a_sample \nReference level:  a_sample = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0     0.00     0.0000       0.00       0.00\n1     1.06     0.0101       1.04       1.08\n\n\nThe treatment effect is given as a 1.06 unit change in the outcome across the sample population, with a confidence interval from 1.04 to 1.08.\nNext, we obtain the true (oracle) treatment effect for the population under the same intervention.\n\n## note the population effect is different\n\n#obtain true ate\nfit_std_population &lt;-\n  stdReg::stdGlm(model_population, data = population_data, X = \"a_population\")\n\n# summary\nsummary(fit_std_population,\n        contrast = \"difference\",\n        reference = 0)\n\n\nFormula: y_population ~ a_population * z_population\nFamily: gaussian \nLink function: identity \nExposure:  a_population \nReference level:  a_population = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0     0.00    0.00000       0.00       0.00\n1     1.25    0.00327       1.24       1.26\n\n\nNote, the true treatment effect is a 1.25 unit change in the population, with a confidence bound between 1.24 and 1.26. This is well outside the ATE that we obtain from the sample population!\nNext, consider the ATE in the weighted regression, where the sample was weighted to the target population’s true distribution of effect modifiers.\n\n## next try weights adjusted ate where we correctly assign population weights to the sample\nfit_std_weighted_sample_weights &lt;- stdReg::stdGlm( model_weighted_sample, \n    data = sample_data, \n    X = \"a_sample\")\n\n# this gives us the right answer\nsummary(fit_std_weighted_sample_weights, \n    contrast = \"difference\", \n    reference = 0)\n\n\nFormula: y_sample ~ a_sample * z_sample\nFamily: gaussian \nLink function: identity \nExposure:  a_sample \nReference level:  a_sample = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0     0.00     0.0000       0.00       0.00\n1     1.25     0.0172       1.22       1.29\n\n# Moral of the story. When we marginalise over the entire sample we need to weight estimates to the target population. \n\nWe find that we obtain the population-level causal effect estimate with accurate coverage by weighting the sample to the target population. So with appropriate weights, our results generalise from the sample to the target population."
  },
  {
    "objectID": "content/07-content.html#lessons-from-lab-2",
    "href": "content/07-content.html#lessons-from-lab-2",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "Lessons from Lab 2",
    "text": "Lessons from Lab 2\n\nRegression coefficients do not clarify the problem of sample/target population mismatch – or selection bias as discussed in this manuscript.\nThe correct advice to investigators is that they should not rely on regression coefficients when evaluating the biases that arise from sample attrition. This advice applies to both methods that the authors use to investigate threats of bias. That is, to implement this advice, the authors must first take it.\nGenerally, observed data are insufficient for assessing threats. Observed data do not clarify structural sources of bias, nor do they clarify effect-modification in the full counterfactual data condition in which all receive the treatment and all do not receive the treatment (at the same level).\nTo properly assess bias, one would need access to the counterfactual outcome—what would have happened to the missing participants had they not been lost to follow-up or had they responded. Again, the join distributions over “full data” are inherently unobservable (Van Der Laan and Rose 2011).\nIn simple settings like the one we just simulated, we may address the gap between the sample and target population using methods such as modelling the censoring (e.g., censoring weighting). However, we never know what setting we are in or whether it is simple—such modelling must be handled with care. There is a large and growing epidemiology literature on this topic (see, for example, Li, Miao, and Tchetgen Tchetgen (2023)). \n\n\nVan Der Laan, Mark J., and Sherri Rose. 2011. Targeted Learning: Causal Inference for Observational and Experimental Data. Springer Series in Statistics. New York, NY: Springer. https://link.springer.com/10.1007/978-1-4419-9782-1.\n\nLi, Wei, Wang Miao, and Eric Tchetgen Tchetgen. 2023. “Non-Parametric Inference about Mean Functionals of Non-Ignorable Non-Response Data Without Identifying the Joint Distribution.” Journal of the Royal Statistical Society Series B: Statistical Methodology 85 (3): 913–35."
  },
  {
    "objectID": "content/07-content.html#appendix-a",
    "href": "content/07-content.html#appendix-a",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "Appendix A",
    "text": "Appendix A\n\nEvidence for effect-modification is relative to inclusion of other variables in the model\nThe ‘sharp-null hypothesis’ states there is no effect of the exposure on the outcome for any unit in the target population. Unless the ‘sharp-null hypothesis’ is false, there may be effect-modification. For any study worth conducting, we cannot evaluate whether the sharp-null hypothesis is false. If we could the experiment would be otiose. Therefore, we must assume the possibility of effect-modification. Notably, whether a variable is an effect-modifier also depends on which other variables are included in the model. That is, just as for the concept of a ‘confounder’, where a variable is an ‘effect-modifier’ cannot be stated without reference to an assumed causal order and an explicit statement about which other variables will be included in the model (Tyler J. VanderWeele 2012).\n\nVanderWeele, Tyler J. 2012. “Confounding and Effect Modification: Distribution and Measure.” Epidemiologic Methods 1 (1): 55–82. https://doi.org/10.1515/2161-962X.1004.\nAs illustrated in Figure 4, the marginal association between A and Y is unbiased. Here, exposure A is unconditionally associated with Y. Recall our convention G denotes effect-modification with conditioning and Z indicates effect-modification without conditioning.\n\n\n\n\n\n\n\n\nFigure 4: Consider a randomised experiment. There is no confounding. Here, the marginal association between A and Y provides an unbiased estimate for the causal effect of A on Y. Does the conditional association of A on Y vary within levels of G? The causal diagram allows for a classification of G as an effect modifier of A on Y by proxy. G modifies A’s effect on Y in virtue of G’s relationship to Z, which, according to this graph, is a direct effect modifier for the effect of A on Y.\n\n\n\n\n\nFigure 5 presents the same a randomised experiment as in the previous causal diagram. We again assume that there is no confounding of the marginal association between the exposure, A, and the outcome, Y. However, suppose we were to adjust for Z and ask, does the conditional association of A on Y vary within levels of G, after adjusting for Z? That is, does G remain an effect-modifier of the exposure on the outcome? Tyler J. VanderWeele and Robins (2007b) proved that for effect-modification to occur, at least one other arrow besides the treatment must enter into the outcome. According to Figure 5 the only arrow into Y other than A arrives from Z. Because Y is independent of G conditional on Z we may infer that G is no longer an effect modifier for the effect of A on Y. Viewed another way, G no longer co-varies with Y conditional on Z and so cannot act as an effect-modifier.\n\n\n\n\n\n\n\n\nFigure 5: Conditioning on Z renders G independent of Y. G is no longer an effect modifier after conditioning on Z because G is independent of Y. Although Z is an unconditional effect modifier, G is not.\n\n\n\n\n\nFigure 6 presents the same a randomised experiment as in the previous graph. We assume a true effect of A \\rightarrow Y. If we do not condition on B, then G will not modify the effect of A  \\rightarrow Y because G will not be associated with Y. However, if we were to condition on B, then both B (an effect modifier by proxy) and G may become effect-modifiers for the causal effect of A on Y. In this setting, both B and G are conditional effect-modifiers.\nNote that casual graphs help us to evaluate classifications of conditional and unconditional effect modifiers. They may also help to clarify conditions in which conditioning on unconditional effect-modifiers may remove conditional effect-modification. However we cannot not tell from a causal diagram whether the ancestors of an unconditional effect-modifier will be conditional effect-modifiers for the effect of the exposure on the outcome; see: Tyler J. VanderWeele and Robins (2007b), also Suzuki et al. (2013). Causal diagrams express non-parametric relations. I have adopted an off-label colouring convention to denote instances of effect-modification to highlight possible pathways for effect-modification, which may be relative to other variables in a model.\n\nVanderWeele, Tyler J., and James M. Robins. 2007b. “Four types of effect modification: a classification based on directed acyclic graphs.” Epidemiology (Cambridge, Mass.) 18 (5): 561–68. https://doi.org/10.1097/EDE.0b013e318127181b.\n\n\n\n\n\n\n\n\nFigure 6: Blue path denotes effect-modification for G by conditioning on B. Both B and G are conditional effect modifiers.\n\n\n\n\n\nFigure 7 reveals the relativity of effect-modification. If investigators do not condition on B, then G cannot be a conditional effect-modifier because G would then be independent of Z because B is a collider. However, as we observed in Figure 6, conditioning on B, a collider, may open a path for effect-modification of G by Z. Both B and G are conditional effect modifiers.\n\n\n\n\n\n\n\n\nFigure 7: Blue path denotes effect-modification. Here G is not an effect modifier because B, a common effect (collider) of G and Z, is not conditioned on. Any conditional effect modification for G would require conditioning on B, and not-conditioning on G. Otherwise G will be d-separated from Y.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Blue path denotes effect-modification. Neither, G nor B are unconditional effect-modifiers for the effect of A on Y after Z is conditioned upon. If investigators condition on Z, the causal diagram implies they will not find evidence for effect-modification by B or G, which are conditionally independent of Y once Z is conditioned upon.\n\n\n\n\n\nFigure 8 considers the implications of conditioning on Z, which is the only unconditional effect-modifier on the graph. If Z is measured, conditioning on Z will remove effect-modification for B and G because B,G\\coprod Y |Z. This examples again reveals the context dependency of effect-modification. Here, causal diagrams are useful for clarifying features of dependent and independent effect modification. For further discussion, see: Suzuki et al. (2013); Tyler J. VanderWeele (2009).\n\nSuzuki, Etsuji, Toshiharu Mitsuhashi, Toshihide Tsuda, and Eiji Yamamoto. 2013. “A Counterfactual Approach to Bias and Effect Modification in Terms of Response Types.” BMC Medical Research Methodology 13 (1): 1–17.\n\nVanderWeele, Tyler J. 2009. “On the Distinction Between Interaction and Effect Modification.” Epidemiology, 863–71.\n\n\nPackages\n\nreport::cite_packages()\n\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 0.3.1.1 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Firke S (2024). _janitor: Simple Tools for Examining and Cleaning Dirty Data_. R package version 2.2.1, &lt;https://CRAN.R-project.org/package=janitor&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 &lt;https://doi.org/10.21105/joss.02445&gt;.\n  - Müller K (2020). _here: A Simpler Way to Find Your Files_. R package version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Sjoberg D, Whiting K, Curry M, Lavery J, Larmarange J (2021). \"Reproducible Summary Tables with the gtsummary Package.\" _The R Journal_, *13*, 570-580. doi:10.32614/RJ-2021-053 &lt;https://doi.org/10.32614/RJ-2021-053&gt;, &lt;https://doi.org/10.32614/RJ-2021-053&gt;.\n  - Sjolander A, Dahlqwist E (2021). _stdReg: Regression Standardization_. R package version 3.4.1, &lt;https://CRAN.R-project.org/package=stdReg&gt;.\n  - Waring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, Ellis S (2022). _skimr: Compact and Flexible Summaries of Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=skimr&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Xie Y (2024). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.54, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/12-content.html",
    "href": "content/12-content.html",
    "title": "Student Presentations",
    "section": "",
    "text": "ReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "content/12-content.html#overview",
    "href": "content/12-content.html#overview",
    "title": "Measurement from a Causal Perspective",
    "section": "Overview",
    "text": "Overview\nBy the end of this lecture you will:\n\nUnderstand the causal assumptions implied by the factor analytic interpretation of the formative and reflective models.\nBe able to distinguish between statistical and structural interpretations of these models.\nUnderstand why Vanderweele thinks consistent causal estimation is possible using the theory of multiple versions of treatments for constructs with multiple indicators"
  },
  {
    "objectID": "content/12-content.html#two-ways-of-thinking-about-measurement-in-psychometric-research.",
    "href": "content/12-content.html#two-ways-of-thinking-about-measurement-in-psychometric-research.",
    "title": "Measurement from a Causal Perspective",
    "section": "Two ways of thinking about measurement in psychometric research.",
    "text": "Two ways of thinking about measurement in psychometric research.\nIn psychometric research, formative and reflective models describe the relationship between latent variables and their respective indicators. VanderWeele discusses this in the assigned reading for this week (Tyler J. VanderWeele 2022).\n\nVanderWeele, Tyler J. 2022. “Constructed Measures and Causal Inference: Towards a New Model of Measurement for Psychosocial Constructs.” Epidemiology 33 (1): 141. https://doi.org/10.1097/EDE.0000000000001434.\n\nReflective Model (Factor Analysis)\nIn a reflective measurement model, also known as an effect indicator model, the latent variable is understood to cause the observed variables. In this model, changes in the latent variable cause changes in the observed variables. Each indicator (observed variable) is a ‘reflection’ of the latent variable. In other words, they are effects or manifestations of the latent variable. These relations are presented in Figure 1.\nThe reflective model may be expressed:\nX_i = \\lambda_i \\eta + \\varepsilon_i\nHere, X_i is an observed variable (indicator), \\lambda_i is the factor loading for X_i, \\eta is the latent variable, and \\varepsilon_i is the error term associated with X_i. It is assumed that all the indicators are interchangeable and have a common cause, which is the latent variable \\eta.\nIn the conventional approach of factor analysis, the assumption is that a common latent variable is responsible for the correlation seen among the indicators. Thus, any fluctuation in the latent variable should immediately lead to similar changes in the indicators.These assumptions are presented in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Reflective model: assume univariate latent variable η giving rise to indicators X1…X3. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\nThe Formative Model (Factor Analysis)\nIn a formative measurement model, the observed variables are seen as causing or determining the latent variable. Here again, there is a single latent variable. However this latent variable is taken to be an effect of the underlying indicators. These relations are presented in Figure 2.\nThe formative model may be expressed:\n\\eta = \\sum_i\\lambda_i X_i + \\varepsilon\nIn this equation, \\eta is the latent variable, \\lambda_i is the weight for X_i (the observed variable), and \\varepsilon is the error term. The latent variable \\eta is a composite of the observed variables X_i.\nIn the context of a formative model, correlation or interchangeability between indicators is not required. Each indicator contributes distinctively to the latent variable. As such, a modification in one indicator doesn’t automatically imply a corresponding change in the other indicators.\n\n\n\n\n\n\n\n\nFigure 2: Formative model:: assume univariate latent variable from which the indicators X1…X3 give rise. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
  },
  {
    "objectID": "content/12-content.html#structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis",
    "href": "content/12-content.html#structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis",
    "title": "Measurement from a Causal Perspective",
    "section": "Structural Interpretation of the formative model and reflective models (Factor Analysis)",
    "text": "Structural Interpretation of the formative model and reflective models (Factor Analysis)\n\nHowever, this analysis of reflective and formative models assumed that the latent η was causally efficacious. This may not be the case (VanderWeele 2022)\n\nVanderWeele distinguishes between statistical and structural interpretations of the equations preesented above.\n\nStatistical Model: a mathematical construct that shows how observable variables, also known as indicators, are related to latent or unseen variables. These are presented in the equations above\nStructural Model: A structural model refers to the causal assumptions or hypotheses about the relationships among variables in a statistical model. The assumptions of the factor analytic tradition are presented in Figure 2 and Figure 1 are structural models.\n\nWe have seen that the reflective model statistically implies that the observed variables (indicators) are reflections or manifestations of the latent variable, expressed as X_i = \\lambda_i \\eta + \\varepsilon_i. However, the factor analytic tradition makes the additional structural assumption that a univariate latent variable is causally efficacious and influences the observed variables, as in: Figure 3 (a).\nWe have also seen that the formative model statistically implies that the latent variable is formed or influenced by the observed variables, expressed as \\eta = \\sum_i\\lambda_i X_i + \\varepsilon. However, the factor analytic tradition makes the additional assumption that the observed variables give rise to a univariate latent variable, as in Figure 3 (b).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Reflective Model: causal assumptions. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Formative model: causal assumptions. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\nThe reflective model implies X_i = \\lambda_i \\eta + \\varepsilon_i, which factor analysts take to imply Figure 3 (a).\n\n\n\n\nFigure 3: The formative model implies \\eta = \\sum_i\\lambda_i X_i + \\varepsilon, which factor analysts take to imply Figure 3 (b)."
  },
  {
    "objectID": "content/12-content.html#problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.",
    "href": "content/12-content.html#problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.",
    "title": "Measurement from a Causal Perspective",
    "section": "Problems with the structural interpretations of the reflective and formative factor models.",
    "text": "Problems with the structural interpretations of the reflective and formative factor models.\nWhile the statistical model X_i = \\lambda_i \\eta + \\varepsilon_i aligns with Figure 3 (a), it also alings with Figure 4. Cross-sectional data, unfortunately, do not provide enough information to discern between these different structural interpretations.\nSimilarly, the statistical model \\eta = \\sum_i\\lambda_i X_i + \\varepsilon agrees with Figure 3 (b) but it also agrees with@fig-dag-reflectiveassumptions-compatible_again. Here too, cross-sectional data cannot decide between these two potential structural interpretations.\nThere are other, compatible structural interprestations as well. The formative and reflective conceptions of factor analysis are compatible with indicators having causal effects as shown in (fig_dag_multivariate_reality_again?). They are also compatible with a multivariate reality giving rise to multiple indicators as shown in Figure 6.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Formative model is compatible with indicators causing outcome.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Reflective model is compatible with indicators causing the outcome. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate reality gives rise to the indicators, from which we draw our measures. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Although we take our constructs, A, to be functions of indicators, X, such that, perhaps only one or several of the indicators are efficacious.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\nVanderWeele’s key observation is this:\n\n\n\n\nWhile cross-sectional data can provide insights into the relationships between variables, they cannot conclusively determine the causal direction of these relationships.\n\n\n\n\nThis results is worrying. The structural assumptions of factor analysis underpin nearly all psychological research. If the cross-sectional data used to derive factor structures cannot decide whether the structural interpretations of factor models are accurate, where does that leave us?\n\n\n\n\nMore worrying still, VanderWeele discusses several longitudinal tests for structural interpretations of univariate latent variables that do not pass.\n\n\n\n\nWhere does that leave us? In psychology we have heard about a replication crisis. We might describe the reliance on factor models as an aspect of a much larger, and more worrying “causal crisis” (Bulbulia 2023)\n\nBulbulia, Joseph A. 2023. “A Workflow for Causal Inference in Cross-Cultural Psychology.” Religion, Brain & Behavior 13 (3): 291–306. https://doi.org/10.1080/2153599X.2022.2070245."
  },
  {
    "objectID": "content/12-content.html#review-of-the-theory-of-multiple-versions-of-treatment",
    "href": "content/12-content.html#review-of-the-theory-of-multiple-versions-of-treatment",
    "title": "Measurement from a Causal Perspective",
    "section": "Review of the theory of multiple versions of treatment",
    "text": "Review of the theory of multiple versions of treatment\n\n\n\n\n\nMultiple Versions of treatment. Heae, A is regarded to bbe a coarseneed version of K\n\n\n\n\nPerhaps not all is lost. VanderWeele looks to the theory of multiple versions of treatment for solace.\nRecall, a causal effect is defined as the difference in the expected potential outcome when everyone is exposed (perhaps contrary to fact) to one level of a treatment, conditional on their levels of a confounder, with the expected potential outcome when everyone is exposed to a a different level of a treatement (perhaps contrary to fact), conditional on their levels of a counfounder.\n \\delta = \\sum_l \\left( \\mathbb{E}[Y|A=a,l] - \\mathbb{E}[Y|A=a^*,l] \\right) P(l)\nwhere \\delta is the causal estimand on the difference scale (\\mathbb{E}[Y^0 - Y^0]).\nIn causal inference, the multiple versions of treatment theory allows us to handle situations where the treatment isn’t uniform, but instead has several variations. Each variation of the treatment, or “version”, can have a different impact on the outcome. Consistency is not violated because it is redefined: for each version of the treatment, the outcome under that version is equal to the observed outcome when that version is received. Put differently we may think of the indicator A as corresponding to many version of the true treament K. Where conditional independence holds such that there is a absence of confounding for the effect of K on Y given L, we have: Y_k \\coprod A|K,L. This states conditional on L, A gives no information about Y once K and L are accounted for. When Y = Y_k if K = k and Y_k is independent of K, condition on L, then A may be thought of as a coarsened indicator of K, as shown in (fig_dag_multiple_version_treatment_dag?). We may estimate consistent causal effects where:\n \\delta = \\sum_{k,l} \\mathbb{E}[Y_k|l] P(k|a,l) P(l) - \\sum_{k,l} \\mathbb{E}[Y_k|l] P(k|a^*,l) P(l)\nThe scenario represents a hypothetical randomised trial where within strata of covariates L, individuals in one group receive a treatment K version randomly assigned from the distribution of K distribution (A = 1, L = l) sub-population. Meanwhile, individuals in the other group receive a randomly assigned K version from (A = 0, L = l)\nThis theory finds its utility in practical scenarios where treatments seldom resemble each other – we discussed the example of obesity last week (see: (Tyler J. VanderWeele and Hernan 2013)).\n\nVanderWeele, Tyler J, and Miguel A Hernan. 2013. “Causal Inference Under Multiple Versions of Treatment.” Journal of Causal Inference 1 (1): 1–20.\n\nReflective and formative measurement models may be approached as multiple versions of treatment\nVanderweele applies the following substitution:\n\\delta = \\sum_{\\eta,l} \\mathbb{E}[Y_\\eta|l] P(\\eta|A=a+1,l) P(l) - \\sum_{\\eta,l} \\mathbb{E}[Y_\\eta|l] P(\\eta|A=a,l) P(l)\nSpecifically, we substitue K with \\eta from the previous section, and compare the measurement response A = a + 1 with A = a. We discover that if the influence of \\eta on Y is not confounded given L, then the multiple versions of reality consistent with the reflective and formative statistical models of reality will not lead to biased estimation. \\delta retains its interpretability as a comparison in a hypothetical randomised trial in which the distribution of coarsened measures of \\eta_A are balanced within levels of the treatment, conditional on \\eta_L.\nThis connection between measurement and the multiple versions of treatment framework provides a hope for consistent causal inference varying reliabilities of measurement.\nHowever, as with the theory of multiple treatments, we might not known how to interpret our results because we don’t know the true relationships between our measured indicators and underlying reality.\nHow can we do better?\n\n\n\n\n\n\n\n\nFigure 7: Multiple Versions of treatment applied to measuremen.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
  },
  {
    "objectID": "content/12-content.html#vanderweeles-model-of-reality",
    "href": "content/12-content.html#vanderweeles-model-of-reality",
    "title": "Measurement from a Causal Perspective",
    "section": "VanderWeele’s model of reality",
    "text": "VanderWeele’s model of reality\nVanderWeele’s article concludes as follows:\n\nA preliminary outline of a more adequate approach to the construction and use of psychosocial measures might thus be summarized by the following propositions, that I have argued for in this article: (1) Traditional univariate reflective and formative models do not adequately capture the relations between the underlying causally relevant phenomena and our indicators and measures. (2) The causally relevant constituents of reality related to our constructs are almost always multidimensional, giving rise both to our indicators from which we construct measures, and also to our language and concepts, from which we can more precisely define constructs. (3) In measure construction, we ought to always specify a definition of the underlying construct, from which items are derived, and by which analytic relations of the items to the definition are made clear. (4) The presumption of a structural univariate reflective model impairs measure construction, evaluation, and use. (5) If a structural interpretation of a univariate reflective factor model is being proposed this should be formally tested, not presumed; factor analysis is not sufficient for assessing the relevant evidence. (6) Even when the causally relevant constituents of reality are multidimensional, and a univariate measure is used, we can still interpret associations with outcomes using theory for multiple versions of treatment, though the interpretation is obscured when we do not have a clear sense of what the causally relevant constituents are. (7) When data permit, examining associations item-by-item, or with conceptually related item sets, may give insight into the various facets of the construct.\n\n\nA new integrated theory of measurement for psychosocial constructs is needed in light of these points – one that better respects the relations between our constructs, items, indicators, measures, and the underlying causally relevant phenomena. (VanderWeele 2022)\n\n\n\n\n\n\n\n\n\nFigure 8: Multivariate reality gives rise to the latent variables.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\nThis seems to me sensible. However, Figure 8 this is not a causal graph. The arrows to not clearly represent causal relations. It leaves me unclear about what to practically do.\nLet’s return to the three wave many-outcomes model described in previous weeks. How should we revise this model in light of measurement theory?"
  },
  {
    "objectID": "content/12-content.html#how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement",
    "href": "content/12-content.html#how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement",
    "title": "Measurement from a Causal Perspective",
    "section": "How theory of dependent and directed measurement error might be usefully employed to develop a pragmatic responses to construct measurement",
    "text": "How theory of dependent and directed measurement error might be usefully employed to develop a pragmatic responses to construct measurement\nBy now you are all familiar with The New Zealand Attitudes and Values Study (NZAVS),which is a national probability survey collects a wide range of information, including data on distress, exercise habits, and cultural backgrounds.\n\n\n\n\n\n\n\n\nFigure 9: Uncorrelated non-differential measurement error does not bias estimates under the null. Note, however, we assume that L is measured with sufficient precision to block the path from A_eta –&gt; L_eta –&gt; Y_eta, which, otherwise, we would assume to be open.\n\n\n\n\n\nConsider a study that seeks to use this dataset to investigate the effect of regular exercise on psychological distress. In contrast to previous graphs, let us allow for latent reality to affect our measurements, as well as the discrepencies between our measurements and true underlying reality. We shall use Figure 9 as our initial guide.\nWe represent the true exercise by \\eta_A. We represent true psychological distress by \\eta_Y. Let \\eta_L denote a persons true workload, and assume that this state of work affects both levels of excercise and psychological distress.\nTo bring the model into contact with measurement theory, Let us describe measurements of these latent true underlying realities as functions of multiple indicators: L_{f(X_1\\dots X_n)}, A_{f(X_1\\dots X_n)}, and A_{f(X_1\\dots X_n)}. These constructs are measured realisations of the underlying true states. We assume that the true states of these variables affect their corresponding measured states, and so draw arrows from \\eta_L\\rightarrow{L_{f(X_1\\dots X_n)}}, \\eta_A\\rightarrow{A_{f(X_1\\dots X_n)}}, \\eta_Y\\rightarrow{Y_{f(X_1\\dots X_n)}}.\nWe also assume unmeasured sources of error that affect the measurements: U_{L} \\rightarrow L_{f(X_1\\dots X_n)}, U_{A} \\rightarrow A_{f(X_1\\dots X_n)}, and U_{Y} \\rightarrow Y_{f(X_1\\dots X_n)}. That is, we allow that our measured indicators may “see as through a mirror, in darkness,” the underlying true reality they hope to capture (Corinthians 13:12). We use U_{L}, U_{A} and U_{Y} to denote the unmeasured sources of error in the measured indicators. These are the unknown, and perhaps unknowable, darkness and mirror.\nAllow that the true underlying reality represented by the \\eta_{var} may be multivariate. Similarly, allow the true underlying reality represented by \\U_{var} is multivariate.\nWe now have a causal diagramme that more precisely captures VanderWeele’s thinking as presented in Figure 8. In our Figure 9, we have fleshed out \\mathcal{R} in a way that may include natural language concepts and scientific language, or constructs, as latent realities and latent unmeasured sources of error in our constructs.\nThe utility of describing the measurement dynamics using causal graphs is apparrent. We can understand that the measured states, once conditioned upon create collider biases which opens path between the unmeasured sources of error and the true underlying state that gives rise to our measurements. This is depicted by a the arrows U_{var} and from \\eta_{var} into each var_{f(X1, X2,\\dots X_n)}\nNotice: where true unmeasured (multivariate) psycho-physical states are related to true unmeasured (multivariate) sources of error in the measurement of those states, the very act of measurement opens pathways to confounding.\nIf for each measured construct var_{f(X1, X2,\\dots X_n)}, the sources of error U_{var} and the unmeasured consituents of reality that give rise to our measures \\eta_{var} are uncorrelated with other variables U\\prime_{var} and from \\eta\\prime_{var} and var\\prime_{f(X1, X2,\\dots X_n)}, our estimates may be downwardly biased toward the null. However, d-separation is preserved. Where errors are uncorrelated with true latent realities, there is no new pathway that opens information between our exposure and outcome. Consider the relations presented in Figure 10\n\n\n\n\n\n\n\n\nFigure 10: Measurement error opens an additional pathway to confounding if either there are correlated errors, or a directed effect of the exposure on the errors of measured outcome.\n\n\n\n\n\nHere,\n\\eta_L \\rightarrow L: We assume that the true workload state affects its measurement. This measurement, however, may be affected by an unmeasured error source, U_{L}. Personal perceptions of workload can introduce this error. For instance, a person may perceive their workload differently based on recent personal experiences or cultural backgrounds. Additionally, unmeasured cultural influences like societal expectations of productivity could shape their responses independently of the true workload state. There may be cultural differences - Americans may verstate; the British may present effortless superiority.\n\\eta_A \\rightarrow A: When it comes to exercise, the true state may affect the measured frequency (questions about exercise are not totally uninformative). However, this measurement is also affected by an unmeasured source of error, which we denote by U_{A}. For example, a cultural shift towards valuing physical health might prompt participants toreport higher activity levels, introducing an error, U_{A}.\n\\eta_Y \\rightarrow Y: We assume questions about distress are not totally uninformative: actual distress affects the measured distress. However this measurement is subject to unmeasured error: U_{Y}. For instance, an increased societal acceptance of mental health might change how distress is reported creating an error, U_{Y}, in the measurement of distress. Such norms, moreover, may change over time.\nU_{L} \\rightarrow L, U_{A} \\rightarrow A, and U_{Y} \\rightarrow Y: These edges between the nodes indicate how each unmeasured error source can influence its corresponding measurement, leading to a discrepancy between the true state and the measured state.\nU_{L} \\rightarrow U_{A} and U_{L} \\rightarrow U_{Y}: These relationships indicate that the error in the stress measurement can correlate with those in the exercise and mood measurements. This could stem from a common cultural bias affecting how a participant self-reports across these areas.\n\\eta_A \\rightarrow U_{Y} and \\eta_L \\rightarrow U_{A}: These relationships indicate that the actual state of one variable can affect the error in another variable’s measurement. For example, a cultural emphasis on physical health leading to increased exercise might, in turn, affect the reporting of distress levels, causing an error, U_{Y}, in the distress measurement. Similarly, if a cultural trend pushes people to work more, it might cause them to over or underestimate their exercise frequency, introducing an error, U_{A}, in the exercise measurement.\n\nConfounding control by baseline measures of exposure and outcome: Dependent Directed Measurement Error in Three-Wave Panels\n\nWe propose a three-wave panel design to control confounding. This design adjusts for baseline measurements of both exposure and the outcome.\nUnderstanding this approach in the context of potential directed and correlated measurement errors gives us a clearer picture of its strengths and limitations.\nThis three-wave panel design incorporates baseline measurements of both exposure and confounders. As a result, any bias that could come from unmeasured sources of measurement errors should be uncorrelated with their baseline effects.\nFor instance, if individuals have a social desirability bias at the baseline, they would have to develop a different bias unrelated to the initial one for new bias to occur due to correlated unmeasured sources of measurement errors.\nHowever, we cannot completely eliminate the possibility of such new bias development. There could also be potential new sources of bias from directed effects of the exposure on the error term of the outcome, which can often occur due to panel attrition.\nTo mitigate this risk, we adjust for panel attrition/non-response using methods like multiple imputation. We also consistently perform sensitivity analyses to detect any unanticipated bias.\nDespite these potential challenges, it is worth noting that by including measures of both exposure and outcome at baseline, the chances of new confounding are significantly reduced.\nTherefore, adopting this practice should be a standard procedure in multi-wave studies as it substantially minimizes the likelihood of introducing novel confounding factors.\n\n\n\n\n\n\n\n\n\nFigure 11: TBA\n\n\n\n\n\n\n\nComment on slow changes\nOver long periods of time we can expect additional sources of confounding. Changes in cultural norms and attitudes can occur over the duration of a longitudinal study like the NZAVS, leading to residual confounding. For example, if there is a cultural shift towards increased acceptance of mental health issues, this might change how psychological distress is reported over time, irrespective of baseline responses.\n\n\n\n\n\n\n\n\n\n\n\n\nNeed for Sensitivity Analysis The Key takehome message is that we must always perform sensitivity analyses because we can never be certain that our confounding control strategy has worked."
  },
  {
    "objectID": "content/12-content.html#lab-advice-on-you-final-report",
    "href": "content/12-content.html#lab-advice-on-you-final-report",
    "title": "Measurement from a Causal Perspective",
    "section": "Lab: Advice on you Final Report",
    "text": "Lab: Advice on you Final Report"
  },
  {
    "objectID": "content/12-content.html#intoduction",
    "href": "content/12-content.html#intoduction",
    "title": "Measurement from a Causal Perspective",
    "section": "Intoduction",
    "text": "Intoduction\nAnswer the following:\n\nState the Question: is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nCausality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\nState how you will use time-series data to address causality.\nDefine your exposure.\nDefine your outcome(s)\nExplain how the the exposure and outcome is relevant to your question.\nDefine your causal estimand (see: lecture 9). Hint: it is ATE_g_risk difference = E[Y(1)-(0)|G,L], where G is your multiple-group indicator and L is your set of baseline confounders."
  },
  {
    "objectID": "content/12-content.html#methods",
    "href": "content/12-content.html#methods",
    "title": "Measurement from a Causal Perspective",
    "section": "Methods",
    "text": "Methods\n\nConsider any ethical implications.\nExplain the sample. Provide descriptive statistics\nDiscuss inclusion criteria.\nDiscuss how your sample relates to the “source population” (lecture 9.)\nExplain NZAVS measures. State the questions used in the items\nIn your own words describe how the data meet the following assumptions required for causal inference:\nPositivity: Can we intervene on the exposure at all levels of the covariates? Use the code I provided to test whether there is change in the exposure from the baseline in the source population(s)\nConsistency: Can I interpret what it means to intervene on the exposure?\nExchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? This requires stating baseline confounders and explaining how they may be related to both the exposure and outcome. As part of this, you must explain why the baseline measure of your exposure and outcome are included as potential confounders.\nNote: Unmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? (e.g. childhood exposures that are not measured).\nDraw a causal diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\nMeasurement Error: Have I described potential biases from measurement errors? Return to lecture 11.\nState that you do not have missing data in this synthetic dataset, but that ordinarily missing data would need to be handled.\nState what your estimator will be. Note I’ve given you the following text to modify:\n\n\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\n\n\nStep 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\n\n\nStep 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\n\n\nStep 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\n\n\nStep 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\n\n\nStep 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\nAlso see the appendix here\n\nState what E-values are and how you will use them to clarify the risk of unmeasured confounding."
  },
  {
    "objectID": "content/12-content.html#results",
    "href": "content/12-content.html#results",
    "title": "Measurement from a Causal Perspective",
    "section": "Results",
    "text": "Results\n\nUse the scripts I have provided as a template for your analysis.\nPropensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations: e.g.: A ~ x1 + x2 + x3 + .... using logistic regression, all continuous predictors were transformed to z-scores\n\nWeightIt Package Utilisation: Explicitly mention the use of the ‘WeightIt’ package in R, including any specific options or parameters used in the propensity score estimation process (Greifer 2023).\nReport if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as ‘ebal’, ‘energy’, and ‘ps’.\nIf your exposure is continuous only the ‘energy’ option was used for propensity score estimation.\nSubgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\nCovariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting. The script will generate these plots.\nWeighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit. The script I gave you will generate this information\n\n\n\nGreifer, Noah. 2023. WeightIt: Weighting for Covariate Balance in Observational Studies.\nExample:\n\nWe estimated propensity scores by fitting a model for the exposure A as it is predicted by the set of baseline covariates defined by L. Because we are interested in effect modification by group, we fit different propensity score models for within strata of G using the subgroup command of the WeightIt package. Thus the propensity score is the the probability of receiving a value of a treatment (A=a) conditional on the covariates L, and stratum within G. We compared balance using the following methods of weighting: “ebal” or entropy balancing, “energy” or energy balancing, and “ps” or traditional inverse probability of weighting balancing. Of these methods “ebal” performed the best. Table X and Figure Y present the results of the ebalancing method.\n\n\nInterpretation of Propensity Scores: we interpret the proposensity scores as yeilding good balance across the exposure conditions.\nOutcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation. Example\n\n\nWe fit a linear model using maximum likelihood estimation with the outcome Y predicted by the exposure A. We interacted the exposure with all baseline confounders L. Continuous baseline confounders were converted to z-scores, whereas categorical exposures were not. Also interacted with all baseline confounders was a term for the subgroup interactoin. This allowed uas to flexibily fit non-linearities for the modification of the effect of the exposure within levels of the levels of the cultural group strata of interest. We note that model coefficients have no interpretation in this context so are not reported. The remaining steps of Doubly-Robust estimation were performed as outlined in the Method section. We calculated confidence intervals and standard errors, using the clarify package in R, which relies on simulation based inference for these quantities of interest (Greifer et al. 2023)\n\nGreifer, Noah, Steven Worthington, Stefano Iacus, and Gary King. 2023. Clarify: Simulation-Based Inference for Regression Models. https://iqss.github.io/clarify/.\n\nReport the causal estimates.\n\nATE contrasts for groups in setting the exposure to for each group in setting level A = a and A = a*\ndifferences between groups in the magnitude of the effects. (ATE_group 1 - ATE_group_2)\nReport the E-value: how sensitive are your results to unmeasured confounding?"
  },
  {
    "objectID": "content/12-content.html#discussion",
    "href": "content/12-content.html#discussion",
    "title": "Measurement from a Causal Perspective",
    "section": "Discussion",
    "text": "Discussion\nMake sure to hit these points:\nConsider the following ideas about what to discuss in one’s findings. The order of exposition might be different.\n\nSummary of results: What did you find?\nInterpretation of E-values: Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\nCausal Effect Interpretation: What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\nComparison of Subgroups: Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\nUncertainty and Confidence Intervals: Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\nGeneralisability and Transportability: Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\nAssumptions and Limitations: Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\nTheoretical Relevance: How are these findings relevant to existing theories.\nReplication and Future Research: Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\nReal-World Implications: Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research."
  },
  {
    "objectID": "content/12-content.html#example-anlaysis-week-10",
    "href": "content/12-content.html#example-anlaysis-week-10",
    "title": "Measurement from a Causal Perspective",
    "section": "Example anlaysis (week 10)",
    "text": "Example anlaysis (week 10)\n\nPackages\n\nreport::cite_packages()\n\n  - Arel-Bundock V, Greifer N, Heiss A (2024). \"How to Interpret Statistical Models Using marginaleffects for R and Python.\" _Journal of Statistical Software_, *111*(9), 1-32. doi:10.18637/jss.v111.i09 &lt;https://doi.org/10.18637/jss.v111.i09&gt;.\n  - Barrett M (2021). _ggokabeito: 'Okabe-Ito' Scales for 'ggplot2' and 'ggraph'_. R package version 0.1.0, &lt;https://CRAN.R-project.org/package=ggokabeito&gt;.\n  - Bates D, Mächler M, Bolker B, Walker S (2015). \"Fitting Linear Mixed-Effects Models Using lme4.\" _Journal of Statistical Software_, *67*(1), 1-48. doi:10.18637/jss.v067.i01 &lt;https://doi.org/10.18637/jss.v067.i01&gt;.\n  - Bates D, Maechler M, Jagan M (2025). _Matrix: Sparse and Dense Matrix Classes and Methods_. R package version 1.7-2, &lt;https://CRAN.R-project.org/package=Matrix&gt;.\n  - Bengtsson H (2021). \"A Unifying Framework for Parallel and Distributed Processing in R using Futures.\" _The R Journal_, *13*(2), 208-227. doi:10.32614/RJ-2021-048 &lt;https://doi.org/10.32614/RJ-2021-048&gt;, &lt;https://doi.org/10.32614/RJ-2021-048&gt;.\n  - Bengtsson H (2021). \"A Unifying Framework for Parallel and Distributed Processing in R using Futures.\" _The R Journal_, *13*(2), 208-227. doi:10.32614/RJ-2021-048 &lt;https://doi.org/10.32614/RJ-2021-048&gt;, &lt;https://doi.org/10.32614/RJ-2021-048&gt;.\n  - Bengtsson H (2024). _progressr: An Inclusive, Unifying API for Progress Updates_. R package version 0.15.1, &lt;https://CRAN.R-project.org/package=progressr&gt;.\n  - Blair G, Cooper J, Coppock A, Humphreys M, Bicalho C, Fultz N, Medina L (2021). _DesignLibrary: Library of Research Designs_. R package version 0.1.10, &lt;https://CRAN.R-project.org/package=DesignLibrary&gt;.\n  - Blair G, Cooper J, Coppock A, Humphreys M, Rudkin A, Fultz N (2024). _fabricatr: Imagine Your Data Before You Collect It_. R package version 1.0.2, &lt;https://CRAN.R-project.org/package=fabricatr&gt;.\n  - Blair G, Cooper J, Coppock A, Humphreys M, Sonnet L (2024). _estimatr: Fast Estimators for Design-Based Inference_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=estimatr&gt;.\n  - Blair G, Coppock A, Humphreys M (2023). _Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign_. Princeton University Press, Princeton. &lt;https://book.declaredesign.org&gt;. Blair G, Cooper J, Coppock A, Humphreys M (2019). \"Declaring and Diagnosing Research Designs.\" _American Political Science Review_, *113*, 838-859. &lt;https://declaredesign.org/paper.pdf&gt;.\n  - Brown C (2018). _formula.tools: Programmatic Utilities for Manipulating Formulas, Expressions, Calls, Assignments and Other R Objects_. R package version 1.7.1, &lt;https://CRAN.R-project.org/package=formula.tools&gt;.\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 0.3.1.1 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Chen T, He T, Benesty M, Khotilovich V, Tang Y, Cho H, Chen K, Mitchell R, Cano I, Zhou T, Li M, Xie J, Lin M, Geng Y, Li Y, Yuan J (2024). _xgboost: Extreme Gradient Boosting_. R package version 1.7.8.1, &lt;https://CRAN.R-project.org/package=xgboost&gt;.\n  - Christopher H. Jackson (2011). \"Multi-State Models for Panel Data: The msm Package for R.\" _Journal of Statistical Software_, *38*(8), 1-29. doi:10.18637/jss.v038.i08 &lt;https://doi.org/10.18637/jss.v038.i08&gt;.\n  - Coppock A (2023). _randomizr: Easy-to-Use Tools for Common Forms of Random Assignment and Sampling_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=randomizr&gt;.\n  - Csárdi G, Hester J, Wickham H, Chang W, Morgan M, Tenenbaum D (2024). _remotes: R Package Installation from Remote Repositories, Including 'GitHub'_. R package version 2.5.0, &lt;https://CRAN.R-project.org/package=remotes&gt;.\n  - Eddelbuettel D, Francois R, Allaire J, Ushey K, Kou Q, Russell N, Ucar I, Bates D, Chambers J (2025). _Rcpp: Seamless R and C++ Integration_. R package version 1.0.14, &lt;https://CRAN.R-project.org/package=Rcpp&gt;. Eddelbuettel D, François R (2011). \"Rcpp: Seamless R and C++ Integration.\" _Journal of Statistical Software_, *40*(8), 1-18. doi:10.18637/jss.v040.i08 &lt;https://doi.org/10.18637/jss.v040.i08&gt;. Eddelbuettel D (2013). _Seamless R and C++ Integration with Rcpp_. Springer, New York. doi:10.1007/978-1-4614-6868-4 &lt;https://doi.org/10.1007/978-1-4614-6868-4&gt;, ISBN 978-1-4614-6867-7. Eddelbuettel D, Balamuta J (2018). \"Extending R with C++: A Brief Introduction to Rcpp.\" _The American Statistician_, *72*(1), 28-36. doi:10.1080/00031305.2017.1375990 &lt;https://doi.org/10.1080/00031305.2017.1375990&gt;.\n  - Firke S (2024). _janitor: Simple Tools for Examining and Cleaning Dirty Data_. R package version 2.2.1, &lt;https://CRAN.R-project.org/package=janitor&gt;.\n  - Friedman J, Tibshirani R, Hastie T (2010). \"Regularization Paths for Generalized Linear Models via Coordinate Descent.\" _Journal of Statistical Software_, *33*(1), 1-22. doi:10.18637/jss.v033.i01 &lt;https://doi.org/10.18637/jss.v033.i01&gt;. Simon N, Friedman J, Tibshirani R, Hastie T (2011). \"Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent.\" _Journal of Statistical Software_, *39*(5), 1-13. doi:10.18637/jss.v039.i05 &lt;https://doi.org/10.18637/jss.v039.i05&gt;. Tay JK, Narasimhan B, Hastie T (2023). \"Elastic Net Regularization Paths for All Generalized Linear Models.\" _Journal of Statistical Software_, *106*(1), 1-31. doi:10.18637/jss.v106.i01 &lt;https://doi.org/10.18637/jss.v106.i01&gt;.\n  - Greifer N (2024). _cobalt: Covariate Balance Tables and Plots_. R package version 4.5.5, &lt;https://CRAN.R-project.org/package=cobalt&gt;.\n  - Greifer N (2024). _WeightIt: Weighting for Covariate Balance in Observational Studies_. R package version 1.3.2, &lt;https://CRAN.R-project.org/package=WeightIt&gt;.\n  - Greifer N, Worthington S, Iacus S, King G (2024). _clarify: Simulation-Based Inference for Regression Models_. R package version 0.2.1, &lt;https://CRAN.R-project.org/package=clarify&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Halekoh U, Højsgaard S, Yan J (2006). \"The R Package geepack for Generalized Estimating Equations.\" _Journal of Statistical Software_, *15/2*, 1-11. Yan J, Fine JP (2004). \"Estimating Equations for Association Structures.\" _Statistics in Medicine_, *23*, 859-880. Yan J (2002). \"geepack: Yet Another Package for Generalized Estimating Equations.\" _R-News_, *2/3*, 12-14.\n  - Hansen BB, Klopfer SO (2006). \"Optimal full matching and related designs via network flows.\" _Journal of Computational and Graphical Statistics_, *15*(3), 609-627.\n  - Hastie T (2024). _gam: Generalized Additive Models_. R package version 1.22-5, &lt;https://CRAN.R-project.org/package=gam&gt;.\n  - Helske S, Helske J (2019). \"Mixture Hidden Markov Models for Sequence Data: The seqHMM Package in R.\" _Journal of Statistical Software_, *88*(3), 1-32. doi:10.18637/jss.v088.i03 &lt;https://doi.org/10.18637/jss.v088.i03&gt;. Helske J, Helske S (2023). _seqHMM: Mixture hidden Markov models for social sequence data and other multivariate, multichannel categorical time series_. R package version 1.2.6, &lt;https://cran.r-project.org/package=seqHMM&gt;.\n  - Henry L, Wickham H (2025). _rlang: Functions for Base Types and Core R and 'Tidyverse' Features_. R package version 1.1.5, &lt;https://CRAN.R-project.org/package=rlang&gt;.\n  - Hester J, Bryan J (2024). _glue: Interpreted String Literals_. R package version 1.8.0, &lt;https://CRAN.R-project.org/package=glue&gt;.\n  - Hester J, Wickham H, Csárdi G (2024). _fs: Cross-Platform File System Operations Based on 'libuv'_. R package version 1.6.5, &lt;https://CRAN.R-project.org/package=fs&gt;.\n  - Ho D, Imai K, King G, Stuart E (2011). \"MatchIt: Nonparametric Preprocessing for Parametric Causal Inference.\" _Journal of Statistical Software_, *42*(8), 1-28. doi:10.18637/jss.v042.i08 &lt;https://doi.org/10.18637/jss.v042.i08&gt;.\n  - Honaker J, King G, Blackwell M (2011). \"Amelia II: A Program for Missing Data.\" _Journal of Statistical Software_, *45*(7), 1-47. doi:10.18637/jss.v045.i07 &lt;https://doi.org/10.18637/jss.v045.i07&gt;.\n  - Iannone R, Cheng J, Schloerke B, Hughes E, Lauer A, Seo J, Brevoort K, Roy O (2024). _gt: Easily Create Presentation-Ready Display Tables_. R package version 0.11.1, &lt;https://CRAN.R-project.org/package=gt&gt;.\n  - J L (2006). \"Plotrix: a package in the red light district of R.\" _R-News_, *6*(4), 8-12.\n  - Kassambara A (2023). _ggpubr: 'ggplot2' Based Publication Ready Plots_. R package version 0.6.0, &lt;https://CRAN.R-project.org/package=ggpubr&gt;.\n  - Kay M (2024). \"ggdist: Visualizations of Distributions and Uncertainty in the Grammar of Graphics.\" _IEEE Transactions on Visualization and Computer Graphics_, *30*(1), 414-424. doi:10.1109/TVCG.2023.3327195 &lt;https://doi.org/10.1109/TVCG.2023.3327195&gt;. Kay M (2024). _ggdist: Visualizations of Distributions and Uncertainty_. doi:10.5281/zenodo.3879620 &lt;https://doi.org/10.5281/zenodo.3879620&gt;, R package version 3.3.2, &lt;https://mjskay.github.io/ggdist/&gt;.\n  - Lüdecke D (2018). \"ggeffects: Tidy Data Frames of Marginal Effects from Regression Models.\" _Journal of Open Source Software_, *3*(26), 772. doi:10.21105/joss.00772 &lt;https://doi.org/10.21105/joss.00772&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 &lt;https://doi.org/10.21105/joss.02445&gt;.\n  - Lumley T (2024). \"survey: analysis of complex survey samples.\" R package version 4.4. Lumley T (2004). \"Analysis of Complex Survey Samples.\" _Journal of Statistical Software_, *9*(1), 1-19. R package verson 2.2. Lumley T (2010). _Complex Surveys: A Guide to Analysis Using R: A Guide to Analysis Using R_. John Wiley and Sons.\n  - Microsoft, Weston S (2022). _foreach: Provides Foreach Looping Construct_. R package version 1.5.2, &lt;https://CRAN.R-project.org/package=foreach&gt;.\n  - Milborrow S (2024). _plotmo: Plot a Model's Residuals, Response, and Partial Dependence Plots_. R package version 3.6.4, &lt;https://CRAN.R-project.org/package=plotmo&gt;.\n  - Milborrow S, Hastie T, Tibshirani R (2024). _earth: Multivariate Adaptive Regression Splines_. R package version 5.3.4, &lt;https://CRAN.R-project.org/package=earth&gt;.\n  - Mullen KM, van Stokkum IHM (2024). _nnls: The Lawson-Hanson Algorithm for Non-Negative Least Squares (NNLS)_. R package version 1.6, &lt;https://CRAN.R-project.org/package=nnls&gt;.\n  - Müller K (2020). _here: A Simpler Way to Find Your Files_. R package version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - Ooms J (2024). _katex: Rendering Math to HTML, 'MathML', or R-Documentation Format_. R package version 1.5.0, &lt;https://CRAN.R-project.org/package=katex&gt;.\n  - Ooms J (2024). _pdftools: Text Extraction, Rendering and Converting of PDF Documents_. R package version 3.4.1, &lt;https://CRAN.R-project.org/package=pdftools&gt;.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. R package version 1.3.0, &lt;https://CRAN.R-project.org/package=patchwork&gt;.\n  - Pishgar F, Greifer N, Leyrat C, Stuart E (2021). \"MatchThem:: Matching and Weighting after Multiple Imputation.\" _The R Journal_. doi:10.32614/RJ-2021-073 &lt;https://doi.org/10.32614/RJ-2021-073&gt;, &lt;https://journal.r-project.org/archive/2021/RJ-2021-073/&gt;.\n  - Polley E, LeDell E, Kennedy C, van der Laan M (2024). _SuperLearner: Super Learner Prediction_. R package version 2.0-29, &lt;https://CRAN.R-project.org/package=SuperLearner&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Rich B (2023). _table1: Tables of Descriptive Statistics in HTML_. R package version 1.4.3, &lt;https://CRAN.R-project.org/package=table1&gt;.\n  - Richardson N, Cook I, Crane N, Dunnington D, François R, Keane J, Moldovan-Grünfeld D, Ooms J, Wujciak-Jens J, Apache Arrow (2025). _arrow: Integration to 'Apache' 'Arrow'_. R package version 18.1.0.1, &lt;https://CRAN.R-project.org/package=arrow&gt;.\n  - Robinson D, Hayes A, Couch S (2024). _broom: Convert Statistical Objects into Tidy Tibbles_. R package version 1.0.7, &lt;https://CRAN.R-project.org/package=broom&gt;.\n  - Robitzsch A, Grund S (2024). _miceadds: Some Additional Multiple Imputation Functions, Especially for 'mice'_. R package version 3.17-44, &lt;https://CRAN.R-project.org/package=miceadds&gt;.\n  - Sjoberg D, Whiting K, Curry M, Lavery J, Larmarange J (2021). \"Reproducible Summary Tables with the gtsummary Package.\" _The R Journal_, *13*, 570-580. doi:10.32614/RJ-2021-053 &lt;https://doi.org/10.32614/RJ-2021-053&gt;, &lt;https://doi.org/10.32614/RJ-2021-053&gt;.\n  - Sjolander A, Dahlqwist E (2021). _stdReg: Regression Standardization_. R package version 3.4.1, &lt;https://CRAN.R-project.org/package=stdReg&gt;.\n  - Therneau T (2024). _A Package for Survival Analysis in R_. R package version 3.8-3, &lt;https://CRAN.R-project.org/package=survival&gt;. Terry M. Therneau, Patricia M. Grambsch (2000). _Modeling Survival Data: Extending the Cox Model_. Springer, New York. ISBN 0-387-98784-3.\n  - Tibshirani J, Athey S, Sverdrup E, Wager S (2024). _grf: Generalized Random Forests_. R package version 2.4.0, &lt;https://CRAN.R-project.org/package=grf&gt;.\n  - Tierney N, Cook D (2023). \"Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.\" _Journal of Statistical Software_, *105*(7), 1-31. doi:10.18637/jss.v105.i07 &lt;https://doi.org/10.18637/jss.v105.i07&gt;.\n  - van Buuren S, Groothuis-Oudshoorn K (2011). \"mice: Multivariate Imputation by Chained Equations in R.\" _Journal of Statistical Software_, *45*(3), 1-67. doi:10.18637/jss.v045.i03 &lt;https://doi.org/10.18637/jss.v045.i03&gt;.\n  - van der Wal WM, Geskus RB (2011). \"ipw: An R Package for Inverse Probability Weighting.\" _Journal of Statistical Software_, *43*(13), 1-23. doi:10.18637/jss.v043.i13 &lt;https://doi.org/10.18637/jss.v043.i13&gt;.\n  - Waring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, Ellis S (2022). _skimr: Compact and Flexible Summaries of Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=skimr&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _conflicted: An Alternative Conflict Resolution Strategy_. R package version 1.2.0, &lt;https://CRAN.R-project.org/package=conflicted&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, Bryan J, Barrett M, Teucher A (2024). _usethis: Automate Package and Project Setup_. R package version 3.1.0, &lt;https://CRAN.R-project.org/package=usethis&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Hester J, Chang W, Bryan J (2022). _devtools: Tools to Make Developing R Packages Easier_. R package version 2.4.5, &lt;https://CRAN.R-project.org/package=devtools&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Williams N, Díaz I (2023). \"lmtp: An R package for estimating the causal effects of modified treatment policies.\" _Observational Studies_. &lt;https://muse.jhu.edu/article/883479&gt;. Díaz I, Williams N, Hoffman K, Schneck E (2021). \"Non-parametric causal effects based on longitudinal modified treatment policies.\" _Journal of the American Statistical Association_. doi:10.1080/01621459.2021.1955691 &lt;https://doi.org/10.1080/01621459.2021.1955691&gt;.\n  - Wright MN, Ziegler A (2017). \"ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.\" _Journal of Statistical Software_, *77*(1), 1-17. doi:10.18637/jss.v077.i01 &lt;https://doi.org/10.18637/jss.v077.i01&gt;.\n  - Xie Y (2024). _knitr: A General-Purpose Package for Dynamic Report Generation in R_. R package version 1.49, &lt;https://yihui.org/knitr/&gt;. Xie Y (2015). _Dynamic Documents with R and knitr_, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, &lt;https://yihui.org/knitr/&gt;. Xie Y (2014). \"knitr: A Comprehensive Tool for Reproducible Research in R.\" In Stodden V, Leisch F, Peng RD (eds.), _Implementing Reproducible Computational Research_. Chapman and Hall/CRC. ISBN 978-1466561595.\n  - Xie Y (2024). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.54, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;.\n  - Zeileis A, Croissant Y (2010). \"Extended Model Formulas in R: Multiple Parts and Multiple Responses.\" _Journal of Statistical Software_, *34*(1), 1-13. doi:10.18637/jss.v034.i01 &lt;https://doi.org/10.18637/jss.v034.i01&gt;.\n  - Zeileis A, Köll S, Graham N (2020). \"Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.\" _Journal of Statistical Software_, *95*(1), 1-36. doi:10.18637/jss.v095.i01 &lt;https://doi.org/10.18637/jss.v095.i01&gt;. Zeileis A (2004). \"Econometric Computing with HC and HAC Covariance Matrix Estimators.\" _Journal of Statistical Software_, *11*(10), 1-17. doi:10.18637/jss.v011.i10 &lt;https://doi.org/10.18637/jss.v011.i10&gt;. Zeileis A (2006). \"Object-Oriented Computation of Sandwich Estimators.\" _Journal of Statistical Software_, *16*(9), 1-16. doi:10.18637/jss.v016.i09 &lt;https://doi.org/10.18637/jss.v016.i09&gt;.\n  - Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. R package version 1.4.0, &lt;https://CRAN.R-project.org/package=kableExtra&gt;."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "content/more-old-advice.html",
    "href": "content/more-old-advice.html",
    "title": "Measurement Matters",
    "section": "",
    "text": "Today, we consider deep into data analysis for causal inference as it applies to observational cultural psychology. By the end, you will:\n\nBetter understand how to integrate measurement theory with causal inference\nEnhance your proficiency in causal analysis using doubly robust methods\nGain insights into the application of sensitivity analysis using E-Values"
  },
  {
    "objectID": "content/more-old-advice.html#overview",
    "href": "content/more-old-advice.html#overview",
    "title": "Measurement Matters",
    "section": "",
    "text": "Today, we consider deep into data analysis for causal inference as it applies to observational cultural psychology. By the end, you will:\n\nBetter understand how to integrate measurement theory with causal inference\nEnhance your proficiency in causal analysis using doubly robust methods\nGain insights into the application of sensitivity analysis using E-Values"
  },
  {
    "objectID": "content/more-old-advice.html#set-up-your-workspace-.",
    "href": "content/more-old-advice.html#set-up-your-workspace-.",
    "title": "Measurement Matters",
    "section": "Set up your workspace .",
    "text": "Set up your workspace .\n\n\nCode\n# Before running this source code, make sure to update to the current version of R, and to update all existing packages.\n\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/libs2.R\")\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/funs.R\")\n\n\nsource(\"/Users/joseph/GIT/templates/functions/experimental_funs.R\")\n\n\n# ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB\n# source(\n#   \"https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R\"\n# )\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n\n# A function we will use for our tables.\ntab_ate_subgroup_rd &lt;- function(x,\n                                new_name,\n                                delta = 1,\n                                sd = 1) {\n  # Check if required packages are installed\n  required_packages &lt;- c(\"EValue\", \"dplyr\")\n  new_packages &lt;-\n    required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  require(EValue)\n  require(dplyr)\n  \n  # check if input data is a dataframe\n  if (!is.data.frame(x))\n    stop(\"Input x must be a dataframe\")\n  \n  # Check if required columns are in the dataframe\n  required_cols &lt;- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols &lt;- required_cols[!(required_cols %in% colnames(x))]\n  if (length(missing_cols) &gt; 0)\n    stop(\"Missing columns in dataframe: \",\n         paste(missing_cols, collapse = \", \"))\n  \n  # Check if lower_ci and upper_ci do not contain NA values\n  if (any(is.na(x$lower_ci), is.na(x$upper_ci)))\n    stop(\"Columns 'lower_ci' and 'upper_ci' should not contain NA values\")\n  \n  x &lt;- x %&gt;%\n    dplyr::mutate(across(where(is.numeric), round, digits = 3)) %&gt;%\n    dplyr::rename(\"E[Y(1)]-E[Y(0)]\" = estimate)\n  \n  x$standard_error &lt;- abs(x$lower_ci - x$upper_ci) / 3.92\n  \n  evalues_list &lt;- lapply(seq_len(nrow(x)), function(i) {\n    row_evalue &lt;- EValue::evalues.OLS(\n      x[i, \"E[Y(1)]-E[Y(0)]\"],\n      se = x[i, \"standard_error\"],\n      sd = sd,\n      delta = delta,\n      true = 0\n    )\n    # If E_value is NA, set it to 1\n    if (is.na(row_evalue[2, \"lower\"])) {\n      row_evalue[2, \"lower\"] &lt;- 1\n    }\n    if (is.na(row_evalue[2, \"upper\"])) {\n      row_evalue[2, \"upper\"] &lt;- 1\n    }\n    data.frame(round(as.data.frame(row_evalue)[2, ], 3)) # exclude the NA column\n  })\n  \n  evalues_df &lt;- do.call(rbind, evalues_list)\n  colnames(evalues_df) &lt;- c(\"E_Value\", \"E_Val_bound\")\n  \n  tab_p &lt;- cbind(x, evalues_df)\n  \n  tab &lt;-\n    tab_p |&gt; select(c(\n      \"E[Y(1)]-E[Y(0)]\",\n      \"lower_ci\",\n      \"upper_ci\",\n      \"E_Value\",\n      \"E_Val_bound\"\n    ))\n  \n  return(tab)\n}\n\n# extra packages we need\n# for efa/cfa\nif (!require(psych)) {\n  install.packages(\"psych\")\n  library(\"psych\")\n}\n\n# for reporting\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(\"parameters\")\n}\n\n# for graphing\nif (!require(see)) {\n  install.packages(\"see\")\n  library(\"see\")\n}\n\n# for graphing\nif (!require(lavaan)) {\n  install.packages(\"lavaan\")\n  library(\"lavaan\")\n}\n\n\n# for graphing\nif (!require(datawizard)) {\n  install.packages(\"datawizard\")\n  library(\"datawizard\")\n}\n\n\n\nImport the data\n\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\n\nnzavs_synth &lt;- arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n\nNext, we will inspect column names.\nMake sure to familiarise your self with the variable names here\nIt is alwasy a good idea to plot the data (do on your own time.)"
  },
  {
    "objectID": "content/more-old-advice.html#revisit-the-checklist",
    "href": "content/more-old-advice.html#revisit-the-checklist",
    "title": "Measurement Matters",
    "section": "Revisit the checklist",
    "text": "Revisit the checklist\nIt is essential to remember our checklist:\n\nClearly state your question.\nExplain its relevance.\nEnsure your question is causal.\nDevelop a subgroup analysis question if applicable.\n\nOur discussion today revolves around two main questions:\n\nDoes exercise influence anxiety/depression?\nDo these effects differ among NZ Europeans and Māori?\n\nWhile these questions offer a starting point, they lack specificity. We need to clarify:\n\nThe amount, regularity, and duration of exercise\nThe measures of depression to be used\nThe expected timeline for observing the effects\n\nRemember, we can clarify these by emulating a hypothetical experiment, a concept we call the Target Trial.\nOur initial responses will be guided by the NZAVS measure of exercise, focusing on the hours of activity per week, the 1-year effect on Kessler-6 depression after initiating a change in exercise, and a particular emphasis on effect-modification by NZ European and Māori ethnic identification.\nThis analysis has practical motivation, as the effects of exercise on mental health and possible differences between cultural groups remain largely uncharted territory.\nOur initial responses will be guided by the NZAVS measure of exercise, focusing on the hours of activity per week, the 1-year effect on Kessler-6 depression after initiating a change in exercise, and a particular emphasis on effect-modification by NZ European and Māori ethnic identification. This analysis has practical motivation, as the effects of exercise on mental health and possible differences between cultural groups remain largely uncharted territory.\n\nSculpting the Data: A Hands-On Approach\nAs we venture further, we’ll perform a series of transformations to shape our data according to our needs. Our process will involve:\n\nConstructing a Kessler 6 average score\nBuilding a Kessler 6 sum score\nCoarsening the Exercise score\n\nConsider the ambiguity in the NZAVS exercise question: “During the past week, list ‘Hours spent exercising/physical activity’.” Different people interpret physical activity differently; John may consider any wakeful time as physical activity, while Jane counts only aerobic exercise. Such variation underlines the importance of the consistency assumption in causal inference. But we’ll delve deeper into that later.\nFor now, let’s transform our indicators.\n\n# create sum score of kessler 6\ndt_start &lt;- nzavs_synth %&gt;%\n  arrange(id, wave) %&gt;%\n  rowwise() %&gt;%\n  mutate(lifesat_composite  = mean(\n    c(lifesat_satlife,                         \n    lifesat_ideal) ))|&gt; \n  mutate(kessler_6  = mean(\n    # Specify the Kessler scale items\n    c(\n      kessler_depressed,\n      # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      kessler_hopeless,\n      # During the last 30 days, how often did you feel hopeless?\n      kessler_nervous,\n      # During the last 30 days, how often did you feel nervous?\n      kessler_effort,\n      # During the last 30 days, how often did you feel that everything was an effort?\n      kessler_restless,\n      # During the last 30 days, how often did you feel restless or fidgety ?\n      kessler_worthless  # During the last 30 days, how often did you feel worthless?\n    )\n  )) |&gt;\n  mutate(kessler_6_sum = round(sum(\n    c (\n      kessler_depressed,\n      kessler_hopeless,\n      kessler_nervous,\n      kessler_effort,\n      kessler_restless,\n      kessler_worthless\n    )\n  ),\n  digits = 0)) |&gt;  ungroup() |&gt;\n  # Coarsen 'hours_exercise' into categories\n  mutate(\n    hours_exercise_coarsen = cut(\n      hours_exercise,\n      # Hours spent exercising/ physical activity\n      breaks = c(-1, 3, 8, 200),\n      labels = c(\"inactive\",\n                 \"active\",\n                 \"very_active\"),\n      # Define thresholds for categories\n      levels = c(\"(-1,3]\", \"(3,8]\", \"(8,200]\"),\n      ordered = TRUE\n    ))|&gt;\n  # Create a binary 'urban' variable based on the 'rural_gch2018' variable\n  mutate(urban = factor(\n    ifelse(\n      rural_gch2018 == \"medium_urban_accessibility\" |\n        # Define urban condition\n        rural_gch2018 == \"high_urban_accessibility\",\n      \"urban\",\n      # Label 'urban' if condition is met\n      \"rural\"  # Label 'rural' if condition is not met\n    )\n  ))\n\nWhy do we coarsen the exposure? Recall the consistency assumption of causal inference:\nConsistency: Can I interpret what it means to intervene on the exposure? I should be able to.\nWhat is th hypothetical experiment here for change in exercise?\nThrough data wrangling, we can answer our research questions more effectively by manipulating variables into more meaningful and digestible forms. We imagine an experiment in which people were within one band of the coarsened exercise band and we\nThese data checks will ensure the accuracy and reliability of our transformations, setting the foundation for solid data analysis.\n\n\nCode\n# do some checks\nlevels(dt_start$hours_exercise_coarsen)\ntable(dt_start$hours_exercise_coarsen)\nmax(dt_start$hours_exercise)\nmin(dt_start$hours_exercise)\n# checks\n\n\n# justification for transforming exercise\" has a very long tail\nhist(dt_start$hours_exercise, breaks = 1000)\n# consider only those cases below &lt; or = to 20\nhist(subset(dt_start, hours_exercise &lt;= 20)$hours_exercise)\nhist(as.numeric(dt_start$hours_exercise_coarsen))\n\n\n\n\nCreate variables for the latent factors\nLet’s next get the data into shape for analysis. Here we create a variable for the two factors (see Appendix)\n\n# get two factors from data\ndt_start2 &lt;- dt_start |&gt;\n  arrange(id, wave) |&gt;\n  rowwise() |&gt;\n  mutate(\n    kessler_latent_depression = mean(c(kessler_depressed, kessler_hopeless, kessler_effort), na.rm = TRUE),\n    kessler_latent_anxiety  = mean(c(kessler_effort, kessler_nervous, kessler_restless), na.rm = TRUE)\n  ) |&gt;\n  ungroup()\n\nInspect the data: anxiety\n\n#hist(dt_start2$kessler_latent_anxiety, by = dt_start2$eth_cat)\n\ncreate_histograms_anxiety &lt;- function(df) {\n  # require patchwork\n  library(patchwork)\n  \n  # separate the data by eth_cat\n  df1 &lt;- df %&gt;% filter(eth_cat == \"euro\") # replace \"level_1\" with actual level\n  df2 &lt;- df %&gt;% filter(eth_cat == \"māori\") # replace \"level_2\" with actual level\n  \n  # create the histograms\n  p1 &lt;- ggplot(df1, aes(x=kessler_latent_anxiety)) +\n    geom_histogram(binwidth = 1, fill = \"dodgerblue\", color = \"black\") +\n    ggtitle(\"Kessler Latent Anxiety: NZ Euro\") +\n    xlab(\"kessler_latent_anxiety\") +\n    ylab(\"Count\")\n  \n  p2 &lt;- ggplot(df2, aes(x=kessler_latent_anxiety)) +\n    geom_histogram(binwidth = 1, fill = \"brown\", color = \"black\") +\n    ggtitle(\"Kessler Latent Anxiety: Māori\") +\n    xlab(\"kessler_latent_anxiety\") +\n    ylab(\"Count\")\n  \n  # plot the histograms\n p1 + p2 + plot_annotation(tag_levels = \"a\", title = \"comparison of anxiety histograms\")\n}\n\ncreate_histograms_anxiety(dt_start2)\n\n\ncreate_histograms_depression &lt;- function(df) {\n  # require patchwork\n  library(patchwork)\n  \n  # separate the data by eth_cat\n  df11 &lt;- df %&gt;% filter(eth_cat == \"euro\") # replace \"level_1\" with actual level\n  df22 &lt;- df %&gt;% filter(eth_cat == \"māori\") # replace \"level_2\" with actual level\n  \n  # create the histograms\n  p11 &lt;- ggplot(df11, aes(x=kessler_latent_depression)) +\n    geom_histogram(binwidth = 1, fill = \"dodgerblue\", color = \"black\") +\n    ggtitle(\"Kessler Latent Depression: NZ Euro\") +\n    xlab(\"kessler_latent_depression\") +\n    ylab(\"Count\") + theme_classic()\n  \n  p22 &lt;- ggplot(df22, aes(x=kessler_latent_depression)) +\n    geom_histogram(binwidth = 1, fill = \"brown\", color = \"black\") +\n    ggtitle(\"Kessler Latent Depression: Māori\") +\n    xlab(\"kessler_latent_depression\") +\n    ylab(\"Count\") + theme_classic()\n  \n  # plot the histograms\n p11 + p22 + plot_annotation(tag_levels = \"a\", title = \"comparison of depression histograms\")\n}\n\ncreate_histograms_depression(dt_start2)\n\nWhat do you make of these histograms?"
  },
  {
    "objectID": "content/more-old-advice.html#investigate-assumption-of-positivity",
    "href": "content/more-old-advice.html#investigate-assumption-of-positivity",
    "title": "Measurement Matters",
    "section": "Investigate assumption of positivity:",
    "text": "Investigate assumption of positivity:\nRecall the positive assumption:\nPositivity: Can we intervene on the exposure at all levels of the covariates? We should be able to.\nNot this is just a description of the the summary scores. We do not assess change within indivuals\n\n#  select only the baseline year and the exposure year.  That will give us change in the exposure. ()\ndt_exposure &lt;- dt_start2 |&gt;\n\n  # select baseline year and exposure year\n  filter(wave == \"2018\" | wave == \"2019\") |&gt;\n\n  # select variables of interest\n  select(id, wave, hours_exercise_coarsen,  eth_cat) |&gt;\n\n  # the categorical variable needs to be numeric for us to use msm package to investigate change\n  mutate(hours_exercise_coarsen_n = as.numeric(hours_exercise_coarsen)) |&gt;\n  droplevels()\n\n\n# check\ndt_exposure |&gt;\n  tabyl(hours_exercise_coarsen_n, eth_cat,  wave )\n\nI’ve written a function called transition_table that will help us assess change in the exposure at the individual level.\n\n#   consider people going from active to vary active\nout &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure)\n\n\n# for a function I wrote to create state tables\nstate_names &lt;- c(\"Inactive\", \"Somewhat Active\", \"Active\", \"Extremely Active\")\n\nNext consider Māori only\n\n# Maori only\n\ndt_exposure_maori &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"māori\")\n\nout_m &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_maori)\n\n# with this little support we might consider parametric models\n#t_tab_m&lt;- transition_table_2( out_m, state_names)\n\n#interpretation\n# cat(t_tab_m$explanation)\n# print(t_tab_m$table)\n\n\n# filter euro\ndt_exposure_euro &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"euro\")\n\n# model change\nout_e &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_euro)\n\n\n# creat transition table.\n# t_tab_e &lt;- transition_table_2( out_e, state_names)\n\n#interpretation\n# cat(t_tab_e$explanation)\n# \n# # table\n# print(t_tab_e$table)\n\nOverall we find evidence for change in the exposure variable. This suggest that we are ready to proceed with the next step of causal estimation.\n\nCreate wide data frame for analysis\nRecall, I wrote a function for you that will put the data into temporal order such that measurement of the exposure and outcome appear at baseline, along with a rich set of baseline confounders, the exposure appears in the following wave, and the outcome appears in the wave following the exposure.\nThe graph encodes our assumptions about the world. It is a qualitative instrument to help us understand how to move from our assumptions to decisions about our analysis, in the first instance, the decision about whether to proceed with an analysis.\nIt is perhaps useful here to stop and consider what does this graph implies.\nQuestion: 1. Does the graph imply unmeasured confounding?\nQuestion 2. If there is unmeasured confounding, should we proceed?\n\n\nE-value\n\nThe minimum strength of association on the risk ratio scale that an unmeasured confounder would need to have with both the exposure and the outcome, conditional on the measured covariates, to fully explain away a specific exposure-outcome association\n\nSee: [VanderWeele, Mathur, and Chen (2020)](mathur2018a?)\nFor example, suppose that the lower bound of the the E-value was 1.3 with the lower bound of the confidence interval = 1.12, we might then write:\n\nWith an observed risk ratio of RR=1.3, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 1.3-fold each (or 30%), above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 1.12-fold (or 12%) each could do so, but weaker joint confounder associations could not.\n\nThe equations are as follows (for risk ratios)\n\nE-value_{RR} = RR + \\sqrt{RR \\times (RR - 1)}\n \nE-value_{LCL} = LCL + \\sqrt{LCL \\times (LCL - 1)}\n\nHere is an R function that will calculate E-values\n\ncalculate_e_value &lt;- function(rr, lcl) {\n  e_value_rr = rr + sqrt(rr*(rr - 1))\n  e_value_lcl = lcl + sqrt(lcl*(lcl - 1))\n  \n  list(e_value_rr = e_value_rr, e_value_lcl = e_value_lcl)\n}\n\n# e.g. smoking causes cancer\n\n# finding   RR = 10.73 (95% CI: 8.02, 14.36)\n\ncalculate_e_value(10.73, 8.02)\n\nWe write:\n\nWith an observed risk ratio of RR=10.7, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 20.9-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 15.5-fold each could do so, but weaker joint confounder associations could not.\n\nNote that in this class, most of the outcomes will be (standardised) continuous outcomes. Here’s a function and LaTeX code to describe the approximation.\nThis function takes a linear regression coefficient estimate (est), its standard error (se), the standard deviation of the outcome (sd), a contrast of interest in the exposure (delta, which defaults to 1), and a “true” standardized mean difference (true, which defaults to 0). It calculates the odds ratio using the formula from Chinn (2000) and VanderWeele (2017), and then uses this to calculate the E-value.\n\n#| label: evalue_ols\n\ncompute_evalue_ols &lt;- function(est, se, delta = 1, true = 0) {\n  # Rescale estimate and SE to reflect a contrast of size delta\n  est &lt;- est / delta\n  se &lt;- se / delta\n\n  # Compute transformed odds ratio and confidence intervals\n  odds_ratio &lt;- exp(0.91 * est)\n  lo &lt;- exp(0.91 * est - 1.78 * se)\n  hi &lt;- exp(0.91 * est + 1.78 * se)\n\n  # Compute E-Values based on the RR values\n  evalue_point_estimate &lt;- odds_ratio * sqrt(odds_ratio + 1)\n  evalue_lower_ci &lt;- lo * sqrt(lo + 1)\n\n  # Return the E-values\n  return(list(EValue_PointEstimate = evalue_point_estimate,\n              EValue_LowerCI = evalue_lower_ci))\n}\n\n\n\n\n# exampl:\n# suppose we have an estimate of 0.5, a standard error of 0.1, and a standard deviation of 1.\n# This would correspond to a half a standard deviation increase in the outcome per unit increase in the exposure.\nresults &lt;- compute_evalue_ols(est = 0.5, se = 0.1, delta = 1)\nprint(results)\n\nWe write:\n\nWith an observed risk ratio of RR=2.92, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 2.92-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 2.23-fold each could do so, but weaker joint confounder associations could not.\n\nNote the E-values package will do the computational work for us (note we get slightly different estimates)\nNote:\nFirst, the fucntion converts the estimate to an odds ratio:\n\nOdds Ratio Conversion:\n(OddsRatio = e^{ })\n\nThen, it calculates the confidence intervals for the odds ratio:\n\nConfidence Intervals:\n(LowerConfidenceInterval = e^{log(OddsRatio) - 1.78 SE})\n(UpperConfidenceInterval = e^{log(OddsRatio) + 1.78 SE})\n\nFinally, it calculates the E-value for the point estimate and the lower confidence interval:\n\nE-Values Calculation:\n(EValue_{PointEstimate} = OddsRatio + )\n(EValue_{LowerCI} = LowerConfidenceInterval + )\n\n[[JB: NEED TO CHECK]]\nGenerally, best to use the EValue function.\n\nlibrary(EValue)\n\nEValue::evalues.OLS(est = 0.5, se = 0.1, sd = 1, delta = 1, true = 0)\n\n\n############## ############## ############## ############## ############## ############## ############## ########\n####  ####  ####  CREATE DATA FRAME FOR ANALYSIS ####  ####  ################## ############## ######## #########\n############## ############## ############## ############## ############## ############## ############# #########\n\n\n# I have created a function that will put the data into the correct shape. Here are the steps.\n\n# Step 1: choose baseline variables (confounders).  here we select standard demographic variablees plus personality variables.\n\n# Note again that the function will automatically include the baseline exposure and basline outcome in the baseline variable confounder set so you don't need to include these. \n\n\n# here are some plausible baseline confounders\nbaseline_vars = c(\n  \"edu\",\n  \"male\",\n  \"eth_cat\",\n  \"employed\",\n  \"gen_cohort\",\n  \"nz_dep2018\", # nz dep\n  \"nzsei13\", # occupational prestige\n  \"partner\",\n  \"parent\",\n  \"pol_orient\",\n # \"rural_gch2018\",\n   \"urban\", # use the two level urban varaible. \n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"honesty_humility\",\n  \"openness\",\n  \"neuroticism\",\n  \"modesty\",\n  \"religion_identification_level\"\n)\n\n\n## Step 2, select the exposure variable.  This is the \"cause\"\nexposure_var = c(\"hours_exercise_coarsen\")\n\n\n## step 3. select the outcome variable.  These are the outcomes.\noutcome_vars_reflective = c(\"kessler_latent_anxiety\",\n                            \"kessler_latent_depression\")\n\n\n\n# the function \"create_wide_data\" should be in your environment.\n# If not, make sure to run the first line of code in this script once more.  You may ignore the warnings. or uncomment and run the code below\n# source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\ndt_prepare &lt;-\n  create_wide_data(\n    dat_long = dt_start2,\n    baseline_vars = baseline_vars,\n    exposure_var = exposure_var,\n    outcome_vars = outcome_vars_reflective\n  )"
  },
  {
    "objectID": "content/more-old-advice.html#descriptive-table",
    "href": "content/more-old-advice.html#descriptive-table",
    "title": "Measurement Matters",
    "section": "Descriptive table",
    "text": "Descriptive table\n\n\nCode\n# I have created a function that will allow you to take a data frame and\n# create a table\nbaseline_table(dt_prepare, output_format = \"markdown\")\n\n# but it is not very nice. Next up, is a better table\n\n\n\n# get data into shape\ndt_new &lt;- dt_prepare %&gt;%\n  select(starts_with(\"t0\")) %&gt;%\n  rename_all( ~ stringr::str_replace(., \"^t0_\", \"\")) %&gt;%\n  mutate(wave = factor(rep(\"baseline\", nrow(dt_prepare)))) |&gt;\n  janitor::clean_names(case = \"screaming_snake\")\n\n\n# create a formula string\nbaseline_vars_names &lt;- dt_new %&gt;%\n  select(-WAVE) %&gt;%\n  colnames()\n\ntable_baseline_vars &lt;-\n  paste(baseline_vars_names, collapse = \"+\")\n\nformula_string_table_baseline &lt;-\n  paste(\"~\", table_baseline_vars, \"|WAVE\")\n\ntable1::table1(as.formula(formula_string_table_baseline),\n               data = dt_new,\n               overall = FALSE)\n\n\n# another method for making a table\n# x &lt;- table1::table1(as.formula(formula_string_table_baseline),\n#                     data = dt_new,\n#                     overall = FALSE)\n\n# # some options, see: https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\n# table1::t1kable(x, format = \"html\", booktabs = TRUE) |&gt;\n#   kable_material(c(\"striped\", \"hover\"))\n\nWe need to do some more data wrangling, alas! Data wrangling is the majority of data analysis. The good news is that R makes wrangling relatively straightforward.\n\nmutate(id = factor(1:nrow(dt_prepare))): This creates a new column called id that has unique identification factors for each row in the dataset. It ranges from 1 to the number of rows in the dataset.\nThe next mutate operation is used to convert the t0_eth_cat, t0_urban, and t0_gen_cohort variables to factor type, if they are not already.\nThe filter command is used to subset the dataset to only include rows where the t0_eth_cat is either “euro” or “māori”. The original dataset includes data with four different ethnic categories. This command filters out any row not related to these two groups.\nungroup() ensures that there’s no grouping in the dataframe.\nThe mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) step standardizes all numeric columns in the dataset by subtracting the mean and dividing by the standard deviation (a z-score transformation). The resulting columns are renamed to include “_z” at the end of their original names.\nThe select function is used to keep only specific columns: the id column, any columns that are factors, and any columns that end in “_z”.\nThe relocate functions re-order columns. The first relocate places the id column at the beginning. The next three relocate functions order the rest of the columns based on their names: those starting with “t0_” are placed before “t1_” columns, and those starting with “t2_” are placed after “t1_” columns.\ndroplevels() removes unused factor levels in the dataframe.\nFinally, skimr::skim(dt) will print out a summary of the data in the dt object using the skimr package. This provides a useful overview of the data, including data types and summary statistics.\n\nThis function seems to be part of a data preparation pipeline in a longitudinal or panel analysis, where observations are ordered over time (indicated by t0_, t1_, t2_, etc.).\n\n### ### ### ### ### ### SUBGROUP DATA ANALYSIS: DATA WRANGLING  ### ### ### ###\n\ndt &lt;- dt_prepare|&gt;\n  mutate(id = factor(1:nrow(dt_prepare))) |&gt;\n  mutate(\n  t0_eth_cat = as.factor(t0_eth_cat),\n  t0_urban = as.factor(t0_urban),\n  t0_gen_cohort = as.factor(t0_gen_cohort)\n) |&gt;\n  dplyr::filter(t0_eth_cat == \"euro\" |\n                t0_eth_cat == \"māori\") |&gt; # Too few asian and pacific\n  ungroup() |&gt;\n  # transform numeric variables into z scores (improves estimation)\n  dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %&gt;%\n  # select only factors and numeric values that are z-scores\n  select(id, # category is too sparse\n         where(is.factor),\n         ends_with(\"_z\"), ) |&gt;\n  # tidy data frame so that the columns are ordered by time (useful for more complex models)\n  relocate(id, .before = starts_with(\"t1_\"))   |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |&gt;\n  relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\")) |&gt;\n  droplevels()\n\n# view object\nskimr::skim(dt)\n\n\n# quick cross table\n#table( dt$t1_hours_exercise_coarsen, dt$t0_eth_cat )\n\n# checks\nhist(dt$t2_kessler_latent_depression_z)\nhist(dt$t2_kessler_latent_anxiety_z)\n\ndt |&gt;\n  tabyl(t0_eth_cat, t1_hours_exercise_coarsen ) |&gt;\n  kbl(format = \"markdown\")\n\n# Visualise missingness\nnaniar::vis_miss(dt)\n\n# save your dataframe for future use\n\n# make dataframe\ndt = as.data.frame(dt)\n\n# save data\nsaveRDS(dt, here::here(\"data\", \"dt\"))"
  },
  {
    "objectID": "content/more-old-advice.html#propensity-scores",
    "href": "content/more-old-advice.html#propensity-scores",
    "title": "Measurement Matters",
    "section": "Propensity scores",
    "text": "Propensity scores\nNext we generate propensity scores. Instead of modelling the outcome (t2_y) we will model the exposure (t1_x) as predicted by baseline indicators (t0_c) that we assume may be associated with the outcome and the exposure.\nThe first step is to obtain the baseline variables. note that we must remove “t0_eth_cat” because we are performing separate weighting for each stratum within this variable.\n\n# read  data -- you may start here if you need to repeat the analysis\ndt &lt;- readRDS(here::here(\"data\", \"dt\"))\n\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\n\n# define our exposure\nX &lt;- \"t1_hours_exercise_coarsen\"\n\n# define subclasses\nS &lt;- \"t0_eth_cat\"\n\n# Make sure data is in a data frame format\ndt &lt;- data.frame(dt)\n\n\n# next we use our trick for creating a formula string, which will reduce our work\nformula_str_prop &lt;-\n  paste(X,\n        \"~\",\n        paste(baseline_vars_reflective_propensity, collapse = \"+\"))\n\n# this shows the exposure variable as predicted by the baseline confounders.\n\nFor propensity score analysis, we will try several different approaches. We will want to select the method that produces the best balance.\nI typically use ps (classical propensity scores), ebal and energy. The latter two in my experience yeild good balance. Also energy will work with continuous exposures.\nFor more information, see https://ngreifer.github.io/WeightIt/\n\n# traditional propensity scores-- note we select the ATT and we have a subgroup \ndt_match_ps &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ps\"\n)\n\nsaveRDS(dt_match_ps, here::here(\"data\", \"dt_match_ps\"))\n\n\n# ebalance\ndt_match_ebal &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ebal\"\n)\n\n# save output\nsaveRDS(dt_match_ebal, here::here(\"data\", \"dt_match_ebal\"))\n\n\n\n## energy balance method\ndt_match_energy &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  #focal = \"high\", # for use with ATT\n  method = \"energy\"\n)\nsaveRDS(dt_match_energy, here::here(\"data\", \"dt_match_energy\"))\n\nResults, first for Europeans\n\n#dt_match_energy &lt;- readRDS(here::here(\"data\", \"dt_match_energy\"))\ndt_match_ebal &lt;- readRDS(here::here(\"data\", \"dt_match_ebal\"))\n#dt_match_ps &lt;- readRDS(here::here(\"data\", \"dt_match_ps\"))\n\n# next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2. \n# note that if the variables are unlikely to influence the outcome we can be less strict. \n\n#See: Hainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n# Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of\n# Epidemiology 2008; 168(6):656–664.\n\n# Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies\n# Peter C. Austin, Elizabeth A. Stuart\n# https://onlinelibrary.wiley.com/doi/10.1002/sim.6607\n\n#bal.tab(dt_match_energy$euro)   #  good\nbal.tab(dt_match_ebal$euro)   #  best\n#bal.tab(dt_match_ps$euro)   #  not as good\n\n# here we show only the best tab, but you should put all information into an appendix\n\nResults for Maori\n\n# who only Ebal\n#bal.tab(dt_match_energy$māori)   #  good\nbal.tab(dt_match_ebal$māori)   #  best\n#bal.tab(dt_match_ps$māori)   #  not good\n\n\n# code for summar\nsum_e &lt;- summary(dt_match_ebal$euro)\nsum_m &lt;- summary(dt_match_ebal$māori)\n\n# summary euro\nsum_e\n\n# summary maori\nsum_m\n\n\nlove_plot_e &lt;- love.plot(dt_match_ebal$euro,\n          binary = \"std\",\n          thresholds = c(m = .1))+ labs(title = \"NZ Euro Weighting: method e-balance\")\n\n# plot\nlove_plot_e \n\n\nlove_plot_m &lt;- love.plot(dt_match_ebal$māori,\n          binary = \"std\",\n          thresholds = c(m = .1)) + labs(title = \"Māori Weighting: method e-balance\")\n# plot\nlove_plot_m\n\n\nExample Summary NZ Euro Propensity scores.\nWe estimated propensity score analysis using entropy balancing, energy balancing and traditional propensity scores. Of these approaches, entropy balancing provided the best balance. The results indicate an excellent balance across all variables, with Max.Diff.Adj values significantly below the target threshold of 0.05 across a range of binary and continuous baseline confounders, including gender, generation cohort, urban_location, exercise hours (coarsened, baseline), education, employment status, depression, anxiety, and various personality traits. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs.\nThe effective sample sizes were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 2880, 3927, and 1834, respectively. After adjustment, the effective sample sizes were reduced to 1855.89, 3659.59, and 1052.01, respectively.\nThe weight ranges for the inactive, active, and very active groups varied, with the inactive group showing the widest range (0.2310 to 7.0511) and the active group showing the narrowest range (0.5769 to 1.9603). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\nWe also identified the units with the five most extreme weights by group. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\nWe plotted these results using love plots, visually confirming both the balance in the propensity score model using entropy balanced weights, and the imbalance in the model that does not adjust for baseline confounders.\nOverall, these findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, conditional on the measured covariates included in the model.\n\n\nExample Summary Maori Propensity scores.\nResults:\nThe entropy balancing method was also the best performing method that was applied to a subgroup analysis of the Māori population. Similar to the NZ European subgroup analysis, the method achieved a high level of balance across all treatment pairs for the Māori subgroup. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs for the Māori subgroup.\nThe effective sample sizes for the Māori subgroup were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 307, 354, and 160, respectively. After adjustment, the effective sample sizes were reduced to 220.54, 321.09, and 76.39, respectively\nThe weight ranges for the inactive, active, and very active groups in the Māori subgroup varied, with the inactive group showing the widest range (0.2213 to 3.8101) and the active group showing the narrowest range (0.3995 to 1.9800). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\nThe study also identified the units with the five most extreme weights by group for the Māori subgroup. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\nIn conclusion, the results of the Māori subgroup analysis are consistent with the overall analysis. The entropy balancing method achieved a high level of balance across all treatment pairs, with Max.Diff.Adj values significantly below the target threshold. These findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, even in subgroup analyses.\n\n\nMore data wrangling\nNote that we need to attach the weights from the propensity score model back to the data.\nHowever, because our weighting analysis estimates a model for the exposure, we only need to do this analysis once, no matter how many outcomes we investigate. So there’s a little good news.\n\n# prepare nz_euro data\ndt_ref_e &lt;- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans\n\n# add weights\ndt_ref_e$weights &lt;- dt_match_ebal$euro$weights # get weights from the ps matching model,add to data\n\n# prepare maori data\ndt_ref_m &lt;- subset(dt, t0_eth_cat == \"māori\")# original data subset only maori\n\n# add weights\ndt_ref_m$weights &lt;- dt_match_ebal$māori$weights # get weights from the ps matching model, add to data\n\n# combine data into one data frame\ndt_ref_all &lt;- rbind(dt_ref_e, dt_ref_m) # combine the data into one dataframe."
  },
  {
    "objectID": "content/more-old-advice.html#graph-of-the-result",
    "href": "content/more-old-advice.html#graph-of-the-result",
    "title": "Measurement Matters",
    "section": "Graph of the result",
    "text": "Graph of the result\nI’ve create a function you can use to graph your results. Here is the code, adjust to suit.\n\n# group tables\nsub_group_plot_ate(big_tab, title = \"Effect of Exercise on Anxiety\", subtitle = \"Subgroup Analysis: NZ Euro and Māori\", xlab = \"Groups\", ylab = \"Effects\",\n                 x_offset = -1,\n                           x_lim_lo = -1,\n                           x_lim_hi = 1.5)\n\n\nReport the anxiety result.\n\nFor the New Zealand European group, our results suggest that exercise potentially reduces anxiety, with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of -0.077. The associated confidence interval, ranging from -0.131 to -0.022, does not cross zero, providing more certainty in our estimate.\n\n\nE-values quantify the minimum strength of association that an unmeasured confounding variable would need to have with both the treatment and outcome, to fully explain away our observed effect. In this case, any unmeasured confounder would need to be associated with both exercise and anxiety reduction, with a risk ratio of at least 1.352 to explain away the observed effect, and at least 1.167 to shift the confidence interval to include a null effect.\n\n\nTurning to the Māori group, the data suggest a possible reducing effect of exercise on anxiety, with a causal contrast value of 0.027. Yet, the confidence interval for this estimate (-0.114 to 0.188) also crosses zero, indicating similar uncertainties. An unmeasured confounder would need to have a risk ratio of at least 1.183 with both exercise and anxiety to account for our observed effect, and a risk ratio of at least 1 to render the confidence interval inclusive of a null effect.\n\n\nThus, while our analysis suggests that exercise could potentially reduce anxiety in both New Zealand Europeans and Māori, we advise caution in interpretation. The confidence intervals crossing zero reflect substantial uncertainties, and the possible impact of unmeasured confounding factors further complicates the picture.\n\nHere’s a function that will do much of this work for you. However, you’ll need to adjust it, and supply your own interpretation.\n\n#|label: interpretation function\n#| eval: false\ninterpret_results_subgroup &lt;- function(df, outcome, exposure) {\n  df &lt;- df %&gt;%\n    mutate(\n      report = case_when(\n        E_Val_bound &gt; 1.2 & E_Val_bound &lt; 2 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests stronger confidence in our findings.\"\n        ),\n        E_Val_bound &gt;= 2 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"With an observed risk ratio of RR = \", E_Value, \", an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each could do so, but weaker joint confounder associations could not. Here we find stronger evidence that the result is robust to unmeasured confounding.\"\n        ),\n        E_Val_bound &lt; 1.2 & E_Val_bound &gt; 1 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\"\n        ),\n        E_Val_bound == 1 ~ paste0(\n          \"For the \", group, \", the data suggests a potential effect of \", exposure, \" on \", outcome, \", with a causal contrast value of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"However, the confidence interval for this estimate, ranging from \", `2.5 %`,\" to \", `97.5 %`, \", crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the \", outcome, \" and the \", exposure, \" by a risk ratio of \", E_Value, \" could explain away the observed associations, even after accounting for the measured confounders. \",\n          \"This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of \", exposure, \" on \", outcome, \" for the \", group, \", the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n        )\n      )\n    )\n  return(df$report)\n}\n\nYou run the function like this:\n\ninterpret_results_subgroup(big_tab, outcome = \"Anxiety\", exposure = \"Excercise\")\n\nEasy!\n\n\nEstimate the subgroup contrast\n\n# calculated above\nest_all_anxiety &lt;- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make the sumamry into a dataframe so we can make a table\ndf &lt;- as.data.frame(summary(est_all_anxiety))\n\n# get rownames for selecting the correct row\ndf$RowName &lt;- row.names(df)\n\n# select the correct row -- the group contrast\nfiltered_df &lt;- df |&gt; \n  dplyr::filter(RowName == \"RD_m - RD_e\") \n\n\n# pring the filtered data frame\nlibrary(kableExtra)\nfiltered_df  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3) |&gt; \n  kable_material(c(\"striped\", \"hover\")) \n\nAnother option for making the table using markdown. This would be useful if you were writing your article using qaurto.\n\nfiltered_df  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3, format = \"markdown\")\n\nReport result along the following lines:\n\nThe estimated reduction of anxiety from exercise is higher overall for New Zealand Europeans (RD_e) compared to Māori (RD_m). This is indicated by the estimated risk difference (RD_m - RD_e) of 0.104. However, there is uncertainty in this estimate, as the confidence interval (-0.042 to 0.279) crosses zero. This indicates that we cannot be confident that the difference in anxiety reduction between New Zealand Europeans and Māori is reliable. It’s possible that the true difference could be zero or even negative, suggesting higher anxiety reduction for Māori. Thus, while there’s an indication of higher anxiety reduction for New Zealand Europeans, the uncertainty in the estimate means we should interpret this difference with caution."
  },
  {
    "objectID": "content/more-old-advice.html#depression-analysis-and-results",
    "href": "content/more-old-advice.html#depression-analysis-and-results",
    "title": "Measurement Matters",
    "section": "Depression Analysis and Results",
    "text": "Depression Analysis and Results\n\n### SUBGROUP analysis\ndt_ref_all &lt;- readRDS(here::here(\"data\", \"dt_ref_all\"))\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_depression_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# fit model\nfit_all_dep  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n\n# coefs &lt;- coef(fit_all_dep)\n# table(is.na(coefs))#   \n# insight::get_varcov(fit_all_all)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_dep, n = nsims, vcov = \"HC1\")\n\n\n# simulate effect as modified in europeans\nsim_estimand_all_e_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = TRUE)\n\n\n# note contrast of interest\nsim_estimand_all_e_d &lt;-\n  transform(sim_estimand_all_e_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n\n# simulate effect as modified in māori\nsim_estimand_all_m_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = TRUE\n)\n\n# combine\nsim_estimand_all_m_d &lt;-\n  transform(sim_estimand_all_m_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n\n# summary\n#summary(sim_estimand_all_e_d)\n#summary(sim_estimand_all_m_d)\n\n# rearrange\nnames(sim_estimand_all_e_d) &lt;-\n  paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m_d) &lt;-\n  paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\")\n\n\nest_all_d &lt;- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d)\nest_all_d &lt;- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e)\nsaveRDS(sim_estimand_all_m_d, here::here(\"data\", \"sim_estimand_all_m_d\"))\nsaveRDS(sim_estimand_all_e_d, here::here(\"data\", \"sim_estimand_all_e_d\"))\n\n\nReport anxiety results\n\n# return stored estimates \nsim_estimand_all_e_d &lt;- readRDS(here::here(\"data\",\"sim_estimand_all_e_d\"))\nsim_estimand_all_m_d&lt;- readRDS(here::here(\"data\",\"sim_estimand_all_m_d\"))\n\n# create individual summaries \nsum_e_d &lt;- summary(sim_estimand_all_e_d)\nsum_m_d &lt;- summary(sim_estimand_all_m_d)\n\n\n# create individual tables\ntab_ed &lt;- sub_tab_ate(sum_e_d, new_name = \"NZ Euro Depression\")\ntab_md &lt;- sub_tab_ate(sum_m_d, new_name = \"Māori Depression\")\n\n\n# expand tables \nplot_ed &lt;- sub_group_tab(tab_ed, type= \"RD\")\nplot_md &lt;- sub_group_tab(tab_md, type= \"RD\")\n\nbig_tab_d &lt;- rbind(plot_ed,plot_md)\n\n\n# table for anxiety outcome --format as \"markdown\" if you are using quarto documents\nbig_tab_d |&gt; \n  kbl(format=\"markdown\")\n\n\n\nGraph Anxiety result\n\n# group tables\nsub_group_plot_ate(big_tab_d, title = \"Effect of Exercise on Depression\", subtitle = \"Subgroup Analysis: NZ Euro and Māori\", xlab = \"Groups\", ylab = \"Effects\",\n                 x_offset = -1,\n                           x_lim_lo = -1,\n                           x_lim_hi = 1.5)\n\n\n\nInterpretation\nUse the function, again, modify the outputs to fit with your study and results and provide your own interpretation.\n\ninterpret_results_subgroup(big_tab_d, exposure = \"Exercise\", outcome = \"Depression\")\n\n\n\nEstimate the subgroup contrast\n\n# calculated above\nest_all_d &lt;- readRDS( here::here(\"data\",\"est_all_d\"))\n\n# make the sumamry into a dataframe so we can make a table\ndfd &lt;- as.data.frame(summary(est_all_d))\n\n# get rownames for selecting the correct row\ndfd$RowName &lt;- row.names(dfd)\n\n# select the correct row -- the group contrast\nfiltered_dfd &lt;- dfd |&gt; \n  dplyr::filter(RowName == \"RD_m - RD_e\") \n\n\n# Print the filtered data frame\nlibrary(kableExtra)\nfiltered_dfd  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3) |&gt; \n  kable_material(c(\"striped\", \"hover\")) \n\nReporting might be:\n\nThe estimated reduction of depression from exercise is higher overall for New Zealand Europeans (RD_e) compared to Māori (RD_m). This is suggested by the estimated risk difference (RD_m - RD_e) of 0.068. However, there is a degree of uncertainty in this estimate, as the confidence interval (-0.09 to 0.229) crosses zero. This suggests that we cannot be confident that the difference in depression reduction between New Zealand Europeans and Māori is statistically significant. It’s possible that the true difference could be zero or even negative, implying a greater depression reduction for Māori than New Zealand Europeans. Thus, while the results hint at a larger depression reduction for New Zealand Europeans, the uncertainty in this estimate urges us to interpret this difference with caution.\n\n\n\nDiscusion\nYou’ll need to do this yourself. Here’s a start:\n\nIn our study, we employed a robust statistical method that helps us estimate the impact of exercise on reducing anxiety among different population groups – New Zealand Europeans and Māori. This method has the advantage of providing reliable results even if our underlying assumptions aren’t entirely accurate – a likely scenario given the complexity of real-world data. However, this robustness comes with a trade-off: it gives us wider ranges of uncertainty in our estimates. This doesn’t mean the analysis is flawed; rather, it accurately represents our level of certainty given the data we have.\n\n\nExercise and Anxiety\n\nOur analysis suggests that exercise may have a greater effect in reducing anxiety among New Zealand Europeans compared to Māori. This conclusion comes from our primary causal estimate, the risk difference, which is 0.104. However, it’s crucial to consider our uncertainty in this value. We represent this uncertainty as a range, also known as a confidence interval. In this case, the interval ranges from -0.042 to 0.279. What this means is, given our current data and method, the true effect could plausibly be anywhere within this range. While our best estimate shows a higher reduction in anxiety for New Zealand Europeans, the range of plausible values includes zero and even negative values. This implies that the true effect could be no difference between the two groups or even a higher reduction in Māori. Hence, while there’s an indication of a difference, we should interpret it cautiously given the wide range of uncertainty.\n\n\nThus, although our analysis points towards a potential difference in how exercise reduces anxiety among these groups, the level of uncertainty means we should be careful about drawing firm conclusions. More research is needed to further explore these patterns.\n\n\n\nExercise and Depression\n\nIn addition to anxiety, we also examined the effect of exercise on depression. We do not find evidence for reduction of depression from exercise in either group. We do not find evidence for the effect of weekly exercise – as self-reported – on depression.\n\n\n\nProviso\n\nIt is important to bear in mind that statistical results are only one piece of a larger scientific puzzle about the relationship between excercise and well-being. Other pieces include understanding the context, incorporating subject matter knowledge, and considering the implications of the findings. In the present study, wide confidence intervals suggest the possibility of considerable individual differences.\\dots nevertheless, \\dots"
  },
  {
    "objectID": "content/more-old-advice.html#exercises",
    "href": "content/more-old-advice.html#exercises",
    "title": "Measurement Matters",
    "section": "Exercises",
    "text": "Exercises\n\nGenerate a Kessler 6 binary score (Not Depressed vs. Moderately or Severely Depressed)\n\nand also:\n\nCreate a variable for the log of exercise hour\nTake Home: estimate whether exercise causally affects nervousness, using the single item of the kessler 6 score. Briefly write up your results."
  },
  {
    "objectID": "content/more-old-advice.html#appendix-mg-cfa",
    "href": "content/more-old-advice.html#appendix-mg-cfa",
    "title": "Measurement Matters",
    "section": "Appendix: MG-CFA",
    "text": "Appendix: MG-CFA\n\nCFA for Kessler 6\nWe have learned how to do confirmatory factor analysis. Let’s put this knowledge to use but clarifying the underlying factor structure of Kessler-6\nThe code below will:\n\nLoad required packages.\nSelect the Kessler 6 items\nCheck whether there is sufficient correlation among the variables to support factor analysis.\n\n\n# select the columns we need. \ndt_only_k6 &lt;- dt_start |&gt; select(kessler_depressed, kessler_effort,kessler_hopeless,\n                                 kessler_worthless, kessler_nervous,\n                                 kessler_restless)\n\n\n# check factor structure\nperformance::check_factorstructure(dt_only_k6)\n\nThe code below will allow us to explore the factor structure, on the assumption of n = 3 factors.\n\n# exploratory factor analysis\n# explore a factor structure made of 3 latent variables\nefa &lt;- psych::fa(dt_only_k6, nfactors = 3) %&gt;%\n  model_parameters(sort = TRUE, threshold = \"max\")\n\nefa\n\nThis output describes an exploratory factor analysis (EFA) with 3 factors conducted on the Kessler 6 (K6) scale data. The K6 scale is used to measure psychological distress.\nThe analysis identifies three latent factors, labeled MR1, MR2, and MR3, which collectively account for 66.05% of the variance in the K6 data. The factors MR1, MR2, and MR3 explain 35.14%, 17.17%, and 13.73% of the variance respectively.\nFactor loadings, indicating the strength and direction of the relationship between the K6 items and the latent factors, are as follows:\n\nFactor MR1 is strongly associated with ‘kessler_depressed’, ‘kessler_worthless’, and ‘kessler_hopeless’ with loadings of 0.85, 0.79, and 0.75 respectively.\nFactor MR2 is exclusively linked with ‘kessler_nervous’ with a loading of 1.00.\nFactor MR3 relates to ‘kessler_restless’ and ‘kessler_effort’ with loadings of 0.69 and 0.48 respectively.\n\nThe ‘Uniqueness’ values show the proportion of each variable’s variance that isn’t shared with the other variables.\nThe ‘Complexity’ values give a measure of how each item loads on more than one factor. All the items are either loading exclusively on one factor (complexity=1.00) or slightly more than one factor. ‘kessler_effort’ with complexity of 1.66 shows it’s the item most shared between the factors.\nThe analysis suggests these K6 items measure may measure three somewhat distinct, yet related, factors of psychological distress.\nHowever, the meaning of these factors would need to be interpreted in the context of the variables and the theoretical framework of the study.\nNotably, there are many many theoretical frameworks for in measurement theory. Here is a brief description of the different conclusions one might make, depending on one’s preferred theory.\n\n\nCode\nn &lt;- n_factors(dt_only_k6)\n\n# plot\nplot(n) + theme_classic()\n\n\n\n\nConfirmatory factor analysis (ignoring groups)\n\n# first partition the data \npart_data &lt;- datawizard::data_partition(dt_only_k6, traing_proportion = .7, seed = 123)\n\n\n# set up training data\ntraining &lt;- part_data$p_0.7\ntest &lt;- part_data$test\n\n\n# one factor model\nstructure_k6_one &lt;- psych::fa(training, nfactors = 1) |&gt;\n  efa_to_cfa()\n\n# two factor model model\nstructure_k6_two &lt;- psych::fa(training, nfactors = 2) |&gt;\n  efa_to_cfa()\n\n# three factor model\nstructure_k6_three &lt;- psych::fa(training, nfactors = 3) %&gt;%\n  efa_to_cfa()\n\n# inspect models\nstructure_k6_one\nstructure_k6_two\nstructure_k6_three\n\nNext we perform the confirmatory factor analysis.\n\n# fit and compare models\n\n# one latent model\none_latent &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_one, data = test))\n\n# two latents model\ntwo_latents &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_two, data = test))\n\n# three latents model\nthree_latents &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_three, data = test))\n\n\n# compare models\ncompare &lt;-\n  performance::compare_performance(one_latent, two_latents, three_latents, verbose = FALSE)\n\n# view as html table\nas.data.frame(compare) |&gt;\n  kbl(format = \"markdown\")\n\nThis table provides the results of three different Confirmatory Factor Analysis (CFA) models: one that specifies a single latent factor, one that specifies two latent factors, and one that specifies three latent factors. The results include a number of goodness-of-fit statistics, which can be used to assess how well each model fits the data.\n\nOne_latent CFA:\nThis model assumes that there is only one underlying latent factor contributing to all variables. This model has a chi-square statistic of 1359.7 with 14 degrees of freedom, which is highly significant (p&lt;0.001), indicating a poor fit of the model to the data. Other goodness-of-fit indices like GFI, AGFI, NFI, NNFI, and CFI are all high (above 0.9), generally indicating good fit, but these indices can be misleading in the presence of large sample sizes. RMSEA is above 0.1 which indicates a poor fit. The SRMR is less than 0.08 which suggests a good fit, but given the high Chi-square and RMSEA values, we can’t solely rely on this index. The Akaike information criterion (AIC), Bayesian information criterion (BIC) and adjusted BIC are used for comparing models, with lower values indicating better fit.\n\n\nTwo_latents CFA\nThis model assumes that there are two underlying latent factors. The chi-square statistic is lower than the one-factor model (317.97 with 13 df), suggesting a better fit. The p-value is still less than 0.05, indicating a statistically significant chi-square, which typically suggests a poor fit. However, all other fit indices (GFI, AGFI, NFI, NNFI, and CFI) are above 0.9 and the RMSEA is 0.051, which generally indicate good fit. The SRMR is also less than 0.08 which suggests a good fit. This model has the lowest AIC and BIC values among the three models, indicating the best fit according to these criteria.\n\n\nThree_latents CFA\nThis model assumes three underlying latent factors. The chi-square statistic is 747.87 with 12 df, higher than the two-factor model, suggesting a worse fit to the data. Other fit indices such as GFI, AGFI, NFI, NNFI, and CFI are below 0.97 and the RMSEA is 0.083, which generally indicate acceptable but not excellent fit. The SRMR is less than 0.08 which suggests a good fit. The AIC and BIC values are higher than the two-factor model but lower than the one-factor model, indicating a fit that is better than the one-factor model but worse than the two-factor model.\nBased on these results, the two-latents model seems to provide the best fit to the data among the three models, according to most of the fit indices and the AIC and BIC. Note, all models have significant chi-square statistics, which suggests some degree of misfit. It’s also important to consider the substantive interpretation of the factors, to make sure the model makes sense theoretically.\n\n\n\nMulti-group Confirmatory Factor Analysis\nThis script runs multi-group confirmatory factor analysis (MG-CFA)\n\n# select needed columns plus 'ethnicity'\n# filter dataset for only 'euro' and 'maori' ethnic categories\ndt_eth_k6_eth &lt;- dt_start |&gt; \n  filter(eth_cat == \"euro\" | eth_cat == \"maori\") |&gt; \n  select(kessler_depressed, kessler_effort,kessler_hopeless,\n         kessler_worthless, kessler_nervous,\n         kessler_restless, eth_cat)\n\n# partition the dataset into training and test subsets\n# stratify by ethnic category to ensure balanced representation\npart_data_eth &lt;- datawizard::data_partition(dt_eth_k6_eth, traing_proportion = .7, seed = 123, group = \"eth_cat\")\n\ntraining_eth &lt;- part_data_eth$p_0.7\ntest_eth &lt;- part_data_eth$test\n\n# run confirmatory factor analysis (CFA) models for configural invariance across ethnic groups\n# models specify one, two, and three latent variables\none_latent_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", data = test_eth))\ntwo_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", data = test_eth))\nthree_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\", data = test_eth))\n\n# compare model performances for configural invariance\ncompare_eth_configural &lt;- performance::compare_performance(one_latent_eth_configural, two_latents_eth_configural, three_latents_eth_configural, verbose = FALSE)\n\n# run CFA models for metric invariance, holding factor loadings equal across groups\n# models specify one, two, and three latent variables\none_latent_eth_metric &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\ntwo_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\nthree_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal = \"loadings\", data = test_eth))\n\n# compare model performances for metric invariance\ncompare_eth_metric  &lt;- performance::compare_performance(one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric, verbose = FALSE)\n\n# run CFA models for scalar invariance, holding factor loadings and intercepts equal across groups\n# models specify one, two, and three latent variables\none_latent_eth_scalar &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = c(\"loadings\",\"intercepts\"), data = test_eth))\ntwo_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\nthree_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\n\n# compare model performances for scalar invariance\ncompare_eth_scalar  &lt;- performance::compare_performance(one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar, verbose = FALSE)\n\nRecall, in the context of measurement and factor analysis, the concepts of configural, metric, and scalar invariance relate to the comparability of a measurement instrument, such as a survey or test, across different groups.\nWe saw in part 1 of this course that these invariance concepts are frequently tested in the context of cross-cultural, multi-group, or longitudinal studies.\nLet’s first define these concepts, and then apply them to the context of the Kessler 6 (K6) Distress Scale used among Maori and New Zealand Europeans.\n\nConfigural invariance refers to the most basic level of measurement invariance, and it is established when the same pattern of factor loadings and structure is observed across groups. This means that the underlying or “latent” constructs (factors) are defined the same way for different groups. This doesn’t mean the strength of relationship between items and factors (loadings) or the item means (intercepts) are the same, just that the items relate to the same factors in all groups.\n\nIn the context of the K6 Distress Scale, configural invariance would suggest that the same six items are measuring the construct of psychological distress in the same way for both Māori and New Zealand Europeans, even though the strength of the relationship between the items and the construct (distress), or the average scores, might differ.\n\nMetric invariance (also known as “weak invariance”) refers to the assumption that factor loadings are equivalent across groups, meaning that the relationship or association between the measured items and their underlying factor is the same in all groups. This is important when comparing the strength of relationships with other variables across groups.\n\nIf metric invariance holds for the K6 Distress Scale, this would mean that a unit change in the latent distress factor would correspond to the same change in each item score (e.g., feeling nervous, hopeless, restless, etc.) for both Māori and New Zealand Europeans.\n\nScalar invariance (also known as “strong invariance”) involves equivalence of both factor loadings and intercepts (item means) across groups. This means that not only are the relationships between the items and the factors the same across groups (as with metric invariance), but also the zero-points or origins of the scales are the same. Scalar invariance is necessary when one wants to compare latent mean scores across groups.\n\nIn the context of the K6 Distress Scale, if scalar invariance holds, it would mean that a specific score on the scale would correspond to the same level of the underlying distress factor for both Māori and New Zealand Europeans. It would mean that the groups do not differ systematically in how they interpret and respond to the items. If this holds, one can make meaningful comparisons of distress level between Maori and New Zealand Europeans based on the scale scores.\nNote: each of these levels of invariance is a progressively stricter test of the equivalence of the measurement instrument across groups. Demonstrating scalar invariance, for example, also demonstrates configural and metric invariance. On the other hand, failure to demonstrate metric invariance means that scalar invariance also does not hold. These tests are therefore usually conducted in sequence. The results of these tests should be considered when comparing group means or examining the relationship between a scale and other variables across groups.\n\n\nConfigural invariance\n\nas.data.frame(compare_eth_configural)|&gt;\n  kbl(format = \"markdown\")\n\nThe table represents the comparison of three multi-group confirmatory factor analysis (CFA) models conducted to test for configural invariance across different ethnic categories (eth_cat). Configural invariance refers to whether the pattern of factor loadings is the same across groups. It’s the most basic form of measurement invariance.\nLooking at the results, we can draw the following conclusions:\n\nChi2 (Chi-square): A lower value suggests a better model fit. In this case, the two_latents_eth_configural model exhibits the lowest Chi2 value, suggesting it has the best fit according to this metric.\nGFI (Goodness of Fit Index) and AGFI (Adjusted Goodness of Fit Index): These values range from 0 to 1, with values closer to 1 suggesting a better fit. The two_latents_eth_configural model has the highest GFI and AGFI values, indicating it is the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, also called TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting it is the best fit according to these metrics.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 considered good and up to 0.08 considered acceptable. In this table, the two_latents_eth_configural model has an RMSEA of 0.05, which falls within the acceptable range.\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models exhibit acceptable RMR and SRMR values, with the two_latents_eth_configural model having the lowest.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting the best fit according to these measures.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_configural model has the lowest AIC and BIC, suggesting it is the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_configural model is non-significant, suggesting a good fit.\n\nOverall, the two_latents_eth_configural model appears to provide the best fit across multiple indices, suggesting configural invariance (i.e., the same general factor structure) across ethnic categories with a two-factor solution. As with the previous assessment, theoretical soundness and other substantive considerations should also be taken into account when deciding on the final model. We will return to these issues next week.\n\n\nMetric Equivalence\n\nas.data.frame(compare_eth_metric)|&gt;\n  kbl(format = \"markdown\")\n\nThis table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test metric equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric) were run with a constraint of equal factor loadings across groups, which is a requirement for metric invariance.\nHere’s the interpretation of the fit indices:\n\nChi2 (Chi-square): Lower values indicate better model fit. The two_latents_eth_metric model has the lowest Chi2 value, suggesting the best fit according to this measure.\nGFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_metric model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. For these indices, the one_latent_eth_metric model has the highest values, suggesting the best fit according to these measures.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_metric model has an RMSEA within the acceptable range (0.051).\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_metric model having the lowest, indicating the best fit.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_metric model has the highest values, suggesting the best fit according to these indices.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_metric model has the lowest AIC and BIC, indicating the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Statistically non-significant values at the traditional threshold (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_metric model is statistically non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_metric model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the metric equivalence (equal factor loadings) assumption is supported across ethnic categories. However, one must also take into consideration the theoretical soundness of the model and other substantive considerations when deciding on the final model.\n\n\nScalar Equivalence\n\n# view as html table\nas.data.frame(compare_eth_scalar)|&gt;\n  kbl(format = \"markdown\")\n\nThe table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test scalar equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar) were run with constraints on both factor loadings and intercepts to be equal across groups, a requirement for scalar invariance.\nHere’s the interpretation of the fit indices:\n\nChi2 (Chi-square): Lower values indicate better model fit. The two_latents_eth_scalar model has the lowest Chi2 value, suggesting the best fit according to this measure.\nGFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_scalar model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these measures.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_scalar model has an RMSEA within the acceptable range (0.05).\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_scalar model having the lowest, indicating the best fit.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these indices.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_scalar model has the lowest AIC and BIC, indicating the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_scalar model is non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_scalar model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the scalar equivalence (equal factor loadings and intercepts) assumption is supported across ethnic categories. However, we must also consider the theoretical soundness of the model and other substantive considerations when deciding on the final model (a matter to which we will return next week.)\nOverall it seems that we have good evidence for the two-factor model of Kessler-6."
  },
  {
    "objectID": "content/more-old-advice.html#solutions",
    "href": "content/more-old-advice.html#solutions",
    "title": "Measurement Matters",
    "section": "Solutions",
    "text": "Solutions\n\nGenerate a Kessler 6 binary score (Not Depressed vs. Moderately or Severely Depressed)\n\nand also:\n\nCreate a variable for the log of exercise hour\n\n\n# functions \n#source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\n\n\n# experimental functions (more functions)\n#source(\n # \"https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R\"\n#)\n\n\n\nnzavs_synth &lt;-\n  arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n\n\ndt_new &lt;- nzavs_synth %&gt;%\n  arrange(id, wave) %&gt;%\n  rowwise() %&gt;%\n  mutate(kessler_6  = mean(sum(\n    # Specify the Kessler scale items\n    c(\n      kessler_depressed,\n      # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      kessler_hopeless,\n      # During the last 30 days, how often did you feel hopeless?\n      kessler_nervous,\n      # During the last 30 days, how often did you feel nervous?\n      kessler_effort,\n      # During the last 30 days, how often did you feel that everything was an effort?\n      kessler_restless,\n      # During the last 30 days, how often did you feel restless or fidgety ?\n      kessler_worthless  # During the last 30 days, how often did you feel worthless?\n    )\n  ))) |&gt;\n  mutate(kessler_6_sum = round(sum(\n    c (\n      kessler_depressed,\n      kessler_hopeless,\n      kessler_nervous,\n      kessler_effort,\n      kessler_restless,\n      kessler_worthless\n    )\n  ),\n  digits = 0)) |&gt;  ungroup() |&gt;\n  # Create a categorical variable 'kessler_6_coarsen' based on the sum of Kessler scale items\n  mutate(\n    kessler_6_coarsen = cut(\n      kessler_6_sum,\n      breaks = c(0, 5, 24),\n      labels = c(\"not_depressed\",\n                 \"mildly_to_severely_depressed\"),\n      include.lowest = TRUE,\n      include.highest = TRUE,\n      na.rm = TRUE,\n      right = FALSE\n    )\n  ) |&gt;\n  # Transform 'hours_exercise' by applying the log function to compress its scale\n  mutate(hours_exercise_log = log(hours_exercise + 1)) # Add 1 to avoid undefined log(0). Hours spent exercising/physical activity\n\n\nTake Home: estimate whether exercise causally affects nervousness, using the single item of the kessler 6 score. Briefly write up your results."
  },
  {
    "objectID": "content/more-old-advice.html#readings",
    "href": "content/more-old-advice.html#readings",
    "title": "Measurement Matters",
    "section": "Readings",
    "text": "Readings\n(He and Vijver 2012)\n(vandevijver2021?)\n(Berry 1989)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Accessing Lectures and Readings\n\n\n\n\nSeminar Time/Location: Tuesdays, 9:00-11:50am Easterfield Building Room: EA201\nCourse Outline: find a detailed schedule of topics, readings, and assignments in the Course Outline tab.\nReadings: links to readings are directly within the Course Outline tab, essential for lecture preparation.\nLecture Materials: access slides, video recordings, and more under the Content tab, organised by week for ease of use.\nTests: in the same room as the seminar\n\n\n\n\n\nTest/Quiz Location IN CLASS\n\n\n\nThe Contents tab offers direct access to weekly seminar and lab materials, including lecture outlines and lab resources.\n\nAccess it from the top right of the course platform by selecting the appropriate week.\nLab materials are available one week before the lecture; seminar review materials post-seminar.\n\n\n\n\n\n\n\n\n\n\nCourse Coordinator Prof Joseph Bulbulia joseph.bulbulia@vuw.ac.nz\nCourse Coordinator’s Office EA324\nR Help from  Dr.Inkuk Kim inkuk.kim@vuw.ac.nz\n\n\n\n\n\n\n\n\nAssessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n1,2,3\n10\nWeekly\n\n\nIn-class Test\n2\n25\n8 April (w7)\n\n\nIntroduction/Methods Research Report\n2\n25\n20 May (w11)\n\n\nFull Research Report\n1,2,3\n40\n30 May (end of w12)\n\n\n\n\n\n\n\n\n\n\nThe official description:\nThis course will focus on theoretical and practical challenges for conducting research involving individuals from more than one cultural background or ethnicity. Topics are likely to include defining and measuring culture; developing culture-sensitive studies, choice of language and translation; communication styles and bias; questionnaire and interview design; qualitative and quantitative data analysis for cultural and cross-cultural research; minorities, power and ethics in cross-cultural research; and ethno-methodologies and indigenous research methodologies. Appropriate background for this course: PSYC 338.\n\n\n\nPreamble: in this advanced course, students will develop foundational skills in cross-cultural psychological research with a strong emphasis on causal inference, a new and critical methodological approach.\n\nProgramming in R students will learn the basics of programming in the statistical language R, gaining essential computational tools for psychological research. The skills you acquire will lay the foundation for applying data analysis techniques in a causal inference framework and beyond.\nUnderstanding Causal Inference. students will develop a robust understanding of causal inference concepts and approaches, with particular emphasis on how they mitigate common pitfalls in cross-cultural research. We will focus on designing studies, analysing data, and drawing strong conclusions about cause-and-effect relationships across cultures.\nUnderstanding Measurement in Comparative Settings. students will learn techniques for constructing and validating psychometrically sound measures across diverse cultures. We will examine how to ensure measurements are reliable, cross-culturally valid, and aligned with theoretical constructs while focusing strongly on causal reasoning.\n\n\n\n\n\n\n\n\n\nAssessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n1,2,3\n10\nWeekly\n\n\nIn-class Test\n2\n25\n8 April (w7)\n\n\nIntroduction/Methods Research Report\n2\n25\n20 May (w11)\n\n\nFull Research Report\n1,2,3\n40\n30 May (end of w12)\n\n\n\n\n\n\n\n\n\n\n\nClass attendance and active participation.\n\n\nLab attendance and active participation.\n\n\nPresentation (Week 12)\n\n\n\n\n\n\n\n\n\n\nTest duration is one hour. The allocated time is nearly two hours.\n\n\n\n\n\n\n\nImportant\n\n\n\n\nTHE TEST IS IN CLASS (i.e. come to class with a writing instrument).\n\n\n\n\nEach lecture starts and ends with key concept definitions and reviews for the test.\nR or RStudio knowledge isn’t part of the test. R support aims to enhance research report skills.\nTests are conducted in the lecture room without the aids of notes, a computer, or a phone.\nRequired: pen/pencil.\nTest (50 minutes, total time allowed: 1 hour 50 minutes):\n\nFocuses on revising core statistical and methodological concepts.\nAims to refresh basic statistical knowledge foundational for later course material.\n\n\n\n\n\n\nTake Home Asssessment (~6 Hours) due May 06**:\nBuilds upon the course concepts, emphasising the application of basic conceptual, statistical, and theoretical knowledge as applied to your research question.\nSpecifically, how to formulate a research question and develop a strategy for answering it.\n\nTask\n\nWrite a draft Introduction to your final writing assessment.\nWrite a draft Method section for your writing assessment.\nPrepare an in-class presentation of 10 minutes summaring your study.\nYou are encouraged to use AI.\nHowever, be warned: I will mark hallucinations and errors harshly, so you’ll need to to internalise understanding!\nAlthough I encourage you to use AI, I also encourage you to you write in your own voice. Cultivating your self-expression will make you interesting, and employable.\n(* Note that even the best AI models make mistakes and hallucinate, particularly in causal inference)\n\n\n\n\n\n\nState your question: is your question clearly stated?\nRelevance: have you explained its scientific importance?\nCausality: Is your question causal?\nSubgroup analysis: does your question involve a subgroups (e.g., cultural group)? Which?\nExplain the framework: Have you explained the causal inference framework in a way that is comprehensible to non-specialists?\nEthics/Policy interests have you explained how this question might practically affect people?\nData source: are your data from the NZAVS simulated data set? (if not, consult with me)\nData waves: are your data using three waves?\n\n\n\n\n\nOutcome variable: is your outcome variable Y well-defined?\nMultiple outcomes: do you assess multiple outcomes are are these well-defined?\nOutcome relevance: can you explain how the outcome variable/s relate to your question?\nOutcome type: is your outcome binary and rare? … etc.\nOutcome timing: does your outcome appear after your exposure?\n\n\n\n\n\nExposure variable: is your exposure variable A well-defined?\nMultiple exposures: are there multiple exposures? (If yes, for this study, reassess).\nExposure relevance: have you explained how the exposure variable relates to your question?\nPositivity: can we intervene on the exposure at all levels of the covariates?\nConsistency: can we interpret what it means to intervene on the exposure?\nExchangeability: are there different versions of the exposure conditionally exchangeable given measured baseline confounders?\nExposure type: is the exposure binary or continuous?\nShift intervention: do you contrast static interventions or modified treatment policies?\nExposure timing: does your exposure appear before the outcome? (It should.)\n\n\n\n\n\nBaseline confounders: Have you defined your baseline confounders L?\nJustification: Can you explain how the baseline confounders could affect both A and Y?\nTiming: Are the baseline confounders measured before the exposure?\nInclusion: Is the baseline measure of the exposure and the baseline outcome included in the set of baseline confounders?\nSufficiency: Are the baseline confounders sufficient to ensure balance on the exposure, such that A is independent of Y given L? If not, explain your sensitivity analysis (E-values)\nConfounder type: Are the confounders continuous or binary? If so, consider converting them to z-scores. If they are categorical with three or more levels, do not convert them to z-scores, but rather use one-hot encoding (see lecture 9.)\n\n\n\n\n\nCausal diagram: Have you drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\nMeasurement error: Have you described potential biases from measurement errors?\nTemporal order: Does your DAG have time indicators to ensure correct temporal order?\nTime consistency: Is your DAG organized so that time follows in a consistent direction?\n\n\n\n\n\nWhat is your casual contrast?\nHave you stated your causal contrast clearly?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulations identified: Have you explained how your sample relates to your target populations?\n\n\n\n\n\nCriteria stated: Have you stated the eligibility criteria for the study?\n\n\n\n\n\nDescriptive statistics: have you provided descriptive statistics for demographic information taken at baseline?\nExposure change: Have you described the magnitudes of change in the exposure from baseline to the exposure interval\nReferences: Have you included references for more information about the sample (e.g. the NZAVS website)? I should have.\nDATA ARE SIMULATED: Have you made it clear you are working with simulated data?\n\n\n\n\n\nMissing data checks: Have you checked for missing data?\nMissing data plan: If there are missing data, have you described how you will address the problem? (IPCW, see week 9)\n\n\n\n\n\nApproach decision: G-computation, IPTW, or Doubly-Robust Estimation?\nModel specification: Model Specification?\nMachine Learning: have you explained how machine learning works?\nOutcome Specifics: If the outcome is rare and binary, have you specified logistic regression? If it’s continuous, have I considered converting it to z-scores?\nSensitivity analysis: Have you described your sensitivity analysis (e.g. E-values.)\n\nNote most of these tasks can be ticked off in a sentence or two, but all need to be covered.\nLength: ~ 1,000 - 2,000 words (note it is a draft introduction and draft methods section).\n\n\n\n\n\n\n\n\n\n\nResearch Report Instructions\n\n\n\n\nWe will supply the data.\nLab sessions are designed to support you in this assignment.\nWe assume no statistical background.\n\n\n\n\nTitle: “Causal Inference in Cultural Psychology: Examining Exposure Effects on Dimensions of Well-being Modified by Cultural or Sociodemographic Categories”.\nObjective:\n\nTo quantify the causal effect of a specific exposure on well-being dimensions, modified by sociodemographic categories (born_nz, eth_cat, big_doms, gen_cohort) using the NZAVS longitudinal synthetic dataset.\n\nInstructions:\n\nTheoretical Interest and Research Question:\n\nDescribe the significance of your chosen exposure and its potential impact on the selected outcomes, modified by the cultural or sociodemographic category.\nState the research question clearly.\n\nDirected Acyclic Graph (DAG):\n\nConstruct a DAG illustrating the relationships between exposure, outcomes, sociodemographic category, and potential bias sources. Ensure clarity in labelling.\n\nConfounding Control Strategy:\n\nOutline your strategy for confounding control, justifying the chosen confounders.\n\nMeasurement Biases:\n\nAddress and analyse measurement biases as relevant.\n\nAssumptions and Statistical Models:\n\nDiscuss the assumptions of your causal inference approach and your statistical model, including their limitations.\n\n\nRequirements:\n\nIntroduction: 1,500 words limit.\nConclusion: 1,500 words limit.\nMethod and Results sections should be concise; no specific word limit.\nUse any useful sources, citing appropriately to avoid academic misconduct.\nFollow APA style for citations and references.\nInclude tables/figures as needed.\nSubmit as a single PDF, including R code in an appendix.\nPresentations of Study in Week 12 (or by arrangement.)\n\nEvaluation Criteria:\n\nClarity of theoretical framework, research question, and design.\nValidity of confounding control strategy.\nDiscussion on assumptions and statistical models.\nPresentation quality (10%)\n\nclearly and efficiently presents study\n\n\n\n\n\n\n\nExtensions:\n\nNegotiate a new due date by writing (email) before the mid-term test.\nEvery reasonable request will be accepted (e.g. too many assignments falling in the same week, you want another week to complete.)\n\nPenalties:\n\nLate submissions incur a one full grade-per-week penalty, e.g. if late by one day, B \\to C, one week later, C \\to D.\nOver-length assignments will be penalised.\n\nUnforeseeable Events:\n\nExtensions will require evidence (e.g., medical certificate).\n\n\n\n\n\n\nBring a laptop with R and RStudio installed for data analysis sessions. Contact the instructor if you lack computer access.\nFor in-class tests, bring a writing utensil. Again, electronic devices are not permitted."
  },
  {
    "objectID": "index.html#class-times-and-locations",
    "href": "index.html#class-times-and-locations",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Test/Quiz Location IN CLASS"
  },
  {
    "objectID": "index.html#contents-tab",
    "href": "index.html#contents-tab",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "The Contents tab offers direct access to weekly seminar and lab materials, including lecture outlines and lab resources.\n\nAccess it from the top right of the course platform by selecting the appropriate week.\nLab materials are available one week before the lecture; seminar review materials post-seminar."
  },
  {
    "objectID": "index.html#names-and-contact-details",
    "href": "index.html#names-and-contact-details",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Course Coordinator Prof Joseph Bulbulia joseph.bulbulia@vuw.ac.nz\nCourse Coordinator’s Office EA324\nR Help from  Dr.Inkuk Kim inkuk.kim@vuw.ac.nz"
  },
  {
    "objectID": "index.html#assignments-and-due-dates",
    "href": "index.html#assignments-and-due-dates",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Assessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n1,2,3\n10\nWeekly\n\n\nIn-class Test\n2\n25\n8 April (w7)\n\n\nIntroduction/Methods Research Report\n2\n25\n20 May (w11)\n\n\nFull Research Report\n1,2,3\n40\n30 May (end of w12)"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "The official description:\nThis course will focus on theoretical and practical challenges for conducting research involving individuals from more than one cultural background or ethnicity. Topics are likely to include defining and measuring culture; developing culture-sensitive studies, choice of language and translation; communication styles and bias; questionnaire and interview design; qualitative and quantitative data analysis for cultural and cross-cultural research; minorities, power and ethics in cross-cultural research; and ethno-methodologies and indigenous research methodologies. Appropriate background for this course: PSYC 338."
  },
  {
    "objectID": "index.html#course-learning-objectives",
    "href": "index.html#course-learning-objectives",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Preamble: in this advanced course, students will develop foundational skills in cross-cultural psychological research with a strong emphasis on causal inference, a new and critical methodological approach.\n\nProgramming in R students will learn the basics of programming in the statistical language R, gaining essential computational tools for psychological research. The skills you acquire will lay the foundation for applying data analysis techniques in a causal inference framework and beyond.\nUnderstanding Causal Inference. students will develop a robust understanding of causal inference concepts and approaches, with particular emphasis on how they mitigate common pitfalls in cross-cultural research. We will focus on designing studies, analysing data, and drawing strong conclusions about cause-and-effect relationships across cultures.\nUnderstanding Measurement in Comparative Settings. students will learn techniques for constructing and validating psychometrically sound measures across diverse cultures. We will examine how to ensure measurements are reliable, cross-culturally valid, and aligned with theoretical constructs while focusing strongly on causal reasoning."
  },
  {
    "objectID": "index.html#assignments-and-due-dates-1",
    "href": "index.html#assignments-and-due-dates-1",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Assessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n1,2,3\n10\nWeekly\n\n\nIn-class Test\n2\n25\n8 April (w7)\n\n\nIntroduction/Methods Research Report\n2\n25\n20 May (w11)\n\n\nFull Research Report\n1,2,3\n40\n30 May (end of w12)\n\n\n\n\n\n\n\n\n\n\n\nClass attendance and active participation.\n\n\nLab attendance and active participation.\n\n\nPresentation (Week 12)"
  },
  {
    "objectID": "index.html#take-home-assessment",
    "href": "index.html#take-home-assessment",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Take Home Asssessment (~6 Hours) due March 12**:\nBuilds upon the course concepts, emphasising the application of basic conceptual, statistical, and theoretical knowledge as applied to your research question.\nSpecifically, how to formulate a research question and develop a strategy for answering it.\n\nTask\n\nWrite a draft Introduction to your final writing assessment.\nWrite a draft Method section for your writing assessment.\nPrepare an in-class presentation of 10 minutes summaring your study.\nYou are encouraged to use AI.\nHowever, be warned: I will mark hallucinations and errors harshly, so you’ll need to to internalise understanding!\nAlthough I encourage you to use AI, I also encourage you to you write in your own voice. Cultivating your self-expression will make you interesting, and employable.\n(* Note that even the best AI models make mistakes and hallucinate, particularly in causal inference)\n\n\n\n\n\n\nState your question: is your question clearly stated?\nRelevance: have you explained its scientific importance?\nCausality: Is your question causal?\nSubgroup analysis: does your question involve a subgroups (e.g., cultural group)? Which?\nExplain the framework: Have you explained the causal inference framework in a way that is comprehensible to non-specialists?\nEthics/Policy interests have you explained how this question might practically affect people?\nData source: are your data from the NZAVS simulated data set? (if not, consult with me)\nData waves: are your data using three waves?\n\n\n\n\n\nOutcome variable: is your outcome variable Y well-defined?\nMultiple outcomes: do you assess multiple outcomes are are these well-defined?\nOutcome relevance: can you explain how the outcome variable/s relate to your question?\nOutcome type: is your outcome binary and rare? … etc.\nOutcome timing: does your outcome appear after your exposure?\n\n\n\n\n\nExposure variable: is your exposure variable A well-defined?\nMultiple exposures: are there multiple exposures? (If yes, for this study, reassess).\nExposure relevance: have you explained how the exposure variable relates to your question?\nPositivity: can we intervene on the exposure at all levels of the covariates?\nConsistency: can we interpret what it means to intervene on the exposure?\nExchangeability: are there different versions of the exposure conditionally exchangeable given measured baseline confounders?\nExposure type: is the exposure binary or continuous?\nShift intervention: do you contrast static interventions or modified treatment policies?\nExposure timing: does your exposure appear before the outcome? (It should.)\n\n\n\n\n\nBaseline confounders: Have you defined your baseline confounders L?\nJustification: Can you explain how the baseline confounders could affect both A and Y?\nTiming: Are the baseline confounders measured before the exposure?\nInclusion: Is the baseline measure of the exposure and the baseline outcome included in the set of baseline confounders?\nSufficiency: Are the baseline confounders sufficient to ensure balance on the exposure, such that A is independent of Y given L? If not, explain your sensitivity analysis (E-values)\nConfounder type: Are the confounders continuous or binary? If so, consider converting them to z-scores. If they are categorical with three or more levels, do not convert them to z-scores, but rather use one-hot encoding (see lecture 9.)\n\n\n\n\n\nCausal diagram: Have you drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\nMeasurement error: Have you described potential biases from measurement errors?\nTemporal order: Does your DAG have time indicators to ensure correct temporal order?\nTime consistency: Is your DAG organized so that time follows in a consistent direction?\n\n\n\n\n\nWhat is your casual contrast?\nHave you stated your causal contrast clearly?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulations identified: Have you explained how your sample relates to your target populations?\n\n\n\n\n\nCriteria stated: Have you stated the eligibility criteria for the study?\n\n\n\n\n\nDescriptive statistics: have you provided descriptive statistics for demographic information taken at baseline?\nExposure change: Have you described the magnitudes of change in the exposure from baseline to the exposure interval\nReferences: Have you included references for more information about the sample (e.g. the NZAVS website)? I should have.\nDATA ARE SIMULATED: Have you made it clear you are working with simulated data?\n\n\n\n\n\nMissing data checks: Have you checked for missing data?\nMissing data plan: If there are missing data, have you described how you will address the problem? (IPCW, see week 9)\n\n\n\n\n\nApproach decision: G-computation, IPTW, or Doubly-Robust Estimation?\nModel specification: Model Specification?\nMachine Learning: have you explained how machine learning works?\nOutcome Specifics: If the outcome is rare and binary, have you specified logistic regression? If it’s continuous, have I considered converting it to z-scores?\nSensitivity analysis: Have you described your sensitivity analysis (e.g. E-values.)\n\nNote most of these tasks can be ticked off in a sentence or two, but all need to be covered.\nLength: ~ 1,000 - 2,000 words (note it is a draft introduction and draft methods section).\n\n\n\n\n\n\n\n\n\n\nResearch Report Instructions\n\n\n\n\nWe will supply the data.\nLab sessions are designed to support you in this assignment.\nWe assume no statistical background.\n\n\n\n\nTitle: “Causal Inference in Cultural Psychology: Examining Exposure Effects on Dimensions of Well-being Modified by Cultural or Sociodemographic Categories”.\nObjective:\n\nTo quantify the causal effect of a specific exposure on well-being dimensions, modified by sociodemographic categories (born_nz, eth_cat, big_doms, gen_cohort) using the NZAVS longitudinal synthetic dataset.\n\nInstructions:\n\nTheoretical Interest and Research Question:\n\nDescribe the significance of your chosen exposure and its potential impact on the selected outcomes, modified by the cultural or sociodemographic category.\nState the research question clearly.\n\nDirected Acyclic Graph (DAG):\n\nConstruct a DAG illustrating the relationships between exposure, outcomes, sociodemographic category, and potential bias sources. Ensure clarity in labelling.\n\nConfounding Control Strategy:\n\nOutline your strategy for confounding control, justifying the chosen confounders.\n\nMeasurement Biases:\n\nAddress and analyse measurement biases as relevant.\n\nAssumptions and Statistical Models:\n\nDiscuss the assumptions of your causal inference approach and your statistical model, including their limitations.\n\n\nRequirements:\n\nIntroduction: 1,500 words limit.\nConclusion: 1,500 words limit.\nMethod and Results sections should be concise; no specific word limit.\nUse any useful sources, citing appropriately to avoid academic misconduct.\nFollow APA style for citations and references.\nInclude tables/figures as needed.\nSubmit as a single PDF, including R code in an appendix.\n\nEvaluation Criteria:\n\nClarity of theoretical framework, research question, and design.\nValidity of confounding control strategy.\nDiscussion on assumptions and statistical models.\nOrganisation and presentation quality.\n\n\n\n\n\n\nExtensions:\n\nNegotiate a new due date by writing (email) before March 11th, 2025, if necessary.\nEvery reasonable request will be accepted (e.g. too many assignments falling in the same week, you want another week to complete.)\n\nPenalties:\n\nLate submissions incur a one full grade-per-week penalty, e.g. if late by one day, B \\to C, one week later, C \\to D.\nOver-length assignments will be penalised.\n\nUnforeseeable Events:\n\nExtensions after March 11th will require evidence (e.g., medical certificate).\n\n\n\n\n\n\nBring a laptop with R and RStudio installed for data analysis sessions. Contact the instructor if you lack computer access.\nFor in-class tests, bring a writing utensil. Again, electronic devices are not permitted."
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Preliminaries",
    "section": "",
    "text": "Class times and locations Lectures Day/Time: Tuesday, 1:10-3:00pm\nLecture Location: Easterfield Building EA407"
  },
  {
    "objectID": "slides/index.html#where-do-we-meet",
    "href": "slides/index.html#where-do-we-meet",
    "title": "Preliminaries",
    "section": "",
    "text": "Class times and locations Lectures Day/Time: Tuesday, 1:10-3:00pm\nLecture Location: Easterfield Building EA407"
  },
  {
    "objectID": "slides/index.html#course-learning-objectives",
    "href": "slides/index.html#course-learning-objectives",
    "title": "Preliminaries",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\n\nEthical Reasoning. Beginning in week 1, we will explore questions that are typically reserved for philosophy courses but are central to the conduct of psychology: why do we need to think about right and wrong in science? What if different people have different ideas about what is right and wrong in science? Is there hope for ethical progress? Can science play a role in ethical progress? We will provide you with a set of strategies for addressing these questions.\nProgramming in R. Beginning in week 2, we will teach you the basics of programming in the statistical language R. The past several decades have brought extraordinary new tools to psychological scientists, many of which require literacy in computer programming. While some of you may find the thought of programming thrilling, others may find it terrifying or boring! We promise that programming can be fun. This course is designed to help you find that joy.\nUnderstanding measurement. Beginning in week 3 the course will turn its focus to developing skills for constructing and validating measures in cross-cultural research. Again these skills will be invaluable for a wide range of tasks you may face in psychological science and will help you to address problems that will arise in other areas of your research and work. Measurement will occupy our attention through week 6.\nUnderstanding causal inference. Beginning in week 7 and for the following four weeks, the course will impart skills that you require to disentangle causation from correlation. Again our focus will be on the special problems that arise for cultural datasets. Notably, in psychological science, causal inference remains underdeveloped, and the material in this part of the course will position you to make potentially important contributions, whether or not your interests lie in cross-cultural psychology."
  },
  {
    "objectID": "slides/index.html#assignments-and-due-dates",
    "href": "slides/index.html#assignments-and-due-dates",
    "title": "Preliminaries",
    "section": "Assignments and due dates",
    "text": "Assignments and due dates\n\n\n\n\n\nAssessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n*\n10\nWeekly\n\n\nTake-home\n3,4\n10\nMarch 19 (w3)\n\n\nTheoretical application\n1,2,3\n30\n9 May (w9)\n\n\nPre-reg review\n1,2,3,4\n10\n28 May (w11)\n\n\nStatistical application\n1,2,3\n30\n11 June (w13)\n\n\nLetter to the reviewer\n1,2,3\n10\n11 June (w13)"
  },
  {
    "objectID": "slides/index.html#assessment-1-class-participation",
    "href": "slides/index.html#assessment-1-class-participation",
    "title": "Preliminaries",
    "section": "Assessment 1: Class Participation",
    "text": "Assessment 1: Class Participation\n\n\nyour willingness to ask/answer questions and generally contribute to class discussion; and\n\n\nthe quality of your contributions\n\nNOTE: if you’re sick, you shouldn’t come to class. Just let me know in advance. Your participation will not be affected."
  },
  {
    "objectID": "slides/index.html#assessment-2-take-home-research-concepts-workbook",
    "href": "slides/index.html#assessment-2-take-home-research-concepts-workbook",
    "title": "Preliminaries",
    "section": "Assessment 2: Take Home Research Concepts Workbook",
    "text": "Assessment 2: Take Home Research Concepts Workbook\n\nThis test will help you revise core statistical and methodological issues. It is a chance to brush up on basic statistical knowledge and will help you re-familiarize yourself with material that we will build on later in this course.\nThe key focus is the revision of basic terms, including correlation, regression, and on basic R applications.\nThe test contains a theoretical and a practical component. Therefore, it is essential that you complete these questions since you will need to understand the material for the more advanced techniques and approaches that we cover in this course.\nThere is no word limit (be reasonable)\nYou can use any source that you find useful.\nWarning: AI chatbots may be used but they make errors confidently.\nGenerally, it is better practice, to use peer-reviewed publications.\nAny resource you use must be cited. Failure to cite your source is a form of academic misconduct.\nYou may complete the test in your own study time. We ask that you work individually."
  },
  {
    "objectID": "slides/index.html#assessment-3theoretical-application-5000-word-maximum-including-references",
    "href": "slides/index.html#assessment-3theoretical-application-5000-word-maximum-including-references",
    "title": "Preliminaries",
    "section": "Assessment 3:Theoretical Application (5,000 word maximum including references)",
    "text": "Assessment 3:Theoretical Application (5,000 word maximum including references)\nChoose one particular psychological concept or variable of interest to you. Provide a brief introduction to the concept or variable. Provide a sketch of the methodology for your study (what analysis will you run: equivalence tests, regressions, etc.) following the provided pre-registration template. Identify and justify your research participants’ cultural background. This assessment is equivalent to the introduction and methods part of an empirical paper.\nThe word limit for this assignment is 5000 words max (including references), and it must be typed and double spaced with a 12point Times New Roman font (or similar)."
  },
  {
    "objectID": "slides/index.html#pre-registration-review-1000-words-maximum-including-references",
    "href": "slides/index.html#pre-registration-review-1000-words-maximum-including-references",
    "title": "Preliminaries",
    "section": "Pre-Registration Review (1000 words maximum, including references)",
    "text": "Pre-Registration Review (1000 words maximum, including references)\nYou will be invited to provide a review of one submitted pre-registration from another student. Using what you have learned about cross-cultural research provide feedback on both theory and suggested methods. Make sure to provide concrete and respectful feedback.\nStatistical Application (4,000 word maximum excluding tables and any references)\nOption 1 Measurement (see outline)\nOption 2 Causality (see outline)"
  },
  {
    "objectID": "slides/index.html#letter-to-the-reviewer",
    "href": "slides/index.html#letter-to-the-reviewer",
    "title": "Preliminaries",
    "section": "Letter to the Reviewer",
    "text": "Letter to the Reviewer\nUsing the reviewer letter you received, respond to the points raised by the reviewer. Where did you alter your methods and pre-registration, where do results support your reviewer, where do they not support their claims.\nSubmission and return of work"
  },
  {
    "objectID": "slides/index.html#assessments-or-detailed-feedback-on-your-performance-will-typically-be-returned-to-you-within-10-working-days.",
    "href": "slides/index.html#assessments-or-detailed-feedback-on-your-performance-will-typically-be-returned-to-you-within-10-working-days.",
    "title": "Preliminaries",
    "section": "Assessments or detailed feedback on your performance will typically be returned to you within 10 working days.",
    "text": "Assessments or detailed feedback on your performance will typically be returned to you within 10 working days."
  },
  {
    "objectID": "slides/index.html#extensions",
    "href": "slides/index.html#extensions",
    "title": "Preliminaries",
    "section": "Extensions",
    "text": "Extensions\nIf any due date does not suit you, you may negotiate a new due date, in writing, that suits you. Any request for revision must be submitted in writing (by email) to your instructor before class on March 14th, 2023"
  },
  {
    "objectID": "slides/index.html#penalties",
    "href": "slides/index.html#penalties",
    "title": "Preliminaries",
    "section": "Penalties",
    "text": "Penalties\nThe submission of late assignments is strongly discouraged. A penalty of one grade per day (e.g., B down to B-) from the hand-in date will be deducted from the final grade for any late work."
  },
  {
    "objectID": "slides/index.html#unforeseeable-events",
    "href": "slides/index.html#unforeseeable-events",
    "title": "Preliminaries",
    "section": "Unforeseeable events",
    "text": "Unforeseeable events\nIn general, we require evidence (medical certificate etc.) to grant an extension."
  },
  {
    "objectID": "slides/index.html#word-limits",
    "href": "slides/index.html#word-limits",
    "title": "Preliminaries",
    "section": "Word limits",
    "text": "Word limits\nAssignments that are over the word limit (see above) will be penalized. All submitted assessments are expected to be your own. You should neither give nor receive any aid on the assessments. Giving or receiving aid on assessments will be considered academic misconduct, resulting in registration in the Academic Misconduct Register (AMR). You may use any source, but the source must be cited. Penalties appropriate for the level of academic misconduct may be applied. Failure to cite your source will be considered academic misconduct and will result in registration in the Academic Misconduct Register (AMR)."
  },
  {
    "objectID": "slides/index.html#materials-and-equipment",
    "href": "slides/index.html#materials-and-equipment",
    "title": "Preliminaries",
    "section": "Materials and equipment",
    "text": "Materials and equipment\nYou should bring paper and a writing utensil or laptop/tablet to seminars. For the data analysis sessions, having your personal laptop with the relevant computer programs (R and RStudio) installed is essential. Most sessions will require data analysis.\n\nNote: If you do not have a laptop, let me know. We will find a solution."
  },
  {
    "objectID": "slides/index.html#help",
    "href": "slides/index.html#help",
    "title": "Preliminaries",
    "section": "Help",
    "text": "Help\nDr.Inkuk Kim inkuk.kim@vuw.ac.nz\n\n\n\n\n\n\n\n\n\nDr. In Kuk Kim\n\n\n\n\n\nTorven Schalk torven.schalk@vuw.ac.nz\n\n\n\n\n\n\n\n\n\nTorven Schalk\n\n\n\n\n\nJoseph Bulbulia joseph.bulbulia@vuw.ac.nz\n\n\n\n\n\n\n\n\n\nJoseph Bulbulia"
  },
  {
    "objectID": "slides/index.html#thanks-to-johannes-karl-and-ron-fischer",
    "href": "slides/index.html#thanks-to-johannes-karl-and-ron-fischer",
    "title": "Preliminaries",
    "section": "Thanks to Johannes Karl and Ron Fischer",
    "text": "Thanks to Johannes Karl and Ron Fischer\nFor developing measurement components of the course\n\n\n\n\n\n\n\n\n\nJohannes Karl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRonald Fischer"
  },
  {
    "objectID": "slides/01-slides-lab.html",
    "href": "slides/01-slides-lab.html",
    "title": "Week 1: Installing and Using R",
    "section": "",
    "text": "Have R and R-studio Downloaded on your machine\nBe able to use R for basic analysis and graphing"
  },
  {
    "objectID": "slides/01-slides-lab.html#introduction",
    "href": "slides/01-slides-lab.html#introduction",
    "title": "Week 1: Installing and Using R",
    "section": "Introduction",
    "text": "Introduction\n\nWhy learn R?\nYou’ll need it for your final report.\nSupports your psychology coursework.\nEnhances your coding skills."
  },
  {
    "objectID": "slides/01-slides-lab.html#install-r",
    "href": "slides/01-slides-lab.html#install-r",
    "title": "Week 1: Installing and Using R",
    "section": "Install R",
    "text": "Install R\n\nVisit the comprehensive r archive network (cran) at https://cran.r-project.org/\nSelect the version of r suitable for your operating system (windows, mac, or linux)\nDownload and install it by following the on-screen instructions"
  },
  {
    "objectID": "slides/01-slides-lab.html#install-rstudio",
    "href": "slides/01-slides-lab.html#install-rstudio",
    "title": "Week 1: Installing and Using R",
    "section": "Install RStudio",
    "text": "Install RStudio\n\nVisit rstudio download page at https://www.rstudio.com/products/rstudio/download/\nChoose the free version of rstudio desktop,\nDownload it for your operating system\nInstall and open"
  },
  {
    "objectID": "slides/01-slides-lab.html#create-new-project",
    "href": "slides/01-slides-lab.html#create-new-project",
    "title": "Week 1: Installing and Using R",
    "section": "Create new project",
    "text": "Create new project\n\nfile &gt; new project\nChoose new directory\nSpecify the location where the project folder will be created\nClick create project"
  },
  {
    "objectID": "slides/01-slides-lab.html#exercise-1-install-tidyverse",
    "href": "slides/01-slides-lab.html#exercise-1-install-tidyverse",
    "title": "Week 1: Installing and Using R",
    "section": "Exercise 1: Install tidyverse",
    "text": "Exercise 1: Install tidyverse\n\nOpen rstudio: launch rstudio on your computer\ntools &gt; install packages\nType tidyverse\nClick on the install button\nType library(tidyverse) in the console and press enter"
  },
  {
    "objectID": "slides/01-slides-lab.html#execut-code",
    "href": "slides/01-slides-lab.html#execut-code",
    "title": "Week 1: Installing and Using R",
    "section": "Execut Code",
    "text": "Execut Code\n\nUse Ctrl + Enter (Windows/Linux) or Cmd + Enter (Mac)."
  },
  {
    "objectID": "slides/01-slides-lab.html#assignment-operator",
    "href": "slides/01-slides-lab.html#assignment-operator",
    "title": "Week 1: Installing and Using R",
    "section": "Assignment Operator",
    "text": "Assignment Operator\nThe assignment operator in R is &lt;-. This operator assigns values to variables.\n\nx &lt;- 10 # assigns the value 10 to x\ny &lt;- 5  # assigns the value 5 to y\n\nAlternative assignment:\n\nx = 10\ny = 5\n\nComparing values:\n\n10 == 5 # returns FALSE\n\n[1] FALSE"
  },
  {
    "objectID": "slides/01-slides-lab.html#rstudio-assignment-operator-shortcut",
    "href": "slides/01-slides-lab.html#rstudio-assignment-operator-shortcut",
    "title": "Week 1: Installing and Using R",
    "section": "RStudio Assignment Operator Shortcut",
    "text": "RStudio Assignment Operator Shortcut\n\nFor macOS: Option + - inserts &lt;-.\nFor Windows and Linux: Alt + - inserts &lt;-."
  },
  {
    "objectID": "slides/01-slides-lab.html#keyboard-shortcuts",
    "href": "slides/01-slides-lab.html#keyboard-shortcuts",
    "title": "Week 1: Installing and Using R",
    "section": "Keyboard Shortcuts",
    "text": "Keyboard Shortcuts\nExplore keyboard shortcuts in RStudio through Tools -&gt; Keyboard Shortcuts Help."
  },
  {
    "objectID": "slides/01-slides-lab.html#concatenation",
    "href": "slides/01-slides-lab.html#concatenation",
    "title": "Week 1: Installing and Using R",
    "section": "Concatenation",
    "text": "Concatenation\nThe c() function combines multiple elements into a vector.\n\nnumbers &lt;- c(1, 2, 3, 4, 5) # a vector of numbers\nprint(numbers)\n\n[1] 1 2 3 4 5"
  },
  {
    "objectID": "slides/01-slides-lab.html#arithmetic-operations",
    "href": "slides/01-slides-lab.html#arithmetic-operations",
    "title": "Week 1: Installing and Using R",
    "section": "Arithmetic Operations",
    "text": "Arithmetic Operations\nAddition and subtraction in R:\n\nsum &lt;- x + y\nprint(sum)\n\n[1] 15\n\ndifference &lt;- x - y\nprint(difference)\n\n[1] 5"
  },
  {
    "objectID": "slides/01-slides-lab.html#multiplication-and-division",
    "href": "slides/01-slides-lab.html#multiplication-and-division",
    "title": "Week 1: Installing and Using R",
    "section": "Multiplication and Division",
    "text": "Multiplication and Division\n\n# Scalar operations\nproduct &lt;- x * y\nquotient &lt;- x / y\n\n\n# vector multiplication and division\nvector1 &lt;- c(1, 2, 3)\nvector2 &lt;- c(4, 5, 6)\n\n# Vector operations\nvector_product &lt;- vector1 * vector2\nvector_division &lt;- vector1 / vector2\n\nBe cautious with division by zero:\n\nresult &lt;- 10 / 0 # Inf\nzero_division &lt;- 0 / 0 # NaN\n\nInteger division and modulo operation:\n\ninteger_division &lt;- 10 %/% 3\nremainder &lt;- 10 %% 3"
  },
  {
    "objectID": "slides/01-slides-lab.html#logical-operators",
    "href": "slides/01-slides-lab.html#logical-operators",
    "title": "Week 1: Installing and Using R",
    "section": "Logical Operators",
    "text": "Logical Operators\nExamples of NOT, NOT EQUAL, and EQUAL operations:\n\nx_not_y &lt;- x != y\nx_equal_10 &lt;- x == 10\n\nOR and AND operations:\n\nvector_or &lt;- c(TRUE, FALSE) | c(FALSE, TRUE)\nsingle_or &lt;- TRUE || FALSE\n\nvector_and &lt;- c(TRUE, FALSE) & c(FALSE, TRUE)\nsingle_and &lt;- TRUE && FALSE"
  },
  {
    "objectID": "slides/01-slides-lab.html#integers",
    "href": "slides/01-slides-lab.html#integers",
    "title": "Week 1: Installing and Using R",
    "section": "Integers",
    "text": "Integers\n\nWhole numbers without decimal points, defined with an L suffix\n\n\nx &lt;- 42L\nstr(x) # check type\n\n int 42\n\n\n\nConversion to numeric\n\n\ny &lt;- as.numeric(x)\nstr(y)\n\n num 42"
  },
  {
    "objectID": "slides/01-slides-lab.html#characters",
    "href": "slides/01-slides-lab.html#characters",
    "title": "Week 1: Installing and Using R",
    "section": "Characters",
    "text": "Characters\n\nText strings enclosed in quotes\n\n\nname &lt;- \"alice\""
  },
  {
    "objectID": "slides/01-slides-lab.html#factors",
    "href": "slides/01-slides-lab.html#factors",
    "title": "Week 1: Installing and Using R",
    "section": "Factors",
    "text": "Factors\n\nRepresent categorical data with limited values\n\n\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#ordered-factors",
    "href": "slides/01-slides-lab.html#ordered-factors",
    "title": "Week 1: Installing and Using R",
    "section": "Ordered Factors",
    "text": "Ordered Factors\n\nFactors with and without inherent order\n\n\neducation_levels &lt;- c(\"high school\", \"bachelor\", \"master\", \"ph.d.\")\neducation_factor_no_order &lt;- factor(education_levels, ordered = FALSE)\neducation_factor &lt;- factor(education_levels, ordered = TRUE)\neducation_ordered_explicit &lt;- factor(education_levels, levels = education_levels, ordered = TRUE)\n\n\nOperations with ordered factors\n\n\nedu1 &lt;- ordered(\"bachelor\", levels = education_levels)\nedu2 &lt;- ordered(\"master\", levels = education_levels)\nedu2 &gt; edu1 # logical comparison\n\n[1] TRUE\n\n\n\nModifying ordered factors\n\n\nnew_levels &lt;- c(\"primary school\", \"high school\", \"bachelor\", \"master\", \"ph.d.\")\neducation_updated &lt;- factor(education_levels, levels = new_levels, ordered = TRUE)\nstr(education_updated)\n\n Ord.factor w/ 5 levels \"primary school\"&lt;..: 2 3 4 5\n\ntable(education_updated)\n\neducation_updated\nprimary school    high school       bachelor         master          ph.d. \n             0              1              1              1              1"
  },
  {
    "objectID": "slides/01-slides-lab.html#strings",
    "href": "slides/01-slides-lab.html#strings",
    "title": "Week 1: Installing and Using R",
    "section": "Strings",
    "text": "Strings\n\nSequences of characters\n\n\nyou &lt;- 'world!'\ngreeting &lt;- paste(\"hello,\", you)\n# hello world\ngreeting\n\n[1] \"hello, world!\""
  },
  {
    "objectID": "slides/01-slides-lab.html#vectors",
    "href": "slides/01-slides-lab.html#vectors",
    "title": "Week 1: Installing and Using R",
    "section": "Vectors",
    "text": "Vectors\n\nFundamental data structure in R\n\n\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"cherry\")\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, FALSE)\n\n\nManipulating vectors\n\n\nvector_sum &lt;- numeric_vector + 10\nvector_multiplication &lt;- numeric_vector * 2\nvector_greater_than_three &lt;- numeric_vector &gt; 3"
  },
  {
    "objectID": "slides/01-slides-lab.html#table-function",
    "href": "slides/01-slides-lab.html#table-function",
    "title": "Week 1: Installing and Using R",
    "section": "table() Function",
    "text": "table() Function\n\nGenerates frequency tables for categorical data\n\n\ntable(vector_greater_than_three)\n\nvector_greater_than_three\nFALSE  TRUE \n    3     2"
  },
  {
    "objectID": "slides/01-slides-lab.html#dataframes",
    "href": "slides/01-slides-lab.html#dataframes",
    "title": "Week 1: Installing and Using R",
    "section": "Dataframes",
    "text": "Dataframes\n\nCreating and manipulating data frames\n\n\n# clear previous `df` object (if any)\nrm(df)\ndf &lt;- data.frame(\n  name = c(\"alice\", \"bob\", \"charlie\"),\n  age = c(25, 30, 35),\n  gender = c(\"female\", \"male\", \"male\")\n)\n# look at structure\nhead(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\nstr(df)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"alice\" \"bob\" \"charlie\"\n $ age   : num  25 30 35\n $ gender: chr  \"female\" \"male\" \"male\"\n\ntable(df$gender)\n\n\nfemale   male \n     1      2 \n\ntable(df$age)\n\n\n25 30 35 \n 1  1  1 \n\ntable(df$name)\n\n\n  alice     bob charlie \n      1       1       1"
  },
  {
    "objectID": "slides/01-slides-lab.html#access-data-frame-elements",
    "href": "slides/01-slides-lab.html#access-data-frame-elements",
    "title": "Week 1: Installing and Using R",
    "section": "Access Data Frame Elements",
    "text": "Access Data Frame Elements\n\nBy column name and row/column indexing\n\n\n# by column name\nnames &lt;- df$name\n# by row and column\nsecond_person &lt;- df[2, ]\nage_column &lt;- df[, \"age\"]"
  },
  {
    "objectID": "slides/01-slides-lab.html#using-subset-function",
    "href": "slides/01-slides-lab.html#using-subset-function",
    "title": "Week 1: Installing and Using R",
    "section": "Using subset() Function",
    "text": "Using subset() Function\n\nExtracting rows based on conditions\n\n\nvery_old_people &lt;- subset(df, age &gt; 25)\nsummary(very_old_people$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.00   31.25   32.50   32.50   33.75   35.00 \n\nmean(very_old_people$age)\n\n[1] 32.5\n\nmin(very_old_people$age)\n\n[1] 30"
  },
  {
    "objectID": "slides/01-slides-lab.html#explore-data-frames",
    "href": "slides/01-slides-lab.html#explore-data-frames",
    "title": "Week 1: Installing and Using R",
    "section": "Explore Data Frames",
    "text": "Explore Data Frames\n\nUsing head(), tail(), and str()\n\n\nhead(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\ntail(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\nstr(df)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"alice\" \"bob\" \"charlie\"\n $ age   : num  25 30 35\n $ gender: chr  \"female\" \"male\" \"male\""
  },
  {
    "objectID": "slides/01-slides-lab.html#modify-data-frames",
    "href": "slides/01-slides-lab.html#modify-data-frames",
    "title": "Week 1: Installing and Using R",
    "section": "Modify Data Frames",
    "text": "Modify Data Frames\n\nAdd and modify columns and rows\n\n\n# add columns\ndf$employed &lt;- c(TRUE, TRUE, FALSE)\n# add rows\nnew_person &lt;- data.frame(name = \"diana\", age = 28, gender = \"female\", employed = TRUE)\ndf &lt;- rbind(df, new_person)\n# modify values\ndf[4, \"age\"] &lt;- 26\ndf\n\n     name age gender employed\n1   alice  25 female     TRUE\n2     bob  30   male     TRUE\n3 charlie  35   male    FALSE\n4   diana  26 female     TRUE"
  },
  {
    "objectID": "slides/01-slides-lab.html#rbind-and-cbind",
    "href": "slides/01-slides-lab.html#rbind-and-cbind",
    "title": "Week 1: Installing and Using R",
    "section": "rbind() and cbind()",
    "text": "rbind() and cbind()\n\nAdding rows and columns to data frames\n\n\n# add rows with `rbind()`\nnew_person &lt;- data.frame(name = \"eve\", age = 32, gender = \"female\", employed = TRUE)\ndf &lt;- rbind(df, new_person)\n# add columns with `cbind()`\noccupation_vector &lt;- c(\"engineer\", \"doctor\", \"artist\", \"teacher\", \"doctor\")\ndf &lt;- cbind(df, occupation_vector)\ndf\n\n     name age gender employed occupation_vector\n1   alice  25 female     TRUE          engineer\n2     bob  30   male     TRUE            doctor\n3 charlie  35   male    FALSE            artist\n4   diana  26 female     TRUE           teacher\n5     eve  32 female     TRUE            doctor"
  },
  {
    "objectID": "slides/01-slides-lab.html#data-structure-view",
    "href": "slides/01-slides-lab.html#data-structure-view",
    "title": "Week 1: Installing and Using R",
    "section": "Data Structure View",
    "text": "Data Structure View\n\nUsing summary(), str(), head(), and tail()\n\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntail(iris)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica"
  },
  {
    "objectID": "slides/01-slides-lab.html#statistical-functions",
    "href": "slides/01-slides-lab.html#statistical-functions",
    "title": "Week 1: Installing and Using R",
    "section": "Statistical Functions",
    "text": "Statistical Functions\n\nmean(), sd(), min(), max(), and table()\n\n\n#  seed for reproducibility\nset.seed(12345)\nvector &lt;- rnorm(n = 40, mean = 0, sd = 1)\nmean(vector)  # calculates mean\n\n[1] 0.2401853\n\nsd(vector)  # computes standard deviation\n\n[1] 1.038425\n\nmin(vector)  # finds minimum value\n\n[1] -1.817956\n\nmax(vector)  # finds maximum value\n\n[1] 2.196834"
  },
  {
    "objectID": "slides/01-slides-lab.html#introduction-to-ggplot2",
    "href": "slides/01-slides-lab.html#introduction-to-ggplot2",
    "title": "Week 1: Installing and Using R",
    "section": "Introduction to ggplot2",
    "text": "Introduction to ggplot2\n\nVisualizing data with ggplot2\n\n\n#  seed for reproducibility\nset.seed(12345)\n# ensure ggplot2 is installed and loaded\nif (!require(ggplot2)) install.packages(\"ggplot2\")\nlibrary(ggplot2)\n# simulate student data\nstudent_data &lt;- data.frame(\n  name = c(\"alice\", \"bob\", \"charlie\", \"diana\", \"ethan\", \"fiona\", \"george\", \"hannah\"),\n  score = sample(80:100, 8, replace = TRUE),\n  stringsasfactors = FALSE\n)\nstudent_data$passed &lt;- ifelse(student_data$score &gt;= 90, \"passed\", \"failed\")\nstudent_data$passed &lt;- factor(student_data$passed, levels = c(\"failed\", \"passed\"))\nstudent_data$study_hours &lt;- sample(5:15, 8, replace = TRUE)"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-barplot-score-for-each-name",
    "href": "slides/01-slides-lab.html#ggplot2-barplot-score-for-each-name",
    "title": "Week 1: Installing and Using R",
    "section": "ggplot2 Barplot: score for each name",
    "text": "ggplot2 Barplot: score for each name\n\nggplot(student_data, aes(x = name, y = score)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nenhanced bar plot with titles, axis labels, and modified colours\n\n\nggplot(student_data, aes(x = name, y = score, fill = passed)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\")) +\n  labs(title = \"student scores\", x = \"student name\", y = \"score\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-scatterplot-student-scores-against-study-hours",
    "href": "slides/01-slides-lab.html#ggplot2-scatterplot-student-scores-against-study-hours",
    "title": "Week 1: Installing and Using R",
    "section": "ggplot2 Scatterplot: student scores against study hours",
    "text": "ggplot2 Scatterplot: student scores against study hours\n\nggplot(student_data, aes(x = study_hours, y = score, color = passed)) +\n  geom_point(size = 4) +\n  labs(title = \"student scores vs. study hours\", x = \"study hours\", y = \"score\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-boxplot-scores-by-passfail-status",
    "href": "slides/01-slides-lab.html#ggplot2-boxplot-scores-by-passfail-status",
    "title": "Week 1: Installing and Using R",
    "section": "ggplot2 Boxplot: scores by pass/fail status",
    "text": "ggplot2 Boxplot: scores by pass/fail status\n\nggplot(student_data, aes(x = passed, y = score, fill = passed)) +\n  geom_boxplot() +\n  labs(title = \"score distribution by pass/fail status\", x = \"status\", y = \"score\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-histogram-distribution-of-scores",
    "href": "slides/01-slides-lab.html#ggplot2-histogram-distribution-of-scores",
    "title": "Week 1: Installing and Using R",
    "section": "ggplot2 Histogram: distribution of scores",
    "text": "ggplot2 Histogram: distribution of scores\n\nggplot(student_data, aes(x = score, fill = passed)) +\n  geom_histogram(binwidth = 5, color = \"black\", alpha = 0.7) +\n  labs(title = \"histogram of scores\", x = \"score\", y = \"count\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-lineplot",
    "href": "slides/01-slides-lab.html#ggplot2-lineplot",
    "title": "Week 1: Installing and Using R",
    "section": "ggplot2 Lineplot",
    "text": "ggplot2 Lineplot\n\n# prep data\nmonths &lt;- factor(month.abb[1:8], levels = month.abb[1:8])\nstudy_hours &lt;- c(0, 3, 15, 30, 35, 120, 18, 15)\nstudy_data &lt;- data.frame(month = months, study_hours = study_hours)\n\n# line plot\nggplot(study_data, aes(x = month, y = study_hours, group = 1)) +\n  geom_line(linewidth = 1, color = \"blue\") +\n  geom_point(color = \"red\", size = 1) +\n  labs(title = \"monthly study hours\", x = \"month\", y = \"study hours\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-scatter-plot-scores-vs.-study-hours",
    "href": "slides/01-slides-lab.html#base-r-scatter-plot-scores-vs.-study-hours",
    "title": "Week 1: Installing and Using R",
    "section": "Base R Scatter Plot: scores vs. study hours",
    "text": "Base R Scatter Plot: scores vs. study hours\n\n# scatter plot\nplot(student_data$study_hours, student_data$score,\n     main = \"scatter plot of scores vs. study hours\",\n     xlab = \"study hours\", ylab = \"score\",\n     pch = 19, col = ifelse(student_data$passed == \"passed\", \"blue\", \"red\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-histogram",
    "href": "slides/01-slides-lab.html#base-r-histogram",
    "title": "Week 1: Installing and Using R",
    "section": "Base R Histogram",
    "text": "Base R Histogram\n\nhistogram to visualise distribution of student scores\n\n\n# histogram \nhist(student_data$score,\n     breaks = 5,\n     col = \"skyblue\",\n     main = \"histogram of student scores\",\n     xlab = \"scores\",\n     border = \"white\")"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-boxplots-distribution-by-passfail",
    "href": "slides/01-slides-lab.html#base-r-boxplots-distribution-by-passfail",
    "title": "Week 1: Installing and Using R",
    "section": "Base R Boxplots: Distribution by Pass/Fail",
    "text": "Base R Boxplots: Distribution by Pass/Fail\n\n# boxplot\nboxplot(score ~ passed, data = student_data,\n        main = \"score distribution by pass/fail status\",\n        xlab = \"status\", ylab = \"scores\",\n        col = c(\"red\", \"blue\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-barplot-score-distributions",
    "href": "slides/01-slides-lab.html#base-r-barplot-score-distributions",
    "title": "Week 1: Installing and Using R",
    "section": "Base R Barplot: Score Distributions",
    "text": "Base R Barplot: Score Distributions\n\n# prep data for the barplot\nscores_table &lt;- table(student_data$score)\nbarplot(scores_table,\n        main = \"Barplot of Scores\",\n        xlab = \"Scores\",\n        ylab = \"Frequency\",\n        col = \"skyblue\",\n        border = \"white\")"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-line-plot",
    "href": "slides/01-slides-lab.html#base-r-line-plot",
    "title": "Week 1: Installing and Using R",
    "section": "Base R Line Plot",
    "text": "Base R Line Plot\n\n# convert 'month' to a numeric scale for plotting positions\nmonths_num &lt;- 1:length(study_data$month) # Simple numeric sequence\n\n# Plotting points with suppressed x-axis\nplot(months_num, study_data$study_hours, \n     type = \"p\", # Points\n     pch = 19,   # Type of point\n     col = \"red\", \n     xlab = \"Month\", \n     ylab = \"Study Hours\", \n     main = \"Monthly Study Hours\",\n     xaxt = \"n\") # Suppress the x-axis\n\n# add lines between points\nlines(months_num, study_data$study_hours, \n      col = \"blue\", \n      lwd = 1) # Line width\n\n# add custom month labels to the x-axis at appropriate positions\naxis(1, at = months_num, labels = study_data$month, las=2) # `las=2` makes labels perpendicular to axis"
  },
  {
    "objectID": "slides/01-slides-lab.html#what-have-your-learned",
    "href": "slides/01-slides-lab.html#what-have-your-learned",
    "title": "Week 1: Installing and Using R",
    "section": "What have your learned?",
    "text": "What have your learned?\n\nYou have Base R and R-studio Downloaded on your machine\nYou are able able to use R for basic analysis and graphing\nYou will need to practice, and will have lots of opporunity."
  },
  {
    "objectID": "slides/01-slides-lab.html#where-to-get-help",
    "href": "slides/01-slides-lab.html#where-to-get-help",
    "title": "Week 1: Installing and Using R",
    "section": "Where to Get Help",
    "text": "Where to Get Help\n\nLarge Language Models (LLMs): LLMs are trained on extensive datasets. They are extremely good coding tutors. Open AI’s GPT-4 considerably outperforms GPT-3.5. However GPT 3.5 should be good enough. Gemini has a two-month free trial. LLM’s are rapidly evolving. However, presently, to use these tools, and to spot their errors, you will need to know how to code. Which is fortunate because coding makes you smarter!\n\nNote: you will not be assessed for R-code. Help from LLM’s for coding does not consitute a breach of academic integrity in this course. Your tests are in-class; no LLM’s allowed. For your final report, you will need to cite all sources, and how you used them, including LLMs.\n\nStack Overflow: an outstanding resource for most problems. Great community.\nCross-validated the best place to go for stats advice. (LLM’s are only safe for standard statistics. They do not perform well for causal inference.)\nDeveloper Websites and GitHub Pages: Tidyverse\nYour tutors and course coordinator. We care. We’re here to help you!"
  },
  {
    "objectID": "slides/01-slides-lab.html#references",
    "href": "slides/01-slides-lab.html#references",
    "title": "Week 1: Installing and Using R",
    "section": "References",
    "text": "References\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. O’Reilly Media. [Available online](https://r4ds.had.co.nz\nA helpful resource for learning R is Megan Hall’s lecture available at: https://meghan.rbind.io/talk/neair/.\nRStudio has compiled numerous accessible materials for learning R, which can be found here: https://education.rstudio.com/learn/beginner/."
  },
  {
    "objectID": "index.html#test-guidelines",
    "href": "index.html#test-guidelines",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Test duration is one hour. The allocated time is nearly two hours.\n\n\n\n\n\n\n\nImportant\n\n\n\n\nTHE TEST IS IN CLASS (i.e. come to class with a writing instrument).\n\n\n\n\nEach lecture starts and ends with key concept definitions and reviews for the test.\nR or RStudio knowledge isn’t part of the test. R support aims to enhance research report skills.\nTests are conducted in the lecture room without the aids of notes, a computer, or a phone.\nRequired: pen/pencil.\nTest (50 minutes, total time allowed: 1 hour 50 minutes):\n\nFocuses on revising core statistical and methodological concepts.\nAims to refresh basic statistical knowledge foundational for later course material."
  },
  {
    "objectID": "content/10-content.html#part-2-quarto-manuscripts",
    "href": "content/10-content.html#part-2-quarto-manuscripts",
    "title": "Hands On Working With Quarto Manuscript",
    "section": "Part 2: Quarto manuscripts",
    "text": "Part 2: Quarto manuscripts\n\n\n\n\n\n\nNote\n\n\n\nRequired Download Quarto here: - Use the prelease version: https://quarto.org/docs/download/ Optional - (Bulbulia 2024) link - (Hoffman et al. 2023) link\n\n\n\nHoffman, Katherine L., Diego Salazar-Barreto, Kara E. Rudolph, and Iván Díaz. 2023. “Introducing Longitudinal Modified Treatment Policies: A Unified Framework for Studying Complex Exposures,” April. https://doi.org/10.48550/arXiv.2304.09460.\n\nBulbulia, J. A. 2024. “A Practical Guide to Causal Inference in Three-Wave Panel Studies.” OSF. https://doi.org/10.31234/osf.io/uyg3d.\n\n\n\n\n\n\nKey concepts\n\n\n\n\nWriting up your manuscript\n\n\n\n\n\n\n\n\n\nFor the lab, download the script\n\n\n\n\nWe will go through this script step-by-step.\n\n\n\n\nTo Do\n\nDownload the script (from week 9) and store in our R directory: link to analysis script\nDownload the manuscript template and store in our R directory: link to manuscript template\nDownload the following tex file and save it to your R directory: latex macros\nDownload the followign tex file and save it to your R directory: title preamble\nDownload the following csl file and save it to your R directory: csl preamble\nMake sure you can install all libraries required of the manuscript template.\nCome to the seminar prepared to work through the analysis"
  },
  {
    "objectID": "content/10-content.html#what-you-will-learn",
    "href": "content/10-content.html#what-you-will-learn",
    "title": "Hands On Working With Quarto Manuscript",
    "section": "What You Will Learn",
    "text": "What You Will Learn\n\nHow to create a publication quality manuscript\nHow to create a workflow for references\nHow to import results into your manuscript\nHow to make graphs of your results (using) margot\nHow to report your results\nHow to interpret your results\n\n\nPackages\n\nreport::cite_packages()\n\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 1.0.37 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont &lt;https://doi.org/10.32614/CRAN.package.extrafont&gt;, R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. doi:10.32614/CRAN.package.tibble &lt;https://doi.org/10.32614/CRAN.package.tibble&gt;, R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. doi:10.32614/CRAN.package.forcats &lt;https://doi.org/10.32614/CRAN.package.forcats&gt;, R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. doi:10.32614/CRAN.package.stringr &lt;https://doi.org/10.32614/CRAN.package.stringr&gt;, R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. doi:10.32614/CRAN.package.dplyr &lt;https://doi.org/10.32614/CRAN.package.dplyr&gt;, R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. doi:10.32614/CRAN.package.purrr &lt;https://doi.org/10.32614/CRAN.package.purrr&gt;, R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. doi:10.32614/CRAN.package.readr &lt;https://doi.org/10.32614/CRAN.package.readr&gt;, R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. doi:10.32614/CRAN.package.tidyr &lt;https://doi.org/10.32614/CRAN.package.tidyr&gt;, R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "",
    "text": "Readings\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html"
  },
  {
    "objectID": "content/02-content.html#definitions",
    "href": "content/02-content.html#definitions",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Definitions",
    "text": "Definitions\n\nDefinition 1 We say internal validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the sample population as defined at baseline.\n\n\nDefinition 2 We say external validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the target population as defined at baseline.\n\nThe concept of “confounding bias” helps to clarify what it is at stake when evaluating the internal validity of a study. As we shall see, there are several equivalent definitions of “confounding bias,” which we will describe during the upcoming weeks.\nThe definition of confounding bias that we will examine today is:\n\nDefinition 3 We say there is confounding bias if there is an open back-door path between the treatment and outcome or if the path between the treatment and outcome is blocked.\n\nToday, our purpose will be to clarify the meaning of each term in this definition. To that end, we will introduce the five elementary graphical structures employed in causal diagrams. We will then explain the four elementary rules that allow investigators to identify causal effects from the asserted relations in a causal diagram. First, what are causal diagrams?"
  },
  {
    "objectID": "content/02-content.html#introduction-to-causal-diagrams.",
    "href": "content/02-content.html#introduction-to-causal-diagrams.",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Introduction to Causal Diagrams.",
    "text": "Introduction to Causal Diagrams.\nCausal diagrams, also called causal graphs, Directed Acyclic Graphs, and Causal Directed Acyclic Graphs, are graphical tools whose primary purpose is to enable investigators to detect confounding biases.\nRemarkably, causal diagrams are rarely used in psychology!\nBefore describing how causal diagrams work, we first define the meanings of their symbols. Note there is no single convention for creating causal diagrams, so it is important that we are clear when defining our meanings.\n\nThe meaning of our symbols\nThe conventions that describe the meanings of our symbols are given in Figure 1.\n\n\n\n\n\n\nFigure 1: Our variable naming conventions. This figure is adapted from (Bulbulia 2024)\n\nBulbulia, J. A. 2024. “Methods in Causal Inference Part 1: Causal Diagrams and Confounding.” Evolutionary Human Sciences 6: e40. https://doi.org/10.1017/ehs.2024.35.\n\n\n\nFor us:\nX denotes a variable without reference to its role;\nA denotes the “treatment” or “exposure” variable. This is the variable for which we seek to understand the effect of intervening on it. It is the “cause;”\nY denotes the outcome or response of an intervention. It is the “effect.” Last week we considered whether marriage A causes happiness Y.\nY(a) denotes the counterfactual or potential state of Y in response to setting the level of the exposure to a specific level, A=a. As we will consider in the second half of the course, to consistently estimate causal effects we will need to evaluate counterfactual or potential states of the world. Keeping to our example, we will need to do more than evaluate marriage and happiness in people over time. We will need to evaluate how happy the unmarried people would have been had they been married and how happy the married people would have been had they not been married. Of course, these events cannot be directly observed. Thus to address fundamental questions in psychology, we need to contrast counterfactual states of the world. This might seem like science fiction; however, we are already familiar with methods for obtaining such counterfactual contrasts – namely, randomised controlled experiments! We will return to this concept later, but for now, it will be useful for you to understand the notation.\nL denotes a measured confounder or set of confounders is defined as a variable which, if conditioned upon, closes an open back-door path between the treatment A and the outcome Y. Consider the scenario where happiness at time 0 (L) affects both the probability of getting married at time 1 (A) and one’s happiness at time 2 (Y). In this case, L serves as a confounder because it influences both the treatment (marriage at time 1) and the outcome (happiness at time 2), potentially opening a back-door path that confounds the estimated effect of marriage on happiness.\nTo accurately estimate the causal effect of marriage on happiness, then, it is essential to control for L. With cross-sectional data, such control might be difficult.\nU denotes an unmeasured confounder – that is a variable that may affect both the treatment and the outcome, but for which we have no direct measurement. Suppose cultural upbringing affects both whether someone gets married and whether they are happy. If this variable is not measured, we cannot accurately estimate a causal effect of marriage on happiness.\nM denotes a mediator or a variable along the path from exposure to outcome. For example, perhaps marriage causes wealth and wealth causes happiness. As we shall see, conditioning on “wealth” when estimating the effect of marriage on happiness will make it seem that marriage does not cause happiness when it does, through wealth.\n\\bar{X} denotes a sequence of variables, for example, a sequence of treatments. Imagine we were interested in the causal effect of marriage and remarriage on well-being. In this case, there are two treatments A_0 and A_1 and four potential contrasts. For the scenario of marriage and remarriage affecting well-being, we denote the potential outcomes as Y(a_0, a_1), where a_0 and a_1 represent the specific values taken by A_0 and A_1, respectively. Given two treatments, A_0 and A_1, four primary contrasts of interest correspond to the different combinations of these treatments. These contrasts allow us to compare the causal effects of being married versus not and remarried versus not on well-being. The potential outcomes under these conditions can be specified as follows:\n\nY(0, 0): The potential outcome when there is no marriage.\nY(0, 1): The potential outcome when there is marriage.\nY(1, 0): The potential outcome when there is divorce.\nY(1, 1): The potential outcome from marriage prevalence.\n\nEach of these outcomes allows for a specific contrast to be made, comparing the well-being under different scenarios of marriage and remarriage. Which do we want to contrast? Note, the question about ‘the causal effects of marriage on happiness’ is ambiguous because we have not stated the causal contrast we are interested in.\n\\mathcal{R} denotes a randomisation or a chance event.\n\n\nElements of our Causal Graphs\nThe conventions that describe components of our causal graphs are given in Figure 2.\n\n\n\n\n\n\nFigure 2: Nodes, Edges, Conditioning Conventions. This figure is adapted from (Bulbulia 2024)\n\nBulbulia, J. A. 2024. “Methods in Causal Inference Part 1: Causal Diagrams and Confounding.” Evolutionary Human Sciences 6: e40. https://doi.org/10.1017/ehs.2024.35.\n\n\n\n\nTime indexing\nIn our causal diagrams, we will implement two conventions to accurately depict the temporal order of events.\nFirst, the layout of a causal diagram will be structured from left to right to reflect the sequence of causality as it unfolds in reality. This orientation is crucial because causal diagrams must inherently be acyclic and because causality itself is inherently temporal.\nSecond, we will enhance the representation of the event sequence within our diagrams by systematically indexing our nodes according to the relative timing of events. If an event represented by X_0 precedes another event represented by X_1, the indexing will indicate this chronological order.\n\n\nRepresenting uncertainty in timing explicitly\nIn settings in which the sequence of events is ambiguous or cannot be definitively known, particularly in the context of cross-sectional data where all measurements are taken at a single point in time, we adopt a specific convention to express causality under uncertainty: X_{\\phi t}. This notation allows us to propose a temporal order without clear, time-specific measurements, acknowledging our speculation.\nFor instance, when the timing between events is unclear, we denote an event that is presumed to occur first as X_{\\phi 0} and a subsequent event as X_{\\phi 1}, indicating a tentative ordering where X_{\\phi 0} is thought to precede X_{\\phi 1}. However, it is essential to underscore that this notation signals our uncertainty regarding the actual timing of events; our measurements do not give us the confidence to assert this sequence definitively.\n\n\nArrows\nAs indicated in Figure 2, black arrows denote causality, red arrows reveal an open backdoor path, dashed black arrows denote attenuation, and red dashed arrows denote bias in a true causal association between A and Y. Finally, a blue arrow with a circle point denotes effect-measure modification, also known as “effect modification.” We might be interested in treatment effect heterogeneity without evaluating the causality in the sources of this heterogeneity. For example, we cannot typically imagine any intervention in which people could be randomised into cultures. However, we may be interested in whether the effects of an intervention that might be manipulable, such as marriage, differ by culture. To clarify this interest, we require a non-causal arrow.\n\\mathcal{R}\\to A denotes a random treatment assignment.\n\n\nBoxes\nWe use a black box to denote conditioning that reduces confounding or that is inert.\nWe use a red box to describe settings in which conditioning on a variable introduces confounding bias.\nOccasionally we will use a dashed circle do denote a latent variable, that is, a variable that is either not measured or not conditioned upon.\n\n\nTerminology for Conditional Independence\nThe bottom panel of Figure 2 shows some mathematical notation. Do not be alarmed, we are safe! Part 1 of the course will not require more complicated math than this notation. And we shall see that the notation is a compact way to describe intuitions that can be expressed less compactly in words:\n\nStatistical Independence (\\coprod): in the context of causal inference, statistical independence between the treatment and potential outcomes, denoted as A \\coprod Y(a), means the treatment assignment is independent of the potential outcomes. This assumption is critical for estimating causal effects without bias.\nStatistical Dependence (\\cancel\\coprod): conversely, \\cancel\\coprod denotes statistical dependence, indicating that the distribution of one variable is influenced by the other. For example, A \\cancel\\coprod Y(a) implies that the treatment assignment is related to the potential outcomes, potentially introducing bias into causal estimates.\nConditioning (|): conditioning, denoted by the vertical line |, allows for specifying contexts or conditions under which independence or dependence holds.\n\nConditional Independence (A \\coprod Y(a)|L): This means that once we account for a set of variables L, the treatment and potential outcomes are independent. This condition is often the basis for strategies aiming to control for confounding.\nConditional Dependence (A \\cancel\\coprod Y(a)|L): States that potential outcomes and treatments are not independent after conditioning on L, indicating a need for careful consideration in the analysis to avoid biased causal inferences."
  },
  {
    "objectID": "content/02-content.html#the-five-elementary-structures-of-causality",
    "href": "content/02-content.html#the-five-elementary-structures-of-causality",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "The Five Elementary Structures of Causality",
    "text": "The Five Elementary Structures of Causality\nJudea Pearl proved that all elementary structures of causality can be represented graphically (Pearl 2009). Figure 3 presents this five elementary structures.\n\nPearl, Judea. 2009. Causality. Cambridge University Press.\n\n\n\n\n\n\nFigure 3: Five elementary structures. This figure is adapted from (Bulbulia 2024).\n\nBulbulia, J. A. 2024. “Methods in Causal Inference Part 1: Causal Diagrams and Confounding.” Evolutionary Human Sciences 6: e40. https://doi.org/10.1017/ehs.2024.35.\n\n\n\nThe structures are as follows:\n\nTwo Variables:\n\nCausality Absent: There is no causal effect between variables A and B. They do not influence each other, denoted as A \\coprod B, indicating they are statistically independent.\nCausality: Variable A causally affects variable B. This relationship suggests an association between them, denoted as A \\cancel\\coprod B, indicating they are statistically dependent.\n\nThree Variables:\n\nFork: Variable A causally affects both B and C. Variables B and C are conditionally independent given A, denoted as B \\coprod C | A. This structure implies that knowing A removes any association between B and C due to their common cause.\nChain: A causal chain exists where C is affected by B, which in turn is affected by A. Variables A and C are conditionally independent given B, denoted as A \\coprod C | B. This indicates that B mediates the effect of A on C, and knowing B breaks the association between A and C.\nCollider: Variable C is affected by both A and B, which are independent. However, conditioning on C induces an association between A and B, denoted as A \\cancel\\coprod B | C. This structure is unique because it suggests that A and B, while initially independent, become associated when we account for their common effect C.\n\n\nOnce we understand the basic relationships between two variables, we can build upon these to create more complex relationships. These structures help us see how statistical independences and dependencies emerge from the data, allowing us to clarify the causal relationships we presume exist. Such clarity is crucial for ensuring that confounders are balanced across treatment groups, given all measured confounders, so that Y(a) \\coprod A | L.\nYou might wonder, “If not from the data, where do our assumptions about causality come from?” This question will come up repeatedly throughout the course. The short answer is that our assumptions are based on existing knowledge. This reliance on current knowledge might seem counterintuitive for buiding scientific knowledge-— shouldn’t we use data to build knowledge, not the other way around? Yes, but it is not that straightforward. Data often hold the answers we’re looking for but can be ambiguous. When the causal structure is unclear, it is important to sketch out different causal diagrams, explore their implications, and, if necessary, conduct separate analyses based on these diagrams.\nOtto Neurath, an Austrian philosopher and a member of the Vienna Circle, famously used the metaphor of a ship that must be rebuilt at sea to describe the process of scientific theory and knowledge development.\n\nDuhem has shown … that every statement about any happening is saturated with hypotheses of all sorts and that these in the end are derived from our whole world-view. We are like sailors who on the open sea must reconstruct their ship but are never able to start afresh from the bottom. Where a beam is taken away a new one must at once be put there, and for this the rest of the ship is used as support. In this way, by using the old beams and driftwood, the ship can be shaped entirely anew, but only by gradual reconstruction. (Neurath 1973, 199)\n\nNeurath, Otto. 1973. “Anti-Spengler.” In Empiricism and Sociology, edited by Marie Neurath and Robert S. Cohen, 158–213. Dordrecht: Springer Netherlands. https://doi.org/10.1007/978-94-010-2525-6_6.\n\nThis quotation emphasises the iterative process that accumulates scientific knowledge; new insights are cast from the foundation of existing knowledge. Causal diagrams are at home in Neurath’s boat. The tradition of science that believes that knowledge develops from the results of statistical tests applied to data should be resisted. The data alone typically do not contain the answers we seek."
  },
  {
    "objectID": "content/02-content.html#the-four-rules-of-confounding-control",
    "href": "content/02-content.html#the-four-rules-of-confounding-control",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "The Four Rules of Confounding Control",
    "text": "The Four Rules of Confounding Control\nFigure 4 describe the four elementary rules of confounding control:\n\n\n\n\n\n\nFigure 4: Four rules of confounding control\n\n\n\n\nCondition on Common Cause or its Proxy: this rule applies to settings in which the treatment (A) and the outcome (Y) share common causes. By conditioning on these common causes, we block the open backdoor paths that could introduce bias into our causal estimates. Controlling for these common causes (or their proxies) helps tp isolate the specific effect of A on Y. (We do not draw a path from $ A Y$ because we do not assume this path.)\nDo Not Condition on a Mediator: this rule applies to settings in which the variable L is a mediator of A \\to Y. Here, conditioning on a mediator will bias the total causal effect estimate. Later in the course, we will discuss the assumptions required for causal mediation. For now, if we are interested in total effect estimates, we must not condition on a mediator. Here we draw the path from A \\to Y to ensure that if such a path exists, it will not become biased from our conditioning strategy.\nDo Not Condition on a Collider: this rule applies to settings in which we L is a common effect of A and Y. Conditioning on a collider may invoke a spurious association. Last week we considered an example in which marriage caused wealth and happiness caused wealth. Conditioning on wealth in this setting will induce an association between happiness and marriage. Why? If we know the outcome, wealth, then we know there are at least two ways of wealth. Among those wealthy but low on happiness, we can predict that they are more likely to be married, for how else would they be wealthy? Similarly, among those who are wealthy and are not married, we can predict that they are happy, for how else would they be wealthy if not through marriage? These relationships are predictable entirely without a causal association between marriage and happiness!\nProxy Rule: Conditioning on a Descendent Is Akin to Conditioning on Its Parent: this rule applies to settings in which we L’ is an effect from another variable L. The graph considers when L’ is downstream of a collider. For example, suppose we condition on home ownership, which is an effect of wealth. Such conditioning will open up a non-causal path without causation because home ownership is a proxy for wealth. Consider, if someone owns a house but is not married, they are more likely to be happy, for how else could they accumulate the wealth required for home ownership? Likewise, if someone is unhappy and owns a house, we can infer that they are more likely to be married because how else would they be wealthy? Conditioning on a proxy for a collider here is akin to conditioning on the collider itself.\n\nHowever, we can also use the proxy rule to reduce bias. Return to the earlier example in which there is an unmeasured common cause of marriage and happiness, which we called “cultural upbringing” Suppose we have not measured this variable but have measured proxies for this variable, such as country of birth, childhood religion, number of languages one speaks, and others. By controlling for baseline values of these proxies, we can exert more control over unmeasured confounding. Even if bias is not eliminated, we should reduce bias wherever possible, which includes not introducing new biases, such as mediator bias, along the way. Later in the course, we will teach you how to perform sensitivity analyses to verify the robustness of your results to unmeasured confounding. Sensitivity analysis is critical because where the data are observational, we cannot entirely rule out unmeasured confounding."
  },
  {
    "objectID": "content/02-content.html#simulating-data-in-r-outcome-treatment",
    "href": "content/02-content.html#simulating-data-in-r-outcome-treatment",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Simulating Data in R: Outcome ~ Treatment",
    "text": "Simulating Data in R: Outcome ~ Treatment\n\nStep 1: Set Up Your R Environment\nEnsure R or RStudio is installed and open.\n\n\nStep 2: Set a Seed for Reproducibility\nTo ensure that your simulated data can be reproduced exactly. Again, it is good practice to set a seed before generating random data. This makes your analyses and simulations replicable.\n\nset.seed(123) # use any number to set the seed\n\n\n\nStep 3: Simulate Continuous Data: One Variable\nTo simulate continuous data, you can use functions like rnorm() for normal distributions, runif() for uniform distributions, etc. Here we simulate 100 normally distributed data points with a mean of 50 and a standard deviation of 10:\n\nn &lt;- 100 # number of observations\nmean &lt;- 50\nsd &lt;- 10\ndata_continuous &lt;- rnorm(n, mean, sd)\n\n# view\nhead(data_continuous)\n\n[1] 44.39524 47.69823 65.58708 50.70508 51.29288 67.15065\n\n# view using base R histogram\nhist(data_continuous)\n\n\n\n\n\n\n\n\n\n\nStep 4: Simulate Categorical Data\nCategorical data can be simulated using the sample() function. Here, we simulate a binary variable (gender) with two levels for 100 observations. There is equal probability of assignment.\n\nlevels &lt;- c(\"Male\", \"Female\")\ndata_categorical &lt;- sample(levels, n, replace = TRUE)\n\n# view\nhead(data_categorical)\n\n[1] \"Female\" \"Female\" \"Female\" \"Male\"   \"Female\" \"Female\"\n\n# check\ntable(data_categorical)\n\ndata_categorical\nFemale   Male \n    49     51 \n\n\nTo generate categories with unequal probabilities, you can use the sample() function by specifying the prob parameter, which defines the probability of selecting each level. This allows for simulating categorical data where the distribution between categories is not uniform.\nBelow is an example that modifies your initial code to create a categorical variable with unequal probabilities for “Male” and “Female”. Here is an example with unequal probabilities:\n\n# define levels and number of observations\nlevels &lt;- c(\"Male\", \"Female\")\nn &lt;- 100 # total number of observations\n\n# generate categorical data with unequal probabilities\ndata_categorical_unequal &lt;- sample(levels, n, replace = TRUE, prob = c(0.3, 0.7))\n\n# view the first few elements\n# head(data_categorical_unequal)\n\n# check\ntable(data_categorical_unequal)\n\ndata_categorical_unequal\nFemale   Male \n    69     31"
  },
  {
    "objectID": "content/02-content.html#simulating-outcomes-from-treatments",
    "href": "content/02-content.html#simulating-outcomes-from-treatments",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Simulating Outcomes from Treatments",
    "text": "Simulating Outcomes from Treatments\nIn this example, the prob parameter is set to c(0.3, 0.7), indicating a 30% probability for “Male” and a 70% probability for “Female”. This results in a simulated dataset where approximately 30% of the observations are “Male” and 70% are “Female”, reflecting the specified unequal probabilities. Adjust the probabilities as needed to fit the scenario you wish to simulate.\n\n#|fig-cap: \"Box plot of simulated scores by groups.\"\n# note: running this code will do no harm if the libraries are already installed.\n# install (if necessary) libraries\n# load libraries\n# libraries\n# graphs\nif (!require(ggplot2)) {\n  install.packages(\"ggplot2\")\n  library(ggplot2)\n} else {\n  library(ggplot2)\n}\n\n# data wrangling\nif (!require(tidyverse)) {\n  install.packages(\"tidyverse\")\n  library(tidyverse)\n} else {\n  library(tidyverse)\n}\n\n# tables\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(parameters)\n} else {\n  library(parameters)\n}\n\n# reporting\nif (!require(report)) {\n  install.packages(\"report\")\n  library(report)\n} else {\n  library(report)\n}\n\n\n# predictive graphs\nif (!require(ggeffects)) {\n  install.packages(\"ggeffects\")\n  library(ggeffects)\n} else {\n  library(ggeffects)\n}\n\n# assembling graphs\nif (!require(patchwork)) {\n  install.packages(\"patchwork\")\n  library(patchwork)\n} else {\n  library(patchwork)\n}\n\n# if libraries are already install, start here\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(parameters)\nlibrary(report)\n\n\nset.seed(123) # reproducibility\ngroupA_scores &lt;- rnorm(100, mean = 100, sd = 15) # simulate scores for group A\ngroupB_scores &lt;- rnorm(100, mean = 105, sd = 15) # simulate scores for group B\n\n# ombine into a data frame\ndf_scores &lt;- data.frame(group = rep(c(\"A\", \"B\"), each = 100), scores = c(groupA_scores, groupB_scores))\n\n# commands to view data\n #str(df_scores)\n\n# summary of columns\n#summary(df_scores)\n\n# top rows (uncomment)\n# head(df_scores)\n\n# bottom rows  (uncomment)\n# tail(df_scores)\n\n# check structure of data  (uncomment)\n# str(df_scores)\n\n\n# make group a factor (not strictly necessary here, but useful in outher applications)\ndf_scores_1 &lt;- df_scores |&gt; \n  mutate(group = as.factor(group))\n\nhead(df_scores_1)\n\n  group    scores\n1     A  91.59287\n2     A  96.54734\n3     A 123.38062\n4     A 101.05763\n5     A 101.93932\n6     A 125.72597"
  },
  {
    "objectID": "content/02-content.html#visualising-our-simulated-data",
    "href": "content/02-content.html#visualising-our-simulated-data",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Visualising Our Simulated Data",
    "text": "Visualising Our Simulated Data\nUnderstanding your data visually is as important as the statistical analysis itself. Let’s create a simple plot to compare the score distributions between the two groups.\n\n# plot your data\nggplot(df_scores_1, aes(x = group, y = scores, fill = group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Score Distribution by Group\", x = \"Group\", y = \"Scores\")\n\n\n\n\nScore Distribution by Group"
  },
  {
    "objectID": "content/02-content.html#sec-histogram",
    "href": "content/02-content.html#sec-histogram",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Histogram",
    "text": "Histogram\n\n#fig-cap: \"Histogram of simulated scores, by group, using facet_wrap()\"\nlibrary(ggplot2)\n\n# H=histograms for both groups\nggplot(df_scores_1, aes(x = scores, fill = group)) +\n  geom_histogram(binwidth = 5, color = \"black\") +\n  labs(title = \"Distribution of Scores by Group\",\n       x = \"Scores\",\n       y = \"Frequency\") +\n  facet_wrap(~group, ncol = 1) +   theme_minimal()"
  },
  {
    "objectID": "content/02-content.html#excercise-1",
    "href": "content/02-content.html#excercise-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Excercise 1",
    "text": "Excercise 1\n\nModify the simulation parameters to change each group’s mean and standard deviation. Observe how these changes affect the distribution.\nGo to the histogram. Experiment with different bin widths. In your own words, how do large and small numbers speak differently to the data? When might you use one histogram and not another."
  },
  {
    "objectID": "content/02-content.html#simulating-data-for-familiar-statistical-tests",
    "href": "content/02-content.html#simulating-data-for-familiar-statistical-tests",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Simulating data for familiar statistical tests",
    "text": "Simulating data for familiar statistical tests\n\n# tbl-caption: \"results of a one-sample t-test.\"\n# simulate some data\nset.seed(123)\ndata &lt;- rnorm(100, mean = 5, sd = 1) # 100 random normal values with mean = 5\n\n# perform one-sample t-test\n# testing if the mean of the data is reliably different from 4\nmod_first_test&lt;- t.test(data, mu = 4)\n\n# table\nparameters::parameters(mod_first_test)\n\nOne Sample t-test\n\nParameter | Mean |   mu | Difference |       95% CI | t(99) |      p\n--------------------------------------------------------------------\ndata      | 5.09 | 4.00 |       1.09 | [4.91, 5.27] | 11.95 | &lt; .001\n\nAlternative hypothesis: true mean is not equal to 4\n\n\nWe can automatically report these results using the report package:\n\n# report results automatically\nreport::report(mod_first_test)\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe One Sample t-test testing the difference between data (mean = 5.09) and mu\n= 4 suggests that the effect is positive, statistically significant, and large\n(difference = 1.09, 95% CI [4.91, 5.27], t(99) = 11.95, p &lt; .001; Cohen's d =\n1.19, 95% CI [0.94, 1.45])\n\n\n\n# simulate data for two groups\nset.seed(123)\n\ngroup1 &lt;- rnorm(50, mean = 5, sd = 1) # 50 random normal values, mean = 5\ngroup2 &lt;- rnorm(50, mean = 5.5, sd = 1) # 50 random normal values, mean = 5.5\n\n# two-sample t-test\nmod_t_test_result &lt;- t.test(group1, group2)\n\nreport::report(mod_t_test_result)\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference between group1 and group2\n(mean of x = 5.03, mean of y = 5.65) suggests that the effect is negative,\nstatistically significant, and medium (difference = -0.61, 95% CI [-0.98,\n-0.25], t(97.95) = -3.34, p = 0.001; Cohen's d = -0.67, 95% CI [-1.07, -0.26])\n\nparameters::parameters(mod_t_test_result)\n\nWelch Two Sample t-test\n\nParameter1 | Parameter2 | Mean_Parameter1 | Mean_Parameter2 | Difference\n------------------------------------------------------------------------\ngroup1     |     group2 |            5.03 |            5.65 |      -0.61\n\nParameter1 |         95% CI | t(97.95) |     p\n----------------------------------------------\ngroup1     | [-0.98, -0.25] |    -3.34 | 0.001\n\nAlternative hypothesis: true difference in means is not equal to 0\n\n\n\n# simulate pre-test and post-test scores\nset.seed(123)\n\npre_test &lt;- rnorm(30, mean = 80, sd = 10)\npost_test &lt;- rnorm(30, mean =  pre_test + 5, sd = 5) # assume an increase\n\n# perform paired t-test\nmod_pre_post &lt;- t.test(pre_test, post_test, paired = TRUE)\n\nreport::report(mod_pre_post)\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Paired t-test testing the difference between pre_test and post_test (mean\ndifference = -5.89) suggests that the effect is negative, statistically\nsignificant, and large (difference = -5.89, 95% CI [-7.45, -4.33], t(29) =\n-7.73, p &lt; .001; Cohen's d = -1.41, 95% CI [-1.91, -0.90])\n\nparameters::parameters(mod_pre_post)\n\nPaired t-test\n\nParameter |     Group | Difference | t(29) |      p |         95% CI\n--------------------------------------------------------------------\npre_test  | post_test |      -5.89 | -7.73 | &lt; .001 | [-7.45, -4.33]\n\nAlternative hypothesis: true mean difference is not equal to 0"
  },
  {
    "objectID": "content/02-content.html#understanding-regression-using-data-simulation",
    "href": "content/02-content.html#understanding-regression-using-data-simulation",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Understanding Regression Using Data Simulation",
    "text": "Understanding Regression Using Data Simulation\n\nSimulating Continuous Treatment Variable and Outcome Variable\n\n# library for enhanced model reporting\nset.seed(123)\n\nlibrary(parameters)\nlibrary(report)\n\n\n# set seed for reproducibility\nset.seed(123) # choose a seed number for consistency\n\n# define the number of observations\nn &lt;- 100 # total observations\n\n# simulate continuous treatment variable A\ntreatment &lt;- rnorm(n, mean = 50, sd = 10) # mean = 50, sd = 10 for A\n\n# specify the effect size of A on Y\nbeta_a &lt;- 2 # explicit effect size\n\n# simulate outcome variable Y including an error term\noutcome &lt;- 5 + beta_a * treatment + rnorm(n, mean = 0, sd = 20) # Y = intercept + beta_a*A + error\n\n# create a dataframe\ndf &lt;- data.frame(treatment = treatment,outcome = outcome)\n\n# view the structure and first few rows of the data frame\nstr(df)\n\n'data.frame':   100 obs. of  2 variables:\n $ treatment: num  44.4 47.7 65.6 50.7 51.3 ...\n $ outcome  : num  79.6 105.5 131.2 99.5 88.6 ...\n\nhead(df)\n\n  treatment   outcome\n1  44.39524  79.58236\n2  47.69823 105.53412\n3  65.58708 131.24033\n4  50.70508  99.45932\n5  51.29288  88.55338\n6  67.15065 138.40075\n\n\n\n\nRun Linear Model\nBefore moving on to regression analysis, ensure students understand the structure and distribution of the simulated data. Encourage them to use summary(), plot(), and other exploratory data analysis functions.\n\nRegression Analysis of Continuous Treatment Effect\n\nset.seed(123)\n\n# perform linear regression of Y on A\nfit &lt;- lm(outcome ~ treatment, data = df)\n\n# display the regression model summary\nsummary(fit)\n\n\nCall:\nlm(formula = outcome ~ treatment, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-38.15 -13.67  -1.75  11.61  65.81 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   8.1911    11.0530   0.741     0.46    \ntreatment     1.8951     0.2138   8.865  3.5e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.41 on 98 degrees of freedom\nMultiple R-squared:  0.4451,    Adjusted R-squared:  0.4394 \nF-statistic:  78.6 on 1 and 98 DF,  p-value: 3.497e-14\n\n# report the model in a reader-friendly format\nreport_fit &lt;- report::report(fit)\n\n\n# uncomment to print the model report\n# print(report_fit)\n\n# use ggeffects to view predicted values\nlibrary(ggeffects)\n\npredicted_values &lt;- ggeffects::ggemmeans(fit,\n                     terms = c(\"treatment\"))\n\n# plot see\nplot(predicted_values,  dot_alpha = 0.35,   show_data = TRUE, jitter = .1)\n\n\n\n\nGraph of regression line showing uncertainty."
  },
  {
    "objectID": "content/02-content.html#equivalence-of-anova-and-regression",
    "href": "content/02-content.html#equivalence-of-anova-and-regression",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Equivalence of ANOVA and Regression",
    "text": "Equivalence of ANOVA and Regression\nWe will simulate data in R to show that a one-way ANOVA is a particular case of linear regression with categorical predictors. We will give some reasons for preferring regression (in some settings)."
  },
  {
    "objectID": "content/02-content.html#method",
    "href": "content/02-content.html#method",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Method",
    "text": "Method\nFirst, we simulate a dataset with one categorical independent variable with three levels (groups) and a continuous outcome (also called a “dependant”) variable. This setup allows us to apply both ANOVA and linear regression for comparison.\n\n# nice tables\nlibrary(parameters)\n\nset.seed(123) # reproducibility\nn &lt;- 90 # total number of observations\nk &lt;- 3 # number of groups\n\n# simulate independent variable (grouping factor)\ngroup &lt;- factor(rep(1:k, each = n/k))\n\n# inspect\nstr(group)\n\n Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 1 1 ...\n\n# simulate outcome variable\nmeans &lt;- c(100, 100, 220) # Mean for each group\nsd &lt;- 15 # Standard deviation (same for all groups)\n\n# generate random data\ny &lt;- rnorm(n, mean = rep(means, each = n/k), sd = sd)\n\n\n# make data frame\ndf_1 &lt;- cbind.data.frame(y, group)\n\nanova_model &lt;- aov(y ~ group, data = df_1)\n# summary(anova_model)\ntable_anova &lt;- model_parameters(anova_model)\n\n# report the model\nreport::report(anova_model)\n\nThe ANOVA (formula: y ~ group) suggests that:\n\n  - The main effect of group is statistically significant and large (F(2, 87) =\n786.88, p &lt; .001; Eta2 = 0.95, 95% CI [0.93, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations.\n\n\nNext, we analyse the same data using linear regression. In R, regression models automatically convert categorical variables into dummy variables.\n\n# for tables (just installed)\nlibrary(parameters)\n\n# regression model \nfit_regression &lt;- lm(y ~ group, data = df_1)\n\n# uncomment if you want an ordinary summary\n# summary(regression_model)\n\ntable_fit &lt;- parameters::model_parameters(fit_regression)\n\n# print table\ntable_fit\n\nParameter   | Coefficient |   SE |           95% CI | t(87) |      p\n--------------------------------------------------------------------\n(Intercept) |       99.29 | 2.46 | [ 94.41, 104.18] | 40.40 | &lt; .001\ngroup [2]   |        3.38 | 3.48 | [ -3.53,  10.29] |  0.97 | 0.333 \ngroup [3]   |      121.07 | 3.48 | [114.16, 127.98] | 34.83 | &lt; .001\n\n\n\nlibrary(parameters)\nlibrary(report)\n\n# report the model\nreport_fit &lt;- report_parameters(fit_regression)\n\n#print\nreport_fit\n\n  - The intercept is statistically significant and positive (beta = 99.29, 95% CI [94.41, 104.18], t(87) = 40.40, p &lt; .001; Std. beta = -0.71, 95% CI [-0.80, -0.63])\n  - The effect of group [2] is statistically non-significant and positive (beta = 3.38, 95% CI [-3.53, 10.29], t(87) = 0.97, p = 0.333; Std. beta = 0.06, 95% CI [-0.06, 0.18])\n  - The effect of group [3] is statistically significant and positive (beta = 121.07, 95% CI [114.16, 127.98], t(87) = 34.83, p &lt; .001; Std. beta = 2.08, 95% CI [1.96, 2.20])"
  },
  {
    "objectID": "content/02-content.html#combination-plots",
    "href": "content/02-content.html#combination-plots",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Combination Plots",
    "text": "Combination Plots\nWe can create and combine individual plots, as showin in Figure 5, as follows:\n\n# regression --------------------------------------------------------------\n# graph the output of the parameters table and assign to object\ncoefficient_plot &lt;- plot(table_fit)\n\n\n# ggeffects plot - create predictive plot.\n# this gives the expected values by group \npredictive_plot &lt;- plot(\n  ggeffects::ggpredict(fit_regression, terms = \"group\"),\n  dot_alpha = 0.35,\n  show_data = TRUE,\n  jitter = .1,\n  colors =  \"reefs\"\n) +\n  scale_y_continuous(limits = c(0, 260)) + # change y axis\n  labs(title = \"Predictive Graph\", x = \"Treatment Group\", y = \"Response\")\n\n# view (uncomment to see it alone)\n#predictive_plot\n\n# show all color palettes (uncomment to see colour options)\n# show_pals()\n\n\n# multiple plots using `patchwork`\nlibrary(patchwork)\n\n# create a plot\nmy_first_combination_plot &lt;- coefficient_plot / predictive_plot  +\n  plot_annotation(title = \"Coefficient and Predictive plots with two panels \",\n                  tag_levels = \"A\")\n\n# save a plot to your directory, make sure to create one called \"figs\"\n\n## check directory (uncomments)\n# here::here()\n\n# save (change values if necessary )\nggsave(\n  my_first_combination_plot,\n  path = here::here(\"figs\"),\n  width = 12,\n  height = 8,\n  units = \"in\",\n  filename = \"my_first_combination_plot.jpeg\",\n  device = 'jpeg',\n  limitsize = FALSE,\n  dpi = 600\n)\n\n# view\nmy_first_combination_plot\n\n\n\n\n\n\n\nFigure 5: This is a combined plot"
  },
  {
    "objectID": "content/02-content.html#upshot",
    "href": "content/02-content.html#upshot",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Upshot",
    "text": "Upshot\nANOVA partitions variance into between-group and within-group components. Regression analysis estimates the mean of the dependent variable as a linear function of the independent (including categorical) variables. For many questions, ANOVA is appropriate, however, when comparing groups, we often want a finer-grained interpretation. Regression is built for obtaining this finer grain comparisons. Note: comparisons do not establish causality. We will return to regression over the next few weeks and use regression to hone your skills in R. Later, along the way, you’ll learn more about data visualisation, modelling, and reporting. These skills are essential for your final report. They are also skills that you will serve you beyond this course."
  },
  {
    "objectID": "content/02-content.html#exercise-2",
    "href": "content/02-content.html#exercise-2",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Exercise 2",
    "text": "Exercise 2\nPerform a linear regression analysis using R. Follow the detailed instructions below to simulate the necessary data, execute the regression, and report your findings:\n\nSimulate Data:\n\nGenerate two continuous variables, Y and A, with n = 100 observations each.\nThe variable A should have a mean of 50 and a standard deviation (sd) of 10.\n\nDefine the Relationship:\n\nSimulate the variable Y such that it is linearly related to A with a specified effect size. The effect size of A on Y must be explicitly defined as 2.\n\nIncorporate an Error Term:\n\nWhen simulating Y, include an error term with a standard deviation (sd) of 20 to introduce variability.\n\nRegression Analysis:\n\nUse the lm() function in R to regress Y on A.\nEnsure the regression model captures the specified effect of A on Y.\n\nReport the Results:\n\nOutput the regression model summary to examine the coefficients, including the effect of A on Y, and assess the model’s overall fit and significance.\n\n\nHere is a template to get you started. Copy the code and paste it into your R script.\n\nlibrary(parameters)\n#  seed for reproducibility\nset.seed( ) # numbers go in brackets\n\n# number of observations\nn &lt;-   # number goes here\n\n# simulate data for variable A with specified mean and sd\nA &lt;- rnorm(n, \n           mean = , # set your number here \n           sd = )# set your number here \n\n# define the specified effect size of A on Y\nbeta_A &lt;-   # define your effect with a number here \n\n\n# simulate data and make data frame in one step\n\ndf_3 &lt;- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A = A, # from above\n  Y = 5 + beta_A * A + rnorm(n, mean = 0, sd = 20) #  effect is intercept + ...\n)\n\n# view\nhead(df_3)\nstr(df_3)\n\n#  linear regression of Y on A\nfit_3 &lt;- lm(Y ~ A, data = df_3)\n\n#  results (standard code)\n# summary(model)\n\n# time saving reports\nparameters::model_parameters(fit_3)\nreport(fit_3)"
  },
  {
    "objectID": "content/02-content.html#what-you-have-learned",
    "href": "content/02-content.html#what-you-have-learned",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "What You Have Learned",
    "text": "What You Have Learned\n\nData simulation:\n\nYou’ve learned to simulate datasets in R. This is a important skill for exploring statistical concepts and causal inference. Congratulations!\n\nData visualisation:\n\nYou’ve expanded your capacity for data visualisation.\n\nStatistical tests: You’ve conducted basic statistical tests, including t-tests and ANOVA, and regression.\nUnderstanding ANOVA and regression:\n\nYou’ve examined the equivalence of ANOVA and regression analysis"
  },
  {
    "objectID": "content/02-content.html#for-more-information-about-the-packages-used-here",
    "href": "content/02-content.html#for-more-information-about-the-packages-used-here",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "For more information about the packages used here:",
    "text": "For more information about the packages used here:\n\nggplot2: A system for declaratively creating graphics, based on The Grammar of Graphics.\nParameters package: Provides utilities for processing model parameters and their metrics.\nReport package: Facilitates the automated generation of reports from statistical models.\nMontgomery, Nyhan, and Torres (2018) describes confounding in experiments - well worth a read.\n\n\nMontgomery, Jacob M., Brendan Nyhan, and Michelle Torres. 2018. “How Conditioning on Posttreatment Variables Can Ruin Your Experiment and What to Do about It.” American Journal of Political Science 62 (3): 760–75. https://doi.org/10.1111/ajps.12357."
  },
  {
    "objectID": "content/02-content.html#appendix-a",
    "href": "content/02-content.html#appendix-a",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix A: Solutions",
    "text": "Appendix A: Solutions\n\nSolution Excercise 2: simulate data and regression reporting\n\nlibrary(parameters)\n#  seed for reproducibility\nset.seed(12345)\n\n# number of observations\nn &lt;- 100\n\n# simulate data for variable A with specified mean and sd\nA &lt;- rnorm(n, mean = 50, sd = 10)\n\n# define the specified effect size of A on Y\nbeta_A &lt;- 2\n\n\n# simulate data and make data frame in one step\n\ndf_3 &lt;- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A =  rnorm(n, mean = 50, sd = 10),\n  Y = 5 +  # intercept (optional)\n    beta_A * A + rnorm(n, mean = 0, sd = 20)\n)\n\n# view\nhead(df_3)\n\n         A         Y\n1 52.23925  87.98766\n2 38.43777 106.60413\n3 54.22419 107.68437\n4 36.75245 117.09730\n5 51.41084 133.74473\n6 44.63952  70.74512\n\nstr(df_3)\n\n'data.frame':   100 obs. of  2 variables:\n $ A: num  52.2 38.4 54.2 36.8 51.4 ...\n $ Y: num  88 107 108 117 134 ...\n\n# linear regression of Y on A\n\nfit_3 &lt;- lm(Y ~ A, data = df_3)\n\n# results \n# summary(model)\n\n# nicer report\nparameters::model_parameters(fit_3)\n\nParameter   | Coefficient |    SE |          95% CI | t(98) |      p\n--------------------------------------------------------------------\n(Intercept) |      109.17 | 14.62 | [80.17, 138.18] |  7.47 | &lt; .001\nA           |   -3.80e-03 |  0.28 | [-0.57,   0.56] | -0.01 | 0.989 \n\nreport(fit_3)\n\nWe fitted a linear model (estimated using OLS) to predict Y with A (formula: Y\n~ A). The model explains a statistically not significant and very weak\nproportion of variance (R2 = 1.83e-06, F(1, 98) = 1.79e-04, p = 0.989, adj. R2\n= -0.01). The model's intercept, corresponding to A = 0, is at 109.17 (95% CI\n[80.17, 138.18], t(98) = 7.47, p &lt; .001). Within this model:\n\n  - The effect of A is statistically non-significant and negative (beta =\n-3.80e-03, 95% CI [-0.57, 0.56], t(98) = -0.01, p = 0.989; Std. beta =\n-1.35e-03, 95% CI [-0.20, 0.20])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "content/02-content.html#appendix-b-how-anova-can-deliver-the-functionality-of-linear-regressions",
    "href": "content/02-content.html#appendix-b-how-anova-can-deliver-the-functionality-of-linear-regressions",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix B: How ANOVA can deliver the functionality of Linear Regressions",
    "text": "Appendix B: How ANOVA can deliver the functionality of Linear Regressions\nWe can get group comparisons with ANOVA, for example:\n\n# Conduct Tukey's HSD test for post hoc comparisons\ntukey_post_hoc &lt;- TukeyHSD(anova_model)\n\n# Display the results\nprint(tukey_post_hoc)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ group, data = df_1)\n\n$group\n          diff        lwr       upr     p adj\n2-1   3.381631  -4.906624  11.66989 0.5958959\n3-1 121.072862 112.784607 129.36112 0.0000000\n3-2 117.691231 109.402975 125.97949 0.0000000\n\nplot(tukey_post_hoc)\n\n\n\n\n\n\n\n\nRegression and ANOVA are equivalent"
  },
  {
    "objectID": "content/02-content.html#appendix-c-adding-complexity-in-simulation",
    "href": "content/02-content.html#appendix-c-adding-complexity-in-simulation",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix C: Adding Complexity in Simulation",
    "text": "Appendix C: Adding Complexity in Simulation\n\nSimulating Data… Continued\nData frames are used in R to store data tables. To simulate a dataset with both continuous and categorical data, you can combine the above steps:\n\n# create a data frame with simulated data for ID, Gender, Age, and Income\ndata_frame &lt;- data.frame(\n  # generate a sequence of IDs from 1 to n\n  ID = 1:n,\n  \n  # randomly assign 'Male' or 'Female' to each observation\n  Gender = sample(c(\"Male\", \"Female\"), n, replace = TRUE),\n  \n  # simulate 'Age' data: normally distributed with mean 30 and sd 5\n  Age = rnorm(n, mean = 30, sd = 5),\n  \n  # simulate 'Income' data: normally distributed with mean 50000 and sd 10000\n  Income = rnorm(n, mean = 50000, sd = 10000)\n)\n\nNote that you can sample probabilistically for your groups\n\nn &lt;- 100 # total number of observations\n\n# sample 'Gender' with a 40/60 proportion for Male/Female\nGender = sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(0.4, 0.6))\n\n\n\nMore complexity\n\n# set the number of observations\nn &lt;- 100\n\n# simulate the 'Age' variable\nmean_age &lt;- 30\nsd_age &lt;- 5\nAge &lt;- rnorm(n, mean = mean_age, sd = sd_age)\n\n# define coefficients explicitly\nintercept &lt;- 20000   # Intercept for the income equation\nbeta_age &lt;- 1500     # Coefficient for the effect of age on income\nerror_sd &lt;- 10000    # Standard deviation of the error term\n\n# simulate 'Income' based on 'Age' and defined coefficients\nIncome &lt;- intercept + beta_age * Age + rnorm(n, mean = 0, sd = error_sd)\n\n# create a data frame to hold the simulated data\ndata_complex &lt;- data.frame(Age, Income)\n\n\n\nVisualising Simulated Data\nVisualising your simulated data can help understand its distribution and relationships. Use the ggplot2 package for this:\n\nlibrary(ggplot2)\nggplot(data_complex, aes(x = Age, y = Income)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Simulated Age vs. Income\", x = \"Age\", y = \"Income\")"
  },
  {
    "objectID": "content/02-content.html#appendix-d-causal-inference-glossary",
    "href": "content/02-content.html#appendix-d-causal-inference-glossary",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix D: Causal Inference Glossary",
    "text": "Appendix D: Causal Inference Glossary\nNote, as of yet, we have only encountered several of the terms in this glossary.\nGlossary of key terms in causal inference\n\nPackages\n\nreport::cite_packages()\n\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Lüdecke D (2018). \"ggeffects: Tidy Data Frames of Marginal Effects from Regression Models.\" _Journal of Open Source Software_, *3*(26), 772. doi:10.21105/joss.00772 &lt;https://doi.org/10.21105/joss.00772&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 &lt;https://doi.org/10.21105/joss.02445&gt;.\n  - Makowski D, Lüdecke D, Patil I, Thériault R, Ben-Shachar M, Wiernik B (2023). \"Automated Results Reporting as a Practical Tool to Improve Reproducibility and Methodological Best Practices Adoption.\" _CRAN_. &lt;https://easystats.github.io/report/&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. R package version 1.3.0, &lt;https://CRAN.R-project.org/package=patchwork&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Xie Y (2024). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.54, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "",
    "text": "Note\n\n\n\nRequired\n\n(Bulbulia 2024a) link\nsee simplified reading\n\nOptional\n\n(Hernan and Robins 2020) Chapter 6-9 link\n(Miguel A. Hernán, Hernández-Díaz, and Robins 2004) link\n(M. A. Hernán 2017) link\n(Miguel A. Hernán and Cole 2009) link\n(VanderWeele and Hernán 2012) link"
  },
  {
    "objectID": "content/04-content.html#learning-outcomes",
    "href": "content/04-content.html#learning-outcomes",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou will learn the elementary structures of measurement error bias and be able to use them to explain how measurement can go wrong.\nYou will begin to understand the relationship between structural sources of bias and meaasurement in cross-cultural studies.\nYou will begin to understand how the concepts of target population and sample population clarify external validity/transportability of results in cross-cultural research."
  },
  {
    "objectID": "content/04-content.html#common-causal-questions-presented-as-causal-graphs",
    "href": "content/04-content.html#common-causal-questions-presented-as-causal-graphs",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Common Causal Questions Presented as Causal Graphs",
    "text": "Common Causal Questions Presented as Causal Graphs\n\n\n\n\n\n\nFigure 1: This figure is adapted from (Bulbulia 2024b)\n\n———. 2024b. “Methods in Causal Inference Part 3: Measurement Error and External Validity Threats.” Evolutionary Human Sciences 6: e42. https://doi.org/10.1017/ehs.2024.33."
  },
  {
    "objectID": "content/04-content.html#a-typology-of-measurement-error-bias",
    "href": "content/04-content.html#a-typology-of-measurement-error-bias",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "A Typology of Measurement Error Bias",
    "text": "A Typology of Measurement Error Bias\n\n\n\n\n\n\nFigure 2: This figure is adapted from (Bulbulia 2024b)\n\n———. 2024b. “Methods in Causal Inference Part 3: Measurement Error and External Validity Threats.” Evolutionary Human Sciences 6: e42. https://doi.org/10.1017/ehs.2024.33."
  },
  {
    "objectID": "content/04-content.html#threats-to-external-validity",
    "href": "content/04-content.html#threats-to-external-validity",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Threats to External Validity",
    "text": "Threats to External Validity\n\n\n\n\n\n\nFigure 3: This figure is adapted from [Bulbulia (2024b)][Download PDF](externalvalidity.pdf){.btn .btn-primary .btn-sm}\n\n———. 2024b. “Methods in Causal Inference Part 3: Measurement Error and External Validity Threats.” Evolutionary Human Sciences 6: e42. https://doi.org/10.1017/ehs.2024.33."
  },
  {
    "objectID": "content/04-content.html#load-libraries",
    "href": "content/04-content.html#load-libraries",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Load libraries",
    "text": "Load libraries\n\n# data-wrangling\nif (!require(tidyverse, quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n  library(tidyverse)\n}\n\n# graphing \nif (!require(ggplot2, quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n  library(ggplot2)\n}\n\n\n# automated causal diagrams\nif (!require(ggdag, quietly = TRUE)) {\n  install.packages(\"ggdag\")\n  library(ggdag)\n}\n\nWe can use the ggdag package to evaluate confounding .\n\nOmitted Variable Bias Causal Graph\nLet’s use ggdag to identify confounding arising from omitting Z in our regression of X on Y.\nFirst we write out the DAG as follows:\n\n# code for creating a DAG\ngraph_fork &lt;- dagify(Y ~ L,\n                   A ~ L,\n                   exposure = \"A\",\n                   outcome = \"Y\") |&gt;\n  tidy_dagitty(layout = \"tree\")\n\n# plot the DAG\ngraph_fork |&gt;\n  ggdag() + theme_dag_blank() + labs(title = \"L is a common cause of A and Y\")\n\n\n\n\n\n\n\n\nNext we ask ggdag which variables we need to include if we are to obtain an unbiased estimate of the outcome from the exposure:\n\n# use this code\n\nggdag::ggdag_adjustment_set( graph_fork ) +  theme_dag_blank() + labs(title = \"{L} is the exclusive member of the confounder set for A and Y. Conditioning on L 'd-separates' A and Y \")\n\n\n\n\n\n\n\n\nThe causal graph tells us to obtain an unbiased estimate of A on Y we must condition on L.\nAnd indeed, when we included the omitted variable L in our simulated dateset it breaks the association between X and Y:\n\n# set seed\nset.seed(123)\n\n# number of observations\nN = 1000\n\n# confounder\nL = rnorm(N)\n\n# A is caused by \nA = rnorm(N, L)\n\n# Y draws randomly from L but is not caused by A\nY = rnorm(N, L)\n\n# note we did not need to make a data frame\n\n# regress Y on A without control\nfit_fork  &lt;- lm(Y ~ A)\n\n# A is \"significant\nparameters::model_parameters(fit_fork)\n\nParameter   | Coefficient |   SE |        95% CI | t(998) |      p\n------------------------------------------------------------------\n(Intercept) |       -0.03 | 0.04 | [-0.11, 0.04] |  -0.89 | 0.373 \nA           |        0.50 | 0.03 | [ 0.45, 0.54] |  19.72 | &lt; .001\n\n# regress Y on A with control\nfit_fork_controlled  &lt;- lm(Y ~ A + L)\n\n# A and Y are no longer associated, conditioning worked.\nparameters::model_parameters(fit_fork_controlled)\n\nParameter   | Coefficient |   SE |        95% CI | t(997) |      p\n------------------------------------------------------------------\n(Intercept) |       -0.02 | 0.03 | [-0.08, 0.04] |  -0.68 | 0.499 \nA           |        0.03 | 0.03 | [-0.03, 0.09] |   0.89 | 0.372 \nL           |        0.95 | 0.05 | [ 0.86, 1.04] |  20.77 | &lt; .001"
  },
  {
    "objectID": "content/04-content.html#mediation-and-causation",
    "href": "content/04-content.html#mediation-and-causation",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Mediation and causation",
    "text": "Mediation and causation\nSuppose we were interested in the causal effect of X on Y. We have a direct effect of X on Y as well as an indirect effect of X on Y through M. We use ggdag to draw the DAG:\n\ngraph_mediation &lt;- dagify(Y ~  M,\n                 M ~ A,\n                exposure = \"A\",\n                outcome = \"Y\") |&gt;\n  ggdag::tidy_dagitty(layout = \"tree\")\n\ngraph_mediation |&gt;\n  ggdag() +   \n  theme_dag_blank() + \n  labs(title = \"Mediation Graph\")\n\n\n\n\n\n\n\n\nHere is another way\n\ngraph_mediation_full &lt;- ggdag_mediation_triangle(x = \"A\", \n                         y = \"Y\", \n                         m = \"M\", \n                         x_y_associated = FALSE) \n\n\ngraph_mediation_full +  theme_dag_blank() + \n  labs(title = \"Fully Mediated Graph\")\n\n\n\n\n\n\n\n\nWhat should we condition on if we are interested in the causal effect of changes in X on changes in Y?\nWe can pose the question to ggdag:\n\n# ask ggdag which variables to condition on:\n\nggdag::ggdag_adjustment_set(graph_fork)\n\n\n\n\n\n\n\n\n‘Backdoor Paths Unconditionally Closed’ means that, assuming the DAG we have drawn is correct, we may obtain an unbiased estimate of X on Y without including additional variables.\nLater we shall understand why this is the case.1\n1 We shall see there is no “backdoor path” from X to Y that would bias our estimate, hence the estimate X-&gt;Y is an unbiased causal estimate – again, conditional on our DAG.For now, we can enrich our language for causal inference by considering the concept of d-connected and d-separated:\nTwo variables are d-connected if information flows between them (condional on the graph), and they are d-separated if they are conditionally independent of each other.\n\n# use this code to examine d-connectedness\nggdag::ggdag_dconnected(graph_mediation)\n\n\n\n\n\n\n\n\nIn this case, d-connection is a good thing because we can estimate the causal effect of A on Y.\nIn other cases, d-connection will spoil the model. We have seen this for omitted variable bias. A and Y are d-separated conditional on L, and that’s our motivation for including L. These concepts are tricky, but they get easier with practice.\nTo add some grit to our exploration of mediation lets simulate data that are consistent with our mediation DAG\n\nset.seed(123)\nN &lt;- 100\nx &lt;- rnorm(N)# sim x\nm &lt;- rnorm(N , x) # sim X -&gt; M\ny &lt;- rnorm(N , x + m) # sim M -&gt; Y\ndf &lt;- data.frame(x, m, y)\n\ndf &lt;- df |&gt;\n  dplyr::mutate(x_s = scale(x),\n                m_s = scale(m))\n\nFirst we ask, is X is related to Y?\n\nfit_mediation &lt;- lm(y ~ x_s, data = df)\nparameters::model_parameters(fit_mediation)\n\nParameter   | Coefficient |   SE |        95% CI | t(98) |      p\n-----------------------------------------------------------------\n(Intercept) |        0.19 | 0.14 | [-0.08, 0.47] |  1.41 | 0.161 \nx s         |        1.66 | 0.14 | [ 1.38, 1.93] | 12.00 | &lt; .001\n\n\nYes.\nNext we ask, is A related to Y conditional on M?\n\nfit_total_mediated_effect &lt;- lm(y ~ x_s + m_s, data = df)\nparameters::model_parameters(fit_total_mediated_effect) |&gt; parameters::print_html()\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(97)\np\n\n\n\n\n(Intercept)\n0.19\n0.10\n(4.92e-03, 0.38)\n2.04\n0.044\n\n\nx s\n0.77\n0.13\n(0.51, 1.02)\n6.00\n&lt; .001\n\n\nm s\n1.33\n0.13\n(1.07, 1.58)\n10.34\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\nYes, but notice this is a different question. The effect of X is attenuated because M contributes to the causal effect of Y.\n\nfit_total_effect &lt;- lm(y ~ x_s, data = df)\nparameters::model_parameters(fit_total_effect) |&gt; parameters::print_html()\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(98)\np\n\n\n\n\n(Intercept)\n0.19\n0.14\n(-0.08, 0.47)\n1.41\n0.161\n\n\nx s\n1.66\n0.14\n(1.38, 1.93)\n12.00\n&lt; .001"
  },
  {
    "objectID": "content/04-content.html#pipe-confounding-full-mediation",
    "href": "content/04-content.html#pipe-confounding-full-mediation",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Pipe confounding (full mediation)",
    "text": "Pipe confounding (full mediation)\nSuppose we are interested in the effect of x on y, in a scenario when m fully mediates the relationship of x on y.\n\nmediation_triangle(\n  x = NULL,\n  y = NULL,\n  m = NULL,\n  x_y_associated = FALSE\n) |&gt;\n  ggdag()\n\n\n\n\n\n\n\n\nWhat variables do we need to include to obtain an unbiased estimate of X on Y?\nLet’s fill out this example out by imagining an experiment.\nSuppose we want to know whether a ritual action condition (X) influences charity (Y). We have good reason to assume the effect of X on Y happens entirely through perceived social cohesion (M):\nX\\toM\\toZ or ritual \\to social cohesion \\to charity\nLets simulate some data\n\nset.seed(123)\n# Participants\nN &lt;-100\n\n# initial charitable giving\nc0 &lt;- rnorm(N ,10 ,2)\n\n# assign treatments and simulate charitable giving and increase in social cohesion\nritual &lt;- rep( 0:1 , each = N/2 )\ncohesion &lt;- ritual * rnorm(N,.5,.2)\n\n# increase in charity\nc1 &lt;- c0 + ritual * cohesion \n\n# dataframe\nd &lt;- data.frame( c0 = c0 , \n                 c1=c1 , \n                 ritual = ritual , \n                 cohesion = cohesion )\nskimr::skim(d)\n\n\n\n\n\nName\nd\n\n\nNumber of rows\n100\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nc0\n0\n1\n10.18\n1.83\n5.38\n9.01\n10.12\n11.38\n14.37\n▁▃▇▅▂\n\n\nc1\n0\n1\n10.43\n1.85\n5.89\n9.17\n10.30\n11.57\n14.99\n▂▆▇▆▂\n\n\nritual\n0\n1\n0.50\n0.50\n0.00\n0.00\n0.50\n1.00\n1.00\n▇▁▁▁▇\n\n\ncohesion\n0\n1\n0.25\n0.29\n0.00\n0.00\n0.12\n0.48\n1.15\n▇▃▃▁▁\n\n\n\n\n\nDoes the ritual increase charity?\nIf we only include the ritual condition in the model, we find that ritual condition reliable predicts increases in charitable giving:\n\nparameters::model_parameters(\n  lm(c1 ~  c0 + ritual, data = d)\n  )\n\nParameter   | Coefficient |       SE |        95% CI |  t(97) |      p\n----------------------------------------------------------------------\n(Intercept) |        0.08 |     0.08 | [-0.07, 0.23] |   1.05 | 0.297 \nc0          |        0.99 | 7.26e-03 | [ 0.98, 1.01] | 136.74 | &lt; .001\nritual      |        0.51 |     0.03 | [ 0.46, 0.56] |  19.33 | &lt; .001\n\n\nDoes the ritual increase charity adjusting for levels of social cohesion?\n\nparameters::model_parameters(\n  lm(c1 ~  c0 + ritual + cohesion, data = d)\n  )\n\nParameter   | Coefficient |       SE |         95% CI |    t(96) |      p\n-------------------------------------------------------------------------\n(Intercept) |   -1.05e-14 | 1.02e-15 | [ 0.00,  0.00] |   -10.31 | &lt; .001\nc0          |        1.00 | 9.78e-17 | [ 1.00,  1.00] | 1.02e+16 | &lt; .001\nritual      |    7.36e-16 | 7.78e-16 | [ 0.00,  0.00] |     0.95 | 0.346 \ncohesion    |        1.00 | 1.36e-15 | [ 1.00,  1.00] | 7.35e+14 | &lt; .001\n\n\nThe answer is that the (direct) effect of ritual entirely drops out when we include both ritual and social cohesion. Why is this? The answer is that once our model knows m it does not obtain any new information by knowing x.\nIf we were interested in assessing x\\toy but x were to effect y through m (i.e x\\tom\\toy) then conditioning on m would block the path from x\\toy. Including m leads to Pipe Confounding.\nIn experiments we should never condition on a post-treatment variable."
  },
  {
    "objectID": "content/04-content.html#masked-relationships",
    "href": "content/04-content.html#masked-relationships",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Masked relationships",
    "text": "Masked relationships\nImagine two variables were to affect an outcome. Both are correlated with each other. One affects the outcome positively and the other affects the outcome negatively. How shall we investigate the causal role of the focal predictor?\nConsider two correlated variables that jointly predict Political conservatism (C), religion (R). Imagine that one variable has a positive effect and the other has a negative effect on distress (K6).\nFirst consider this relationship, where conservatism causes religion\n\nlibrary(ggdag)\ndag_m1 &lt;- dagify(K ~ C + R,\n                 R ~ C,\n                 exposure = \"C\",\n                 outcome = \"K\") |&gt;\n  tidy_dagitty(layout = \"tree\")\n\n# graph\ndag_m1|&gt;\n  ggdag()\n\n\n\n\n\n\n\n\nWe can simulate the data:\n\n# C -&gt; K &lt;- R\n# C -&gt; R\nset.seed(123)\nn &lt;- 100\nC &lt;- rnorm( n )\nR &lt;- rnorm( n , C )\nK &lt;- rnorm( n , R - C )\n\nd_sim &lt;- data.frame(K=K,R=R,C=C)\n\nFirst we only condition on conservatism\n\nms1 &lt;- parameters::model_parameters(\n  lm(K  ~ C, data = d_sim)\n)\nplot(ms1)\n\n\n\n\n\n\n\nms1\n\nParameter   | Coefficient |   SE |        95% CI | t(98) |     p\n----------------------------------------------------------------\n(Intercept) |        0.03 | 0.14 | [-0.24, 0.30] |  0.22 | 0.829\nC           |       -0.19 | 0.15 | [-0.49, 0.11] | -1.24 | 0.219\n\n\nNext, only religion:\n\nms2&lt;- parameters::model_parameters(\n  lm(K  ~ R, data = d_sim)\n)\nplot(ms2)\n\n\n\n\n\n\n\n\nWhen we add both C and R, we see them “pop” in opposite directions, as is typical of masking:\n\nms3&lt;- parameters::model_parameters(\n  lm(K  ~ C + R, data = d_sim)\n)\nplot(ms3)\n\n\n\n\n\n\n\n\nNote that when you ask ggdag to assess how to obtain an unbiased estimate of C on K it will tell you you don’t need to condition on R.\n\ndag_m1|&gt;\n  ggdag_adjustment_set()\n\n\n\n\n\n\n\n\nYet recall when we just assessed the relationship of C on K we got this:\n\nplot(ms1)\n\n\n\n\n\n\n\n\nIs the DAG wrong?\nNo. The fact that C\\toR is positive and R\\toK is negative means that if we were to increase C, we wouldn’t reliably increase K. The total effect of C just isn’t reliable"
  },
  {
    "objectID": "content/04-content.html#collider-confounding",
    "href": "content/04-content.html#collider-confounding",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Collider Confounding",
    "text": "Collider Confounding\nThe selection-distortion effect (Berkson’s paradox)\nThis example is from the book Statistical Rethinking. Imagine in science there is no relationship between the newsworthiness of science and its trustworthiness. Imagine further that selection committees make decisions on the basis of the both newsworthiness and the trustworthiness of scientific proposals.\nThis presents us with the following graph\n\ndag_sd &lt;- dagify(S ~ N,\n                 S ~ T,\n                 labels = c(\"S\" = \"Selection\",\n                            \"N\" = \"Newsworthy\",\n                            \"T\" = \"Trustworthy\")) |&gt;\n  tidy_dagitty(layout = \"nicely\")\n\n# Graph\ndag_sd |&gt;\n  ggdag(text = FALSE, use_labels = \"label\") + theme_dag_blank()\n\n\n\n\n\n\n\n\nWhen two arrows enter into an variable, it opens a path of information between the two variables.\nVery often this openning of information has disasterous implications. In the human sciences, included variable bias is a woefully underrated problem.\n\nggdag_dseparated(\n  dag_sd,\n  from = \"T\",\n  to = \"N\",\n  controlling_for = \"S\",\n  text = FALSE,\n  use_labels = \"label\"\n) + theme_dag_blank()\n\n\n\n\n\n\n\n\nWe can use the ggdag package to find colliders among our variables:\n\n# code for finding colliders\n\nggdag::ggdag_collider(dag_sd,\n                      text = FALSE,\n                      use_labels = \"label\")\n\n\n\n\n\n\n\n\nThe following simulation (by Solomon Kurz) illustrates the selection-distortion effect, which Richard McElreath discusses in Statistical Rethinking:\nFirst simulated uncorrelated variables and a process of selection for sub-populations score high on both indicators.\n\n# simulate selection distortion effect, following Solomon Kurz\n# https://bookdown.org/content/4857/the-haunted-dag-the-causal-terror.html\nset.seed(123)\nn &lt;- 1000  # number of grant proposals\np &lt;- 0.05  # proportion to select\n\nd &lt;-\n  # uncorrelated newsworthiness and trustworthiness\n  dplyr::tibble(\n    newsworthiness  = rnorm(n, mean = 0, sd = 1),\n    trustworthiness = rnorm(n, mean = 0, sd = 1)\n  ) |&gt;\n  # total_score\n  dplyr::mutate(total_score = newsworthiness + trustworthiness) |&gt;\n  # select top 10% of combined scores\n  dplyr::mutate(selected = ifelse(total_score &gt;= quantile(total_score, 1 - p), TRUE, FALSE))\n\nNext filter out the high scoring examples, and assess their correlation.\nNote that the act of selection induces a correlation within our dataset.\n\nd |&gt; \n  dplyr::filter(selected == TRUE) |&gt; \n  dplyr::select(newsworthiness, trustworthiness) |&gt; \n  cor()\n\n                newsworthiness trustworthiness\nnewsworthiness       1.0000000      -0.7318408\ntrustworthiness     -0.7318408       1.0000000\n\n\nThis makes it seems as if there is a relationship between Trustworthiness and Newsworthiness in science, even when there isn’t any.\n\n# we'll need this for the annotation\nlibrary(ggplot2)\ntext &lt;-\n  dplyr::tibble(\n    newsworthiness  = c(2, 1),\n    trustworthiness = c(2.25, -2.5),\n    selected        = c(TRUE, FALSE),\n    label           = c(\"selected\", \"rejected\")\n  )\n\nd |&gt;\n  ggplot2::ggplot(aes(x = newsworthiness, y = trustworthiness, color = selected)) +\n  ggplot2::geom_point(aes(shape = selected), alpha = 3 / 4) +\n  ggplot2::geom_text(data = text,\n            aes(label = label)) +\n  ggplot2::geom_smooth(\n    data = d |&gt; filter(selected == TRUE),\n    method = \"lm\",\n    fullrange = T,\n    color = \"lightblue\",\n    se = F,\n    size = 1\n  ) +\n  # scale_color_manual(values = c(\"black\", \"lightblue\")) +\n  ggplot2::scale_shape_manual(values = c(1, 19)) +\n  ggplot2::scale_x_continuous(limits = c(-3, 3.9), expand = c(0, 0)) +\n  ggplot2::coord_cartesian(ylim = range(d$trustworthiness)) +\n  ggplot2::theme(legend.position = \"none\") +\n  ggplot2::xlab(\"Newsworthy\") +\n  ggplot2::ylab(\"Trustworthy\") + theme_bw()\n\n\n\n\n\n\n\n\nOnce we know a proposal has been selected, if it is newsworthy we can predict that it is less trustworthy. Our simulation produces this prediction even though we simulated a world in which there is no relationship between trustworthiness and newsworthiness.\nSelection bias is commonplace."
  },
  {
    "objectID": "content/04-content.html#collider-bias-within-experiments",
    "href": "content/04-content.html#collider-bias-within-experiments",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Collider bias within experiments",
    "text": "Collider bias within experiments\nWe noted that conditioning on a post-treatment variable can induce bias by blocking the path between the experimental manipulation and the outcome. However, such conditioning can open a path even when there is no experimental effect.\n\ndag_ex2 &lt;- dagify(\n  C1 ~ C0 + U,\n  Ch ~ U + R,\n  labels = c(\n    \"R\" = \"Ritual\",\n    \"C1\" = \"Charity-post\",\n    \"C0\" = \"Charity-pre\",\n    \"Ch\" = \"Cohesion\",\n    \"U\" = \"Religiousness (Unmeasured)\"\n  ),\n  exposure = \"R\",\n  outcome = \"C1\",\n  latent = \"U\"\n) |&gt;\n  control_for(c(\"Ch\",\"C0\"))  \n\ndag_ex2 |&gt;\n  ggdag( text = FALSE,\n    use_labels = \"label\")\n\n\n\n\n\n\n\n\nHow do we avoid collider-bias here?\nNote what happens if we condition on cohesion?\n\ndag_ex2 |&gt;\n  ggdag_collider(\n    text = FALSE,\n    use_labels = \"label\"\n  )  +\n  ggtitle(\"Cohesion is a collider that opens a path from ritual to charity\")\n\n\n\n\n\n\n\n\nDon’t condition on a post-treatment variable!\n\ndag_ex3 &lt;- dagify(\n  C1 ~ C0,\n  C1 ~ U,\n  Ch ~ U + R,\n  labels = c(\n    \"R\" = \"Ritual\",\n    \"C1\" = \"Charity-post\",\n    \"C0\" = \"Charity-pre\",\n    \"Ch\" = \"Cohesion\",\n    \"U\" = \"Religiousness (Unmeasured)\"\n  ),\n  exposure = \"R\",\n  outcome = \"C1\",\n  latent = \"U\"\n)\nggdag_adjustment_set(dag_ex3)"
  },
  {
    "objectID": "content/04-content.html#taxonomy-of-confounding",
    "href": "content/04-content.html#taxonomy-of-confounding",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Taxonomy of confounding",
    "text": "Taxonomy of confounding\nThere is good news. Remember, ultimately are only four basic types of confounding:\n\nThe Fork (omitted variable bias)\n\nconfounder_triangle(x = \"Coffee\",\n                    y = \"Lung Cancer\",\n                    z = \"Smoking\") |&gt;\n  ggdag_dconnected(text = FALSE,\n                   use_labels = \"label\")\n\n\n\n\n\n\n\n\n\n\nThe Pipe (fully mediated effects)\n\nmediation_triangle(\n  x = NULL,\n  y = NULL,\n  m = NULL,\n  x_y_associated = FALSE\n) |&gt;\n  tidy_dagitty(layout = \"nicely\") |&gt;\n  ggdag()\n\n\n\n\n\n\n\n\n\n\nThe Collider\n\ncollider_triangle() |&gt;\n  ggdag_dseparated(controlling_for = \"m\")\n\n\n\n\n\n\n\n\n\n\nConfounding by proxy\nIf we “control for” a descendant of a collider, we will introduce collider bias.\n\ndag_sd &lt;- dagify(\n  Z ~ X,\n  Z ~ Y,\n  D ~ Z,\n  labels = c(\n    \"Z\" = \"Collider\",\n    \"D\" = \"Descendant\",\n    \"X\" = \"X\",\n    \"Y\" = \"Y\"\n  ),\n  exposure = \"X\",\n  outcome = \"Y\"\n) |&gt;\n  control_for(\"D\") \n\ndag_sd |&gt;\n  ggdag_dseparated(\n    from = \"X\",\n    to = \"Y\",\n    controlling_for = \"D\",\n    text = FALSE,\n    use_labels = \"label\"\n  )  +\n  ggtitle(\"X --&gt; Y, controlling for D\",\n          subtitle = \"D induces collider bias\")"
  },
  {
    "objectID": "content/04-content.html#rules-for-avoiding-confounding",
    "href": "content/04-content.html#rules-for-avoiding-confounding",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Rules for avoiding confounding",
    "text": "Rules for avoiding confounding\nFrom Statistical Rethinking, p.286\n\nList all of the paths connecting X (the potential cause of interest) and Y (the outcome).\n\n\nClassify each path by whether it is open or closed. A path is open unless it contains a collider.\n\n\nClassify each path by whether it is a backdoor path. A backdoor path has an arrow entering X.\n\n\nIf there are any open backdoor paths, decide which variable(s) to condition on to close it (if possible).\n\n\n# Examle\n# call ggdag model\n# write relationships:\n\nlibrary(ggdag)\ndg_1 &lt;- ggdag::dagify(\n  b ~  im + ordr + rel + sr  + st,\n  rel ~  age + ses + edu + male + cny,\n  ses ~ cny + edu + age,\n  edu ~ cny + male + age,\n  im ~ mem + rel + cny,\n  mem ~ age + edu + ordr,\n  exposure = \"sr\",\n  outcome = \"b\",\n  labels = c(\n    \"b\" = \"statement credibility\",\n    \"sr\" = \"source\",\n    \"st\" = \"statement\",\n    \"im\" = \"importance\",\n    \"mem\" = \"memory\",\n    \"s\" = \"source\",\n    \"rel\" = \"religious\",\n    \"cny\" = \"country\",\n    \"mem\" = \"memory\",\n    \"male\" = \"male\",\n    \"ordr\" = \"presentation order\",\n    \"ses\" = \"perceived SES\",\n    \"edu\" = \"education\",\n    \"age\" = \"age\"\n  )\n) |&gt;\n  control_for(\"rel\")\n\nggdag::ggdag_collider(dg_1, text = FALSE, use_labels = \"label\")\n\n\n\n\n\n\n\n\nNote the colliders induced from the “controls” that we had included in the study:\n\np3 &lt;- ggdag::ggdag_dseparated(\n  dg_1,\n  from = \"sr\",\n  to = \"b\",\n  controlling_for = c(\"ses\", \"age\", \"cny\", \"im\", \"edu\", \"mem\", \"male\", \"rel\"),\n  text = FALSE,\n  use_labels  = \"label\"\n) +\n  theme_dag_blank() +\n  labs(title = \"Collider Confounding occurs when we `control for` a bunch of variables\")\np3\n\n\n\n\n\n\n\n\nHow do we fix the problem? Think hard about the causal network and let ggdag do the work.\n\n# find adjustment set\np2 &lt;- ggdag::ggdag_adjustment_set(dg_1,\n                                  text = FALSE,\n                                  use_labels  = \"label\") +\n  theme_dag_blank() +\n  labs(title = \"Adjustment set\",\n       subtite = \"Model for Source credibility from belief \")\np2"
  },
  {
    "objectID": "content/04-content.html#inference-depends-on-assumptions-that-are-not-contained-in-the-data.",
    "href": "content/04-content.html#inference-depends-on-assumptions-that-are-not-contained-in-the-data.",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Inference depends on assumptions that are not contained in the data.",
    "text": "Inference depends on assumptions that are not contained in the data.\n\nregression itself does not provide the evidence you need to justify a causal model. Instead, you need some science.” – Richard McElreath: “Statistical Rethinking, Chapter 6”\n\n\n“…the data alone can never tell you which causal model is correct”- Richard McElreath: “Statistical Rethinking” Chapter 5\n\n\n“The parameter estimates will always depend upon what you believe about the causal model, because typically several (or very many) causal models are consistent with any one set of parameter estimates.” “Statistical Rethinking” Chapter 5\n\nSuppose we assume that the source condition affects religion, say through priming. We then have the following dag:\n\n## adding religion to effect on edu\ndg_3 &lt;- ggdag::dagify(\n  b ~  im + ordr + rel  + st + sr,\n  rel ~  age + ses + edu + male + cny + sr,\n  ses ~ cny + edu + age,\n  edu ~ cny + male + age,\n  im ~ mem + rel + cny,\n  mem ~ age + edu + ordr,\n  exposure = \"rel\",\n  outcome = \"b\",\n  labels = c(\n    \"b\" = \"statement credibility\",\n    \"sr\" = \"source\",\n    \"st\" = \"statement\",\n    \"im\" = \"importance\",\n    \"mem\" = \"memory\",\n    \"s\" = \"source\",\n    \"rel\" = \"religious\",\n    \"cny\" = \"country\",\n    \"mem\" = \"memory\",\n    \"male\" = \"male\",\n    \"ordr\" = \"presentation order\",\n    \"ses\" = \"perceived SES\",\n    \"edu\" = \"education\",\n    \"age\" = \"age\"\n   )\n)|&gt;\n  control_for(\"rel\")\n\nggdag(dg_3, text = FALSE, use_labels  = \"label\")\n\n\n\n\n\n\n\n\nWe turn to our trusted oracle, and and ask: “What do we condition on to obtain an unbiased causal estimate?”\nThe oracle replies:\n\nggdag::ggdag_adjustment_set(\n  dg_3,\n  exposure = \"sr\",\n  outcome = \"b\",\n  text = FALSE,\n  use_labels  = \"label\"\n) +\n  theme_dag_blank() +\n  labs(title = \"Adjustment set\",\n       subtite = \"Model for Source credibility from belief \")\n\n\n\n\n\n\n\n\nYour data cannot answer your question."
  },
  {
    "objectID": "content/04-content.html#more-examples-of-counfoundingde-confounding",
    "href": "content/04-content.html#more-examples-of-counfoundingde-confounding",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "More examples of counfounding/de-confounding",
    "text": "More examples of counfounding/de-confounding\nHere’s another example from recent NZAVS research\n\ntidy_ggdag &lt;- dagify(\n  WB ~ belief + age_within + age_between + partner + nzdep + urban + male + pols + empl,\n  WB ~~ partner,\n  belief ~ age_within + age_between + male + ethn,\n  partner ~ nzdep + age_within + age_between + belief, \n  nzdep ~ empl + age_within + age_between,\n  pols ~ age_within + age_between + empl + ethn,\n  empl ~  edu + ethn + age_within + age_between,\n  exposure =  \"belief\",\n  outcome =   \"WB\")|&gt;\n  tidy_dagitty()\n\n# graph\ntidy_ggdag |&gt;\n  ggdag()\n\n\n\n\n\n\n\n\nWe can examine which variables to select, conditional on the causal assumptions of this dag\n\n# graph adjustment sets\nggdag::ggdag_adjustment_set(tidy_ggdag, node_size = 14) + \n  theme(legend.position = \"bottom\") + theme_dag_blank()\n\n\n\n\n\n\n\n\nThis method reveals two adjustments sets: {age, employment, male, political conservativism, and time}, and {age, ethnicty, male, and time.} We report the second set because employment is likely to contain more measurement error: some are not employed because they cannot find employment, others because they are not seeking employment (e.g. retirement).\n\nUnmeasured causes\nReturn to the previous example of R and C on K6 distress, but imagine an underlying common cause of both C and R (say childhood upbringing) called “U”:\n\ndag_m3 &lt;- dagify(\n  K ~ C + R,\n  C ~ U,\n  R ~ U,\n  exposure = \"C\",\n  outcome = \"K\",\n  latent = \"U\"\n) |&gt;\n  tidy_dagitty(layout = \"nicely\")\n\ndag_m3 |&gt;\n  ggdag()\n\n\n\n\n\n\n\n\nHow do we assess the relationship of C on K?\nWe can close the backdoor from U through R by conditioning on R\n\nggdag::ggdag_adjustment_set(dag_m3)\n\n\n\n\n\n\n\n\nAside, we can simulate this relationship using the following code:\n\n# C -&gt; K &lt;- R\n# C &lt;- U -&gt; R\nn &lt;- 100\nU &lt;- rnorm( n )\nR &lt;- rnorm( n , U )\nC &lt;- rnorm( n , U )\nK &lt;- rnorm( n , R - C )\nd_sim3 &lt;- data.frame(K = K, R = R, U = U, C = C )\n\n\n\nWhat is the relationship between smoking and cardiac arrest?\nThis example is from the ggdag package, by Malcolm Barrett here\n\nsmoking_ca_dag &lt;- dagify(\n  cardiacarrest ~ cholesterol,\n  cholesterol ~ smoking + weight,\n  smoking ~ unhealthy,\n  weight ~ unhealthy,\n  labels = c(\n    \"cardiacarrest\" = \"Cardiac\\n Arrest\",\n    \"smoking\" = \"Smoking\",\n    \"cholesterol\" = \"Cholesterol\",\n    \"unhealthy\" = \"Unhealthy\\n Lifestyle\",\n    \"weight\" = \"Weight\"\n  ),\n  latent = \"unhealthy\",\n  exposure = \"smoking\",\n  outcome = \"cardiacarrest\"\n)\n\nggdag(smoking_ca_dag,\n      text = FALSE,\n      use_labels = \"label\")\n\n\n\n\n\n\n\n\nWhat do we condition on to close any open backdoor paths, while avoiding colliders? We imagine that unhealthy lifestyle is unmeasured.\n\nggdag_adjustment_set(\n  smoking_ca_dag,\n  text = FALSE,\n  use_labels = \"label\",\n  shadow = TRUE\n)\n\n\n\n\n\n\n\n\nWhat if we control for cholesterol?\n\nggdag_dseparated(\n  smoking_ca_dag,\n  controlling_for = c(\"weight\", \"cholesterol\"),\n  text = FALSE,\n  use_labels = \"label\",\n  collider_lines = FALSE\n)\n\n\n\n\n\n\n\n\n\nControlling for intermediate variables may also induce bias, because it decomposes the total effect of x on y into its parts. (ggdag documentation)\n\n\n\nSelection bias in sampling\nThis example is from https://ggdag.malco.io/articles/bias-structures.html\n\nLet’s say we’re doing a case-control study and want to assess the effect of smoking on glioma, a type of brain cancer. We have a group of glioma patients at a hospital and want to compare them to a group of controls, so we pick people in the hospital with a broken bone, since that seems to have nothing to do with brain cancer. However, perhaps there is some unknown confounding between smoking and being in the hospital with a broken bone, like being prone to reckless behavior. In the normal population, there is no causal effect of smoking on glioma, but in our case, we’re selecting on people who have been hospitalized, which opens up a back-door path:\n\n\ncoords_mine &lt;- tibble::tribble(\n  ~name,           ~x,  ~y,\n  \"glioma\",         1,   2,\n  \"hospitalized\",   2,   3,\n  \"broken_bone\",    3,   2,\n  \"reckless\",       4,   1,\n  \"smoking\",        5,   2\n)\n\ndagify(hospitalized ~ broken_bone + glioma,\n       broken_bone ~ reckless,\n       smoking ~ reckless,\n       labels = c(hospitalized = \"Hospitalization\",\n                  broken_bone = \"Broken Bone\",\n                  glioma = \"Glioma\",\n                  reckless = \"Reckless \\nBehavior\",\n                  smoking = \"Smoking\"),\n       coords = coords_mine) |&gt; \n  ggdag_dconnected(\"glioma\", \"smoking\", controlling_for = \"hospitalized\", \n                   text = FALSE, use_labels = \"label\", collider_lines = FALSE)\n\n\n\n\n\n\n\n\n\nEven though smoking doesn’t actually cause glioma, it will appear as if there is an association. Actually, in this case, it may make smoking appear to be protective against glioma, since controls are more likely to be smokers.\n\n\n\nSelection bias in longitudinal research\nSuppose we want to estimate the effect of ethnicity on ecological orientation in a longitudinal dataset where there is selection bias from homeownership (it is easier to reach homeowners by the mail.)\nSuppose the following DAG:\n\ndag_sel &lt;- dagify(\n  retained ~ homeowner,\n  homeowner ~ income + ethnicity,\n  ecologicalvalues ~  ethnicity + income,\n  labels = c(\n    retained = \"retained\",\n    homeowner = \"homeowner\",\n    ethnicity = \"ethnicity\",\n    income = \"income\",\n    ecologicalvalues = \"Ecological \\n Orientation\"\n  ),\n  exposure = \"ethnicity\",\n  outcome = \"ecologicalvalues\"\n) |&gt;\n  control_for(\"retained\")\n\n\ndag_sel |&gt;\n  ggdag_adjust(\n    \"retained\",\n    layout = \"mds\",\n    text = FALSE,\n    use_labels = \"label\",\n    collider_lines = FALSE\n  )\n\n\n\n\n\n\n\n\nNotice that “retained” falls downstream from a collider, “home ownership”\n\nggdag_collider(dag_sel)\n\n\n\n\n\n\n\n\nBecause we are stratifying on “retained”, we introduce collider bias in our estimate of ethnicity on ecological values.\n\nggdag_dseparated(\n  dag_sel,\n  controlling_for = \"retained\",\n  text = FALSE,\n  use_labels = \"label\",\n  collider_lines = TRUE\n)\n\n\n\n\n\n\n\n\nHowever we have an adjustment set\n\nggdag_adjustment_set(dag_sel)"
  },
  {
    "objectID": "content/04-content.html#workflow",
    "href": "content/04-content.html#workflow",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Workflow",
    "text": "Workflow\n\nImport your data\nCheck that data types are correct\nGraph your data\nConsider your question\nIf causal, draw your DAG/S\nExplain your DAG’s\nWrite your model\nRun your model\nGraph and interpret your results\nReturn to your question, and assess what you have learned.\n\n(Typically there are multiple iterations between these steps in your workflow. Annotate your scripts; keep track of your decisions)"
  },
  {
    "objectID": "content/04-content.html#summary",
    "href": "content/04-content.html#summary",
    "title": "Causal Diagrams: The Structures of Interaction/Effect Modification, Measurement Bias, Selection Bias",
    "section": "Summary",
    "text": "Summary\n\nWe control for variables to avoid omitted variable bias\nOmitted variable bias is real, but also commonplace is included variable bias\nIncluded variable biases arise from “pipes”, “colliders”, and conditioning on descendant of colliders.\nThe ggdag package can help you to obtain causal inference, but it relies on assumptions that are not part of your data.\nClarify your assumption.\n\n\nPackages\n\nreport::cite_packages()\n\n  - Barrett M (2024). _ggdag: Analyze and Create Elegant Directed Acyclic Graphs_. R package version 0.2.13, &lt;https://CRAN.R-project.org/package=ggdag&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.56, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "",
    "text": "Note\n\n\n\nRequired Reading\n\n(Hernan and Robins 2020) Chapters 4-5 link\n\nOptional Reading\n\n(Tyler J. VanderWeele and Robins 2007a) link\n(Tyler J. VanderWeele 2009) link"
  },
  {
    "objectID": "content/06-content.html#learning-outcomes",
    "href": "content/06-content.html#learning-outcomes",
    "title": "Causal Inference: Average (Marginal) Treatment Effects",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou will understand why causation is never directly observed.\nYou will understand how experiments address this “causal gap.”\nYou will understand how the application of three principles from experimental research allow human scientists to close this “causal gap” when making inferences about a population as whole – that is, inferences about “marginal effects.”"
  },
  {
    "objectID": "content/06-content.html#opening",
    "href": "content/06-content.html#opening",
    "title": "Causal Inference: Average (Marginal) Treatment Effects",
    "section": "Opening",
    "text": "Opening\nRobert Frost writes,\n\nTwo roads diverged in a yellow wood, And sorry I could not travel both And be one traveler, long I stood And looked down one as far as I could To where it bent in the undergrowth;\n\n\nThen took the other, as just as fair, And having perhaps the better claim, Because it was grassy and wanted wear; Though as for that the passing there Had worn them really about the same,\n\n\nAnd both that morning equally lay In leaves no step had trodden black. Oh, I kept the first for another day! Yet knowing how way leads on to way, I doubted if I should ever come back.\n\n\nI shall be telling this with a sigh Somewhere ages and ages hence: Two roads diverged in a wood, and I— I took the one less traveled by, And that has made all the difference. – The Road Not Taken"
  },
  {
    "objectID": "content/06-content.html#introduction-motivating-example",
    "href": "content/06-content.html#introduction-motivating-example",
    "title": "Causal Inference: Average (Marginal) Treatment Effects",
    "section": "Introduction: Motivating Example",
    "text": "Introduction: Motivating Example\nConsider the following cross-cultural question:\n\nDoes bilingualism improve cognitive abilities in children?\n\nThere is evidence that bilingual children perform better oat cognitive tasks, but is learning more than one language a confounding factor?\nHow can we know? Each child might respond as the traveller in Frost’s poem:\n“And sorry I could not travel both. And be one traveller\\dots”"
  },
  {
    "objectID": "content/06-content.html#part-1-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "href": "content/06-content.html#part-1-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "title": "Causal Inference: Average (Marginal) Treatment Effects",
    "section": "Part 1: The Fundamental Problem of Causal Inference as a Missing Data Problem",
    "text": "Part 1: The Fundamental Problem of Causal Inference as a Missing Data Problem\nHumans think in images, words, and songs – sometimes dance. However, a little mathematical notation helps to clarify the key concepts in causal inference.\nTo understand the fundamental problem of causal inference, we first define two potential outcomes for each individual in our study:\nLet Y and A denote random variables.\nMathematically, we formulate a causal question by asking whether assignment to treatment A = a will lead to a difference to the outcome Y.\nLet A = 1 denote getting the “bilingual” treatment, and A = 0 denote getting the monolingual treatment. Assume these are the only two treatments of interest.\n\nY_i(a = 1) The cognitive ability of child i if they were bilingual. This is the counterfactual outcome when treatement A = (a  =  1).\nY_i(a = 0):: The cognitive ability of child i if they were monolingual. This is the counterfactual outcome when A = (a = 0).\n\nUsing this notation, we may define a quantitative causal effect of bilingualism on cognitive ability for individual i as the difference between these potential outcomes:\n\n\\text{Causal Effect}_i = Y_i(1) - Y_i(0)\n\nWe say there is a causal effect if:\n\nY_i(1) - Y_i(0)  \\neq 0\n\nWriting out our contrast of interest this way makes it clear that, because individuals experience only one treatment condition, we cannot compute this contrast directly from any data we may observe. We call the missing observation “counterfactual.”\n\nY_i|A_i = 1 \\implies Y_i(0)|A_i = 1~ \\text{is counterfactual}\n Furthermore:\n\nY_i|A_i = 0 \\implies Y_i(1)|A_i = 1~ \\text{is counterfactual}\n\nAnd sorry I could not travel both And be one traveller, long I stood \\dots\n\nAverage Treatment Effect in randomised controlled experiments work from assumptions\nConsider inherently missing observations in an experiment:\n\n\n\n\n\\text{ATE} = \\left[ \\begin{aligned}\n&\\left( \\underbrace{\\mathbb{E}[Y(1)|A = 1]}_{\\text{observed}} + \\textcolor{red}{\\underbrace{\\mathbb{E}[Y(1)|A = 0]}_{\\text{unobserved}}} \\right) \\\\\n&- \\left( \\underbrace{\\mathbb{E}[Y(0)|A = 0]}_{\\text{observed}} + \\textcolor{red}{\\underbrace{\\mathbb{E}[Y(0)|A = 1]}_{\\text{unobserved}}} \\right)\n\\end{aligned} \\right]\n\n\n\nTable 1\n\n\n\nIn Table 1, the expression, \\mathbb{E}[Y(1)|A = 1] represents the average outcome when the treatment is given, which is observable. However, \\mathbb{E}[Y(1)|A = 0] represents the average outcome if the treatment had been given to those who were untreated, which remains unobservable. Similarly, the quantity \\mathbb{E}[Y(0)|A = 1] also remains unobservable. Note that the problem isn’t merely one of statistical analysis on the data. The problem is that the relevant data to identify individual causal effects are missing. Thus, it is evident that the fundamental problem of causal inference is an ever-present concern even in experiments!\nAll the data we require for causal inferences at the individual level are missing, Nevertheless, because the treatments have been randomised, the data we observe from randomised controlled experiments can allow us to infer what would happen, on average if the treatment were applied to the population from which the participants were sampled.\nWe call this causal affect the “Average Treatment Effect” or equivalently, the “Marginal Effect of the Treatment.”\n\n\nHow do we obtain Average Treatment Effects from non-randomised observational data?\nUnder stronger assumptions, we may sometimes infer causal effects from associations in data, even if the treatment variable A has not been randomised.\n\n\\text{ATE} = \\sum_{l} \\large( \\mathbb{E}[Y|A=1, \\textcolor{blue}{L=l}] - \\mathbb{E}[Y|A=0, \\textcolor{blue}{L=l}] \\large) \\times \\textcolor{blue}{\\Pr(L=l)}\n\nWhere L denotes a set of measured covariates, we can express this principle of “no confounding” mathematically in two complementary ways. Recall our notation: A \\coprod B signifies that A is independent of B, and vice versa:\n\nPotential Outcomes Independent of Treatment (given L): Y(a) \\coprod A \\mid L\nTreatment Assignment Independent of Potential Outcomes (given L): A \\coprod Y(a) \\mid L\n\nThese formulations are crucial when working with causal diagrams, which visually encode these principles. The key idea is straightforward: ensuring a balance of confounders across treatment groups is fundamental to experimental and observational causal inference strategies.\nWith sufficiently large numbers of individuals, randomisation facilitates this balance, achieving A \\coprod Y(a)*\n\n\nThe Three Fundamental Assumptions of Causal Inference.\nReviewing causal inference in experimental settings highlights three core assumptions essential for causal analysis.\n\nFundamental Assumption 1: Conditional Exchangeability\nWe say that conditional exchangability holds if the potential outcomes and treatment assignments are statistically independent, considering all measured confounders. This principle enables us to attribute observed group differences directly to the treatment. Randomisation provides unconditional exchangeability, simplifying the analytical process.\nMany experiments, such as child-hood bilingualism, cannot be performed.\n\nChallenge in Satisfying Conditional Exchangeability in Observational Settings\nAchieving conditional exchangability is challenging in observational studies. This condition requires the groups being compared to be similar in every aspect except for the treatment. Consider bilingualism. In real-world data, individuals with access to green spaces may differ from those without access in several ways:\n\nSocioeconomic status: the economic capacity of individuals often determines their opportunities for education and language learning, thereby affecting cognitive development.\nAge demographics: language development is not even throughout childhood; when one learns a second language may make a difference.\nLifestyle choices: Children who learn different language\nPersonal values and social connections: environmental values and community ties may influence both the choice of residence and the utilisation of green spaces.\n\nThese and other unmeasured factors can introduce biases, complicating the interpretation of causal relationships in observational studies.\n\n\n\nFundamental Assumption 2: Causal Consistency\nWe say that causal consistency holds if there is no heterogeneity in the treatments that would prevent us from assuming that the observed outcomes under treatments correspond to their potential outcomes. For an individual ‘i’, we must be able to assume:\n\n\\begin{aligned}\nY_{i}(1) &= (Y_{i}|A_{i} = 1) \\quad \\text{(Potential outcome if treated)} \\\\\nY_{i}(0) &= (Y_{i}|A_{i} = 0) \\quad \\text{(Potential outcome if untreated)}\n\\end{aligned}\n\nIf this assumption holds, as well as the assumptions of conditional exchangeability and positivity (reviewed below), we can calculate the Average Treatment Effect (ATE) from observed data as:\n\n\\begin{aligned}\n\\text{ATE} &= \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)] \\\\\n&= \\mathbb{E}(Y|A=1) - \\mathbb{E}(Y|A=0)\n\\end{aligned}\n\nThis contrast assumes that the potential outcome under treatment is observable when the treatment is administered, setting Y_i(a) to Y_i|A_i=a.\n\nChallenges in Satisfying the Causal Consistency Assumption in Observational Settings\nThe standardisation of treatments in randomised controlled experiments generally ensures the validity of the causal consistency assumption, which is seldom disputed. However, in observational settings, we cannot typically control the treatments that people receive. This fact imposes considerable challenges for satisfying this assumption. For example:\n\nCultures: the language one starts with, and the language one learns next, and the order of learning, may affect cognitive development.\nParents: the way in which one learns languages, either at schools or at home environments, may affect cognitive development.\nLanguage: language impose different demands, consider the demands of a sign language (body-brain), a highly tonal language, the demands of different writing systems, the cognitive demands that arise in cultures with no written scripts…\n\nPut another ways, causal inference does not merely require randomisation, it also requires control.\n\n\n\nFundamental Assumption 3: Positivity\nWe must assume that there is a non-zero probability of receiving each treatment level within covariate-defined subgroups.\n\nP(A = a | L= l) &gt; 0\n\nThis assumption is also met by the control that experimentalists exert over randomised controlled experiments and is rarely stated explicitly. However, in observational settings, this condition must be verified to avoid extrapolating results beyond observed data.\n\nChallenges in Satisfying The Positivity Assumption in Observational Settings: The Relevant Treatments Do Not Exist in The Data\nPositivity demands that each individual has the possibility of experiencing every level of the treatment to be compared. However, real-world constraints, such as the availability of bilingualism within different cultural settings, may preclude some groups from accessing bilinual learning. Where the treatments of interest are absent or scarcely represented in our dataset, the resulting causal inferences will lack empirical support. Consequently, any coefficients derived will represent extrapolations from statistical models, challenging the validity of our causal inferences (Westreich and Cole 2010; Hernan and Robins 2023).\n\nWestreich, Daniel, and Stephen R. Cole. 2010. “Invited commentary: positivity in practice.” American Journal of Epidemiology 171 (6). https://doi.org/10.1093/aje/kwp436.\n\nHernan, M. A., and J. M. Robins. 2023. Causal Inference. Chapman & Hall/CRC Monographs on Statistics &Applied Probab. Taylor & Francis. https://books.google.co.nz/books?id=\\_KnHIAAACAAJ.\n\n\n\n\nSummary\nWe described the fundamental problem of causal inference, focusing on the critical distinction between correlation – associations in the data, and causation – the contrast between potential outcomes only one of which, at most, can be observed. We discussed how controlled experiments facilitate the estimation of average treatment effects (ATE) by systematically manipulating the variable of interest, allowing for the distribution of variables that might affect the outcome to be balanced across the treatment conditions. Our discussion then shifted to observational data, emphasising the challenges inherent in extracting causal relationships from data without the benefit of controlled interventions. We underscored the need to satisfy three key assumptions —- conditional exchangeability, causal consistency, and positivity—for inferring average treatment effects from observational data. These assumptions ensure that the treatment groups are comparable, the treatment effect is consistent across the population, and every individual has a non-zero probability of receiving each treatment level, respectively.\nA note on causal diagrams: we have seen that causal diagrams are graphical tools offer a systematic approach to identifying and controlling for confounding variables – for helping to understand the conditions in which the assumption of “no unmeasured confounding” or equivalently of “balance in the confounders across treatments” may be satisfied. Causal diagrams, however, do not obviate the need for these assumptions; instead, they provide a framework for evaluating whether and how the first of the three assumptions – conditional exchangeability – may be satisfied in a given study."
  },
  {
    "objectID": "content/06-content.html#part-2-lab",
    "href": "content/06-content.html#part-2-lab",
    "title": "Causal Inference: Average (Marginal) Treatment Effects",
    "section": "Part 2: Lab",
    "text": "Part 2: Lab\n\nFocus\n\nApplication of regression and simulation in R for ATE estimation\n\n\n\nSetting up your r script.\nFirst, reinstall the margot package: https://go-bayes.github.io/margot/\n\n# functions explained here: https://go-bayes.github.io/margot/\n\n# installation\nif (!require(devtools, quietly = TRUE)) {\n  install.packages(\"devtools\")\n  library(devtools)\n}\n\n# reinstall the `margot` packagewith updates\n# devtools::install_github(\"go-bayes/margot\", quietly = TRUE)\n\n# call package\nlibrary(\"margot\")\nlibrary(\"tidyverse\")\nlibrary(\"parameters\")\nlibrary(\"skimr\")\nlibrary(\"haven\")\nlibrary(\"stdReg\")\nlibrary('mice')\nlibrary(\"clarify\")\n\n# uncomment and check simulated data\n# head(df_nz)\n\n\n\nDownload a copy of the data directory from the NZAVS OSF website\nFind it the data directory here: https://osf.io/75snb/\nThe variables in the simulated data df_nz correspond to a subset of the variables in this directory.\n\n\nCheck N\n\n# check total n in data\n# total nzavs participants\nn_total &lt;- skimr::n_unique(df_nz$id)\nprint(n_total)\n\n[1] 20000\n\n\n\n\nGet data into shape for analysis\n\n#### Data wrangling: select columns and transpose the data from long to wide\nlibrary(tidyverse)\n\n# filter the original dataset for these IDs three waves\ndf_nz &lt;- as.data.frame(df_nz)\ndf_nz &lt;- haven::zap_formats(df_nz)\ndf_nz &lt;- haven::zap_label(df_nz)\ndf_nz &lt;- haven::zap_widths(df_nz)\n\nname_exposure &lt;-  \"perfectionism\"\n\n# obtain ids for individuals who participated in 2018 and have no missing baseline exposure\nids_2018 &lt;- df_nz %&gt;%\n   dplyr::filter(year_measured == 1, wave == 2018) %&gt;%\n   dplyr::filter(!is.na(!!sym(name_exposure))) |&gt; # criteria, no missing\n  pull(id)\n\n# obtain ids for individuals who participated in 2019\nids_2019 &lt;- df_nz %&gt;%\n   dplyr::filter(year_measured == 1, wave == 2019) %&gt;%\n   dplyr::filter(!is.na(!!sym(name_exposure))) |&gt; # criteria, no missing\n  pull(id)\n\n# intersect IDs from 2018 and 2019 to ensure participation in both years\nids_2018_2019 &lt;- intersect(ids_2018, ids_2019)\n\n\n# data wrangling\ndat_long &lt;- df_nz |&gt;\n  dplyr::filter(id %in% ids_2018_2019 &\n                  wave %in% c(2018, 2019, 2020)) |&gt;\n  arrange(id, wave) |&gt;\n  select(\n    \"id\",\n    \"wave\",\n    \"year_measured\",\n    \"age\",\n    \"male\",\n    \"born_nz\",\n    \"eth_cat\",\n    #factor(EthCat, labels = c(\"Euro\", \"Maori\", \"Pacific\", \"Asian\")),\n    \"employed\",\n    # Are you currently employed? (this includes self-employment or casual work)\n\n    \"edu\",\n    \"kessler6_sum\",\n    # \"gen_cohort\",\n    \"household_inc\",\n    \"partner\",\n    # 0 = no, 1 = yes\n    \"parent\",\n    # 0 = no, 1 = yes\n    \"political_conservative\",\n    \"hours_exercise\",\n    \"agreeableness\",\n    # Mini-IPIP6 Agreeableness (also modelled as empathy facet)\n    # Sympathize with others' feelings.\n    # Am not interested in other people's problems.\n    # Feel others' emotions.\n    # Am not really interested in others.\n    \"conscientiousness\",\n    # see mini ipip6\n    # Get chores done right away.\n    # Like order.\n    # Make a mess of things.\n    # Often forget to put things back in their proper place.\n    \"extraversion\",\n    # Mini-IPIP6 Extraversion\n    # Am the life of the party.\n    # Don't talk a lot.\n    # Keep in the background.\n    # Talk to a lot of different people at parties.\n    \"honesty_humility\",\n    # see mini ipip6\n    # Would like to be seen driving around in a very expensive car.\n    # Would get a lot of pleasure from owning expensive luxury goods.\n    # Feel entitled to more of everything.\n    # Deserve more things in life.\n    \"openness\",\n    # see mini ipip6\n    # Have a vivid imagination.\n    # Have difficulty understanding abstract ideas.\n    # Do not have a good imagination.\n    # Am not interested in abstract ideas.\n    \"neuroticism\",\n    # see mini ipip6\n    # Have frequent mood swings.\n    # Am relaxed most of the time.\n    # Get upset easily.\n    # Seldom feel blue.\n    \"modesty\",\n    # # see mini ipip6\n    # # I want people to know that I am an important person of high status,\n    # # I am an ordinary person who is no better than others.\n    # # I wouldn’t want people to treat me as though I were superior to them.\n    # # I think that I am entitled to more respect than the average person is\n    #\"w_gend_age_ethnic\",\n    \"sample_weights\",\n    \"neighbourhood_community\",\n    # #I feel a sense of community with others in my local neighbourhood.\n    \"belong\",\n    \"rural_gch_2018_l\",\n    \"support\",\n    # \"support_help\",\n    # # 'There are people I can depend on to help me if I really need it.\n    # \"support_turnto\",\n    # # There is no one I can turn to for guidance in times of stress.\n    # \"support_rnoguidance\",\n    #There is no one I can turn to for guidance in times of stress.\n    \"perfectionism\",\n    \"kessler6_sum\"\n  ) |&gt;\n  mutate(\n    #initialize 'censored'\n    censored = ifelse(lead(year_measured) == 1, 1, 0),\n    \n    # modify 'censored' based on the condition; no need to check for NA here as 'censored' is already defined in the previous step\n    censored =  ifelse(is.na(censored) &\n                         year_measured == 1, 1, censored)\n    \n    # # Apply the case_when condition for setting 'censored' based on 'wave' and the dynamic column specified by 'nzavs_exposure'\n    # censored = case_when(\n    #   # Add this condition to keep previous modifications unless the specific condition is met!is.na(censored) ~ censored,\n    #\n    #   # Then check if 'wave' is 2019 and the specified exposure is NA, adjusting the condition to reflect the accurate logic\n    #   wave == 2019 & !is.na(!!sym(nzavs_exposure)) ~ 1,\n    #\n    #   # Default case if none of the above apply; might not be necessary if all possibilities are covered\n    #   TRUE ~ 0\n    # )\n  ) |&gt;\n  select(-year_measured) |&gt;\n  dplyr::mutate(\n    household_inc_log = log(household_inc + 1),\n    hours_exercise_log = log(hours_exercise + 1)  ) |&gt;\n  dplyr::select(\n    -c(\n      household_inc,\n      hours_exercise\n    )\n  ) |&gt;\n  droplevels() |&gt;\n  # dplyr::rename(sample_weights = w_gend_age_ethnic,\n  #               sample_origin =  sample_origin_names_combined) |&gt;\n  arrange(id, wave) |&gt;\n  droplevels() |&gt;\n  data.frame() |&gt;\n  droplevels() |&gt;\n  arrange(id, wave) |&gt;\n  mutate(\n  rural_gch_2018_l = as.numeric(as.character(rural_gch_2018_l)),\n  #   parent = as.numeric(as.character(parent)),\n  partner = as.numeric(as.character(partner)),\n  born_nz = as.numeric(as.character(born_nz)),\n  censored = as.numeric(as.character(censored)),\n  employed = as.numeric(as.character(employed))\n  ) |&gt;\n  droplevels() |&gt;\n  arrange(id, wave) |&gt;\n  data.frame()\n\n# check n in this sample\nskimr::n_unique(dat_long$id)\n\n[1] 14439\n\n#check\n#head(dat_long)\n\n\n\nConvert data from long to wide and impute baseline values\n\n# baseline variables\nbaseline_vars = c(\"age\", \"male\", \"edu\", \"partner\", \"employed\")\nexposure_var = c(\"perfectionism\", \"censored\") # we will use the censored variable later\noutcome_vars = c(\"kessler6_sum\")\n\n\n# function will add exposure and outcome to baseline\ndat_long_wide &lt;- margot::margot_wide_impute_baseline(dat_long, baseline_vars = baseline_vars, exposure_var = exposure_var, outcome_vars =outcome_vars)\n\n\n iter imp variable\n  1   1  t0_employed  t0_edu  t0_kessler6_sum  t0_partner\n  2   1  t0_employed  t0_edu  t0_kessler6_sum  t0_partner\n  3   1  t0_employed  t0_edu  t0_kessler6_sum  t0_partner\n  4   1  t0_employed  t0_edu  t0_kessler6_sum  t0_partner\n  5   1  t0_employed  t0_edu  t0_kessler6_sum  t0_partner\n\n# check\nhead(dat_long_wide)\n\n  id   t0_age t0_male t0_edu t0_partner t0_employed t0_perfectionism\n1  1 40.31341       1      8          1           1         4.303349\n2  3 47.14449       1      3          1           0         2.660515\n3  4 74.60987       0      6          1           0         4.297503\n4  5 57.46461       0      6          1           1         4.046052\n5  6 50.32225       0      7          1           0         4.030903\n6  8 53.36900       1      6          1           1         1.967654\n  t0_censored t0_kessler6_sum t1_perfectionism t1_censored t2_kessler6_sum\n1           1        3.044502         3.670650           1        5.005375\n2           1        2.962828         3.333177           0              NA\n3           1        1.006293         4.350912           1        3.972263\n4           1        1.986856         2.045632           1        2.038406\n5           1        6.018567         4.350877           1        4.978472\n6           1        2.987620         2.991798           1        2.029845\n\n# standardise vars\ndt &lt;- dat_long_wide |&gt; \n  dplyr::mutate(\n    t0_age_z = scale(t0_age),\n    t0_edu_z = scale(t0_edu),\n    t0_kessler6_sum_z =  scale(t0_kessler6_sum),\n    t0_perfectionism_z = scale(t0_perfectionism),\n    t1_perfectionism_z = scale(t1_perfectionism),\n    t2_kessler6_sum_z = scale(t2_kessler6_sum)) |&gt; \n  data.frame()\n\n# we are not handling missing data well, and this will throw bugs down stream\n# to avoid this problem we remove attributes\ndt&lt;- margot::remove_numeric_attributes(dt)\n\n\n\nUse regression to obtain conditional average treatment effect\n\n# note that we are not handling missing data appropriately here\n\n# take interaction of treatment and baseline vars\nfit_1 &lt;- glm(t2_kessler6_sum_z  ~ t1_perfectionism_z * (t0_age_z +  t0_edu_z + t0_kessler6_sum_z + t0_perfectionism_z), data = dt)\n\n# summarise\nregression_table  &lt;- parameters::model_parameters(fit_1)\n\nThis regression coefficient has a causal interpretation (it is the ate, at the value when all variables in the model are set to zero)\n\nregression_table[2, ]\n\nParameter          | Coefficient |       SE |       95% CI | t(11539) |      p\n------------------------------------------------------------------------------\nt1 perfectionism z |        0.22 | 9.36e-03 | [0.20, 0.24] |    23.40 | &lt; .001\n\n\n\n\nAverage Treatment Effect\nFirst we’ll use the stdReg package:\n\n# fit the ate\nfit_ate &lt;- stdReg::stdGlm(fit_1,data = dt, X = \"t1_perfectionism_z\", x=seq(0,1))\nprint(summary(fit_ate))\n\n# summarise the ate\nate_stdreg_method &lt;- summary(fit_ate,  contrast = \"difference\",  reference = 0)\n\nprint( ate_stdreg_method )\n# graph ate using stdReg method\nplot(ate_stdreg_method)\n\n\nObtain ATE by hand using g-computation\n\n# recall our fit\nfit_1 &lt;- glm(t2_kessler6_sum_z ~ t1_perfectionism_z + t0_age_z + t0_edu_z + t0_kessler6_sum_z + t0_perfectionism_z, data = dt, family = gaussian())\n\n# step 2: create a new dataset where everyone receives the treatment\ndt_treated &lt;- dt\ndt_treated$t1_perfectionism_z &lt;- mean(dt$t1_perfectionism_z)  # setting treatment to the mean can be adjusted as needed\n\n# step 3: predict outcomes under the treatment scenario\ndt_treated$predicted_outcome &lt;- predict(fit_1, newdata = dt_treated, type = \"response\")\n\n# step 4: calculate the mean outcome under this treatment scenario\nmean_outcome_treated &lt;- mean(dt_treated$predicted_outcome)\n\n# repeat Steps 2-4 for control or another treatment level\ndt_control &lt;- dt\ndt_control$t1_perfectionism_z &lt;- 1  # one sd increase in perfectionism\n\ndt_control$predicted_outcome &lt;- predict(fit_1, newdata = dt_control, type = \"response\")\nmean_outcome_control &lt;- mean(dt_control$predicted_outcome)\n\n# step 5: calculate the average treatment effect\nate_gcomp &lt;- round( mean_outcome_treated - mean_outcome_control, 2)\nprint(paste(\"Average Treatment Effect (ATE) of t1_perfectionism_z using g-computation: \", ate_gcomp))\n\n[1] \"Average Treatment Effect (ATE) of t1_perfectionism_z using g-computation:  -0.22\"\n\n\n\n\nNext obtain the ATE and standard errors by simulation using clarify\n\nlibrary(\"clarify\")\n\n# setup simulation with clarify to manipulate the treatment variable and predict outcomes\nset.seed(123)\nsim_coefs &lt;- sim(fit_1)\n\n# compute ate\nsim_est &lt;- sim_ame(sim_coefs, contrast = \"diff\", var =  \"t1_perfectionism_z\")\n\n# summarise\nsummary(sim_est)\n\n                            Estimate 2.5 % 97.5 %\nE[dY/d(t1_perfectionism_z)]    0.221 0.203  0.240"
  },
  {
    "objectID": "content/06-content.html#appendix-a",
    "href": "content/06-content.html#appendix-a",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Appendix A",
    "text": "Appendix A\n\nEvidence for effect-modification is relative to inclusion of other variables in the model\nThe ‘sharp-null hypothesis’ states there is no effect of the exposure on the outcome for any unit in the target population. Unless the ‘sharp-null hypothesis’ is false, there may be effect-modification. For any study worth conducting, we cannot evaluate whether the sharp-null hypothesis is false. If we could the experiment would be otiose. Therefore, we must assume the possibility of effect-modification. Notably, whether a variable is an effect-modifier also depends on which other variables are included in the model. That is, just as for the concept of a ‘confounder’, where a variable is an ‘effect-modifier’ cannot be stated without reference to an assumed causal order and an explicit statement about which other variables will be included in the model (Tyler J. VanderWeele 2012).\n\nVanderWeele, Tyler J. 2012. “Confounding and Effect Modification: Distribution and Measure.” Epidemiologic Methods 1 (1): 55–82. https://doi.org/10.1515/2161-962X.1004.\nAs illustrated in Figure 4, the marginal association between A and Y is unbiased. Here, exposure A is unconditionally associated with Y. Recall our convention G denotes effect-modification with conditioning and Z indicates effect-modification without conditioning.\n\n\n\n\n\n\n\n\nFigure 4: Consider a randomised experiment. There is no confounding. Here, the marginal association between A and Y provides an unbiased estimate for the causal effect of A on Y. Does the conditional association of A on Y vary within levels of G? The causal diagram allows for a classification of G as an effect modifier of A on Y by proxy. G modifies A’s effect on Y in virtue of G’s relationship to Z, which, according to this graph, is a direct effect modifier for the effect of A on Y.\n\n\n\n\n\nFigure 5 presents the same a randomised experiment as in the previous causal diagram. We again assume that there is no confounding of the marginal association between the exposure, A, and the outcome, Y. However, suppose we were to adjust for Z and ask, does the conditional association of A on Y vary within levels of G, after adjusting for Z? That is, does G remain an effect-modifier of the exposure on the outcome? Tyler J. VanderWeele and Robins (2007b) proved that for effect-modification to occur, at least one other arrow besides the treatment must enter into the outcome. According to Figure 5 the only arrow into Y other than A arrives from Z. Because Y is independent of G conditional on Z we may infer that G is no longer an effect modifier for the effect of A on Y. Viewed another way, G no longer co-varies with Y conditional on Z and so cannot act as an effect-modifier.\n\n\n\n\n\n\n\n\nFigure 5: Conditioning on Z renders G independent of Y. G is no longer an effect modifier after conditioning on Z because G is independent of Y. Although Z is an unconditional effect modifier, G is not.\n\n\n\n\n\nFigure 6 presents the same a randomised experiment as in the previous graph. We assume a true effect of A \\rightarrow Y. If we do not condition on B, then G will not modify the effect of A  \\rightarrow Y because G will not be associated with Y. However, if we were to condition on B, then both B (an effect modifier by proxy) and G may become effect-modifiers for the causal effect of A on Y. In this setting, both B and G are conditional effect-modifiers.\nNote that casual graphs help us to evaluate classifications of conditional and unconditional effect modifiers. They may also help to clarify conditions in which conditioning on unconditional effect-modifiers may remove conditional effect-modification. However we cannot not tell from a causal diagram whether the ancestors of an unconditional effect-modifier will be conditional effect-modifiers for the effect of the exposure on the outcome; see: Tyler J. VanderWeele and Robins (2007b), also Suzuki et al. (2013). Causal diagrams express non-parametric relations. I have adopted an off-label colouring convention to denote instances of effect-modification to highlight possible pathways for effect-modification, which may be relative to other variables in a model.\n\nVanderWeele, Tyler J., and James M. Robins. 2007b. “Four types of effect modification: a classification based on directed acyclic graphs.” Epidemiology (Cambridge, Mass.) 18 (5): 561–68. https://doi.org/10.1097/EDE.0b013e318127181b.\n\n\n\n\n\n\n\n\nFigure 6: Blue path denotes effect-modification for G by conditioning on B. Both B and G are conditional effect modifiers.\n\n\n\n\n\nFigure 7 reveals the relativity of effect-modification. If investigators do not condition on B, then G cannot be a conditional effect-modifier because G would then be independent of Z because B is a collider. However, as we observed in Figure 6, conditioning on B, a collider, may open a path for effect-modification of G by Z. Both B and G are conditional effect modifiers.\n\n\n\n\n\n\n\n\nFigure 7: Blue path denotes effect-modification. Here G is not an effect modifier because B, a common effect (collider) of G and Z, is not conditioned on. Any conditional effect modification for G would require conditioning on B, and not-conditioning on G. Otherwise G will be d-separated from Y.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Blue path denotes effect-modification. Neither, G nor B are unconditional effect-modifiers for the effect of A on Y after Z is conditioned upon. If investigators condition on Z, the causal diagram implies they will not find evidence for effect-modification by B or G, which are conditionally independent of Y once Z is conditioned upon.\n\n\n\n\n\nFigure 8 considers the implications of conditioning on Z, which is the only unconditional effect-modifier on the graph. If Z is measured, conditioning on Z will remove effect-modification for B and G because B,G\\coprod Y |Z. This examples again reveals the context dependency of effect-modification. Here, causal diagrams are useful for clarifying features of dependent and independent effect modification. For further discussion, see: Suzuki et al. (2013); Tyler J. VanderWeele (2009).\n\nSuzuki, Etsuji, Toshiharu Mitsuhashi, Toshihide Tsuda, and Eiji Yamamoto. 2013. “A Counterfactual Approach to Bias and Effect Modification in Terms of Response Types.” BMC Medical Research Methodology 13 (1): 1–17.\n\nVanderWeele, Tyler J. 2009. “On the Distinction Between Interaction and Effect Modification.” Epidemiology, 863–71.\n\n\nPackages\n\nreport::cite_packages()\n\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 0.3.3.3 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Firke S (2024). _janitor: Simple Tools for Examining and Cleaning Dirty Data_. R package version 2.2.1, &lt;https://CRAN.R-project.org/package=janitor&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 &lt;https://doi.org/10.21105/joss.02445&gt;.\n  - Müller K (2020). _here: A Simpler Way to Find Your Files_. R package version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Rich B (2023). _table1: Tables of Descriptive Statistics in HTML_. R package version 1.4.3, &lt;https://CRAN.R-project.org/package=table1&gt;.\n  - Sjoberg D, Whiting K, Curry M, Lavery J, Larmarange J (2021). \"Reproducible Summary Tables with the gtsummary Package.\" _The R Journal_, *13*, 570-580. doi:10.32614/RJ-2021-053 &lt;https://doi.org/10.32614/RJ-2021-053&gt;, &lt;https://doi.org/10.32614/RJ-2021-053&gt;.\n  - Sjolander A, Dahlqwist E (2021). _stdReg: Regression Standardization_. R package version 3.4.1, &lt;https://CRAN.R-project.org/package=stdReg&gt;.\n  - Waring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, Ellis S (2022). _skimr: Compact and Flexible Summaries of Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=skimr&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;."
  },
  {
    "objectID": "content/05-content.html#slides",
    "href": "content/05-content.html#slides",
    "title": "Quiz",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/11-content.html",
    "href": "content/11-content.html",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "",
    "text": "(Fischer and Karl 2019) link\n\n\n\n\n\n(Vijver et al. 2021) link\n(He and Vijver 2012) link\n(J. [et. al]. Harkness 2003) link"
  },
  {
    "objectID": "content/11-content.html#overview",
    "href": "content/11-content.html#overview",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Overview",
    "text": "Overview\nBy the conclusion of our session, you will gain proficiency in:\n\nExploratory Factor Analysis,\nConfirmatory Factor Analysis (CFA),\nMultigroup Confirmatory Factor Analysis,\nPartial Invariance (configural, metric, and scalar equivalence)\n\nWe will learn these concepts by doing an analysis."
  },
  {
    "objectID": "content/11-content.html#focus-on-kessler-6-anxiety",
    "href": "content/11-content.html#focus-on-kessler-6-anxiety",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Focus on Kessler-6 Anxiety",
    "text": "Focus on Kessler-6 Anxiety\nThe code below will:\n\nLoad required packages.\nSelect the Kessler 6 items\nCheck whether there is sufficient correlation among the variables to support factor analysis.\n\n\nSelect A Scale To Validate: Kessler 6 Distress\n\n# get synthetic data\nlibrary(margot)\nlibrary(tidyverse)\nlibrary(performance)\n\n# update margot\n# uncomment\n# devtools::install_github(\"go-bayes/margot\")\n\n\n# select the columns of the kesser-6 need. \ndt_only_k6 &lt;- df_nz |&gt; \n  filter(wave == 2018) |&gt; \n  select(\n    kessler_depressed,\n    kessler_effort,\n    kessler_hopeless,\n    kessler_worthless,\n    kessler_nervous,\n    kessler_restless\n  )\n\n\n# check factor structure\nperformance::check_factorstructure(dt_only_k6)\n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(15) = 50402.32, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.87). The individual KMO scores are: kessler_depressed (0.85), kessler_effort (0.90), kessler_hopeless (0.86), kessler_worthless (0.85), kessler_nervous (0.90), kessler_restless (0.88).\n\n\n\n\nPractical Definitions of the Kessler-6 Items\n\nThe df_nz is loaded with the margot package. It is a synthetic dataset.\ntake items from the Kessler-6 (K6) scale: depressed, effort, hopeless, worthless, nervous, and restless (R.  C. Kessler et al. 2002; R. C. Kessler et al. 2010).\nThe Kessler-6 is used as a diagnostic screening tool for depression for physicians in New Zealand\n\n\nKessler, R. C., G. Andrews, L. J. Colpe, E. Hiripi, D. K. Mroczek, S.-L. T. Normand, E. E. Walters, and A. M. Zaslavsky. 2002. “Short Screening Scales to Monitor Population Prevalences and Trends in Non-Specific Psychological Distress.” Psychological Medicine 32 (6): 959–76. https://doi.org/10.1017/S0033291702006074.\n\nKessler, Ronald C., Jennifer Greif Green, Michael J. Gruber, Nancy A. Sampson, Evelyn Bromet, Marius Cuitan, Toshi A. Furukawa, et al. 2010. “Screening for Serious Mental Illness in the General Population with the K6 Screening Scale: Results from the WHO World Mental Health (WMH) Survey Initiative.” International Journal of Methods in Psychiatric Research 19 (S1): 4–22. https://doi.org/10.1002/mpr.310.\nThe Kessler-6 (K6) is a widely-used diagnostic screening tool designed to identify levels of psychological distress that may indicate mental health disorders such as depression and anxiety. Physicians in New Zealand to screen patients quickly (krynen2013measuring?). Each item on the Kessler-6 asks respondents to reflect on their feelings and behaviors over the past 30 days, with responses provided on a five-point ordinal scale:\n\n“…you feel hopeless”\n\nInterpretation: This item measures the frequency of feelings of hopelessness. It assesses a core symptom of depression, where the individual perceives little or no optimism about the future.\n\n“…you feel so depressed that nothing could cheer you up”\n\nInterpretation: This statement gauges the depth of depressive feelings and the inability to gain pleasure from normally enjoyable activities, a condition known as anhedonia.\n\n“…you feel restless or fidgety”\n\nInterpretation: This item evaluates agitation and physical restlessness, which are common in anxiety disorders but can also be present in depressive states.\n\n“…you feel that everything was an effort”\n\nInterpretation: This query assesses feelings of fatigue or exhaustion with everyday tasks, reflecting the loss of energy that is frequently a component of depression.\n\n“…you feel worthless”\n\nInterpretation: This item measures feelings of low self-esteem or self-worth, which are critical indicators of depressive disorders.\n\n“…you feel nervous”\n\nInterpretation: This question is aimed at identifying symptoms of nervousness or anxiety, helping to pinpoint anxiety disorders.\n\n\n\nResponse Options\nThe ordinal response options provided for the Kessler-6 are designed to capture the frequency of these symptoms, which is crucial for assessing the severity and persistence of psychological distress:\n\n1. “None of the time”: The symptom was not experienced at all.\n2. “A little of the time”: The symptom was experienced infrequently.\n3. “Some of the time”: The symptom was experienced occasionally.\n4. “Most of the time”: The symptom was experienced frequently.\n5. “All of the time”: The symptom was constantly experienced.\n\nIn clinical practice, higher scores on the Kessler-6 are indicative of greater distress and a higher likelihood of a mental health disorder. Physicians use a sum score of 13 to decide on further diagnostic evaluations or immediate therapeutic interventions.\nThe simplicity and quick administration of the Kessler-6 make it an effective tool for primary care settings in New Zealand, allowing for the early detection and management of mental health issues. Let’s stop to consider this measure. Do the items in this scale cohere? Do they all relate to depression? Might we quantitatively evaluate “coherence” in this scale?\n\n\n\nExploratory Factor Analysis\nWe employ performance::check_factorstructure() to evaluate the data’s suitability for factor analysis. Two tests are reported:\n\nBartlett’s Test of Sphericity\n\nBartlett’s Test of Sphericity is used in psychometrics to assess the appropriateness of factor analysis for a dataset. It tests the hypothesis that the observed correlation matrix of is an identity matrix, which would suggest that all variables are orthogonal (i.e., uncorrelated) and therefore, factor analysis is unlikely to be appropriate. (Appendix A.)\nThe outcome:\n\nChi-square (Chisq): 50402.32 with Degrees of Freedom (15) and a p-value &lt; .001\n\nThis highly reliable result (p &lt; .001) confirms that the observed correlation matrix is not an identity matrix, substantiating the factorability of the dataset.\n\nKaiser-Meyer-Olkin (KMO) Measure\n\nThe KMO test assesses sampling adequacy by comparing the magnitudes of observed correlation coefficients to those of partial correlation coefficients. A KMO value nearing 1 indicates appropriateness for factor analysis. The results are:\n\nOverall KMO: 0.87\n\nThis value suggests good sampling adequacy, indicating that the sum of partial correlations is relatively low compared to the sum of correlations, thus supporting the potential for distinct and reliable factors.\n\n\nEach item’s KMO value exceeds the acceptable threshold of 0.5,so suitabale for factor analysis.\n\nExplore Factor Structure\nThe following R code allows us to perform exploratory factor analysis (EFA) on the Kessler 6 (K6) scale data, assuming three latent factors.\n\n# exploratory factor analysis\n# explore a factor structure made of 3 latent variables\n\nlibrary(\"psych\")\nlibrary(\"parameters\")\n\n\n# do efa\nefa &lt;- psych::fa(dt_only_k6, nfactors = 3) |&gt;\n  model_parameters(sort = TRUE, threshold = \"max\")\n\nprint( efa )\n\n# Rotated loadings from Factor Analysis (oblimin-rotation)\n\nVariable          |  MR3 |  MR1 |  MR2 | Complexity | Uniqueness\n----------------------------------------------------------------\nkessler_hopeless  | 0.79 |      |      |       1.00 |       0.32\nkessler_worthless | 0.79 |      |      |       1.01 |       0.34\nkessler_depressed |      | 0.99 |      |       1.00 |   4.98e-03\nkessler_restless  |      |      | 0.72 |       1.03 |       0.47\nkessler_nervous   |      |      | 0.43 |       1.91 |       0.57\nkessler_effort    |      |      | 0.38 |       2.00 |       0.53\n\nThe 3 latent factors (oblimin rotation) accounted for 62.94% of the total variance of the original data (MR3 = 28.20%, MR1 = 17.56%, MR2 = 17.18%).\n\n\n\n\nExplore Factor Structure\n\nlibrary(psych)\nefa &lt;- psych::fa(dt_only_k6, nfactors = 3) |&gt;\n  model_parameters(sort = TRUE, threshold = \"max\")\nprint(efa)\n\n# Rotated loadings from Factor Analysis (oblimin-rotation)\n\nVariable          |  MR3 |  MR1 |  MR2 | Complexity | Uniqueness\n----------------------------------------------------------------\nkessler_hopeless  | 0.79 |      |      |       1.00 |       0.32\nkessler_worthless | 0.79 |      |      |       1.01 |       0.34\nkessler_depressed |      | 0.99 |      |       1.00 |   4.98e-03\nkessler_restless  |      |      | 0.72 |       1.03 |       0.47\nkessler_nervous   |      |      | 0.43 |       1.91 |       0.57\nkessler_effort    |      |      | 0.38 |       2.00 |       0.53\n\nThe 3 latent factors (oblimin rotation) accounted for 62.94% of the total variance of the original data (MR3 = 28.20%, MR1 = 17.56%, MR2 = 17.18%).\n\n\n\n\nWhat is “Rotation”?\nIn factor analysis, rotation is a mathematical technique applied to the factor solution to make it more interpretable. Rotation is like adjusting the angle of a camera to get a better view. When we rotate the factors, we are not changing the underlying data, just how we are looking at it, to make the relationships between variables and factors clearer and more meaningful.\nThe main goal of rotation is to achieve a simpler and more interpretable factor structure. This simplification is achieved by making the factors as distinct as possible, by aligning them closer with specific variables, which makes it easier to understand what each factor represents. Think of orthogonal rotation like organising books on a shelf so that each book only belongs to one category. Each category (factor) is completely independent of the others.\nThere are two types:\nOrthogonal rotations (such as Varimax), which assume that the factors are uncorrelated and keep the axes at 90 degrees to each other. This is useful when we assume that the underlying factors are independent.\nOblique rotations (such as Oblimin), which allow the factors to correlate. Returning to our analogy, imagine a more complex library system where some categories of books overlap; for example, “history” might overlap with “political science”. Oblique rotation recognises and allows these overlaps.This is more realistic in psychological and social sciences, … here, we believe that stress and anxiety might naturally correlate with each other, so Oblique rotation is a better option.\n\n\nResults\nUsing oblimin rotation, the items loaded as follows on the three factors:\n\nMR3: Strongly associated with ‘kessler_hopeless’ (0.79) and ‘kessler_worthless’ (0.79). This factor might be capturing aspects related to feelings of hopelessness and worthlessness, often linked with depressive affect.\nMR1: Mostly linked with ‘kessler_depressed’ (0.99), suggesting this factor represents core depressive symptoms.\nMR2: Includes ‘kessler_restless’ (0.72), ‘kessler_nervous’ (0.43), and ‘kessler_effort’ (0.38). This factor seems to encompass symptoms related to anxiety and agitation.\n\nThe complexity values indicate the number of factors each item loads on “significantly.” A complexity near 1.00 suggests that the item predominantly loads on a single factor, which is seen with most of the items except for ‘kessler_nervous’ and ‘kessler_effort’, which show higher complexity and thus share variance with more than one factor.\nUniqueness values represent the variance in each item not explained by the common factors. Lower uniqueness values for items like ‘kessler_depressed’ indicate that the factor explains most of the variance for that item.\n\n\nVariance Explained\nThe three factors together account for 62.94% of the total variance in the data, distributed as follows:\n\nMR3: 28.20%\nMR1: 17.56%\nMR2: 17.18%\n\nThis indicates a substantial explanation of the data’s variance by the model, with the highest contribution from the factor associated with hopelessness and worthlessness.\n\n\nConsensus View?\nThere are different algorithms for assessing the factor structure. The performance package allows us to consider a ‘consensus’ view.\n\n\nCode\nn &lt;-n_factors(dt_only_k6)\nn\n\n\n# Method Agreement Procedure:\n\nThe choice of 1 dimensions is supported by 8 (50.00%) methods out of 16 (Optimal coordinates, Acceleration factor, Parallel analysis, Kaiser criterion, Scree (SE), Scree (R2), VSS complexity 1, Velicer's MAP).\n\n\nCode\n# plot\nplot(n) + theme_classic()\n\n\n\n\n\n\n\n\n\nOutput:\n\nThe choice of 1 dimensions is supported by 8 (50.00%) methods out of 16 (Optimal coordinates, Acceleration factor, Parallel analysis, Kaiser criterion, Scree (SE), Scree (R2), VSS complexity 1, Velicer’s MAP).\n\nThe result indicates that a single dimension is supported by half of the methods used (8 out of 16). However science isn’t a matter of voting. Also, does it make sense that there is one latent factor here? Let’s press on…\n\n\n\nConfirmatory Factor Analysis (ignoring groups)\nCFA to validate the hypothesised factor structures derived from EFA.\n\nOne-factor model: assumes all items measure a single underlying construct.\nTwo-factor model: assumes two distinct constructs measured by the items.\nThree-factor model: assumes three distinct constructs measured by the items.\n\nSteps are:\n\n1. Data Partition\nFirst, we take the dataset (dt_only_k6) and partition it into training and testing sets. This division helps in validating the model built on the training data against an unseen test set; this enhances robustness for the factor analysis findings.\n\nset.seed(123)\npart_data &lt;- datawizard::data_partition(dt_only_k6, training_proportion = .7, seed = 123)\ntraining &lt;- part_data$p_0.7\ntest &lt;- part_data$test\n\n\n\n2. Model Setup for CFA\nBases on the EFA results, we consider three different factor structures\n\nOne-factor model: assumes all items measure a single underlying construct.\nTwo-factor model: assumes two distinct constructs measured by the items.\nThree-factor model: assumes three distinct constructs measured by the items.\n\nWe fit each model to the training data:\n\n# One-factor model\nstructure_k6_one &lt;- psych::fa(training, nfactors = 1) |&gt;\n  efa_to_cfa()\n\n# Two-factor model\nstructure_k6_two &lt;- psych::fa(training, nfactors = 2) |&gt;\n  efa_to_cfa()\n\n# Three-factor model\nstructure_k6_three &lt;- psych::fa(training, nfactors = 3) %&gt;%\n  efa_to_cfa()\n\nThen we split our data for cross-validation\n\n# first partition the data \npart_data &lt;- datawizard::data_partition(dt_only_k6, traing_proportion = .7, seed = 123)\n\n\n# set up training data\ntraining &lt;- part_data$p_0.7\ntest &lt;- part_data$test\n\n\n# one factor model\nstructure_k6_one &lt;- psych::fa(training, nfactors = 1) |&gt;\n  efa_to_cfa()\n\n# two factor model model\nstructure_k6_two &lt;- psych::fa(training, nfactors = 2) |&gt;\n  efa_to_cfa()\n\n# three factor model\nstructure_k6_three &lt;- psych::fa(training, nfactors = 3) %&gt;%\n  efa_to_cfa()\n\n# inspect models\nstructure_k6_one\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless + kessler_nervous + kessler_restless + .row_id\n\nstructure_k6_two\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_effort + kessler_nervous + kessler_restless + .row_id\n\nstructure_k6_three\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_effort\nMR3 =~ kessler_nervous + kessler_restless + .row_id\n\n\n\nOne-Factor Model: All items are linked to a single factor (MR1).\nTwo-Factor Model:\n\nMR1 is linked with kessler_depressed, kessler_hopeless, and kessler_worthless, suggesting these items might represent a more depressive aspect of distress.\nMR2 is associated with kessler_effort, kessler_nervous, and kessler_restless, which could indicate a different aspect, perhaps related to anxiety or agitation.\n\nThree-Factor Model:\n\nMR1 includes kessler_depressed, kessler_effort, kessler_hopeless, and kessler_worthless, indicating a broad factor possibly encompassing overall distress.\nMR2 consists solely of kessler_effort.\nMR3 includes kessler_nervous + kessler_restless, which might imply these are distinctivene from other distress components.\n\n\nDo these results make sense? Note they are different from the Exploratory Factor Analysis. Why might that be?\nNext we perform the confirmatory factor analysis itself…\n\n# fit and compare models\n\n# one latent model\none_latent &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_one, data = test))\n\n# two latents model\ntwo_latents &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_two, data = test))\n\n# three latents model\nthree_latents &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_three, data = test))\n\n\n# compare models\ncompare &lt;-\n  performance::compare_performance(one_latent, two_latents, three_latents, verbose = FALSE)\n\n# select cols we want\nkey_columns_df &lt;- compare[, c(\"Model\", \"Chi2\", \"Chi2_df\", \"CFI\", \"RMSEA\", \"RMSEA_CI_low\", \"RMSEA_CI_high\", \"AIC\", \"BIC\")]\n\n# view as html table\nas.data.frame(key_columns_df) |&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nChi2\nChi2_df\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nAIC\nBIC\n\n\n\n\nlavaan\n805.47732\n14\n0.9468004\n0.0985415\n0.0928176\n0.1043876\n195671.0\n195764.3\n\n\nlavaan\n161.24069\n13\n0.9900359\n0.0442564\n0.0382928\n0.0504917\n195028.7\n195128.8\n\n\nlavaan\n80.53628\n12\n0.9953933\n0.0313209\n0.0250361\n0.0379798\n194950.0\n195056.7\n\n\n\n\n\n\n\nMetrics:\n\nChi2 (Chi-Square Test): A lower Chi2 value indicates a better fit of the model to the data.\ndf (Degrees of Freedom): Reflects the model complexity.\nCFI (Comparative Fit Index): Values closer to 1 indicate a better fit. A value above 0.95 is generally considered to indicate a good fit.\nRMSEA (Root Mean Square Error of Approximation): values less than 0.05 indicate a good fit, and values up to 0.08 are acceptable.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): lower values are better, indicating a more parsimonious model, with the BIC imposing a penalty for model complexity.\n\n\n\nModel Selection:\nWhat do you think?\n\nThe Three Latents Model shows the best fit across all indicators, having the lowest Chi2, RMSEA, and the highest CFI. It also has the lowest AIC and BIC scores, suggesting it not only fits well but is also the most parsimonious among the tested models.\n\nBut…\n\nThe CFI for the two-factor model is 0.990, which is close to 1 and suggests a very good fit to the data. This is superior to the one-factor model (CFI = 0.9468) and slightly less than the three-factor model (CFI = 0.9954). A CFI value closer to 1 indicates a better fit of the model to the data.\nRoot Mean Square Error of Approximation (RMSEA): The two-factor model has an RMSEA of 0.0443, which is within the excellent fit range (below 0.05). It significantly improves upon the one-factor model’s RMSEA of 0.0985 and is only slightly worse than the three-factor model’s 0.0313.\n**BIC* isn’t much different, So\n\nWe might say the two-factor model strikes a balance between simplicity and model fit. It has fewer factors than the three-factor model, making it potentially easier to interpret while still capturing the variance in the data.\nLook at the items. What do you think?\nDoes Anxiety appear to differ from Depression?\n\n\n\nMeasurement Invariance in Multi-group Confirmatory Factor Analysis\nWhen we use tools like surveys or tests to measure psychological constructs (like distress, intelligence, satisfaction), we often need to ensure that these tools work similarly across different groups of people. This is crucial for fair comparisons. Think of it as ensuring that a ruler measures inches or centimeters the same way, whether you are in Auckland or Wellington.\n\nLevels of Measurement Invariance\nMeasurement invariance tests how consistently a measure operates across different groups, such as ethnic or gender groups. Consider how we can understand it through the K6 Distress Scale applied to different demographic groups in New Zealand:\n\nConfigural Invariance\n\nWhat it means: The measure’s structure is the same across groups. Imagine you have a toolkit; configural invariance means that everyone has the same set of tools (screwdrivers, hammers, wrenches) in their kits.\nApplication: For the K6 Distress Scale, both Māori and New Zealand Europeans use the same six questions to gauge distress. However, how strongly each question predicts distress can vary between the groups. (Not we are using “prediction” here – we are only assessessing associations.)\n\nMetric Invariance\n\nWhat it means: The strength of the relationship between each tool (question) and the construct (distress) is consistent across groups. If metric invariance holds, turning a screw (answering a question about feeling nervous) tightens the screw by the same amount no matter who uses the screwdriver.\nApplication: A unit change in the latent distress factor affects scores on questions (like feeling nervous or hopeless) equally for Māori and New Zealand Europeans.\n\nScalar Invariance\n\nWhat it means: Beyond having the same tools and relationships, everyone starts from the same baseline. If scalar invariance holds. It is like ensuring that every screwdriver is calibrated to the same torque setting before being used.\nApplication: The actual scores on the distress scale mean the same thing across different groups. If a Māori scores 15 and a New Zealander of European descent scores 15, both are experiencing a comparable level of distress.\n\n\n\n\nConcept of “Partial Invariance”\nSometimes, not all conditions for full metric or scalar invariance are met, which could hinder meaningful comparisons across groups. This is where the concept of “partial invariance” comes into play.\nPartial Invariance occurs when invariance holds for some but not all items of the scale across groups. Imagine if most, but not all, tools in the kits behaved the same way across different groups. If sufficient items (tools) exhibit invariance, the measure might still be usable for certain comparisons.\n\nMetric Partial Invariance: This might mean that while most items relate similarly to the underlying factor across groups, one or two do not. Researchers might decide that there’s enough similarity to proceed with comparisons of relationships (correlations) but should proceed with caution.\nScalar Partial Invariance: Here, most but not all items show the same intercepts across groups. It suggests that while comparisons of the construct means can be made, some scores might need adjustments or nuanced interpretation.\n\nIn practical terms, achieving partial invariance in your analysis allows for some comparisons but signals a need for careful interpretation and potentially more careful analysis. For instance, if partial scalar invariance is found on the K6 Distress Scale, researchers might compare overall distress levels between Māori and New Zealand Europeans but should acknowledge that differences in certain item responses might reflect measurement bias rather than true differences in distress.\nThe upshot is that understanding these levels of invariance helps ensure that when we compare mental health or other constructs across different groups, we are making fair and meaningful comparisons. Partial invariance offers a flexible approach to handle real-world data where not all conditions are perfectly met. This approach allows researchers to acknowledge and account for minor discrepancies while still extracting valuable insights from their analyses.\nThe following script runs multi-group confirmatory factor analysis (MG-CFA) to assess the invariance of the Kessler 6 (K6) distress scale across two ethnic groups: European New Zealanders and Māori.\n\n# select needed columns plus 'ethnicity'\n# filter dataset for only 'euro' and 'maori' ethnic categories\ndt_eth_k6_eth &lt;- df_nz |&gt; \n  filter(wave == 2018) |&gt; \n  filter(eth_cat == \"euro\" | eth_cat == \"maori\") |&gt; \n  select(kessler_depressed, kessler_effort, kessler_hopeless,\n         kessler_worthless, kessler_nervous, kessler_restless, eth_cat)\n\n# partition the dataset into training and test subsets\n# stratify by ethnic category to ensure balanced representation\npart_data_eth &lt;- datawizard::data_partition(dt_eth_k6_eth, training_proportion = .7, seed = 123, group = \"eth_cat\")\n\ntraining_eth &lt;- part_data_eth$p_0.7\ntest_eth &lt;- part_data_eth$test\n\n# configural invariance models\n#run CFA models specifying one, two, and three latent variables without constraining across groups\none_latent_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", data = test_eth))\ntwo_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", data = test_eth))\nthree_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\", data = test_eth))\n\n# compare model performances for configural invariance\ncompare_eth_configural &lt;- performance::compare_performance(one_latent_eth_configural, two_latents_eth_configural, three_latents_eth_configural, verbose = FALSE)\ncompare_eth_configural\n\n# Comparison of Model Performance Indices\n\nName                         |  Model |    Chi2 | Chi2_df | p (Chi2)\n--------------------------------------------------------------------\none_latent_eth_configural    | lavaan | 612.807 |  28.000 |   &lt; .001\ntwo_latents_eth_configural   | lavaan | 129.992 |  26.000 |   &lt; .001\nthree_latents_eth_configural | lavaan |  85.047 |  24.000 |   &lt; .001\n\nName                         | Baseline(42) | p (Baseline) |   GFI |  AGFI\n--------------------------------------------------------------------------\none_latent_eth_configural    |    13146.709 |       &lt; .001 | 0.987 | 0.967\ntwo_latents_eth_configural   |    13146.709 |       &lt; .001 | 0.997 | 0.993\nthree_latents_eth_configural |    13146.709 |       &lt; .001 | 0.998 | 0.995\n\nName                         |   NFI |  NNFI |   CFI | RMSEA |    RMSEA  CI\n---------------------------------------------------------------------------\none_latent_eth_configural    | 0.953 | 0.933 | 0.955 | 0.089 | [0.08, 0.09]\ntwo_latents_eth_configural   | 0.990 | 0.987 | 0.992 | 0.039 | [0.03, 0.05]\nthree_latents_eth_configural | 0.994 | 0.992 | 0.995 | 0.031 | [0.02, 0.04]\n\nName                         | p (RMSEA) |    RMR |  SRMR |   RFI |  PNFI\n-------------------------------------------------------------------------\none_latent_eth_configural    |    &lt; .001 | 33.223 | 0.037 | 0.930 | 0.636\ntwo_latents_eth_configural   |    0.997  | 30.180 | 0.016 | 0.984 | 0.613\nthree_latents_eth_configural |    &gt; .999 | 28.432 | 0.012 | 0.989 | 0.568\n\nName                         |   IFI |   RNI | Loglikelihood |   AIC (weights)\n------------------------------------------------------------------------------\none_latent_eth_configural    | 0.955 | 0.955 |    -87850.050 | 1.8e+05 (&lt;.001)\ntwo_latents_eth_configural   | 0.992 | 0.992 |    -87608.642 | 1.8e+05 (&lt;.001)\nthree_latents_eth_configural | 0.995 | 0.995 |    -87586.170 | 1.8e+05 (&gt;.999)\n\nName                         |   BIC (weights) | BIC_adjusted\n-------------------------------------------------------------\none_latent_eth_configural    | 1.8e+05 (&lt;.001) |    1.759e+05\ntwo_latents_eth_configural   | 1.8e+05 (&lt;.001) |    1.755e+05\nthree_latents_eth_configural | 1.8e+05 (&gt;.999) |    1.754e+05\n\n# metric invariance models\n# run CFA models holding factor loadings equal across groups\none_latent_eth_metric &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\ntwo_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\nthree_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal = \"loadings\", data = test_eth))\n\n# compare model performances for metric invariance\ncompare_eth_metric  &lt;- performance::compare_performance(one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric, verbose = FALSE)\n\n# scalar invariance models\n# run CFA models holding factor loadings and intercepts equal across groups\none_latent_eth_scalar &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = c(\"loadings\",\"intercepts\"), data = test_eth))\ntwo_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\nthree_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\n\n# Compare model performances for scalar invariance\ncompare_eth_scalar &lt;- compare_eth_scalar  &lt;- performance::compare_performance(one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar, verbose = FALSE)\n\n\n\n\nConfigural Invariance Results\n\ncompare_eth_configural_key &lt;- compare_eth_configural[, c(\"Name\", \"Chi2\", \"Chi2_df\",\"RFI\", \"NNFI\", \"CFI\",\"GFI\",\"RMSEA\", \"RMSEA_CI_low\", \"RMSEA_CI_high\", \"AIC\", \"BIC\")]\n\nas.data.frame(compare_eth_configural_key)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nChi2\nChi2_df\nRFI\nNNFI\nCFI\nGFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nAIC\nBIC\n\n\n\n\none_latent_eth_configural\n612.80668\n28\n0.9300806\n0.9330615\n0.9553743\n0.9866606\n0.0887694\n0.0827342\n0.0949488\n175784.1\n176060.3\n\n\ntwo_latents_eth_configural\n129.99163\n26\n0.9840274\n0.9871812\n0.9920646\n0.9972721\n0.0388462\n0.0323467\n0.0456186\n175305.3\n175594.6\n\n\nthree_latents_eth_configural\n85.04735\n24\n0.9886791\n0.9918477\n0.9953416\n0.9982007\n0.0309787\n0.0239993\n0.0382454\n175264.3\n175566.8\n\n\n\n\n\nThe table represents the comparison of three multi-group confirmatory factor analysis (CFA) models conducted to test for configural invariance across different ethnic categories (eth_cat). Configural invariance refers to whether the pattern of factor loadings is the same across groups. It’s the most basic form of measurement invariance.\nLooking at the results, we can draw the following conclusions:\n\nChi2 (Chi-square): a lower value suggests a better model fit. In this case, the three looks best\nGFI (Goodness of Fit Index) and AGFI (Adjusted Goodness of Fit Index): These values range from 0 to 1, with values closer to 1 suggesting a better fit. All models are close.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, also called TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The two and three factors models have the highest values.\nRMSEA (Root Mean Square Error of Approximation): lower values are better, with values below 0.05 considered good and up to 0.08 considered acceptable.Only two and three meet this threshold.\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): three is best.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. Again three is the winner.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): The three factor model win’s again.\n\n\n\nAnalysis of the Results:\n\nOne Latent Model: shows the poorest fit among the models with a high RMSEA and the lowest CFI. The model’s Chi-squared value is also significantly high, indicating a substantial misfit with the observed data.\nTwo Latents Model: displays a much improved fit compared to the one latent model, as evident from its much lower Chi-squared value, lower RMSEA, and higher CFI. This suggests that two factors might be necessary to adequately represent the underlying structure in the data.\nThree Latents Model: provides the best fit metrics among the three configurations.\n\n\n\nMetric Equivalence\n\ncompare_eth_metric &lt;- compare_eth_metric[, c(\"Name\", \"Chi2\", \"Chi2_df\",\"RFI\", \"NNFI\", \"CFI\",\"GFI\",\"RMSEA\", \"RMSEA_CI_low\", \"RMSEA_CI_high\", \"AIC\", \"BIC\")]\n\nas.data.frame(compare_eth_metric)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nChi2\nChi2_df\nRFI\nNNFI\nCFI\nGFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nAIC\nBIC\n\n\n\n\none_latent_eth_metric\n631.4181\n34\n0.9406705\n0.9436854\n0.9544120\n0.9862901\n0.0814209\n0.0759304\n0.0870395\n175790.7\n176027.4\n\n\ntwo_latents_eth_metric\n146.8837\n31\n0.9848628\n0.9880193\n0.9911571\n0.9968735\n0.0375549\n0.0315677\n0.0437721\n175312.2\n175568.6\n\n\nthree_latents_eth_metric\n101.6930\n28\n0.9883971\n0.9915649\n0.9943766\n0.9978055\n0.0315116\n0.0250581\n0.0382135\n175273.0\n175549.2\n\n\n\n\n\nThis table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test metric equivalence (also known as weak measurement invariance) across different ethnic categories (eth_cat).\nThe three factor model wins again.\n\n\nScalar Equivalence\n\n# view as html table\n\ncompare_eth_scalar &lt;- compare_eth_scalar[, c(\"Name\", \"Chi2\", \"Chi2_df\",\"RFI\", \"NNFI\", \"CFI\",\"GFI\",\"RMSEA\", \"RMSEA_CI_low\", \"RMSEA_CI_high\", \"AIC\", \"BIC\")]\n\nas.data.frame(compare_eth_scalar)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nChi2\nChi2_df\nRFI\nNNFI\nCFI\nGFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\nAIC\nBIC\n\n\n\n\none_latent_eth_scalar\n669.8690\n40\n0.9464990\n0.9495325\n0.9519357\n0.9854887\n0.0770781\n0.0720058\n0.0822646\n175817.2\n176014.4\n\n\ntwo_latents_eth_scalar\n177.4574\n36\n0.9842521\n0.9874065\n0.9892056\n0.9961614\n0.0385033\n0.0329544\n0.0442492\n175332.8\n175556.3\n\n\nthree_latents_eth_scalar\n128.2153\n32\n0.9871996\n0.9903636\n0.9926580\n0.9971838\n0.0336809\n0.0277002\n0.0398839\n175291.5\n175541.4\n\n\n\n\n\nOverall, it seems that we have good evidence for the three-factor model of Kessler-6, but two-factor is close.\nConsider: when might we prefer a two-factor model? When might we prefer a three-factor model? When might we prefer a one-factor model?\n\n\nConclusion: Understanding the Limits of Association in Factor Models\nThis discussion of measurement invariance across different demographic groups underscores the reliance of factor models on the underlying associations in the data. It is crucial to remember that these models are fundamentally descriptive, not prescriptive; they organize the observed data into a coherent structure based on correlations and assumed relationships among variables.\nNext week, we will consider the causal assumptions inherent in these factor models. Factor analysis assumes that the latent variables (factors) causally influence the observed indicators. This is a stron assumption that can profoundly affect the interpretation of the results. Understanding and critically evaluating our assumptions is important when applying factor analysis to real-world scenarios.\nThe assumption that latent variables cause the observed indicators, rather than merely being associated with them, suggests a directional relationship that can affect decisions made based on the analysis. For instance, if we believe that a latent construct like psychological distress causally influences responses on the K6 scale, interventions might be designed to target this distress directly. However, if the relationship is more complex or bidirectional, such straightforward interventions might not be as effective.\nNext week’s session on causal assumptions will provide a deeper insight into how these assumptions shape our interpretations and the strategies we derive from factor models. This understanding is critical for applying these models appropriately and effectively in psychological research and practice.\n\n\nLab assignment\nUsing the code above, perform MG-CFA on personality measures using the df_nz data set."
  },
  {
    "objectID": "content/11-content.html#appendix-a.-what-is-a-correlation-matrix",
    "href": "content/11-content.html#appendix-a.-what-is-a-correlation-matrix",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Appendix A. What is a Correlation Matrix?",
    "text": "Appendix A. What is a Correlation Matrix?\nA correlation matrix is a square matrix that contains the Pearson correlation coefficients between each pair of variables within a dataset. Each element in the matrix represents the correlation between two variables.\n\nStructure\n\nDimensions: the matrix is p \\times p where p is the number of variables.\nDiagonal Elements: all diagonal elements are 1, because each variable has a perfect correlation with itself.\nOff-Diagonal Elements: These elements, denoted as r_{ij}, are the Pearson correlation coefficients between the i^{th} and j^{th} variables, ranging from -1 to +1.\n\nr_{ij} = 1 indicates a perfect positive linear relationship.\nr_{ij} = -1 indicates a perfect negative linear relationship.\nr_{ij} = 0 indicates no linear relationship.\n\n\n\n\nProperties\n\nSymmetry: the matrix is symmetric around the diagonal, meaning r_{ij} = r_{ji}.\nReal Values: all entries are real numbers.\nBounded Values: values are constrained between -1 and 1, inclusive.\n\n\n\nUse\n\nExploring relationships between variables.\nConducting factor analysis to identify latent factors, as here.\n…\n\n\n# Compute the correlation matrix\nlibrary(margot)\nlibrary(tidyverse)\ndt_only_k6 &lt;- df_nz |&gt; \n  dplyr::filter(wave == 2018) |&gt; \n  dplyr::select(\n    kessler_depressed,\n    kessler_effort,\n    kessler_hopeless,\n    kessler_worthless,\n    kessler_nervous,\n    kessler_restless\n  )\n\ncor_matrix &lt;- cor(dt_only_k6, use = \"pairwise.complete.obs\", method = \"pearson\")\nprint( \n  round( cor_matrix, 2) \n)\n\n                  kessler_depressed kessler_effort kessler_hopeless\nkessler_depressed              1.00           0.50             0.68\nkessler_effort                 0.50           1.00             0.53\nkessler_hopeless               0.68           0.53             1.00\nkessler_worthless              0.68           0.50             0.67\nkessler_nervous                0.42           0.45             0.47\nkessler_restless               0.38           0.46             0.41\n                  kessler_worthless kessler_nervous kessler_restless\nkessler_depressed              0.68            0.42             0.38\nkessler_effort                 0.50            0.45             0.46\nkessler_hopeless               0.67            0.47             0.41\nkessler_worthless              1.00            0.46             0.39\nkessler_nervous                0.46            1.00             0.46\nkessler_restless               0.39            0.46             1.00\n\n\n\nlibrary(tidyr)\n\n#plot\ncor_matrix_df &lt;- as.data.frame(cor_matrix)  # convert matrix to data frame\ncor_matrix_df$variable &lt;- rownames(cor_matrix_df)  # add a new column for rownames\n\nlong_cor_matrix &lt;- tidyr::pivot_longer(cor_matrix_df, \n                                cols = -variable, \n                                names_to = \"comparison\", \n                                values_to = \"correlation\")\n\nggplot(long_cor_matrix, aes(x = variable, y = comparison, fill = correlation)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1,1)) +\n  theme_minimal() +\n  labs(x = \"Variables\", y = \"Variables\", fill = \"Correlation\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nPackages\n\nreport::cite_packages()\n\n  - Barrett M (2021). _ggokabeito: 'Okabe-Ito' Scales for 'ggplot2' and 'ggraph'_. R package version 0.1.0, &lt;https://CRAN.R-project.org/package=ggokabeito&gt;.\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 0.3.1.0 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Firke S (2024). _janitor: Simple Tools for Examining and Cleaning Dirty Data_. R package version 2.2.1, &lt;https://CRAN.R-project.org/package=janitor&gt;.\n  - Greifer N (2024). _WeightIt: Weighting for Covariate Balance in Observational Studies_. R package version 1.3.2, &lt;https://CRAN.R-project.org/package=WeightIt&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 &lt;https://doi.org/10.21105/joss.02445&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Waggoner P, Makowski D (2021). \"performance: An R Package for Assessment, Comparison and Testing of Statistical Models.\" _Journal of Open Source Software_, *6*(60), 3139. doi:10.21105/joss.03139 &lt;https://doi.org/10.21105/joss.03139&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - Patil I, Makowski D, Ben-Shachar M, Wiernik B, Bacher E, Lüdecke D (2022). \"datawizard: An R Package for Easy Data Preparation and Statistical Transformations.\" _Journal of Open Source Software_, *7*(78), 4684. doi:10.21105/joss.04684 &lt;https://doi.org/10.21105/joss.04684&gt;.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. R package version 1.3.0, &lt;https://CRAN.R-project.org/package=patchwork&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Rosseel Y (2012). \"lavaan: An R Package for Structural Equation Modeling.\" _Journal of Statistical Software_, *48*(2), 1-36. doi:10.18637/jss.v048.i02 &lt;https://doi.org/10.18637/jss.v048.i02&gt;.\n  - VanderWeele TJ, Ding P (2011). \"Sensitivity analysis in observational research: introducing the E-value.\" _Annals of Internal Medicine_, *167*(4), 268-274. Mathur MB, VanderWeele TJ (2019). \"Sensitivity analysis for unmeasured confounding in meta-analyses.\" _Journal of the American Statistical Association&gt;_. Smith LH, VanderWeele TJ (2019). \"Bounding bias due to selection.\" _Epidemiology_.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2023). _purrr: Functional Programming Tools_. R package version 1.0.2, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - William Revelle (2024). _psych: Procedures for Psychological, Psychometric, and Personality Research_. Northwestern University, Evanston, Illinois. R package version 2.4.12, &lt;https://CRAN.R-project.org/package=psych&gt;.\n  - Xie Y (2024). _knitr: A General-Purpose Package for Dynamic Report Generation in R_. R package version 1.49, &lt;https://yihui.org/knitr/&gt;. Xie Y (2015). _Dynamic Documents with R and knitr_, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, &lt;https://yihui.org/knitr/&gt;. Xie Y (2014). \"knitr: A Comprehensive Tool for Reproducible Research in R.\" In Stodden V, Leisch F, Peng RD (eds.), _Implementing Reproducible Computational Research_. Chapman and Hall/CRC. ISBN 978-1466561595.\n  - Xie Y (2024). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.54, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;.\n  - Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. R package version 1.4.0, &lt;https://CRAN.R-project.org/package=kableExtra&gt;."
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "",
    "text": "Note\n\n\n\nRequired - (Hernan and Robins 2020) Chapters 1-3 link\nOptional - (Neal 2020) Chapter 1-2 link"
  },
  {
    "objectID": "content/05-content.html#learning-outcomes",
    "href": "content/05-content.html#learning-outcomes",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou will understand why causation is never directly observed.\nYou will understand how experiments address this “causal gap.”\nYou will understand how applying three principles from experimental research allows human scientists to close this “causal gap” when making inferences about a population as a whole — that is, inferences about “marginal effects.”"
  },
  {
    "objectID": "content/05-content.html#opening",
    "href": "content/05-content.html#opening",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Opening",
    "text": "Opening\n\nRobert Frost writes:\n\nTwo roads diverged in a yellow wood,\nAnd sorry I could not travel both\nAnd be one traveler, long I stood\nAnd looked down one as far as I could\nTo where it bent in the undergrowth;\nThen took the other, as just as fair,\nAnd having perhaps the better claim,\nBecause it was grassy and wanted wear;\nThough as for that the passing there\nHad worn them really about the same,\nAnd both that morning equally lay\nIn leaves no step had trodden black.\nOh, I kept the first for another day!\nYet knowing how way leads on to way,\nI doubted if I should ever come back.\nI shall be telling this with a sigh\nSomewhere ages and ages hence:\nTwo roads diverged in a wood, and I—\nI took the one less traveled by,\nAnd that has made all the difference.\n– The Road Not Taken"
  },
  {
    "objectID": "content/05-content.html#introduction-motivating-example",
    "href": "content/05-content.html#introduction-motivating-example",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Introduction: Motivating Example",
    "text": "Introduction: Motivating Example\nConsider the following cross-cultural question:\n\nDoes bilingualism improve cognitive abilities in children?\n\nThere is evidence that bilingual children perform better at cognitive tasks, but is this improvement truly caused by learning more than one language, or is it confounded by other factors (e.g., cultural environment, parental influence)? How can we know? Each child might answer, like the traveller in Frost’s poem:\n“And sorry I could not travel both. And be one traveler \\dots”"
  },
  {
    "objectID": "content/05-content.html#part-1-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "href": "content/05-content.html#part-1-the-fundamental-problem-of-causal-inference-as-a-missing-data-problem",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Part 1: The Fundamental Problem of Causal Inference as a Missing Data Problem",
    "text": "Part 1: The Fundamental Problem of Causal Inference as a Missing Data Problem\nThe fundamental problem of causal inference is that causality is never directly observed.\nLet Y and A denote random variables.\nWe formulate a causal question by asking whether experiencing a exposure A, when this exposure is set to level A = a, would lead to a difference in Y, compared to what would have occurred had the exposure been set to a different level, say A=a' will lead to a difference in outcome Y. For simplicity, we imagine binary exposure such that A = 1 denotes receiving the “bilingual” exposure and A = 0 denotes receiving the “monolingual” exposure. Assume these are the only two exposures of interest:\nLet:\n\nY_i(a = 1) denote the cognitive ability of child i if the child were bilingual (potential outcome when A_i = 1).\nY_i(a = 0) denote the cognitive ability of child i if the child were monolingual (potential outcome when A_i = 0).\n\nWhat does it mean to quantify a causal effect. We may define the individual-level causal effect of bilingualism on cognitive ability for child i as the difference between two states of the world: one for which the child experiences a bilingual exposure and the other for which the child does not. We write this contrast by referring to the potential outcomes under different levels of exposure:\n\n\\text{Causal Effect}_i = Y_i(1) - Y_i(0).\n\nWe say there is a causal effect of the bilingual exposure if\n\nY_i(1) - Y_i(0) \\neq 0.\n\nBecause each child experiences only one exposure condition in reality, we cannot directly compute this difference from any dataset — the missing observation is called the counterfactual:\n\nIf Y_i|A_i = 1 is observed, then Y_i(0)|A_i=1 is counterfactual.\nIf Y_i|A_i = 0 is observed, then Y_i(1)|A_i=1 is counterfactual.\n\n“And sorry I could not travel both / And be one traveler, long I stood \\dots”\nIn short, individuals cannot simultaneously experience both exposure conditions, so one outcome is inevitably missing."
  },
  {
    "objectID": "content/05-content.html#part-2-lab",
    "href": "content/05-content.html#part-2-lab",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Part 2: Lab",
    "text": "Part 2: Lab\n\nFocus\n\nApplication of regression and simulation in R for ATE estimation\n\n\n\nSetting up your r script.\nFirst, reinstall the margot package: https://go-bayes.github.io/margot/\n\n# functions explained here: https://go-bayes.github.io/margot/\n\n# installation\nif (!require(devtools, quietly = TRUE)) {\n  install.packages(\"devtools\")\n  library(devtools)\n}\n\n# reinstall the `margot` packagewith updates\n# devtools::install_github(\"go-bayes/margot\", quietly = TRUE)\n\n# call package\nlibrary(\"margot\")\nlibrary(\"tidyverse\")\nlibrary(\"parameters\")\nlibrary(\"skimr\")\nlibrary(\"haven\")\nlibrary(\"stdReg\")\nlibrary('mice')\nlibrary(\"clarify\")\n\n# uncomment and check simulated data\n# head(df_nz)\n\n\n\nDownload a copy of the data directory from the NZAVS OSF website\nFind it the data directory here: https://osf.io/75snb/\nThe variables in the simulated data df_nz correspond to a subset of the variables in this directory.\n\n\nCheck N\n\n# check total n in data\n# total nzavs participants\nn_total &lt;- skimr::n_unique(df_nz$id)\nprint(n_total)\n\n[1] 20000\n\n\n\n\nGet data into shape for analysis\n\n#### Data wrangling: select columns and transpose the data from long to wide\nlibrary(tidyverse)\n\n# filter the original dataset for these IDs three waves\ndf_nz &lt;- as.data.frame(df_nz)\ndf_nz &lt;- haven::zap_formats(df_nz)\ndf_nz &lt;- haven::zap_label(df_nz)\ndf_nz &lt;- haven::zap_widths(df_nz)\n\nname_exposure &lt;-  \"perfectionism\"\n\n# obtain ids for individuals who participated in 2018 and have no missing baseline exposure\nids_2018 &lt;- df_nz %&gt;%\n   dplyr::filter(year_measured == 1, wave == 2018) %&gt;%\n   dplyr::filter(!is.na(!!sym(name_exposure))) |&gt; # criteria, no missing\n  pull(id)\n\n# obtain ids for individuals who participated in 2019\nids_2019 &lt;- df_nz %&gt;%\n   dplyr::filter(year_measured == 1, wave == 2019) %&gt;%\n   dplyr::filter(!is.na(!!sym(name_exposure))) |&gt; # criteria, no missing\n  pull(id)\n\n# intersect IDs from 2018 and 2019 to ensure participation in both years\nids_2018_2019 &lt;- intersect(ids_2018, ids_2019)\n\n\n# data wrangling\ndat_long &lt;- df_nz |&gt;\n  dplyr::filter(id %in% ids_2018_2019 &\n                  wave %in% c(2018, 2019, 2020)) |&gt;\n  arrange(id, wave) |&gt;\n  select(\n    \"id\",\n    \"wave\",\n    \"year_measured\",\n    \"age\",\n    \"male\",\n    \"born_nz\",\n    \"eth_cat\",\n    #factor(EthCat, labels = c(\"Euro\", \"Maori\", \"Pacific\", \"Asian\")),\n    \"employed\",\n    # Are you currently employed? (this includes self-employment or casual work)\n\n    \"edu\",\n    \"kessler6_sum\",\n    # \"gen_cohort\",\n    \"household_inc\",\n    \"partner\",\n    # 0 = no, 1 = yes\n    \"parent\",\n    # 0 = no, 1 = yes\n    \"political_conservative\",\n    \"hours_exercise\",\n    \"agreeableness\",\n    # Mini-IPIP6 Agreeableness (also modelled as empathy facet)\n    # Sympathize with others' feelings.\n    # Am not interested in other people's problems.\n    # Feel others' emotions.\n    # Am not really interested in others.\n    \"conscientiousness\",\n    # see mini ipip6\n    # Get chores done right away.\n    # Like order.\n    # Make a mess of things.\n    # Often forget to put things back in their proper place.\n    \"extraversion\",\n    # Mini-IPIP6 Extraversion\n    # Am the life of the party.\n    # Don't talk a lot.\n    # Keep in the background.\n    # Talk to a lot of different people at parties.\n    \"honesty_humility\",\n    # see mini ipip6\n    # Would like to be seen driving around in a very expensive car.\n    # Would get a lot of pleasure from owning expensive luxury goods.\n    # Feel entitled to more of everything.\n    # Deserve more things in life.\n    \"openness\",\n    # see mini ipip6\n    # Have a vivid imagination.\n    # Have difficulty understanding abstract ideas.\n    # Do not have a good imagination.\n    # Am not interested in abstract ideas.\n    \"neuroticism\",\n    # see mini ipip6\n    # Have frequent mood swings.\n    # Am relaxed most of the time.\n    # Get upset easily.\n    # Seldom feel blue.\n    \"modesty\",\n    # # see mini ipip6\n    # # I want people to know that I am an important person of high status,\n    # # I am an ordinary person who is no better than others.\n    # # I wouldn’t want people to treat me as though I were superior to them.\n    # # I think that I am entitled to more respect than the average person is\n    #\"w_gend_age_ethnic\",\n    \"sample_weights\",\n    \"neighbourhood_community\",\n    # #I feel a sense of community with others in my local neighbourhood.\n    \"belong\",\n    \"rural_gch_2018_l\",\n    \"support\",\n    # \"support_help\",\n    # # 'There are people I can depend on to help me if I really need it.\n    # \"support_turnto\",\n    # # There is no one I can turn to for guidance in times of stress.\n    # \"support_rnoguidance\",\n    #There is no one I can turn to for guidance in times of stress.\n    \"perfectionism\",\n    \"kessler6_sum\"\n  ) |&gt;\n  mutate(\n    #initialize 'censored'\n    censored = ifelse(lead(year_measured) == 1, 1, 0),\n    \n    # modify 'censored' based on the condition; no need to check for NA here as 'censored' is already defined in the previous step\n    censored =  ifelse(is.na(censored) &\n                         year_measured == 1, 1, censored)\n    \n    # # Apply the case_when condition for setting 'censored' based on 'wave' and the dynamic column specified by 'nzavs_exposure'\n    # censored = case_when(\n    #   # Add this condition to keep previous modifications unless the specific condition is met!is.na(censored) ~ censored,\n    #\n    #   # Then check if 'wave' is 2019 and the specified exposure is NA, adjusting the condition to reflect the accurate logic\n    #   wave == 2019 & !is.na(!!sym(nzavs_exposure)) ~ 1,\n    #\n    #   # Default case if none of the above apply; might not be necessary if all possibilities are covered\n    #   TRUE ~ 0\n    # )\n  ) |&gt;\n  select(-year_measured) |&gt;\n  dplyr::mutate(\n    household_inc_log = log(household_inc + 1),\n    hours_exercise_log = log(hours_exercise + 1)  ) |&gt;\n  dplyr::select(\n    -c(\n      household_inc,\n      hours_exercise\n    )\n  ) |&gt;\n  droplevels() |&gt;\n  # dplyr::rename(sample_weights = w_gend_age_ethnic,\n  #               sample_origin =  sample_origin_names_combined) |&gt;\n  arrange(id, wave) |&gt;\n  droplevels() |&gt;\n  data.frame() |&gt;\n  droplevels() |&gt;\n  arrange(id, wave) |&gt;\n  mutate(\n  rural_gch_2018_l = as.numeric(as.character(rural_gch_2018_l)),\n  #   parent = as.numeric(as.character(parent)),\n  partner = as.numeric(as.character(partner)),\n  born_nz = as.numeric(as.character(born_nz)),\n  censored = as.numeric(as.character(censored)),\n  employed = as.numeric(as.character(employed))\n  ) |&gt;\n  droplevels() |&gt;\n  arrange(id, wave) |&gt;\n  data.frame()\n\n# check n in this sample\nskimr::n_unique(dat_long$id)\n\n[1] 14439\n\n#check\n#head(dat_long)\n\n\n\nConvert data from long to wide and impute baseline values\n\n# baseline variables\nbaseline_vars = c(\"age\", \"male\", \"edu\", \"partner\", \"employed\")\nexposure_var = c(\"perfectionism\", \"censored\") # we will use the censored variable later\noutcome_vars = c(\"kessler6_sum\")\n\n\n# function will add exposure and outcome to baseline\ndat_long_wide &lt;- margot::margot_wide_impute_baseline(dat_long, baseline_vars = baseline_vars, exposure_var = exposure_var, outcome_vars =outcome_vars)\n\n\n iter imp variable\n  1   1  t0_employed  t0_edu  t0_kessler6_sum  t0_partner\n  2   1  t0_employed  t0_edu  t0_kessler6_sum  t0_partner\n  3   1  t0_employed  t0_edu  t0_kessler6_sum  t0_partner\n  4   1  t0_employed  t0_edu  t0_kessler6_sum  t0_partner\n  5   1  t0_employed  t0_edu  t0_kessler6_sum  t0_partner\n\n# check\nhead(dat_long_wide)\n\n  id   t0_age t0_male t0_edu t0_partner t0_employed t0_perfectionism\n1  1 40.31341       1      8          1           1         4.303349\n2  3 47.14449       1      3          1           0         2.660515\n3  4 74.60987       0      6          1           0         4.297503\n4  5 57.46461       0      6          1           1         4.046052\n5  6 50.32225       0      7          1           0         4.030903\n6  8 53.36900       1      6          1           1         1.967654\n  t0_censored t0_kessler6_sum t1_perfectionism t1_censored t2_kessler6_sum\n1           1        3.044502         3.670650           1        5.005375\n2           1        2.962828         3.333177           0              NA\n3           1        1.006293         4.350912           1        3.972263\n4           1        1.986856         2.045632           1        2.038406\n5           1        6.018567         4.350877           1        4.978472\n6           1        2.987620         2.991798           1        2.029845\n\n# standardise vars\ndt &lt;- dat_long_wide |&gt; \n  dplyr::mutate(\n    t0_age_z = scale(t0_age),\n    t0_edu_z = scale(t0_edu),\n    t0_kessler6_sum_z =  scale(t0_kessler6_sum),\n    t0_perfectionism_z = scale(t0_perfectionism),\n    t1_perfectionism_z = scale(t1_perfectionism),\n    t2_kessler6_sum_z = scale(t2_kessler6_sum)) |&gt; \n  data.frame()\n\n# we are not handling missing data well, and this will throw bugs down stream\n# to avoid this problem we remove attributes\ndt&lt;- margot::remove_numeric_attributes(dt)\n\n\n\nUse regression to obtain conditional average treatment effect\n\n# note that we are not handling missing data appropriately here\n\n# take interaction of treatment and baseline vars\nfit_1 &lt;- glm(t2_kessler6_sum_z  ~ t1_perfectionism_z * (t0_age_z +  t0_edu_z + t0_kessler6_sum_z + t0_perfectionism_z), data = dt)\n\n# summarise\nregression_table  &lt;- parameters::model_parameters(fit_1)\n\nThis regression coefficient has a causal interpretation (it is the ate, at the value when all variables in the model are set to zero)\n\nregression_table[2, ]\n\nParameter          | Coefficient |       SE |       95% CI | t(11539) |      p\n------------------------------------------------------------------------------\nt1 perfectionism z |        0.22 | 9.36e-03 | [0.20, 0.24] |    23.49 | &lt; .001\n\n\n\n\nAverage Treatment Effect\nFirst we’ll use the stdReg package:\n\n# fit the ate\nfit_ate &lt;- stdReg::stdGlm(fit_1,data = dt, X = \"t1_perfectionism_z\", x=seq(0,1))\nprint(summary(fit_ate))\n\n# summarise the ate\nate_stdreg_method &lt;- summary(fit_ate,  contrast = \"difference\",  reference = 0)\n\nprint( ate_stdreg_method )\n# graph ate using stdReg method\nplot(ate_stdreg_method)\n\n\nObtain ATE by hand using g-computation\n\n# recall our fit\nfit_1 &lt;- glm(t2_kessler6_sum_z ~ t1_perfectionism_z + t0_age_z + t0_edu_z + t0_kessler6_sum_z + t0_perfectionism_z, data = dt, family = gaussian())\n\n# step 2: create a new dataset where everyone receives the treatment\ndt_treated &lt;- dt\ndt_treated$t1_perfectionism_z &lt;- mean(dt$t1_perfectionism_z)  # setting treatment to the mean can be adjusted as needed\n\n# step 3: predict outcomes under the treatment scenario\ndt_treated$predicted_outcome &lt;- predict(fit_1, newdata = dt_treated, type = \"response\")\n\n# step 4: calculate the mean outcome under this treatment scenario\nmean_outcome_treated &lt;- mean(dt_treated$predicted_outcome)\n\n# repeat Steps 2-4 for control or another treatment level\ndt_control &lt;- dt\ndt_control$t1_perfectionism_z &lt;- 1  # one sd increase in perfectionism\n\ndt_control$predicted_outcome &lt;- predict(fit_1, newdata = dt_control, type = \"response\")\nmean_outcome_control &lt;- mean(dt_control$predicted_outcome)\n\n# step 5: calculate the average treatment effect\nate_gcomp &lt;- round( mean_outcome_treated - mean_outcome_control, 2)\nprint(paste(\"Average Treatment Effect (ATE) of t1_perfectionism_z using g-computation: \", ate_gcomp))\n\n[1] \"Average Treatment Effect (ATE) of t1_perfectionism_z using g-computation:  -0.22\"\n\n\n\n\nNext obtain the ATE and standard errors by simulation using clarify\n\nlibrary(\"clarify\")\n\n# setup simulation with clarify to manipulate the treatment variable and predict outcomes\nset.seed(123)\nsim_coefs &lt;- sim(fit_1)\n\n# compute ate\nsim_est &lt;- sim_ame(sim_coefs, contrast = \"diff\", var =  \"t1_perfectionism_z\")\n\n# summarise\nsummary(sim_est)\n\n                            Estimate 2.5 % 97.5 %\nE[dY/d(t1_perfectionism_z)]    0.222 0.204  0.241"
  },
  {
    "objectID": "content/05-content.html#appendix-a",
    "href": "content/05-content.html#appendix-a",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Appendix A",
    "text": "Appendix A\nConsider that in the causal inference literature, we may write this contrast two potential outcomes under treatment as:\n\n\\text{Causal Effect}_i = Y_i^{a=1} - Y_i^{a=0}\n\nor\n\n\\text{Causal Effect}_i = Y_i^{1} - Y_i^{0}\n\nor:\n\n\\text{Causal Effect}_i = Y_{1} - Y_0\n Where subscripts are dropped. You will soon encounter that many\n\nPackages\n\nreport::cite_packages()\n\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 0.3.3.1 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Greifer N, Worthington S, Iacus S, King G (2024). _clarify: Simulation-Based Inference for Regression Models_. R package version 0.2.1, &lt;https://CRAN.R-project.org/package=clarify&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 &lt;https://doi.org/10.21105/joss.02445&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Sjolander A, Dahlqwist E (2021). _stdReg: Regression Standardization_. R package version 3.4.1, &lt;https://CRAN.R-project.org/package=stdReg&gt;.\n  - van Buuren S, Groothuis-Oudshoorn K (2011). \"mice: Multivariate Imputation by Chained Equations in R.\" _Journal of Statistical Software_, *45*(3), 1-67. doi:10.18637/jss.v045.i03 &lt;https://doi.org/10.18637/jss.v045.i03&gt;.\n  - Waring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, Ellis S (2022). _skimr: Compact and Flexible Summaries of Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=skimr&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, Bryan J, Barrett M, Teucher A (2024). _usethis: Automate Package and Project Setup_. R package version 3.1.0, &lt;https://CRAN.R-project.org/package=usethis&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Hester J, Chang W, Bryan J (2022). _devtools: Tools to Make Developing R Packages Easier_. R package version 2.4.5, &lt;https://CRAN.R-project.org/package=devtools&gt;.\n  - Wickham H, Miller E, Smith D (2023). _haven: Import and Export 'SPSS', 'Stata' and 'SAS' Files_. R package version 2.5.4, &lt;https://CRAN.R-project.org/package=haven&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.56, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/06-quiz.html#slides",
    "href": "content/06-quiz.html#slides",
    "title": "Quiz",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/course-outline.html#week-5---causal-inference-average-treatment-effects",
    "href": "content/course-outline.html#week-5---causal-inference-average-treatment-effects",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Key concepts of Average Treatment Effect (ATE)\nApplication of regression and simulation in R to obtain ATE estimation\n\n\n\n\n\n(Hernan and Robins 2020) Chapters 1-3 link\n\n\n\n\n(Neal 2020) Chapter 1-2 link\n\n\n\n\n\n\nRegression and simulation exercises in R focussed on estimating the ATE"
  },
  {
    "objectID": "content/course-outline.html#week-6---quiztest-in-class-25",
    "href": "content/course-outline.html#week-6---quiztest-in-class-25",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Assessment covering key terms and concepts taught so far"
  },
  {
    "objectID": "index.html#introduction-and-methods-section-for-your-report-take-home-assessment",
    "href": "index.html#introduction-and-methods-section-for-your-report-take-home-assessment",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Take Home Asssessment (~6 Hours) due May 06**:\nBuilds upon the course concepts, emphasising the application of basic conceptual, statistical, and theoretical knowledge as applied to your research question.\nSpecifically, how to formulate a research question and develop a strategy for answering it.\n\nTask\n\nWrite a draft Introduction to your final writing assessment.\nWrite a draft Method section for your writing assessment.\nPrepare an in-class presentation of 10 minutes summaring your study.\nYou are encouraged to use AI.\nHowever, be warned: I will mark hallucinations and errors harshly, so you’ll need to to internalise understanding!\nAlthough I encourage you to use AI, I also encourage you to you write in your own voice. Cultivating your self-expression will make you interesting, and employable.\n(* Note that even the best AI models make mistakes and hallucinate, particularly in causal inference)\n\n\n\n\n\n\nState your question: is your question clearly stated?\nRelevance: have you explained its scientific importance?\nCausality: Is your question causal?\nSubgroup analysis: does your question involve a subgroups (e.g., cultural group)? Which?\nExplain the framework: Have you explained the causal inference framework in a way that is comprehensible to non-specialists?\nEthics/Policy interests have you explained how this question might practically affect people?\nData source: are your data from the NZAVS simulated data set? (if not, consult with me)\nData waves: are your data using three waves?\n\n\n\n\n\nOutcome variable: is your outcome variable Y well-defined?\nMultiple outcomes: do you assess multiple outcomes are are these well-defined?\nOutcome relevance: can you explain how the outcome variable/s relate to your question?\nOutcome type: is your outcome binary and rare? … etc.\nOutcome timing: does your outcome appear after your exposure?\n\n\n\n\n\nExposure variable: is your exposure variable A well-defined?\nMultiple exposures: are there multiple exposures? (If yes, for this study, reassess).\nExposure relevance: have you explained how the exposure variable relates to your question?\nPositivity: can we intervene on the exposure at all levels of the covariates?\nConsistency: can we interpret what it means to intervene on the exposure?\nExchangeability: are there different versions of the exposure conditionally exchangeable given measured baseline confounders?\nExposure type: is the exposure binary or continuous?\nShift intervention: do you contrast static interventions or modified treatment policies?\nExposure timing: does your exposure appear before the outcome? (It should.)\n\n\n\n\n\nBaseline confounders: Have you defined your baseline confounders L?\nJustification: Can you explain how the baseline confounders could affect both A and Y?\nTiming: Are the baseline confounders measured before the exposure?\nInclusion: Is the baseline measure of the exposure and the baseline outcome included in the set of baseline confounders?\nSufficiency: Are the baseline confounders sufficient to ensure balance on the exposure, such that A is independent of Y given L? If not, explain your sensitivity analysis (E-values)\nConfounder type: Are the confounders continuous or binary? If so, consider converting them to z-scores. If they are categorical with three or more levels, do not convert them to z-scores, but rather use one-hot encoding (see lecture 9.)\n\n\n\n\n\nCausal diagram: Have you drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\nMeasurement error: Have you described potential biases from measurement errors?\nTemporal order: Does your DAG have time indicators to ensure correct temporal order?\nTime consistency: Is your DAG organized so that time follows in a consistent direction?\n\n\n\n\n\nWhat is your casual contrast?\nHave you stated your causal contrast clearly?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulations identified: Have you explained how your sample relates to your target populations?\n\n\n\n\n\nCriteria stated: Have you stated the eligibility criteria for the study?\n\n\n\n\n\nDescriptive statistics: have you provided descriptive statistics for demographic information taken at baseline?\nExposure change: Have you described the magnitudes of change in the exposure from baseline to the exposure interval\nReferences: Have you included references for more information about the sample (e.g. the NZAVS website)? I should have.\nDATA ARE SIMULATED: Have you made it clear you are working with simulated data?\n\n\n\n\n\nMissing data checks: Have you checked for missing data?\nMissing data plan: If there are missing data, have you described how you will address the problem? (IPCW, see week 9)\n\n\n\n\n\nApproach decision: G-computation, IPTW, or Doubly-Robust Estimation?\nModel specification: Model Specification?\nMachine Learning: have you explained how machine learning works?\nOutcome Specifics: If the outcome is rare and binary, have you specified logistic regression? If it’s continuous, have I considered converting it to z-scores?\nSensitivity analysis: Have you described your sensitivity analysis (e.g. E-values.)\n\nNote most of these tasks can be ticked off in a sentence or two, but all need to be covered.\nLength: ~ 1,000 - 2,000 words (note it is a draft introduction and draft methods section).\n\n\n\n\n\n\n\n\n\n\nResearch Report Instructions\n\n\n\n\nWe will supply the data.\nLab sessions are designed to support you in this assignment.\nWe assume no statistical background.\n\n\n\n\nTitle: “Causal Inference in Cultural Psychology: Examining Exposure Effects on Dimensions of Well-being Modified by Cultural or Sociodemographic Categories”.\nObjective:\n\nTo quantify the causal effect of a specific exposure on well-being dimensions, modified by sociodemographic categories (born_nz, eth_cat, big_doms, gen_cohort) using the NZAVS longitudinal synthetic dataset.\n\nInstructions:\n\nTheoretical Interest and Research Question:\n\nDescribe the significance of your chosen exposure and its potential impact on the selected outcomes, modified by the cultural or sociodemographic category.\nState the research question clearly.\n\nDirected Acyclic Graph (DAG):\n\nConstruct a DAG illustrating the relationships between exposure, outcomes, sociodemographic category, and potential bias sources. Ensure clarity in labelling.\n\nConfounding Control Strategy:\n\nOutline your strategy for confounding control, justifying the chosen confounders.\n\nMeasurement Biases:\n\nAddress and analyse measurement biases as relevant.\n\nAssumptions and Statistical Models:\n\nDiscuss the assumptions of your causal inference approach and your statistical model, including their limitations.\n\n\nRequirements:\n\nIntroduction: 1,500 words limit.\nConclusion: 1,500 words limit.\nMethod and Results sections should be concise; no specific word limit.\nUse any useful sources, citing appropriately to avoid academic misconduct.\nFollow APA style for citations and references.\nInclude tables/figures as needed.\nSubmit as a single PDF, including R code in an appendix.\nPresentations of Study in Week 12 (or by arrangement.)\n\nEvaluation Criteria:\n\nClarity of theoretical framework, research question, and design.\nValidity of confounding control strategy.\nDiscussion on assumptions and statistical models.\nPresentation quality (10%)\n\nclearly and efficiently presents study\n\n\n\n\n\n\n\nExtensions:\n\nNegotiate a new due date by writing (email) before the mid-term test.\nEvery reasonable request will be accepted (e.g. too many assignments falling in the same week, you want another week to complete.)\n\nPenalties:\n\nLate submissions incur a one full grade-per-week penalty, e.g. if late by one day, B \\to C, one week later, C \\to D.\nOver-length assignments will be penalised.\n\nUnforeseeable Events:\n\nExtensions will require evidence (e.g., medical certificate).\n\n\n\n\n\n\nBring a laptop with R and RStudio installed for data analysis sessions. Contact the instructor if you lack computer access.\nFor in-class tests, bring a writing utensil. Again, electronic devices are not permitted."
  },
  {
    "objectID": "content/07-quiz.html#slides",
    "href": "content/07-quiz.html#slides",
    "title": "Quiz",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "content/06-content.html#if-you-learning-nothing-else-from-this-course",
    "href": "content/06-content.html#if-you-learning-nothing-else-from-this-course",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "If you learning nothing else from this course…",
    "text": "If you learning nothing else from this course…\nTo answer a psychological question we must first ask it."
  },
  {
    "objectID": "content/06-content.html#causal-inference-begins-by-stating-a-counterfactual-contrast",
    "href": "content/06-content.html#causal-inference-begins-by-stating-a-counterfactual-contrast",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "Causal inference begins by stating a counterfactual contrast",
    "text": "Causal inference begins by stating a counterfactual contrast\nIn the simpliest case, we calculate the contrast between two or more “treatments” or equivalently “exposures”\n\n\\text{Average Treatment Effect} = \\mathbb{E}[Y(a^*) -Y(a)]\n\nUnderstanding causal inference is fundamentally about comparing what happened with what could have happened an average under different conditions or treatments in some population, while carefully accounting for factors that might confound or distort the comparison that interests us (i.e. that might give a false answer to the question we asked)."
  },
  {
    "objectID": "content/06-content.html#what-do-the-words-moderation-and-interaction-mean",
    "href": "content/06-content.html#what-do-the-words-moderation-and-interaction-mean",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "What do the words “Moderation” and “Interaction” mean?",
    "text": "What do the words “Moderation” and “Interaction” mean?\nWe here these words used freely in psychology. What do they mean? Causal inference allows us to be precise.\nLet’s set the term moderation to the side, and in its place consider “effect-modification.” Our task today is to use counterfactual notation to distinguish between two concepts:\n\nInteraction\nEffect-modification"
  },
  {
    "objectID": "content/06-content.html#interaction",
    "href": "content/06-content.html#interaction",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "Interaction",
    "text": "Interaction\nAn interaction is a joint intervention\nWhen assessing “interaction” we are interested in the combined effect of two interventions (two “treatments” or “exposures”).\nCall these treatments, A and B. Call the outcome Y.\nRecall that in potential outcomes terminology, we defined the potential outcome under treatment A = \\tilde{a}:\n\nY(\\tilde{a})\n\nAnd we identify Y(\\tilde{a}) from data using the consistency assumption: Y_i(\\tilde{a}) = (Y_i |A_i = \\tilde{a}), assuming that Y_i(\\tilde{a})\\coprod A|L, where, again, L donotes a set of measured covariates sufficient to insure no confounding (no common causes), or equivalently, no open backdoor path between A and Y.\nA joint intervention of two treatments, call them A and B, which may act on Y, requires that we extend our notation:\n\nY(\\tilde{a}, \\tilde{b})\n This is the effect on Y when A is set to \\tilde{a} amd B is set to \\tilde{b}.\nIn addition to requiring Y_i(\\tilde{a}) = (Y_i \\mid A_i = \\tilde{a}), assuming that Y_i(\\tilde{a}) \\coprod A \\mid L, we require that for all i \\in 1\\dots N individuals in the target population, both Y_i(\\tilde{b}) = (Y_i \\mid B_i = \\tilde{b}), and that Y_i(\\tilde{b}) \\coprod B \\mid Q, where Q denotes a set of confounders sufficient to ensure no confounding for the effect of B on Y.\nNotably, the sets L and Q may have overlapping variables, which in set notation we write:\n\nL \\cap Q \\not\\equiv \\emptyset\n\nThat is the union of L and Q does not imply the empty set, which implies that identification may be harder when there are interactions. Statistical estimation is more difficult too, but we’ll ignore that.\n\nHow do we define such a contrast between two interventions of equal status in our model?\nConsider the effect of beliefs in Big Gods (treatment, denoted A) on social complexity (outcome, denotedY), potentially influenced by a culture’s monumental architecture (treatment, denoted B). Suppose we wish to assess the individual and combined effects of A and B on the difference scale. Let assume the treatments are binary variables.\nIn the counterfactual framework, we say there is evidence for interaction only if the following inequality were to hold:\n\\bigg(\\underbrace{\\mathbb{E}[Y(1,1)]}_{\\text{joint exposure}} - \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither exposed}}\\bigg) - \\bigg[ \\bigg(\\underbrace{\\mathbb{E}[Y(1,0)]}_{\\text{only A exposed}} - \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither exposed}}\\bigg) + \\bigg(\\underbrace{\\mathbb{E}[Y(0,1)]}_{\\text{only B exposed}} - \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither exposed}} \\bigg)\\bigg] \\neq 0 \nExpanding the terms inside the brackets:\n \\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(1,0)] + \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)] \nAnd cancelling \\mathbb{E}[Y(0,0)], which appears three times, once with a minus sign and twice with a plus sign, simplifies to:\n \\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)] \nAnd we define interaction on the additive scale:\n \\underbrace{\\mathbb{E}[Y(1,1)]}_{\\text{joint exposure}} - \\underbrace{\\mathbb{E}[Y(1,0)]}_{\\text{only A exposed}} - \\underbrace{\\mathbb{E}[Y(0,1)]}_{\\text{only B exposed}} + \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither exposed}} \\neq 0 \nThe estimand is net interaction effect, after accounting for the individual effects of A and B, as well as the baseline where neither treatment A nor treatment B is given.\nA positive value would indicate evidence for additive interaction. A negative value would indicate evidence for sub-additive interaction. A value near zero would imply no reliable evidence for interaction.\nNote that the order of the terms does not matter:\n\nY(\\tilde{a}, \\tilde{b})\\equiv Y(\\tilde{b}, \\tilde{a})\n\n\nHaving defined the causal estimand, we next consider what would be needed to identify it using data?\nFor this we can write our causal diagram. Figure 1 describes the structure of the identification problem:\n\n\n\n\n\n\n\n\nFigure 1: This diagram presents the individual and joint effects of two exposures, A and B, on outcome Y. We assume that A and B are causally independent. If either exposure affects the other, then we may conduct effect-modification analysis or mediation analysis, but we should avoid causal interaction analysis. The diagram includes confounders L and W. Control for these confounders is necessary to close the backdoor paths that relate each exposure, A and B, to the outcome. Each exposure has equal status in our model: Y(a,b) = Y(b,a). The red path denotes paths of confounding.\n\n\n\n\n\nHere, Figure 1 clarifies the need to evaluate two sources of counfounding, one for each intervention (A and B).\nBy adjusting for L_{0} we obtain an unbiased estimate of the A\\to Y path.\nBy adjusting for Q_{0} we obtain an unbiased estimate of the B\\to Y path. As indicated in Figure 2, we must condition on both L_{0} and Q_{0} to identify causal interaction conceived as a joint interaction.\n\n\n\n\n\n\n\n\n\n\nFigure 2: We adjust for confounding in causal interaction analysis by adjusting for all confounders of the A to Y path as well as all for the B to Y path. The box over the confounders indicates the biasing paths are closed.\n\n\n\n\n\nConsider the following set of plausible confounders:\nL: variables in L that might confound the relationship between beliefs in Big Gods and social complexity:\n\nGeneral religiosity: prevalence of religion might influence both the belief in Big Gods and social complexity.\nCultural norms and values: ingrained social norms and values, not any part of religion.\nHistorical path dependent events: e.g. conquest.\nEducation: affects religious beliefs and the complexity.\nEconomic development: economic conditions might whether people prefer religious practices or resource intensive hobbies and also, social complexity.\nPolitical stability: stable governance can influence both the proliferation of religious practices and the development of complex social structures\nPolitical complexity in the past: huge confounder, must be adjusted for.\nBeliefs in big gods in the past: huge confounder, must be adjusted for.\n\nQ: variables in Q that that might confound the relationship between monumental architecture and social complexity:\n\nTechnology: for constructing of monumental architecture and managing social complexity.\nEconomic resources: as above (overlap)\nHistorical path dependent events: e.g. conquestm as above\nLabour: the availability of builders, slaves, etc.\nGeography: natural resources can affect a society’s ability to build monumental structures and its social development.\nCultural norms and values: ingrained social norms and values, not any part of religion.\nPolitical complexity in the past: huge confounder, must be adjusted for.\nBeliefs in big gods in the past: huge confounder, must be adjusted for.\n\nAs we see:\n\nL \\cap Q \\not\\equiv \\emptyset\n\nHowever, the set of control variables are larger, and note, we are assuming that monumental architecture and big God beliefs do not affect each other."
  },
  {
    "objectID": "content/06-content.html#effect-modification",
    "href": "content/06-content.html#effect-modification",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "Effect Modification",
    "text": "Effect Modification\nIt is often scientifically interesting to consider whether treatment effects vary over levels of other variable without imagining a double intervention. Broadly, we will think of “effect-modification” as a set of related and overlapping concepts pertaining to the understanding variability of a single effect within a population.\n\nHeterogeneous Treatment Effects – a broad concept of effect-modification\nHeterogeneous Treatment Effects (HTE) refer to settings where treatment effects vary across individuals or subgroups within a study population. This variability can arise from differences in baseline characteristics, environmental conditions, histories, …. much of which is unmeasured.\nAppendix A discusses the challenges in identifying HTE from data, but we already know that individual causal effects are not typically identifiable, and so heteroeneous treatment effects, at the limit of individual causal effects, are elusive.\n\n\nConditional Average Treatment Effect (CATE) – a narrower concept of effect-modification.\nWe might be interested estimating average treatment effects within specific levels of measured covariates\nWhen our focus is estimation of the average treatment effect conditional on a specific level of a covariate or set of covariates. Our causal estimand is often written:\n\\text{CATE}(x) = E[Y(1) - Y(0) | X = x]\nwhere X = x, where X denotes the level of a measured covariate or covariates of interest.\nThe question: how does a causal effect vary within the population defined by levels of stratum X=x\n\n\nComparing Effect Heterogenity in Groups.\nWe might be interested in the following causal quantity (estimand), which compares two conditional average treatment effects between levels defined by G = \\tilde{g}:\n{\\gamma} = \\overbrace{\\big( \\mathbb{E}[Y(a^*)|G=g] - \\mathbb{E}[Y(a)|G=g] \\big)}^{{\\delta_g}} - \\overbrace{\\big(\\mathbb{E}[Y(a^*)|G=g^{\\prime}]- \\mathbb{E}[Y(a)|G=g']\\big)}^{{\\delta_{g^{\\prime}}}}\nSuppose A is the treatment, G is the effect-modifier, and Y is the outcome. The analysis of effect-modification assesses whether the effect of A on Y is different across levels of G (i.e., whether the effect of A on Y is different when G = g_1 compared to when G = g_2).\nWhat do we need to identify such effects? Note that G here is not an intervention variable. It might not be coherent to imagine that G can be intervened upon.\nFigure 3 is a causal diagram. Imagine we are intereste in whether the effect of A on Y differs across levels of G. Because we are not interested in the causal effect of G as such, but rather, how the effect of A varies across G, we need not adjust by Q. However, which variables shall we include in our model? What will happen if we assess the G by examining a regression coefficient? Suppose L were not a confounder of A to Y. How then should we interpret our model?\n\n\n\n\n\n\n\n\nFigure 3: Imagine A is an experiment. How shall we investigate effect modification of A on Y by Z? Can you see why regression coefficients will not work?\n\n\n\n\n\nOften, it is useful to obtain causal effects by restricting to one level of a population. Where G denotes a society, consider:\n\nCausal effect within North American societies (G=1): \\delta_{g_1} = \\mathbb{E}[Y(1)|G= g_1] - \\mathbb{E}[Y(0)|G = g_1]\n\nHere, \\delta_{g_2} represents the estimated causal effect of changing the exposure from A = 0 to A = 1 within the North American societies.\n\nCausal effect within Continental societies (G=g_2):\n\\delta_{g2} = \\mathbb{E}[Y(1)|G=g_2] - \\mathbb{E}[Y(0)|G=g_2]\nSimilarly, {\\delta}_{g_2} denotes the estimated causal effect for the Continental societies.\nComparing causal effects across groups:\n{\\gamma} = {\\delta}_{g_1} - {\\delta}_{g_2}\n\nThe quantity {\\gamma} defines the difference in the causal estimands between the two groups. A nonzero \\hat{\\gamma} indicates effect-modification, suggesting that the effect of changing the exposure differs between the two groups. If we were to observe that \\hat{\\gamma} \\neq 0, this would provide evidence for variability in the effect of the exposure on the outcome in different groups. Note that the causal effect for one group might be indistinguishable from zero, and yet we might nevertheless find evidence for effect-modification if the comparison group exhibits reliably different responses from the contrast group that is indistinguishable from zero.\nThat’s it, we now know our causal quantity of interest. Next week, we’ll develop our statistical estimands, estimators, and begin evaluating effect-modification by groups.\n\n\nSummary of what we learned\n\nInteraction targets a joint intervention in a population\nEffect modificiation targets how a single intervention varies by groups within a population\n\nNote, to obtain the group-wise contrasts, we must be able to imagine the groups as belonging to a larger population. When thinking of people, we all belong Later we shall consider how measurement complicates this assumption, even if all humans belong to the same species…"
  },
  {
    "objectID": "content/06-content.html#lab-part-1-setting-up-your-data",
    "href": "content/06-content.html#lab-part-1-setting-up-your-data",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Lab Part 1: Setting up your data",
    "text": "Lab Part 1: Setting up your data\n\nSet up your libraries\n\nlibrary(\"margot\")\nlibrary(\"tidyverse\")\nlibrary(\"ggplot2\")\nlibrary(\"here\")\nlibrary(\"skimr\")\nif (!require(gtsummary)) install.packages(\"gtsummary\")\nif (!require(janitor)) install.packages(\"janitor\")\n\n\n# if you need to update the margot package, uncomment and do this\n# devtools::install_github(\"go-bayes/margot\")\n\n\n\nSet up a path to a folder in your directory\nThis will allow you to save the outputs of models and other information, which will be handy when you are producing your manuscript. Call this push_mods\n\n### Set up a path to a folder in your directory \n\n# create a folder called saved and make a path like this\npush_mods &lt;- here::here('/Users/joseph/v-project\\ Dropbox/data/courses/25-psych-434')\n\n# view it (will be different for you)\npush_mods\n\n[1] \"/Users/joseph/v-project Dropbox/data/courses/25-psych-434\"\n\n# another option\n# saveRDS(object, here::here(push_mods, \"object\"))\n\n\n\nInitial Data Wrangling to select the study sample\n\nWhere to find variable names information\nFind it the data directory here: https://osf.io/75snb/\nAlso see here: https://github.com/go-bayes/templates/tree/main/method\n\n\nThink, what are my eligibility criteria?\n\nParticipated in at baseline\nParticipated at treatment wave\nMay have been lost to follow up at the end of the study\nFull information on the treatment variable (think about this…)\n\n\nlibrary(margot)\nlibrary(tidyverse)\nlibrary(table1)\nlibrary(gtsummary)\n\n# eliminate haven labels\ndf_nz &lt;- as.data.frame(df_nz)\ndf_nz &lt;- haven::zap_formats(df_nz)\ndf_nz &lt;- haven::zap_label(df_nz)\ndf_nz &lt;- haven::zap_widths(df_nz)\n\n# name output folder\npush_mods &lt;- here::here(\"outputs\")\n\n# set exposure name\nname_exposure &lt;-  \"perfectionism\"\n\n# obtain ids for individuals who participated in 2018 and have no missing baseline exposure\nids_2018 &lt;- df_nz %&gt;%\n   dplyr::filter(year_measured == 1, wave == 2018) %&gt;%\n   dplyr::filter(!is.na(!!sym(name_exposure))) |&gt; # criteria, no missing\n  pull(id)\n\n# obtain ids for individuals who participated in 2019\nids_2019 &lt;- df_nz %&gt;%\n   dplyr::filter(year_measured == 1, wave == 2019) %&gt;%\n   dplyr::filter(!is.na(!!sym(name_exposure))) |&gt; # criteria, no missing\n  pull(id)\n\n# intersect IDs from 2018 and 2019 to ensure participation in both years\nids_2018_2019 &lt;- intersect(ids_2018, ids_2019)\n\n# data wrangling\ndat_long &lt;- df_nz |&gt;\n  dplyr::filter(id %in% ids_2018_2019 &\n                  wave %in% c(2018, 2019, 2020)) |&gt;\n  arrange(id, wave) |&gt;\n  select(\n    \"id\",\n    \"wave\",\n    \"year_measured\",\n    \"age\",\n    \"male\",\n    \"born_nz\",\n    \"eth_cat\",\n    #factor(EthCat, labels = c(\"Euro\", \"Maori\", \"Pacific\", \"Asian\")),\n    \"employed\",\n    # Are you currently employed? (this includes self-employment or casual work)\n    \"edu\",\n    # \"gen_cohort\",\n    \"household_inc\",\n    \"partner\",\n    # 0 = no, 1 = yes\n    \"parent\",\n    \"alert_level_combined_lead\", # see bibliography\n    # 0 = no, 1 = yes\n    \"political_conservative\", # see nzavs sheet\n    \"hours_exercise\", # see nzavs sheet\n    \"agreeableness\", \n    # Mini-IPIP6 Agreeableness (also modelled as empathy facet)\n    # Sympathize with others' feelings.\n    # Am not interested in other people's problems.\n    # Feel others' emotions.\n    # Am not really interested in others.\n    \"conscientiousness\",\n    # see mini ipip6\n    # Get chores done right away.\n    # Like order.\n    # Make a mess of things.\n    # Often forget to put things back in their proper place.\n    \"extraversion\",\n    # Mini-IPIP6 Extraversion\n    # Am the life of the party.\n    # Don't talk a lot.\n    # Keep in the background.\n    # Talk to a lot of different people at parties.\n    \"honesty_humility\",\n    # see mini ipip6\n    # Would like to be seen driving around in a very expensive car.\n    # Would get a lot of pleasure from owning expensive luxury goods.\n    # Feel entitled to more of everything.\n    # Deserve more things in life.\n    \"openness\",\n    # see mini ipip6\n    # Have a vivid imagination.\n    # Have difficulty understanding abstract ideas.\n    # Do not have a good imagination.\n    # Am not interested in abstract ideas.\n    \"neuroticism\",\n    # see mini ipip6\n    # Have frequent mood swings.\n    # Am relaxed most of the time.\n    # Get upset easily.\n    # Seldom feel blue.\n    \"modesty\",\n    # # see mini ipip6\n    # # I want people to know that I am an important person of high status,\n    # # I am an ordinary person who is no better than others.\n    # # I wouldn’t want people to treat me as though I were superior to them.\n    # # I think that I am entitled to more respect than the average person is\n    #\"w_gend_age_ethnic\",\n    \"sample_weights\", # see nzavs sheet\n    \"neighbourhood_community\",\n    # #I feel a sense of community with others in my local neighbourhood.\n    \"belong\", # see nzavs sheet\n    \"rural_gch_2018_l\",# see nzavs sheet\n    \"support\",\n    # \"support_help\",\n    # # 'There are people I can depend on to help me if I really need it.\n    # \"support_turnto\",\n    # # There is no one I can turn to for guidance in times of stress.\n    # \"support_rnoguidance\",\n    #There is no one I can turn to for guidance in times of stress.\n    \"perfectionism\",\n    \"religion_religious\",\n    \"kessler_latent_depression\",\n    \"kessler_latent_anxiety\"\n  ) |&gt;\n  mutate(\n    #initialize 'censored'\n    censored = ifelse(lead(year_measured) == 1, 1, 0),\n    \n    # modify 'censored' based on the condition; no need to check for NA here as 'censored' is already defined in the previous step\n    censored =  ifelse(is.na(censored) &\n                         year_measured == 1, 1, censored)\n  ) |&gt;\n  select(-year_measured) |&gt;\n  dplyr::mutate(\n    # rescale these variables, to get all variables on a similar scale\n    # otherwise your models can blow up, or become uninterpretable. \n    household_inc_log = log(household_inc + 1),\n    hours_exercise_log = log(hours_exercise + 1)  ) |&gt;\n  dplyr::select(\n    -c(\n      household_inc,\n      hours_exercise)\n  ) |&gt;\n  droplevels() |&gt;\n  # dplyr::rename(sample_weights = w_gend_age_ethnic,\n  #               sample_origin =  sample_origin_names_combined) |&gt;\n  arrange(id, wave) |&gt;\n  mutate(\n  rural_gch_2018_l = as.numeric(as.character(rural_gch_2018_l)),\n  #   parent = as.numeric(as.character(parent)),\n  partner = as.numeric(as.character(partner)),\n  born_nz = as.numeric(as.character(born_nz)),\n  censored = as.numeric(as.character(censored)),\n  employed = as.numeric(as.character(employed))\n  ) |&gt;\n  droplevels() |&gt;\n  data.frame() |&gt;\n  droplevels() |&gt;\n  arrange(id, wave) |&gt;\n  data.frame()\n\n# check n in this sample\nn_participants &lt;- skimr::n_unique(dat_long$id)\n\n\n# make number pretty\nn_participants&lt;- prettyNum(n_participants,big.mark=\",\")\n\n# save this so that you can use it in your manuscript\nmargot::here_save(n_participants, \"n_participants\")\n\nObject saved to: /Users/joseph/GIT/psych-434-2025/outputs/n_participants.rds\nObject size: 0.00 MB\n👍 Save operation completed successfully!\n\n# try reading \nn_participants_did_it_work_question_mark &lt;- margot::here_read(\"n_participants\")\n\nObject read from: /Users/joseph/GIT/psych-434-2025/outputs/n_participants.rds\nObject size: 0.00 MB\n👍 Read operation completed successfully!\n\n# view\n# n_participants_did_it_work_question_mark\n\nThere are N = 14,439 participants.\n\n\n\nDefine your baseline covariates, treatment (exposure), and outcome\n\n# for example \nbaseline_vars = c(\"age\", \"male\", \"edu\", \"eth_cat\", \"partner\", \"employed\", \"born_nz\", \"neighbourhood_community\", \"household_inc_log\",\n\"parent\", \"religion_religious\", \"rural_gch_2018_l\",\"sample_weights\", \"employed\", \"alert_level_combined_lead\")\n\n# treatment\nexposure_var = c(\"perfectionism\", \"censored\") # we will use the censored variable later\n\n# outcome, can be many\noutcome_vars = c(\"kessler_latent_anxiety\", \"kessler_latent_depression\")\n\n# define waves\nbaseline_wave = \"2018\"\n\n# exposure waves\nexposure_waves = c(\"2018\",\"2019\")\n\n#outcome wave\noutcome_wave = \"2020\"\n\n# exoposure\nname_exposure = \"perfectionism\"\n\n\n\nMake your baseline table\n\n\nMake tables\n\ntables &lt;- margot::margot_summary_tables(dat_long, \nexposure_waves = exposure_waves, \nbaseline_wave = baseline_wave, \noutcome_wave = outcome_wave, \nname_exposure = name_exposure, \nbaseline_vars = baseline_vars, \noutcome_vars = outcome_vars)\n\n\n# save tables\n# margot::here_save_qs(table_exposures, \"table_exposures\", push_mods)"
  },
  {
    "objectID": "content/06-content.html#lab-part-2-simulation-to-clarify-why-the-distribution-of-effect-modifiers-matter-for-estimating-treatment-effects",
    "href": "content/06-content.html#lab-part-2-simulation-to-clarify-why-the-distribution-of-effect-modifiers-matter-for-estimating-treatment-effects",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Lab Part 2: Simulation to Clarify Why The Distribution of Effect Modifiers Matter For Estimating Treatment Effects",
    "text": "Lab Part 2: Simulation to Clarify Why The Distribution of Effect Modifiers Matter For Estimating Treatment Effects\nFirst, we load the stdReg library, which obtains marginal effect estimates by simulating counterfactuals under different levels of treatment (Sjölander 2016). If a treatment is continuous, the levels can be specified.\n\nSjölander, Arvid. 2016. “Regression Standardization with the R Package stdReg.” European Journal of Epidemiology 31 (6): 563–74. https://doi.org/10.1007/s10654-016-0157-3.\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, and Dominique Makowski. 2020. “Extracting, Computing and Exploring the Parameters of Statistical Models Using R.” Journal of Open Source Software 5 (53): 2445. https://doi.org/10.21105/joss.02445.\nWe also load the parameters library, which creates nice tables (Lüdecke et al. 2020).\n\n# to obtain marginal effects\nlibrary(stdReg)\n# to create nice tables\nlibrary(parameters)\n\nNext, we write a function to simulate data for the sample and and target populations.\nWe assume the treatment effect is the same in the sample and target population. We will assume that the coefficient for the effect-modifier and the coefficient for interaction are the same. We assume no unmeasured confounding throughout the study. We assume only selective attrition of one effect modifier such that the baseline population differs from the sample population at the end of the study.\nThat is: the distribution of effect modifiers is the only respect in which the sample will differ from the target population.\nThis function will generate data under a range of scenarios.1\n1 See documentation in the margot package: Bulbulia (2024)\nBulbulia, J. A. 2024. Margot: MARGinal Observational Treatment-Effects. https://doi.org/10.5281/zenodo.10907724.\n\n# function to generate data for the sample and population, \n# along with precise sample weights for the population, there are differences \n# in the distribution of the true effect modifier but no differences in the treatment effect \n# or the effect modification.all that differs between the sample and the population is \n# the distribution of effect-modifiers.\n\n\n# reproducability\nset.seed(123)\n\n# simulate the data -- you can use different parameters\ndata &lt;- margot::simulate_ate_data_with_weights(\n  n_sample = 10000,\n  n_population = 100000,\n  p_z_sample = 0.1,\n  p_z_population = 0.5,\n  beta_a = 1,\n  beta_z = 2.5,\n  noise_sd = 0.5\n)\n\n# view\nstr(data)\n\nList of 2\n $ sample_data    :'data.frame':    10000 obs. of  4 variables:\n  ..$ y_sample: num [1:10000] 1.1854 -0.0834 1.4635 -0.2841 2.6125 ...\n  ..$ a_sample: int [1:10000] 0 0 1 0 0 0 1 1 1 0 ...\n  ..$ z_sample: int [1:10000] 0 0 0 0 1 0 0 0 0 0 ...\n  ..$ weights : num [1:10000] 0.556 0.556 0.556 0.556 5 ...\n $ population_data:'data.frame':    100000 obs. of  3 variables:\n  ..$ y_population: num [1:100000] 0.331 1.982 -0.446 1.117 0.916 ...\n  ..$ a_population: int [1:100000] 1 0 0 1 1 0 0 1 0 0 ...\n  ..$ z_population: int [1:100000] 0 1 0 0 0 0 0 0 0 0 ...\n\n\nOk, we have generated both sample and population data.\nNext, we verify that the distributions of effect modifiers differ in the sample and in the target population:\n\n# obtain the generated data\nsample_data &lt;- data$sample_data\npopulation_data &lt;- data$population_data\n\n\n# check imbalance\ntable(sample_data$z_sample) # type 1 is rare\n\n\n   0    1 \n9055  945 \n\ntable(population_data$z_population) # type 1 is common\n\n\n    0     1 \n49916 50084 \n\n\nGood, the distributions differ. The simulation is working as intended.\nNext, consider the question: “What are the differences in the coefficients that we obtain from the study population at the end of study, as compared with the target population?”\nFirst, we obtain the coefficients for the sample. They are as follows:\n\n# model coefficients sample\nmodel_sample  &lt;-\n  glm(y_sample ~ a_sample * z_sample, data = sample_data)\n\n# summary\nparameters::model_parameters(model_sample, ci_method = \"wald\")\n\nParameter           | Coefficient |       SE |        95% CI | t(9996) |      p\n-------------------------------------------------------------------------------\n(Intercept)         |   -6.89e-03 | 7.38e-03 | [-0.02, 0.01] |   -0.93 | 0.350 \na sample            |        1.01 |     0.01 | [ 0.99, 1.03] |   95.84 | &lt; .001\nz sample            |        2.47 |     0.02 | [ 2.43, 2.52] |  104.09 | &lt; .001\na sample × z sample |        0.51 |     0.03 | [ 0.44, 0.57] |   14.82 | &lt; .001\n\n\nOk, let’s obtain the coefficients for the weighted regression of the sample. Notice that the coefficients are virtually the same:\n\n# model the sample weighted to the population, again note that these coefficients are similar \nmodel_weighted_sample &lt;-\n  glm(y_sample ~  a_sample  * z_sample,\n      data = sample_data,\n      weights = weights)\n\n# summary\nsummary(parameters::model_parameters(model_weighted_sample, ci_method =\n                                       \"wald\"))\n\nParameter           | Coefficient |        95% CI |      p\n----------------------------------------------------------\n(Intercept)         |   -6.89e-03 | [-0.03, 0.01] | 0.480 \na sample            |        1.01 | [ 0.98, 1.04] | &lt; .001\nz sample            |        2.47 | [ 2.45, 2.50] | &lt; .001\na sample × z sample |        0.51 | [ 0.47, 0.55] | &lt; .001\n\nModel: y_sample ~ a_sample * z_sample (10000 Observations)\nSigma: 0.494 (df = 9996)\n\n\nWe might be tempted to infer that weighting wasn’t relevant to the analysis. However, we’ll see that such an interpretation would be a mistake.\nNext, let us obtain model coefficients for the population. Note again there is no difference – only narrower errors owing to the large sample size.\n\n# model coefficients population -- note that these coefficients are very similar. \nmodel_population &lt;-\n  glm(y_population ~ a_population * z_population, data = population_data)\n\nparameters::model_parameters(model_population, ci_method = \"wald\")\n\nParameter                   | Coefficient |       SE |        95% CI | t(99996) |      p\n----------------------------------------------------------------------------------------\n(Intercept)                 |    2.49e-03 | 3.18e-03 | [ 0.00, 0.01] |     0.78 | 0.434 \na population                |        1.00 | 4.49e-03 | [ 0.99, 1.01] |   222.35 | &lt; .001\nz population                |        2.50 | 4.49e-03 | [ 2.49, 2.51] |   556.80 | &lt; .001\na population × z population |        0.50 | 6.35e-03 | [ 0.49, 0.51] |    78.80 | &lt; .001\n\n\nAgain, there is no difference. That is, we find that all model coefficients are practically equivalent. The different distribution of effect modifiers does not result in different coefficient values for the treatment effect, the effect-modifier “effect,” or the interaction of effect modifier and treatment.\nConsider why this is the case: in a large sample where the causal effects are invariant – as we have simulated them to be – we will have good replication in the effect modifiers within the sample, so our statistical model can recover the coefficients for the population – no problem.\nHowever, in causal inference, we are interested in obtaining the marginal effect of the treatment. That is, we seek an estimate for the counterfactual contrast in which everyone in a pre-specified population was subject to one level of treatment compared with a counterfactual condition in which everyone in a population was subject to another level of the same treatment. When the sample population differs in the distribution of effect modifiers from the target population effect, the marginal effect estimates will typically differ.\nTo see this, we use the stdReg package to recover marginal effect estimates, comparing (1) the sample ATE, (2) the true oracle ATE for the population, and (3) the weighted sample ATE. We will use the outputs of the same models above. The only difference is that we will calculate marginal effects from these outputs. We will contrast a difference from an intervention in which everyone receives treatment = 0 with one in which everyone receives treatment = 1, however, this choice is arbitrary, and the general lessons apply irrespective of the estimand.\nFirst, consider this ATE for the sample population.\n\n# What inference do we draw?  We cannot say the models are unbiased for the marginal effect estimates. \n# regression standardisation \nlibrary(stdReg) # to obtain marginal effects \n\n\n# obtain sample ate\nfit_std_sample &lt;-\n  stdReg::stdGlm(model_sample, data = sample_data, X = \"a_sample\")\n\n# summary\nsummary(fit_std_sample,\n        contrast = \"difference\",\n        reference = 0)\n\n\nFormula: y_sample ~ a_sample * z_sample\nFamily: gaussian \nLink function: identity \nExposure:  a_sample \nReference level:  a_sample = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0     0.00     0.0000       0.00       0.00\n1     1.06     0.0101       1.04       1.08\n\n\nThe treatment effect is given as a 1.06 unit change in the outcome across the sample population, with a confidence interval from 1.04 to 1.08.\nNext, we obtain the true (oracle) treatment effect for the population under the same intervention.\n\n## note the population effect is different\n\n#obtain true ate\nfit_std_population &lt;-\n  stdReg::stdGlm(model_population, data = population_data, X = \"a_population\")\n\n# summary\nsummary(fit_std_population,\n        contrast = \"difference\",\n        reference = 0)\n\n\nFormula: y_population ~ a_population * z_population\nFamily: gaussian \nLink function: identity \nExposure:  a_population \nReference level:  a_population = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0     0.00    0.00000       0.00       0.00\n1     1.25    0.00327       1.24       1.26\n\n\nNote, the true treatment effect is a 1.25 unit change in the population, with a confidence bound between 1.24 and 1.26. This is well outside the ATE that we obtain from the sample population!\nNext, consider the ATE in the weighted regression, where the sample was weighted to the target population’s true distribution of effect modifiers.\n\n## next try weights adjusted ate where we correctly assign population weights to the sample\nfit_std_weighted_sample_weights &lt;- stdReg::stdGlm( model_weighted_sample, \n    data = sample_data, \n    X = \"a_sample\")\n\n# this gives us the right answer\nsummary(fit_std_weighted_sample_weights, \n    contrast = \"difference\", \n    reference = 0)\n\n\nFormula: y_sample ~ a_sample * z_sample\nFamily: gaussian \nLink function: identity \nExposure:  a_sample \nReference level:  a_sample = 0 \nContrast:  difference \n\n  Estimate Std. Error lower 0.95 upper 0.95\n0     0.00     0.0000       0.00       0.00\n1     1.25     0.0172       1.22       1.29\n\n# moral of the story. when we marginalise over the entire sample we need to weight estimates to the target population. \n\nWe find that we obtain the population-level causal effect estimate with accurate coverage by weighting the sample to the target population. So with appropriate weights, our results generalise from the sample to the target population."
  },
  {
    "objectID": "content/06-content.html#lessons-from-lab-2",
    "href": "content/06-content.html#lessons-from-lab-2",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Lessons from Lab 2",
    "text": "Lessons from Lab 2\n\nRegression coefficients do not clarify the problem of sample/target population mismatch – or selection bias as discussed here.\nThe correct advice to investigators is that they should not rely on regression coefficients when evaluating the biases that arise from sample attrition. This advice applies to both methods that the authors use to investigate threats of bias. That is, to implement this advice, the authors must first take it.\nGenerally, observed data are insufficient for assessing threats. Observed data do not clarify structural sources of bias, nor do they clarify effect-modification across the full counterfactual data conditions, in which (1) all receive the treatment and (2) all do not receive the treatment (at the same level).\nTo properly assess bias, one would need access to the counterfactual outcome—what would have happened to the missing participants had they not been lost to follow-up or had they responded. Again, the join distributions over “full data” are inherently unobservable (Van Der Laan and Rose 2011).\nIn simple settings like the one we just simulated, we may address the gap between the sample and target population using methods such as modelling the censoring (e.g., censoring weighting). However, we never know what setting we are in or whether it is simple—such modelling must be handled with care. There is a large and growing epidemiology literature on this topic (see, for example, Li, Miao, and Tchetgen Tchetgen (2023)). \n\n\nVan Der Laan, Mark J., and Sherri Rose. 2011. Targeted Learning: Causal Inference for Observational and Experimental Data. Springer Series in Statistics. New York, NY: Springer. https://link.springer.com/10.1007/978-1-4419-9782-1.\n\nLi, Wei, Wang Miao, and Eric Tchetgen Tchetgen. 2023. “Non-Parametric Inference about Mean Functionals of Non-Ignorable Non-Response Data Without Identifying the Joint Distribution.” Journal of the Royal Statistical Society Series B: Statistical Methodology 85 (3): 913–35."
  },
  {
    "objectID": "content/07-quiz.html",
    "href": "content/07-quiz.html",
    "title": "In-Class Test",
    "section": "",
    "text": "Last Year’s In-Class Test\n\n\n\n\n\n\n\n\n\n\nFigure 1: Our variable naming conventions. This figure is adapted from (Bulbulia 2024)\n\n\n\n\n\n\n\n\n\nReferences\n\nBulbulia, J. A. 2024. “Methods in Causal Inference Part 1: Causal Diagrams and Confounding.” Evolutionary Human Sciences 6: e40. https://doi.org/10.1017/ehs.2024.35.\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "content/index.html#slides",
    "href": "content/index.html#slides",
    "title": "Preliminaries",
    "section": "Slides",
    "text": "Slides\nPREVIEW\n\n\n\nOpen in browser here"
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Hands On Working With Quarto Manuscript",
    "section": "",
    "text": "Note\n\n\n\nRequired Download Quarto here: - Use the prelease version: https://quarto.org/docs/download/ Optional - (Bulbulia 2024) link - (Hoffman et al. 2023) link\n\n\n\nHoffman, Katherine L., Diego Salazar-Barreto, Kara E. Rudolph, and Iván Díaz. 2023. “Introducing Longitudinal Modified Treatment Policies: A Unified Framework for Studying Complex Exposures,” April. https://doi.org/10.48550/arXiv.2304.09460.\n\nBulbulia, J. A. 2024. “A Practical Guide to Causal Inference in Three-Wave Panel Studies.” OSF. https://doi.org/10.31234/osf.io/uyg3d.\n\n\n\n\n\n\nKey concepts\n\n\n\n\nWriting up your manuscript\n\n\n\n\n\n\n\n\n\nFor the lab, download the script\n\n\n\n\nWe will go through this script step-by-step.\n\n\n\n\n\n\nCreate a new Rstudio project\nModify these scripts\n\n\nDownload full lab scripts 0\n\n\nDownload full lab scripts 1\n\n\nDownload full lab scripts 2\n\n\nDownload full lab scripts 3\n\n\n\n\n\nDownload the quarto manuscript template and store in your R directory: link to manuscript template\n\n\n\n\nDownload the following tex file and save it to your R directory: title preamble\nDownload the following csl file and save it to your R directory: csl preamble\nMake sure you can install all libraries required of the manuscript template.\nCome to the seminar prepared to work through the analysis"
  },
  {
    "objectID": "content/10-content.html#part-1-quarto-manuscripts",
    "href": "content/10-content.html#part-1-quarto-manuscripts",
    "title": "Hands On Working With Quarto Manuscript",
    "section": "",
    "text": "Note\n\n\n\nRequired Download Quarto here: - Use the prelease version: https://quarto.org/docs/download/ Optional - (Bulbulia 2024) link - (Hoffman et al. 2023) link\n\n\n\nHoffman, Katherine L., Diego Salazar-Barreto, Kara E. Rudolph, and Iván Díaz. 2023. “Introducing Longitudinal Modified Treatment Policies: A Unified Framework for Studying Complex Exposures,” April. https://doi.org/10.48550/arXiv.2304.09460.\n\nBulbulia, J. A. 2024. “A Practical Guide to Causal Inference in Three-Wave Panel Studies.” OSF. https://doi.org/10.31234/osf.io/uyg3d.\n\n\n\n\n\n\nKey concepts\n\n\n\n\nWriting up your manuscript\n\n\n\n\n\n\n\n\n\nFor the lab, download the script\n\n\n\n\nWe will go through this script step-by-step.\n\n\n\n\n\n\nCreate a new Rstudio project\nModify these scripts\n\n\nDownload full lab scripts 0\n\n\nDownload full lab scripts 1\n\n\nDownload full lab scripts 2\n\n\nDownload full lab scripts 3\n\n\n\n\n\nDownload the quarto manuscript template and store in your R directory: link to manuscript template\n\n\n\n\nDownload the following tex file and save it to your R directory: title preamble\nDownload the following csl file and save it to your R directory: csl preamble\nMake sure you can install all libraries required of the manuscript template.\nCome to the seminar prepared to work through the analysis"
  },
  {
    "objectID": "content/11-content.html#part-2.-how-traditional-measurement-theory-fails",
    "href": "content/11-content.html#part-2.-how-traditional-measurement-theory-fails",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "PART 2. How Traditional Measurement Theory Fails",
    "text": "PART 2. How Traditional Measurement Theory Fails\n\n\n\n\n\n\nNote\n\n\n\nRequired\n\n(Tyler J. VanderWeele 2022) link\n\nSuggested\n\n(J. A. Harkness, Van de Vijver, and Johnson 2003) link\n\n\n\n\nHarkness, Janet A, Fons JR Van de Vijver, and Timothy P Johnson. 2003. “Questionnaire Design in Comparative Research.” Cross-Cultural Survey Methods, 19–34.\n\n\n\n\n\n\nKey concepts\n\n\n\n\nThe Formative Model in Factor Analysis\nThe Reflective Model in Factor Analysis\nHow to use causal Diagrams to evaluate assumptions.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nUnderstanding causal assumptions of measurement theory\nGuidance on your final assessment."
  },
  {
    "objectID": "content/11-content.html#overview-1",
    "href": "content/11-content.html#overview-1",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Overview",
    "text": "Overview\nBy the end of this lecture you will:\n\nUnderstand the causal assumptions implied by the factor analytic interpretation of the formative and reflective models.\nBe able to distinguish between statistical and structural interpretations of these models.\nUnderstand why Vanderweele thinks consistent causal estimation is possible using the theory of multiple versions of treatments for constructs with multiple indicators"
  },
  {
    "objectID": "content/11-content.html#two-ways-of-thinking-about-measurement-in-psychometric-research.",
    "href": "content/11-content.html#two-ways-of-thinking-about-measurement-in-psychometric-research.",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Two ways of thinking about measurement in psychometric research.",
    "text": "Two ways of thinking about measurement in psychometric research.\nIn psychometric research, formative and reflective models describe the relationship between latent variables and their respective indicators. VanderWeele discusses this in the assigned reading for this week (Tyler J. VanderWeele 2022).\n\nVanderWeele, Tyler J. 2022. “Constructed Measures and Causal Inference: Towards a New Model of Measurement for Psychosocial Constructs.” Epidemiology 33 (1): 141. https://doi.org/10.1097/EDE.0000000000001434.\n\nReflective Model (Factor Analysis)\nIn a reflective measurement model, also known as an effect indicator model, the latent variable is understood to cause the observed variables. In this model, changes in the latent variable cause changes in the observed variables. Each indicator (observed variable) is a ‘reflection’ of the latent variable. In other words, they are effects or manifestations of the latent variable. These relations are presented in Figure 1.\nThe reflective model may be expressed:\nX_i = \\lambda_i \\eta + \\varepsilon_i\nHere, X_i is an observed variable (indicator), \\lambda_i is the factor loading for X_i, \\eta is the latent variable, and \\varepsilon_i is the error term associated with X_i. It is assumed that all the indicators are interchangeable and have a common cause, which is the latent variable \\eta.\nIn the conventional approach of factor analysis, the assumption is that a common latent variable is responsible for the correlation seen among the indicators. Thus, any fluctuation in the latent variable should immediately lead to similar changes in the indicators.These assumptions are presented in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Reflective model: assume univariate latent variable η giving rise to indicators X1…X3. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\nThe Formative Model (Factor Analysis)\nIn a formative measurement model, the observed variables are seen as causing or determining the latent variable. Here again, there is a single latent variable. However this latent variable is taken to be an effect of the underlying indicators. These relations are presented in Figure 2.\nThe formative model may be expressed:\n\\eta = \\sum_i\\lambda_i X_i + \\varepsilon\nIn this equation, \\eta is the latent variable, \\lambda_i is the weight for X_i (the observed variable), and \\varepsilon is the error term. The latent variable \\eta is a composite of the observed variables X_i.\nIn the context of a formative model, correlation or interchangeability between indicators is not required. Each indicator contributes distinctively to the latent variable. As such, a modification in one indicator doesn’t automatically imply a corresponding change in the other indicators.\n\n\n\n\n\n\n\n\nFigure 2: Formative model:: assume univariate latent variable from which the indicators X1…X3 give rise. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
  },
  {
    "objectID": "content/11-content.html#structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis",
    "href": "content/11-content.html#structural-interpretation-of-the-formative-model-and-reflective-models-factor-analysis",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Structural Interpretation of the formative model and reflective models (Factor Analysis)",
    "text": "Structural Interpretation of the formative model and reflective models (Factor Analysis)\n\nHowever, this analysis of reflective and formative models assumed that the latent η was causally efficacious. This may not be the case (VanderWeele 2022)\n\nVanderWeele distinguishes between statistical and structural interpretations of the equations preesented above.\n\nStatistical Model: a mathematical construct that shows how observable variables, also known as indicators, are related to latent or unseen variables. These are presented in the equations above\nStructural Model: A structural model refers to the causal assumptions or hypotheses about the relationships among variables in a statistical model. The assumptions of the factor analytic tradition are presented in Figure 2 and Figure 1 are structural models.\n\nWe have seen that the reflective model statistically implies that the observed variables (indicators) are reflections or manifestations of the latent variable, expressed as X_i = \\lambda_i \\eta + \\varepsilon_i. However, the factor analytic tradition makes the additional structural assumption that a univariate latent variable is causally efficacious and influences the observed variables, as in: Figure 3 (a).\nWe have also seen that the formative model statistically implies that the latent variable is formed or influenced by the observed variables, expressed as \\eta = \\sum_i\\lambda_i X_i + \\varepsilon. However, the factor analytic tradition makes the additional assumption that the observed variables give rise to a univariate latent variable, as in Figure 3 (b).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Reflective Model: causal assumptions. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Formative model: causal assumptions. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\nThe reflective model implies X_i = \\lambda_i \\eta + \\varepsilon_i, which factor analysts take to imply Figure 3 (a).\n\n\n\n\nFigure 3: The formative model implies \\eta = \\sum_i\\lambda_i X_i + \\varepsilon, which factor analysts take to imply Figure 3 (b)."
  },
  {
    "objectID": "content/11-content.html#problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.",
    "href": "content/11-content.html#problems-with-the-structural-interpretations-of-the-reflective-and-formative-factor-models.",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Problems with the structural interpretations of the reflective and formative factor models.",
    "text": "Problems with the structural interpretations of the reflective and formative factor models.\nWhile the statistical model X_i = \\lambda_i \\eta + \\varepsilon_i aligns with Figure 3 (a), it also alings with Figure 4. Cross-sectional data, unfortunately, do not provide enough information to discern between these different structural interpretations.\nSimilarly, the statistical model \\eta = \\sum_i\\lambda_i X_i + \\varepsilon agrees with Figure 3 (b) but it also agrees with@fig-dag-reflectiveassumptions-compatible_again. Here too, cross-sectional data cannot decide between these two potential structural interpretations.\nThere are other, compatible structural interprestations as well. The formative and reflective conceptions of factor analysis are compatible with indicators having causal effects as shown in (fig_dag_multivariate_reality_again?). They are also compatible with a multivariate reality giving rise to multiple indicators as shown in Figure 6.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Formative model is compatible with indicators causing outcome.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Reflective model is compatible with indicators causing the outcome. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate reality gives rise to the indicators, from which we draw our measures. Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Although we take our constructs, A, to be functions of indicators, X, such that, perhaps only one or several of the indicators are efficacious.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\n\n\n\nVanderWeele’s key observation is this:\n\n\n\n\nWhile cross-sectional data can provide insights into the relationships between variables, they cannot conclusively determine the causal direction of these relationships.\n\n\n\n\nThis results is worrying. The structural assumptions of factor analysis underpin nearly all psychological research. If the cross-sectional data used to derive factor structures cannot decide whether the structural interpretations of factor models are accurate, where does that leave us?\n\n\n\n\nMore worrying still, VanderWeele discusses several longitudinal tests for structural interpretations of univariate latent variables that do not pass.\n\n\n\n\nWhere does that leave us? In psychology we have heard about a replication crisis. We might describe the reliance on factor models as an aspect of a much larger, and more worrying “causal crisis” (Bulbulia 2023)\n\nBulbulia, Joseph A. 2023. “A Workflow for Causal Inference in Cross-Cultural Psychology.” Religion, Brain & Behavior 13 (3): 291–306. https://doi.org/10.1080/2153599X.2022.2070245."
  },
  {
    "objectID": "content/11-content.html#review-of-the-theory-of-multiple-versions-of-treatment",
    "href": "content/11-content.html#review-of-the-theory-of-multiple-versions-of-treatment",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Review of the theory of multiple versions of treatment",
    "text": "Review of the theory of multiple versions of treatment\n\n\n\n\n\nMultiple Versions of treatment. Heae, A is regarded to bbe a coarseneed version of K\n\n\n\n\nPerhaps not all is lost. VanderWeele looks to the theory of multiple versions of treatment for solace.\nRecall, a causal effect is defined as the difference in the expected potential outcome when everyone is exposed (perhaps contrary to fact) to one level of a treatment, conditional on their levels of a confounder, with the expected potential outcome when everyone is exposed to a a different level of a treatement (perhaps contrary to fact), conditional on their levels of a counfounder.\n \\delta = \\sum_l \\left( \\mathbb{E}[Y|A=a,l] - \\mathbb{E}[Y|A=a^*,l] \\right) P(l)\nwhere \\delta is the causal estimand on the difference scale (\\mathbb{E}[Y^0 - Y^0]).\nIn causal inference, the multiple versions of treatment theory allows us to handle situations where the treatment isn’t uniform, but instead has several variations. Each variation of the treatment, or “version”, can have a different impact on the outcome. Consistency is not violated because it is redefined: for each version of the treatment, the outcome under that version is equal to the observed outcome when that version is received. Put differently we may think of the indicator A as corresponding to many version of the true treament K. Where conditional independence holds such that there is a absence of confounding for the effect of K on Y given L, we have: Y_k \\coprod A|K,L. This states conditional on L, A gives no information about Y once K and L are accounted for. When Y = Y_k if K = k and Y_k is independent of K, condition on L, then A may be thought of as a coarsened indicator of K, as shown in (fig_dag_multiple_version_treatment_dag?). We may estimate consistent causal effects where:\n \\delta = \\sum_{k,l} \\mathbb{E}[Y_k|l] P(k|a,l) P(l) - \\sum_{k,l} \\mathbb{E}[Y_k|l] P(k|a^*,l) P(l)\nThe scenario represents a hypothetical randomised trial where within strata of covariates L, individuals in one group receive a treatment K version randomly assigned from the distribution of K distribution (A = 1, L = l) sub-population. Meanwhile, individuals in the other group receive a randomly assigned K version from (A = 0, L = l)\nThis theory finds its utility in practical scenarios where treatments seldom resemble each other – we discussed the example of obesity last week (see: (Tyler J. VanderWeele and Hernan 2013)).\n\nVanderWeele, Tyler J, and Miguel A Hernan. 2013. “Causal Inference Under Multiple Versions of Treatment.” Journal of Causal Inference 1 (1): 1–20.\n\nReflective and formative measurement models may be approached as multiple versions of treatment\nVanderweele applies the following substitution:\n\\delta = \\sum_{\\eta,l} \\mathbb{E}[Y_\\eta|l] P(\\eta|A=a+1,l) P(l) - \\sum_{\\eta,l} \\mathbb{E}[Y_\\eta|l] P(\\eta|A=a,l) P(l)\nSpecifically, we substitue K with \\eta from the previous section, and compare the measurement response A = a + 1 with A = a. We discover that if the influence of \\eta on Y is not confounded given L, then the multiple versions of reality consistent with the reflective and formative statistical models of reality will not lead to biased estimation. \\delta retains its interpretability as a comparison in a hypothetical randomised trial in which the distribution of coarsened measures of \\eta_A are balanced within levels of the treatment, conditional on \\eta_L.\nThis connection between measurement and the multiple versions of treatment framework provides a hope for consistent causal inference varying reliabilities of measurement.\nHowever, as with the theory of multiple treatments, we might not known how to interpret our results because we don’t know the true relationships between our measured indicators and underlying reality.\nHow can we do better?\n\n\n\n\n\n\n\n\nFigure 7: Multiple Versions of treatment applied to measuremen.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434"
  },
  {
    "objectID": "content/11-content.html#vanderweeles-model-of-reality",
    "href": "content/11-content.html#vanderweeles-model-of-reality",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "VanderWeele’s model of reality",
    "text": "VanderWeele’s model of reality\nVanderWeele’s article concludes as follows:\n\nA preliminary outline of a more adequate approach to the construction and use of psychosocial measures might thus be summarized by the following propositions, that I have argued for in this article: (1) Traditional univariate reflective and formative models do not adequately capture the relations between the underlying causally relevant phenomena and our indicators and measures. (2) The causally relevant constituents of reality related to our constructs are almost always multidimensional, giving rise both to our indicators from which we construct measures, and also to our language and concepts, from which we can more precisely define constructs. (3) In measure construction, we ought to always specify a definition of the underlying construct, from which items are derived, and by which analytic relations of the items to the definition are made clear. (4) The presumption of a structural univariate reflective model impairs measure construction, evaluation, and use. (5) If a structural interpretation of a univariate reflective factor model is being proposed this should be formally tested, not presumed; factor analysis is not sufficient for assessing the relevant evidence. (6) Even when the causally relevant constituents of reality are multidimensional, and a univariate measure is used, we can still interpret associations with outcomes using theory for multiple versions of treatment, though the interpretation is obscured when we do not have a clear sense of what the causally relevant constituents are. (7) When data permit, examining associations item-by-item, or with conceptually related item sets, may give insight into the various facets of the construct.\n\n\nA new integrated theory of measurement for psychosocial constructs is needed in light of these points – one that better respects the relations between our constructs, items, indicators, measures, and the underlying causally relevant phenomena. (VanderWeele 2022)\n\n\n\n\n\n\n\n\n\nFigure 8: Multivariate reality gives rise to the latent variables.Figure adapted from VanderWeele: doi: 10.1097/EDE.0000000000001434\n\n\n\n\n\nThis seems to me sensible. However, Figure 8 this is not a causal graph. The arrows to not clearly represent causal relations. It leaves me unclear about what to practically do.\nLet’s return to the three wave many-outcomes model described in previous weeks. How should we revise this model in light of measurement theory?"
  },
  {
    "objectID": "content/11-content.html#how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement",
    "href": "content/11-content.html#how-theory-of-dependent-and-directed-measurement-error-might-be-usefully-employed-to-develop-a-pragmatic-responses-to-construct-measurement",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "How theory of dependent and directed measurement error might be usefully employed to develop a pragmatic responses to construct measurement",
    "text": "How theory of dependent and directed measurement error might be usefully employed to develop a pragmatic responses to construct measurement\nBy now you are all familiar with The New Zealand Attitudes and Values Study (NZAVS),which is a national probability survey collects a wide range of information, including data on distress, exercise habits, and cultural backgrounds.\n\n\n\n\n\n\n\n\nFigure 9: Uncorrelated non-differential measurement error does not bias estimates under the null. Note, however, we assume that L is measured with sufficient precision to block the path from A_eta –&gt; L_eta –&gt; Y_eta, which, otherwise, we would assume to be open.\n\n\n\n\n\nConsider a study that seeks to use this dataset to investigate the effect of regular exercise on psychological distress. In contrast to previous graphs, let us allow for latent reality to affect our measurements, as well as the discrepencies between our measurements and true underlying reality. We shall use Figure 9 as our initial guide.\nWe represent the true exercise by \\eta_A. We represent true psychological distress by \\eta_Y. Let \\eta_L denote a persons true workload, and assume that this state of work affects both levels of excercise and psychological distress.\nTo bring the model into contact with measurement theory, Let us describe measurements of these latent true underlying realities as functions of multiple indicators: L_{f(X_1\\dots X_n)}, A_{f(X_1\\dots X_n)}, and A_{f(X_1\\dots X_n)}. These constructs are measured realisations of the underlying true states. We assume that the true states of these variables affect their corresponding measured states, and so draw arrows from \\eta_L\\rightarrow{L_{f(X_1\\dots X_n)}}, \\eta_A\\rightarrow{A_{f(X_1\\dots X_n)}}, \\eta_Y\\rightarrow{Y_{f(X_1\\dots X_n)}}.\nWe also assume unmeasured sources of error that affect the measurements: U_{L} \\rightarrow L_{f(X_1\\dots X_n)}, U_{A} \\rightarrow A_{f(X_1\\dots X_n)}, and U_{Y} \\rightarrow Y_{f(X_1\\dots X_n)}. That is, we allow that our measured indicators may “see as through a mirror, in darkness,” the underlying true reality they hope to capture (Corinthians 13:12). We use U_{L}, U_{A} and U_{Y} to denote the unmeasured sources of error in the measured indicators. These are the unknown, and perhaps unknowable, darkness and mirror.\nAllow that the true underlying reality represented by the \\eta_{var} may be multivariate. Similarly, allow the true underlying reality represented by \\U_{var} is multivariate.\nWe now have a causal diagramme that more precisely captures VanderWeele’s thinking as presented in Figure 8. In our Figure 9, we have fleshed out \\mathcal{R} in a way that may include natural language concepts and scientific language, or constructs, as latent realities and latent unmeasured sources of error in our constructs.\nThe utility of describing the measurement dynamics using causal graphs is apparrent. We can understand that the measured states, once conditioned upon create collider biases which opens path between the unmeasured sources of error and the true underlying state that gives rise to our measurements. This is depicted by a the arrows U_{var} and from \\eta_{var} into each var_{f(X1, X2,\\dots X_n)}\nNotice: where true unmeasured (multivariate) psycho-physical states are related to true unmeasured (multivariate) sources of error in the measurement of those states, the very act of measurement opens pathways to confounding.\nIf for each measured construct var_{f(X1, X2,\\dots X_n)}, the sources of error U_{var} and the unmeasured consituents of reality that give rise to our measures \\eta_{var} are uncorrelated with other variables U\\prime_{var} and from \\eta\\prime_{var} and var\\prime_{f(X1, X2,\\dots X_n)}, our estimates may be downwardly biased toward the null. However, d-separation is preserved. Where errors are uncorrelated with true latent realities, there is no new pathway that opens information between our exposure and outcome. Consider the relations presented in Figure 10\n\n\n\n\n\n\n\n\nFigure 10: Measurement error opens an additional pathway to confounding if either there are correlated errors, or a directed effect of the exposure on the errors of measured outcome.\n\n\n\n\n\nHere,\n\\eta_L \\rightarrow L: We assume that the true workload state affects its measurement. This measurement, however, may be affected by an unmeasured error source, U_{L}. Personal perceptions of workload can introduce this error. For instance, a person may perceive their workload differently based on recent personal experiences or cultural backgrounds. Additionally, unmeasured cultural influences like societal expectations of productivity could shape their responses independently of the true workload state. There may be cultural differences - Americans may verstate; the British may present effortless superiority.\n\\eta_A \\rightarrow A: When it comes to exercise, the true state may affect the measured frequency (questions about exercise are not totally uninformative). However, this measurement is also affected by an unmeasured source of error, which we denote by U_{A}. For example, a cultural shift towards valuing physical health might prompt participants toreport higher activity levels, introducing an error, U_{A}.\n\\eta_Y \\rightarrow Y: We assume questions about distress are not totally uninformative: actual distress affects the measured distress. However this measurement is subject to unmeasured error: U_{Y}. For instance, an increased societal acceptance of mental health might change how distress is reported creating an error, U_{Y}, in the measurement of distress. Such norms, moreover, may change over time.\nU_{L} \\rightarrow L, U_{A} \\rightarrow A, and U_{Y} \\rightarrow Y: These edges between the nodes indicate how each unmeasured error source can influence its corresponding measurement, leading to a discrepancy between the true state and the measured state.\nU_{L} \\rightarrow U_{A} and U_{L} \\rightarrow U_{Y}: These relationships indicate that the error in the stress measurement can correlate with those in the exercise and mood measurements. This could stem from a common cultural bias affecting how a participant self-reports across these areas.\n\\eta_A \\rightarrow U_{Y} and \\eta_L \\rightarrow U_{A}: These relationships indicate that the actual state of one variable can affect the error in another variable’s measurement. For example, a cultural emphasis on physical health leading to increased exercise might, in turn, affect the reporting of distress levels, causing an error, U_{Y}, in the distress measurement. Similarly, if a cultural trend pushes people to work more, it might cause them to over or underestimate their exercise frequency, introducing an error, U_{A}, in the exercise measurement.\n\nConfounding control by baseline measures of exposure and outcome: Dependent Directed Measurement Error in Three-Wave Panels\n\nWe propose a three-wave panel design to control confounding. This design adjusts for baseline measurements of both exposure and the outcome.\nUnderstanding this approach in the context of potential directed and correlated measurement errors gives us a clearer picture of its strengths and limitations.\nThis three-wave panel design incorporates baseline measurements of both exposure and confounders. As a result, any bias that could come from unmeasured sources of measurement errors should be uncorrelated with their baseline effects.\nFor instance, if individuals have a social desirability bias at the baseline, they would have to develop a different bias unrelated to the initial one for new bias to occur due to correlated unmeasured sources of measurement errors.\nHowever, we cannot completely eliminate the possibility of such new bias development. There could also be potential new sources of bias from directed effects of the exposure on the error term of the outcome, which can often occur due to panel attrition.\nTo mitigate this risk, we adjust for panel attrition/non-response using methods like multiple imputation. We also consistently perform sensitivity analyses to detect any unanticipated bias.\nDespite these potential challenges, it is worth noting that by including measures of both exposure and outcome at baseline, the chances of new confounding are significantly reduced.\nTherefore, adopting this practice should be a standard procedure in multi-wave studies as it substantially minimizes the likelihood of introducing novel confounding factors.\n\n\n\n\n\n\n\n\n\nFigure 11: TBA\n\n\n\n\n\n\n\nComment on slow changes\nOver long periods of time we can expect additional sources of confounding. Changes in cultural norms and attitudes can occur over the duration of a longitudinal study like the NZAVS, leading to residual confounding. For example, if there is a cultural shift towards increased acceptance of mental health issues, this might change how psychological distress is reported over time, irrespective of baseline responses.\n\n\n\n\n\n\n\n\n\n\n\n\nNeed for Sensitivity Analysis The Key takehome message is that we must always perform sensitivity analyses because we can never be certain that our confounding control strategy has worked."
  },
  {
    "objectID": "content/11-content.html#lab-advice-on-you-final-report",
    "href": "content/11-content.html#lab-advice-on-you-final-report",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Lab: Advice on you Final Report",
    "text": "Lab: Advice on you Final Report"
  },
  {
    "objectID": "content/11-content.html#intoduction",
    "href": "content/11-content.html#intoduction",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Intoduction",
    "text": "Intoduction\nAnswer the following:\n\nState the Question: is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nCausality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\nState how you will use time-series data to address causality.\nDefine your exposure.\nDefine your outcome(s)\nExplain how the the exposure and outcome is relevant to your question.\nDefine your causal estimand (see: lecture 9). Hint: it is ATE_g_risk difference = E[Y(1)-(0)|G,L], where G is your multiple-group indicator and L is your set of baseline confounders."
  },
  {
    "objectID": "content/11-content.html#methods",
    "href": "content/11-content.html#methods",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Methods",
    "text": "Methods\n\nConsider any ethical implications.\nExplain the sample. Provide descriptive statistics\nDiscuss inclusion criteria.\nDiscuss how your sample relates to the “source population” (lecture 9.)\nExplain NZAVS measures. State the questions used in the items\nIn your own words describe how the data meet the following assumptions required for causal inference:\nPositivity: Can we intervene on the exposure at all levels of the covariates? Use the code I provided to test whether there is change in the exposure from the baseline in the source population(s)\nConsistency: Can I interpret what it means to intervene on the exposure?\nExchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? This requires stating baseline confounders and explaining how they may be related to both the exposure and outcome. As part of this, you must explain why the baseline measure of your exposure and outcome are included as potential confounders.\nNote: Unmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? (e.g. childhood exposures that are not measured).\nDraw a causal diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\nMeasurement Error: Have I described potential biases from measurement errors? Return to lecture 11.\nState that you do not have missing data in this synthetic dataset, but that ordinarily missing data would need to be handled.\nState what your estimator will be. Note I’ve given you the following text to modify:\n\n\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\n\n\nStep 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\n\n\nStep 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\n\n\nStep 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\n\n\nStep 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\n\n\nStep 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\nAlso see the appendix here\n\nState what E-values are and how you will use them to clarify the risk of unmeasured confounding."
  },
  {
    "objectID": "content/11-content.html#results-1",
    "href": "content/11-content.html#results-1",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Results",
    "text": "Results\n\nUse the scripts I have provided as a template for your analysis.\nPropensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations: e.g.: A ~ x1 + x2 + x3 + .... using logistic regression, all continuous predictors were transformed to z-scores\n\nWeightIt Package Utilisation: Explicitly mention the use of the ‘WeightIt’ package in R, including any specific options or parameters used in the propensity score estimation process (Greifer 2023).\nReport if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as ‘ebal’, ‘energy’, and ‘ps’.\nIf your exposure is continuous only the ‘energy’ option was used for propensity score estimation.\nSubgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\nCovariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting. The script will generate these plots.\nWeighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit. The script I gave you will generate this information\n\n\n\nGreifer, Noah. 2023. WeightIt: Weighting for Covariate Balance in Observational Studies.\nExample:\n\nWe estimated propensity scores by fitting a model for the exposure A as it is predicted by the set of baseline covariates defined by L. Because we are interested in effect modification by group, we fit different propensity score models for within strata of G using the subgroup command of the WeightIt package. Thus the propensity score is the the probability of receiving a value of a treatment (A=a) conditional on the covariates L, and stratum within G. We compared balance using the following methods of weighting: “ebal” or entropy balancing, “energy” or energy balancing, and “ps” or traditional inverse probability of weighting balancing. Of these methods “ebal” performed the best. Table X and Figure Y present the results of the ebalancing method.\n\n\nInterpretation of Propensity Scores: we interpret the proposensity scores as yeilding good balance across the exposure conditions.\nOutcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation. Example\n\n\nWe fit a linear model using maximum likelihood estimation with the outcome Y predicted by the exposure A. We interacted the exposure with all baseline confounders L. Continuous baseline confounders were converted to z-scores, whereas categorical exposures were not. Also interacted with all baseline confounders was a term for the subgroup interactoin. This allowed uas to flexibily fit non-linearities for the modification of the effect of the exposure within levels of the levels of the cultural group strata of interest. We note that model coefficients have no interpretation in this context so are not reported. The remaining steps of Doubly-Robust estimation were performed as outlined in the Method section. We calculated confidence intervals and standard errors, using the clarify package in R, which relies on simulation based inference for these quantities of interest (Greifer et al. 2023)\n\nGreifer, Noah, Steven Worthington, Stefano Iacus, and Gary King. 2023. Clarify: Simulation-Based Inference for Regression Models. https://iqss.github.io/clarify/.\n\nReport the causal estimates.\n\nATE contrasts for groups in setting the exposure to for each group in setting level A = a and A = a*\ndifferences between groups in the magnitude of the effects. (ATE_group 1 - ATE_group_2)\nReport the E-value: how sensitive are your results to unmeasured confounding?"
  },
  {
    "objectID": "content/11-content.html#discussion",
    "href": "content/11-content.html#discussion",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Discussion",
    "text": "Discussion\nMake sure to hit these points:\nConsider the following ideas about what to discuss in one’s findings. The order of exposition might be different.\n\nSummary of results: What did you find?\nInterpretation of E-values: Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\nCausal Effect Interpretation: What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\nComparison of Subgroups: Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\nUncertainty and Confidence Intervals: Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\nGeneralisability and Transportability: Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\nAssumptions and Limitations: Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\nTheoretical Relevance: How are these findings relevant to existing theories.\nReplication and Future Research: Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\nReal-World Implications: Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research."
  },
  {
    "objectID": "content/11-content.html#example-anlaysis-week-10",
    "href": "content/11-content.html#example-anlaysis-week-10",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Example anlaysis (week 10)",
    "text": "Example anlaysis (week 10)\n\nPackages\n\nreport::cite_packages()\n\n  - Arel-Bundock V, Greifer N, Heiss A (2024). \"How to Interpret Statistical Models Using marginaleffects for R and Python.\" _Journal of Statistical Software_, *111*(9), 1-32. doi:10.18637/jss.v111.i09 &lt;https://doi.org/10.18637/jss.v111.i09&gt;.\n  - Barrett M (2021). _ggokabeito: 'Okabe-Ito' Scales for 'ggplot2' and 'ggraph'_. R package version 0.1.0, &lt;https://CRAN.R-project.org/package=ggokabeito&gt;.\n  - Bates D, Mächler M, Bolker B, Walker S (2015). \"Fitting Linear Mixed-Effects Models Using lme4.\" _Journal of Statistical Software_, *67*(1), 1-48. doi:10.18637/jss.v067.i01 &lt;https://doi.org/10.18637/jss.v067.i01&gt;.\n  - Bates D, Maechler M, Jagan M (2025). _Matrix: Sparse and Dense Matrix Classes and Methods_. R package version 1.7-2, &lt;https://CRAN.R-project.org/package=Matrix&gt;.\n  - Bengtsson H (2021). \"A Unifying Framework for Parallel and Distributed Processing in R using Futures.\" _The R Journal_, *13*(2), 208-227. doi:10.32614/RJ-2021-048 &lt;https://doi.org/10.32614/RJ-2021-048&gt;, &lt;https://doi.org/10.32614/RJ-2021-048&gt;.\n  - Bengtsson H (2021). \"A Unifying Framework for Parallel and Distributed Processing in R using Futures.\" _The R Journal_, *13*(2), 208-227. doi:10.32614/RJ-2021-048 &lt;https://doi.org/10.32614/RJ-2021-048&gt;, &lt;https://doi.org/10.32614/RJ-2021-048&gt;.\n  - Bengtsson H (2024). _progressr: An Inclusive, Unifying API for Progress Updates_. R package version 0.15.1, &lt;https://CRAN.R-project.org/package=progressr&gt;.\n  - Blair G, Cooper J, Coppock A, Humphreys M, Bicalho C, Fultz N, Medina L (2021). _DesignLibrary: Library of Research Designs_. R package version 0.1.10, &lt;https://CRAN.R-project.org/package=DesignLibrary&gt;.\n  - Blair G, Cooper J, Coppock A, Humphreys M, Rudkin A, Fultz N (2024). _fabricatr: Imagine Your Data Before You Collect It_. R package version 1.0.2, &lt;https://CRAN.R-project.org/package=fabricatr&gt;.\n  - Blair G, Cooper J, Coppock A, Humphreys M, Sonnet L (2024). _estimatr: Fast Estimators for Design-Based Inference_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=estimatr&gt;.\n  - Blair G, Coppock A, Humphreys M (2023). _Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign_. Princeton University Press, Princeton. &lt;https://book.declaredesign.org&gt;. Blair G, Cooper J, Coppock A, Humphreys M (2019). \"Declaring and Diagnosing Research Designs.\" _American Political Science Review_, *113*, 838-859. &lt;https://declaredesign.org/paper.pdf&gt;.\n  - Brown C (2018). _formula.tools: Programmatic Utilities for Manipulating Formulas, Expressions, Calls, Assignments and Other R Objects_. R package version 1.7.1, &lt;https://CRAN.R-project.org/package=formula.tools&gt;.\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 0.3.1.0 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Chen T, He T, Benesty M, Khotilovich V, Tang Y, Cho H, Chen K, Mitchell R, Cano I, Zhou T, Li M, Xie J, Lin M, Geng Y, Li Y, Yuan J (2024). _xgboost: Extreme Gradient Boosting_. R package version 1.7.8.1, &lt;https://CRAN.R-project.org/package=xgboost&gt;.\n  - Christopher H. Jackson (2011). \"Multi-State Models for Panel Data: The msm Package for R.\" _Journal of Statistical Software_, *38*(8), 1-29. doi:10.18637/jss.v038.i08 &lt;https://doi.org/10.18637/jss.v038.i08&gt;.\n  - Coppock A (2023). _randomizr: Easy-to-Use Tools for Common Forms of Random Assignment and Sampling_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=randomizr&gt;.\n  - Csárdi G, Hester J, Wickham H, Chang W, Morgan M, Tenenbaum D (2024). _remotes: R Package Installation from Remote Repositories, Including 'GitHub'_. R package version 2.5.0, &lt;https://CRAN.R-project.org/package=remotes&gt;.\n  - Eddelbuettel D, Francois R, Allaire J, Ushey K, Kou Q, Russell N, Ucar I, Bates D, Chambers J (2025). _Rcpp: Seamless R and C++ Integration_. R package version 1.0.14, &lt;https://CRAN.R-project.org/package=Rcpp&gt;. Eddelbuettel D, François R (2011). \"Rcpp: Seamless R and C++ Integration.\" _Journal of Statistical Software_, *40*(8), 1-18. doi:10.18637/jss.v040.i08 &lt;https://doi.org/10.18637/jss.v040.i08&gt;. Eddelbuettel D (2013). _Seamless R and C++ Integration with Rcpp_. Springer, New York. doi:10.1007/978-1-4614-6868-4 &lt;https://doi.org/10.1007/978-1-4614-6868-4&gt;, ISBN 978-1-4614-6867-7. Eddelbuettel D, Balamuta J (2018). \"Extending R with C++: A Brief Introduction to Rcpp.\" _The American Statistician_, *72*(1), 28-36. doi:10.1080/00031305.2017.1375990 &lt;https://doi.org/10.1080/00031305.2017.1375990&gt;.\n  - Firke S (2024). _janitor: Simple Tools for Examining and Cleaning Dirty Data_. R package version 2.2.1, &lt;https://CRAN.R-project.org/package=janitor&gt;.\n  - Friedman J, Tibshirani R, Hastie T (2010). \"Regularization Paths for Generalized Linear Models via Coordinate Descent.\" _Journal of Statistical Software_, *33*(1), 1-22. doi:10.18637/jss.v033.i01 &lt;https://doi.org/10.18637/jss.v033.i01&gt;. Simon N, Friedman J, Tibshirani R, Hastie T (2011). \"Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent.\" _Journal of Statistical Software_, *39*(5), 1-13. doi:10.18637/jss.v039.i05 &lt;https://doi.org/10.18637/jss.v039.i05&gt;. Tay JK, Narasimhan B, Hastie T (2023). \"Elastic Net Regularization Paths for All Generalized Linear Models.\" _Journal of Statistical Software_, *106*(1), 1-31. doi:10.18637/jss.v106.i01 &lt;https://doi.org/10.18637/jss.v106.i01&gt;.\n  - Greifer N (2024). _cobalt: Covariate Balance Tables and Plots_. R package version 4.5.5, &lt;https://CRAN.R-project.org/package=cobalt&gt;.\n  - Greifer N (2024). _WeightIt: Weighting for Covariate Balance in Observational Studies_. R package version 1.3.2, &lt;https://CRAN.R-project.org/package=WeightIt&gt;.\n  - Greifer N, Worthington S, Iacus S, King G (2024). _clarify: Simulation-Based Inference for Regression Models_. R package version 0.2.1, &lt;https://CRAN.R-project.org/package=clarify&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Halekoh U, Højsgaard S, Yan J (2006). \"The R Package geepack for Generalized Estimating Equations.\" _Journal of Statistical Software_, *15/2*, 1-11. Yan J, Fine JP (2004). \"Estimating Equations for Association Structures.\" _Statistics in Medicine_, *23*, 859-880. Yan J (2002). \"geepack: Yet Another Package for Generalized Estimating Equations.\" _R-News_, *2/3*, 12-14.\n  - Hansen BB, Klopfer SO (2006). \"Optimal full matching and related designs via network flows.\" _Journal of Computational and Graphical Statistics_, *15*(3), 609-627.\n  - Hastie T (2024). _gam: Generalized Additive Models_. R package version 1.22-5, &lt;https://CRAN.R-project.org/package=gam&gt;.\n  - Helske S, Helske J (2019). \"Mixture Hidden Markov Models for Sequence Data: The seqHMM Package in R.\" _Journal of Statistical Software_, *88*(3), 1-32. doi:10.18637/jss.v088.i03 &lt;https://doi.org/10.18637/jss.v088.i03&gt;. Helske J, Helske S (2023). _seqHMM: Mixture hidden Markov models for social sequence data and other multivariate, multichannel categorical time series_. R package version 1.2.6, &lt;https://cran.r-project.org/package=seqHMM&gt;.\n  - Henry L, Wickham H (2025). _rlang: Functions for Base Types and Core R and 'Tidyverse' Features_. R package version 1.1.5, &lt;https://CRAN.R-project.org/package=rlang&gt;.\n  - Hester J, Bryan J (2024). _glue: Interpreted String Literals_. R package version 1.8.0, &lt;https://CRAN.R-project.org/package=glue&gt;.\n  - Hester J, Wickham H, Csárdi G (2024). _fs: Cross-Platform File System Operations Based on 'libuv'_. R package version 1.6.5, &lt;https://CRAN.R-project.org/package=fs&gt;.\n  - Ho D, Imai K, King G, Stuart E (2011). \"MatchIt: Nonparametric Preprocessing for Parametric Causal Inference.\" _Journal of Statistical Software_, *42*(8), 1-28. doi:10.18637/jss.v042.i08 &lt;https://doi.org/10.18637/jss.v042.i08&gt;.\n  - Honaker J, King G, Blackwell M (2011). \"Amelia II: A Program for Missing Data.\" _Journal of Statistical Software_, *45*(7), 1-47. doi:10.18637/jss.v045.i07 &lt;https://doi.org/10.18637/jss.v045.i07&gt;.\n  - Iannone R, Cheng J, Schloerke B, Hughes E, Lauer A, Seo J, Brevoort K, Roy O (2024). _gt: Easily Create Presentation-Ready Display Tables_. R package version 0.11.1, &lt;https://CRAN.R-project.org/package=gt&gt;.\n  - Kassambara A (2023). _ggpubr: 'ggplot2' Based Publication Ready Plots_. R package version 0.6.0, &lt;https://CRAN.R-project.org/package=ggpubr&gt;.\n  - Kay M (2024). \"ggdist: Visualizations of Distributions and Uncertainty in the Grammar of Graphics.\" _IEEE Transactions on Visualization and Computer Graphics_, *30*(1), 414-424. doi:10.1109/TVCG.2023.3327195 &lt;https://doi.org/10.1109/TVCG.2023.3327195&gt;. Kay M (2024). _ggdist: Visualizations of Distributions and Uncertainty_. doi:10.5281/zenodo.3879620 &lt;https://doi.org/10.5281/zenodo.3879620&gt;, R package version 3.3.2, &lt;https://mjskay.github.io/ggdist/&gt;.\n  - Lüdecke D (2018). \"ggeffects: Tidy Data Frames of Marginal Effects from Regression Models.\" _Journal of Open Source Software_, *3*(26), 772. doi:10.21105/joss.00772 &lt;https://doi.org/10.21105/joss.00772&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 &lt;https://doi.org/10.21105/joss.02445&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Waggoner P, Makowski D (2021). \"performance: An R Package for Assessment, Comparison and Testing of Statistical Models.\" _Journal of Open Source Software_, *6*(60), 3139. doi:10.21105/joss.03139 &lt;https://doi.org/10.21105/joss.03139&gt;.\n  - Lumley T (2024). \"survey: analysis of complex survey samples.\" R package version 4.4. Lumley T (2004). \"Analysis of Complex Survey Samples.\" _Journal of Statistical Software_, *9*(1), 1-19. R package verson 2.2. Lumley T (2010). _Complex Surveys: A Guide to Analysis Using R: A Guide to Analysis Using R_. John Wiley and Sons.\n  - Microsoft, Weston S (2022). _foreach: Provides Foreach Looping Construct_. R package version 1.5.2, &lt;https://CRAN.R-project.org/package=foreach&gt;.\n  - Mullen KM, van Stokkum IHM (2024). _nnls: The Lawson-Hanson Algorithm for Non-Negative Least Squares (NNLS)_. R package version 1.6, &lt;https://CRAN.R-project.org/package=nnls&gt;.\n  - Müller K (2020). _here: A Simpler Way to Find Your Files_. R package version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - Ooms J (2024). _katex: Rendering Math to HTML, 'MathML', or R-Documentation Format_. R package version 1.5.0, &lt;https://CRAN.R-project.org/package=katex&gt;.\n  - Ooms J (2024). _pdftools: Text Extraction, Rendering and Converting of PDF Documents_. R package version 3.4.1, &lt;https://CRAN.R-project.org/package=pdftools&gt;.\n  - Patil I, Makowski D, Ben-Shachar M, Wiernik B, Bacher E, Lüdecke D (2022). \"datawizard: An R Package for Easy Data Preparation and Statistical Transformations.\" _Journal of Open Source Software_, *7*(78), 4684. doi:10.21105/joss.04684 &lt;https://doi.org/10.21105/joss.04684&gt;.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. R package version 1.3.0, &lt;https://CRAN.R-project.org/package=patchwork&gt;.\n  - Pishgar F, Greifer N, Leyrat C, Stuart E (2021). \"MatchThem:: Matching and Weighting after Multiple Imputation.\" _The R Journal_. doi:10.32614/RJ-2021-073 &lt;https://doi.org/10.32614/RJ-2021-073&gt;, &lt;https://journal.r-project.org/archive/2021/RJ-2021-073/&gt;.\n  - Polley E, LeDell E, Kennedy C, van der Laan M (2024). _SuperLearner: Super Learner Prediction_. R package version 2.0-29, &lt;https://CRAN.R-project.org/package=SuperLearner&gt;.\n  - R Core Team (2024). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Rich B (2023). _table1: Tables of Descriptive Statistics in HTML_. R package version 1.4.3, &lt;https://CRAN.R-project.org/package=table1&gt;.\n  - Richardson N, Cook I, Crane N, Dunnington D, François R, Keane J, Moldovan-Grünfeld D, Ooms J, Wujciak-Jens J, Apache Arrow (2025). _arrow: Integration to 'Apache' 'Arrow'_. R package version 18.1.0.1, &lt;https://CRAN.R-project.org/package=arrow&gt;.\n  - Robinson D, Hayes A, Couch S (2024). _broom: Convert Statistical Objects into Tidy Tibbles_. R package version 1.0.7, &lt;https://CRAN.R-project.org/package=broom&gt;.\n  - Robitzsch A, Grund S (2024). _miceadds: Some Additional Multiple Imputation Functions, Especially for 'mice'_. R package version 3.17-44, &lt;https://CRAN.R-project.org/package=miceadds&gt;.\n  - Rosseel Y (2012). \"lavaan: An R Package for Structural Equation Modeling.\" _Journal of Statistical Software_, *48*(2), 1-36. doi:10.18637/jss.v048.i02 &lt;https://doi.org/10.18637/jss.v048.i02&gt;.\n  - Sjoberg D, Whiting K, Curry M, Lavery J, Larmarange J (2021). \"Reproducible Summary Tables with the gtsummary Package.\" _The R Journal_, *13*, 570-580. doi:10.32614/RJ-2021-053 &lt;https://doi.org/10.32614/RJ-2021-053&gt;, &lt;https://doi.org/10.32614/RJ-2021-053&gt;.\n  - Sjolander A, Dahlqwist E (2021). _stdReg: Regression Standardization_. R package version 3.4.1, &lt;https://CRAN.R-project.org/package=stdReg&gt;.\n  - Therneau T (2024). _A Package for Survival Analysis in R_. R package version 3.8-3, &lt;https://CRAN.R-project.org/package=survival&gt;. Terry M. Therneau, Patricia M. Grambsch (2000). _Modeling Survival Data: Extending the Cox Model_. Springer, New York. ISBN 0-387-98784-3.\n  - Tibshirani J, Athey S, Sverdrup E, Wager S (2024). _grf: Generalized Random Forests_. R package version 2.4.0, commit e2a2040690c3e461e793f98bce1c6f7163a8af7b, &lt;https://github.com/grf-labs/grf&gt;.\n  - Tierney N, Cook D (2023). \"Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.\" _Journal of Statistical Software_, *105*(7), 1-31. doi:10.18637/jss.v105.i07 &lt;https://doi.org/10.18637/jss.v105.i07&gt;.\n  - van Buuren S, Groothuis-Oudshoorn K (2011). \"mice: Multivariate Imputation by Chained Equations in R.\" _Journal of Statistical Software_, *45*(3), 1-67. doi:10.18637/jss.v045.i03 &lt;https://doi.org/10.18637/jss.v045.i03&gt;.\n  - van der Wal WM, Geskus RB (2011). \"ipw: An R Package for Inverse Probability Weighting.\" _Journal of Statistical Software_, *43*(13), 1-23. doi:10.18637/jss.v043.i13 &lt;https://doi.org/10.18637/jss.v043.i13&gt;.\n  - VanderWeele TJ, Ding P (2011). \"Sensitivity analysis in observational research: introducing the E-value.\" _Annals of Internal Medicine_, *167*(4), 268-274. Mathur MB, VanderWeele TJ (2019). \"Sensitivity analysis for unmeasured confounding in meta-analyses.\" _Journal of the American Statistical Association&gt;_. Smith LH, VanderWeele TJ (2019). \"Bounding bias due to selection.\" _Epidemiology_.\n  - Waring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, Ellis S (2022). _skimr: Compact and Flexible Summaries of Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=skimr&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _conflicted: An Alternative Conflict Resolution Strategy_. R package version 1.2.0, &lt;https://CRAN.R-project.org/package=conflicted&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, Bryan J, Barrett M, Teucher A (2024). _usethis: Automate Package and Project Setup_. R package version 3.1.0, &lt;https://CRAN.R-project.org/package=usethis&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2023). _purrr: Functional Programming Tools_. R package version 1.0.2, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Hester J, Chang W, Bryan J (2022). _devtools: Tools to Make Developing R Packages Easier_. R package version 2.4.5, &lt;https://CRAN.R-project.org/package=devtools&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - William Revelle (2024). _psych: Procedures for Psychological, Psychometric, and Personality Research_. Northwestern University, Evanston, Illinois. R package version 2.4.12, &lt;https://CRAN.R-project.org/package=psych&gt;.\n  - Williams N, Díaz I (2023). \"lmtp: An R package for estimating the causal effects of modified treatment policies.\" _Observational Studies_. &lt;https://muse.jhu.edu/article/883479&gt;. Díaz I, Williams N, Hoffman K, Schneck E (2021). \"Non-parametric causal effects based on longitudinal modified treatment policies.\" _Journal of the American Statistical Association_. doi:10.1080/01621459.2021.1955691 &lt;https://doi.org/10.1080/01621459.2021.1955691&gt;.\n  - Wright MN, Ziegler A (2017). \"ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.\" _Journal of Statistical Software_, *77*(1), 1-17. doi:10.18637/jss.v077.i01 &lt;https://doi.org/10.18637/jss.v077.i01&gt;.\n  - Xie Y (2024). _knitr: A General-Purpose Package for Dynamic Report Generation in R_. R package version 1.49, &lt;https://yihui.org/knitr/&gt;. Xie Y (2015). _Dynamic Documents with R and knitr_, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, &lt;https://yihui.org/knitr/&gt;. Xie Y (2014). \"knitr: A Comprehensive Tool for Reproducible Research in R.\" In Stodden V, Leisch F, Peng RD (eds.), _Implementing Reproducible Computational Research_. Chapman and Hall/CRC. ISBN 978-1466561595.\n  - Xie Y (2024). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.54, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;.\n  - Zeileis A, Croissant Y (2010). \"Extended Model Formulas in R: Multiple Parts and Multiple Responses.\" _Journal of Statistical Software_, *34*(1), 1-13. doi:10.18637/jss.v034.i01 &lt;https://doi.org/10.18637/jss.v034.i01&gt;.\n  - Zeileis A, Köll S, Graham N (2020). \"Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in R.\" _Journal of Statistical Software_, *95*(1), 1-36. doi:10.18637/jss.v095.i01 &lt;https://doi.org/10.18637/jss.v095.i01&gt;. Zeileis A (2004). \"Econometric Computing with HC and HAC Covariance Matrix Estimators.\" _Journal of Statistical Software_, *11*(10), 1-17. doi:10.18637/jss.v011.i10 &lt;https://doi.org/10.18637/jss.v011.i10&gt;. Zeileis A (2006). \"Object-Oriented Computation of Sandwich Estimators.\" _Journal of Statistical Software_, *16*(9), 1-16. doi:10.18637/jss.v016.i09 &lt;https://doi.org/10.18637/jss.v016.i09&gt;.\n  - Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. R package version 1.4.0, &lt;https://CRAN.R-project.org/package=kableExtra&gt;."
  },
  {
    "objectID": "content/11-content.html#lab-advice-on-your-final-report",
    "href": "content/11-content.html#lab-advice-on-your-final-report",
    "title": "Hands on Measurement: Exploratory Factor Analysis, Confirmatory Factor Analysis (CFA), Multigroup Confirmatory Factor Analysis, Partial Invariance (Configural, Metric, and Scalar equivalence).",
    "section": "Lab: Advice on Your Final Report",
    "text": "Lab: Advice on Your Final Report"
  },
  {
    "objectID": "slides/01-slides.html#goals",
    "href": "slides/01-slides.html#goals",
    "title": "Seminar 1",
    "section": "",
    "text": "Understand the special problems that cultural research presents for measurement.\nUnderstand the deeper, and prior concept of confounding.\n\n\n\n\n\nMeasurement\nValidity\nA confounder"
  },
  {
    "objectID": "slides/01-slides.html#where-does-psychology-start",
    "href": "slides/01-slides.html#where-does-psychology-start",
    "title": "Seminar 1",
    "section": "Where Does Psychology Start?",
    "text": "Where Does Psychology Start?\nPsychology starts with a question about how people think or behave."
  },
  {
    "objectID": "slides/01-slides.html#examples-of-psychological-questions",
    "href": "slides/01-slides.html#examples-of-psychological-questions",
    "title": "Seminar 1",
    "section": "Examples of Psychological Questions?",
    "text": "Examples of Psychological Questions?\n\nHow does early childhood experience affect personality and behaviour?\nWhat are the effects of social media on self-esteem?\nWhy do some people believe in a God or gods and others do not?\nWhy are some people motivated to sacrifice for others?\nDoes marriage make people happy?"
  },
  {
    "objectID": "slides/01-slides.html#example-of-psychological-questions-in-cross-cultural-psychology",
    "href": "slides/01-slides.html#example-of-psychological-questions-in-cross-cultural-psychology",
    "title": "Seminar 1",
    "section": "Example of Psychological Questions in Cross-Cultural Psychology",
    "text": "Example of Psychological Questions in Cross-Cultural Psychology\n\nHow do early childhood experiences differ across cultures, and how do these differences impact personality and behavior development?\nAre there cultural differences in the way social media use affects self-esteem and body image?\nHow do cultural and religious beliefs shape individual attitudes towards the concept of God or gods?\nWhat are the cultural and individual factors that motivate people to engage in acts of altruism or sacrifice for others, and how do these factors vary across cultures?\nAre there cultural differences in the factors that contribute to marital satisfaction and happiness, and how do cultural expectations and values surrounding marriage play a role?"
  },
  {
    "objectID": "slides/01-slides.html#how-might-psychological-scientists-answer-whether-marriage-makes-people-happy",
    "href": "slides/01-slides.html#how-might-psychological-scientists-answer-whether-marriage-makes-people-happy",
    "title": "Seminar 1",
    "section": "How might Psychological Scientists answer whether marriage makes people happy?",
    "text": "How might Psychological Scientists answer whether marriage makes people happy?"
  },
  {
    "objectID": "slides/01-slides.html#wrong-answers-only",
    "href": "slides/01-slides.html#wrong-answers-only",
    "title": "Seminar 1",
    "section": "Wrong Answers Only",
    "text": "Wrong Answers Only\n\n“Ask my married parents if they are happy.”\n“Consult a palm reader.”\nAssert: “Yes marriage always makes people happy”; “No marriage can’t possibly make anyone happy”\nIntuit: “It depends on the gender of the individual. Men are always happier in marriage, while women are never happier.”\nIntuition: “It depends on the cultural background of the individuals. Couples from Western cultures are always happier in marriage, while couples from Eastern cultures are never happier.”\nConduct a literature review of previous research on the association between marriage and happiness, including cross-cultural studies that compare different cultural attitudes and practices regarding marriage and their relationship to happiness.\nConduct a survey of a large and diverse sample of individuals to assess their happiness levels, as well as their marital status assess the relationship."
  },
  {
    "objectID": "slides/01-slides.html#conduct-a-literature-review",
    "href": "slides/01-slides.html#conduct-a-literature-review",
    "title": "Seminar 1",
    "section": "Conduct a Literature Review",
    "text": "Conduct a Literature Review\n\nWhat would this tell us?\n\nwhat other researchers have found.\n\nWhat would this not tell us?\n\nwhat other researchers have not found.\nwhat other researchers got wrong."
  },
  {
    "objectID": "slides/01-slides.html#conduct-a-survey-with-a-large-and-diverse-sample-of-individuals",
    "href": "slides/01-slides.html#conduct-a-survey-with-a-large-and-diverse-sample-of-individuals",
    "title": "Seminar 1",
    "section": "Conduct a Survey with a Large and Diverse Sample of Individuals",
    "text": "Conduct a Survey with a Large and Diverse Sample of Individuals\n\nWhere to begin?"
  },
  {
    "objectID": "slides/01-slides.html#measurement",
    "href": "slides/01-slides.html#measurement",
    "title": "Seminar 1",
    "section": "Measurement",
    "text": "Measurement\nDefinitions:\n\n“Measurement is the numerical quantifcation of the attributes of an object or event, which can be used to compare with other objects or events” [Conventional]\n“Measurement is the assignment of numerals to objects or events according to rules.” [Psychological]\n“Measurement is the process of experimentally obtaining one or more quantity values that can reasonably be attributed to a quantity.” [Metrological] from [@briggs2021]"
  },
  {
    "objectID": "slides/01-slides.html#on-a-scale-of-1-7-how-happy-are-you",
    "href": "slides/01-slides.html#on-a-scale-of-1-7-how-happy-are-you",
    "title": "Seminar 1",
    "section": "“On a scale of 1-7, how happy are you?”",
    "text": "“On a scale of 1-7, how happy are you?”\nHow might this go wrong?\n\nAmbiguity: Respondents may interpret “happiness” in different ways. -e.g. Some people may equate happiness with a momentary positive emotion -e.g. Others may think of happiness as a long-term state of contentment.\nSocial Desirability Bias: Respondents may want to tell you what they think you want to hear. Others might want to frustrate you.\nMood: the context in which the question is asked and the respondent’s current mood might affect the answer (reliability)\nLimited range of scale\nWhat do the endpoints mean?\nCultural Differences:\n\nin meanings\nin constructs themselves"
  },
  {
    "objectID": "slides/01-slides.html#in-survey-research",
    "href": "slides/01-slides.html#in-survey-research",
    "title": "Seminar 1",
    "section": "In Survey Research",
    "text": "In Survey Research\nConstruct: e.g. Happiness - a subjective experience of positive emotions, such as joy, contentment, and satisfaction with life/\nItem: A question or statement used to measure an aspect of happiness. Example: “I feel content with my life.”\nScale: A collection of items designed to measure a construct. Example: a happiness scale might consist of items rated on a Likert-type scale, with an overall score reflecting the level of happiness being measured. Example:\n\n“In general, how happy do you feel?” (rated on a scale from 1 - not at all happy to 5 - extremely happy)\n“How often do you feel positive emotions, such as joy or contentment?” (rated on a scale from 1 - very rarely or never to 5 - very often or always)\n“How satisfied are you with your life as a whole?” (rated on a scale from 1 - very dissatisfied to 5 - very satisfied)"
  },
  {
    "objectID": "slides/01-slides.html#concept-of-validity-in-psychometric-research",
    "href": "slides/01-slides.html#concept-of-validity-in-psychometric-research",
    "title": "Seminar 1",
    "section": "Concept of Validity in Psychometric Research",
    "text": "Concept of Validity in Psychometric Research\n\nContent Validity: The degree an instrument measures what it is intended to measure (the “construct”).\nConstruct Validity: Whether the construct assumed to measure is accurately defined and operationalised.\nCriterion Validity: Whether an instrument accurately predicts performance.\nFace Validity: whether an instrument measure what it is intended to measure, as assessed by experts.\nEcological Validity: whether an instrument accurately reflects real-world situations and behavior [@bandalos2018]\n\n\nMetric equivalence: Factor loadings are similar across groups. Configural equivalence: The factor structure is the same across groups in a multi-group confirmatory factor analysis. Scalar equivalence: Values/Means are also equivalent across groups."
  },
  {
    "objectID": "slides/01-slides.html#what-else-might-go-wrong",
    "href": "slides/01-slides.html#what-else-might-go-wrong",
    "title": "Seminar 1",
    "section": "What else might go wrong?",
    "text": "What else might go wrong?\nSuppose we are confident in measurement, administer survey, and find an relationship.\n\nDoes marriage cause happiness cross culturally?"
  },
  {
    "objectID": "slides/01-slides.html#what-might-go-wrong",
    "href": "slides/01-slides.html#what-might-go-wrong",
    "title": "Seminar 1",
    "section": "What might go wrong?",
    "text": "What might go wrong?\n\nHappiness might cause marriage"
  },
  {
    "objectID": "slides/01-slides.html#what-else-might-go-wrong-1",
    "href": "slides/01-slides.html#what-else-might-go-wrong-1",
    "title": "Seminar 1",
    "section": "What else might go wrong?",
    "text": "What else might go wrong?\n\nAge might a common cause of both marriage and happiness"
  },
  {
    "objectID": "slides/01-slides.html#many-psychologists-will-simply-control-for-income-but-what-if-happiness-and-marriage-cause-income",
    "href": "slides/01-slides.html#many-psychologists-will-simply-control-for-income-but-what-if-happiness-and-marriage-cause-income",
    "title": "Seminar 1",
    "section": "Many psychologists will simply “control for” income, but what if happiness and marriage cause income?",
    "text": "Many psychologists will simply “control for” income, but what if happiness and marriage cause income?\n\nSimulate data in which happiness and marriage cause income\nControlling for income makes it look as though happiness and marriage are negatively related.\nBut we simulated data with no relationship!\nThis is called ‘collider bias’, we will consider how it works in the upcoming weeks.\nMeantime, even if we measure people without error, cross-cultural studies might select people with different incomes, leading to bias even in the absence of measurement error bias (and even if we do not control for income!)\n\n\n## simulate data\n# reproducability\nset.seed(123)\nsim_fun_B &lt;- function() {\n  n &lt;- 10000\n  H &lt;- rnorm(n, 1) # simulates Happiness,\n  M &lt;- rnorm(n, 1) #  simulates Marriage, nothing to do with happiness\n  I &lt;- rnorm(n, .2 * H + .5 * M) # simulate marriage as a function of age + happiness\n\n\n  # simulate dataframe from function\n  simdat_B &lt;- data.frame(\n    H = H,\n    I = I,\n    M = M\n  )\n\n  #  model in which marriage \"predicts\" happiness controlling for age\n  sim_B &lt;- lm(H ~ M + I, data = simdat_B)\n  sim_B # returns output\n}\n\n# replicate 100 times\nr_lm_B &lt;- NA\nr_lm_B &lt;- replicate(100, sim_fun_B(), simplify = FALSE)\n\n# print model results: marriage looks negatively related to happiness!\nparameters::pool_parameters(r_lm_B)\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | Statistic |     df |      p\n-------------------------------------------------------------------------------\n(Intercept) |        0.96 | 0.02 | [ 0.92,  1.00] |     46.96 | 322.51 | &lt; .001\nM           |       -0.10 | 0.02 | [-0.13, -0.07] |     -6.11 | 324.10 | &lt; .001\nI           |        0.19 | 0.01 | [ 0.17,  0.22] |     13.71 | 320.53 | &lt; .001"
  },
  {
    "objectID": "slides/01-slides.html#the-problem-is-confounding-what-can-we-do-about-it",
    "href": "slides/01-slides.html#the-problem-is-confounding-what-can-we-do-about-it",
    "title": "Seminar 1",
    "section": "The problem is confounding: what can we do about it?",
    "text": "The problem is confounding: what can we do about it?\n\nWeeks 2-4: Causal Diagrams"
  },
  {
    "objectID": "slides/01-slides.html#our-approach",
    "href": "slides/01-slides.html#our-approach",
    "title": "Seminar 1",
    "section": "Our Approach:",
    "text": "Our Approach:\n\nExternal Validity\n\nDefinition: the extent to which the findings of a study can be generalised to other situations, people, settings, and time periods.\nImportance: we want to know if our findings carry beyond the people in our study\nChallenges: how can we know?\n\n\n\nInternal Validity\n\nDefinition: the degree to which a study can demonstrate that a causal relationship exists between the ‘independent’ and ‘dependent’ variables, free of confounding.\nWe will use the term ‘outcome’ and ‘treatment’ in place of independent and dependent. This is because causality occurs in time.\nImportance: we cannot interpret the associations we recover if they do not have a causal interpretation.\nChallenges: internal validity requires balance in the confounders that might afffect the treatement and outcome, and observational data do not give us ballance.\nThe Special Challenges in Cross-Cultural Research:\n\nThe concept of measurement itself has causal underpinnings [@vanderweele2022].\nIn cross-cultural research, we have stronger challengs from measurement validity, which, as we shall see, encompasses both external and internal validity.\nWe consider measurment validity within classical framework of external and internal validity\nEthical challenges, which science needs to consider.\nForemost among these ethical challenges, I believe, is doing our best to get inference right!"
  },
  {
    "objectID": "slides/01-slides.html#the-hope",
    "href": "slides/01-slides.html#the-hope",
    "title": "Seminar 1",
    "section": "The Hope",
    "text": "The Hope\n\nThere has been tremendous progress in the health and computer sciences in addressing problems of external and internal validity from a causal perspective\nYou will be among the first to apply these methods to questions in cross-cultural psychology.\nIn doing so, much of what has been confusing to you about psychological research design, data-analysis, ultimately scientific inference will be clearer."
  },
  {
    "objectID": "slides/01-slides.html#what-have-we-learned",
    "href": "slides/01-slides.html#what-have-we-learned",
    "title": "Seminar 1",
    "section": "What have we learned?",
    "text": "What have we learned?\n\n\nMeasurement: attaching magnitudes to features of the world.\nValidity: obtaining accurate scientific inference: a complex set of interrelated problems.\nConfounding; association in data is spurious"
  },
  {
    "objectID": "slides/01-slides.html#references",
    "href": "slides/01-slides.html#references",
    "title": "Seminar 1",
    "section": "References",
    "text": "References\nFor an account of the history of measurement in psychological research, see: [@briggs2021]\nFor an account of key concepts and current debates in psychometrics, see: [@bandalos2018]\n\nFor an accessible introduction to causal inference and its history see: [@pearl2018]\n\nBibliography"
  },
  {
    "objectID": "slides/07-slides.html#goals",
    "href": "slides/07-slides.html#goals",
    "title": "Introduction to Causal Inference",
    "section": "",
    "text": "Understand the definition of “causality” as it is used in the human and health sciences.\nUnderstand the assumptions required for consistently estimating causal effects.\nUnderstand how to use causal diagrammes to assess these assumptions.\n\n\n\n\n\nCause/Effect\nConfounder\nCollider"
  },
  {
    "objectID": "slides/07-slides.html#where-does-psychology-start",
    "href": "slides/07-slides.html#where-does-psychology-start",
    "title": "Introduction to Causal Inference",
    "section": "0Where does psychology start?",
    "text": "0Where does psychology start?\n\nPsychology starts with a question about how people think or behave."
  },
  {
    "objectID": "slides/07-slides.html#group-discussion",
    "href": "slides/07-slides.html#group-discussion",
    "title": "Introduction to Causal Inference",
    "section": "Group discussion",
    "text": "Group discussion\n\nWe know that bilingual children tend to perform better on various cognitive tasks. Why might this be the case?\nHow can we know whether it is bilingualism that causes better performance on various cognitive tasks?"
  },
  {
    "objectID": "slides/07-slides.html#humes-definitions-of-a-causality",
    "href": "slides/07-slides.html#humes-definitions-of-a-causality",
    "title": "Introduction to Causal Inference",
    "section": "Hume’s definitions of a causality",
    "text": "Hume’s definitions of a causality\n\n“we may define a cause to be an object followed by another, and where all the objects, similar to the first, are followed by objects similar to the second [definition 1]. Or, in other words, where, if the first object had not been, the second never would have existed [definition 2].” - David Hume, Enquiries Concerning Human Understanding, and Concerning the Principles of Morals, Section VII"
  },
  {
    "objectID": "slides/07-slides.html#individual-causal-effects",
    "href": "slides/07-slides.html#individual-causal-effects",
    "title": "Introduction to Causal Inference",
    "section": "Individual causal effects",
    "text": "Individual causal effects\nY_i^{a = 1}: The cognitive ability of child i if they were bilingual. This is the counterfactual outcome when A = 1.\nY_i^{a = 0}: The cognitive ability of child i if they were monolingual. This is the counterfactual outcome when A = 0.\n\n\\text{Causal Effect}_i = Y^{a = 1} - Y^{a = 0}"
  },
  {
    "objectID": "slides/07-slides.html#individual-causal-quantities",
    "href": "slides/07-slides.html#individual-causal-quantities",
    "title": "Introduction to Causal Inference",
    "section": "Individual causal quantities",
    "text": "Individual causal quantities\nWe say there is a causal effect for individual i if:\n\nY_i^{a=1} - Y_i^{a=0}  \\neq 0"
  },
  {
    "objectID": "slides/07-slides.html#what-is-the-problem",
    "href": "slides/07-slides.html#what-is-the-problem",
    "title": "Introduction to Causal Inference",
    "section": "What is the problem?",
    "text": "What is the problem?\n\nThese data required to compute this quantity is generally not available."
  },
  {
    "objectID": "slides/07-slides.html#two-roads-diverge-in-a-yellow-wood",
    "href": "slides/07-slides.html#two-roads-diverge-in-a-yellow-wood",
    "title": "Introduction to Causal Inference",
    "section": "Two Roads Diverge in a Yellow Wood",
    "text": "Two Roads Diverge in a Yellow Wood\nRobert Frost writes,\n\nTwo roads diverged in a yellow wood,\n\n\nAnd sorry I could not travel both\u000b\n\n\nAnd be one traveler, long I stood\u000b\n\n\nAnd looked down one as far as I could\u000b\n\n\nTo where it bent in the undergrowth;\u000b\n\n\nThen took the other, as just as fair,\u000b\n\n\nAnd having perhaps the better claim,\u000b\n\n\nBecause it was grassy and wanted wear;\u000b\n\n\nThough as for that the passing there\u000b\n\n\nHad worn them really about the same,\u000b \u000b\n\n\nAnd both that morning equally lay\u000b\n\n\nIn leaves no step had trodden black.\u000b\n\n\nOh, I kept the first for another day!\u000b\n\n\nYet knowing how way leads on to way,\u000b\n\n\nI doubted if I should ever come back.\u000b\n\n\nI shall be telling this with a sigh\u000b\n\n\nSomewhere ages and ages hence:\u000b\n\n\nTwo roads diverged in a wood, and I—\u000b\n\n\nI took the one less traveled by,\u000b\n\n\nAnd that has made all the difference.\n\n\nRobert Frost, The Road Not Taken"
  },
  {
    "objectID": "slides/07-slides.html#how-can-we-identify-causal-effects",
    "href": "slides/07-slides.html#how-can-we-identify-causal-effects",
    "title": "Introduction to Causal Inference",
    "section": "How can we identify causal effects?",
    "text": "How can we identify causal effects?\n\nRecall the answers you proposed to bilingual causal question\n\ne.g. experiment: random assigment to bilingual training.\n\nHow does this work?\n\nConfounders equally distributed\nCount up results\nTake the average of the differences in the two groups."
  },
  {
    "objectID": "slides/07-slides.html#average-treatement-effect-in-a-randomised-experiment",
    "href": "slides/07-slides.html#average-treatement-effect-in-a-randomised-experiment",
    "title": "Introduction to Causal Inference",
    "section": "Average Treatement Effect in a Randomised Experiment",
    "text": "Average Treatement Effect in a Randomised Experiment\n\\begin{align}\nE(\\delta) = E(Y^{a=1} - Y^{a=0})\\\\\n          ~  = E(Y^{a=1}) - E(Y^{a=0}) \\\\\n          ~  = ATE\n\\end{align}\n\n\nE(\\delta) is the “estimand” or causal quantity of interest (the expected difference between the means of two randomised groups, or equivalentally, the mean of the differences)"
  },
  {
    "objectID": "slides/07-slides.html#assumptions-required-for-estimating-causal-effects-from-data",
    "href": "slides/07-slides.html#assumptions-required-for-estimating-causal-effects-from-data",
    "title": "Introduction to Causal Inference",
    "section": "Assumptions Required for Estimating Causal Effects From Data",
    "text": "Assumptions Required for Estimating Causal Effects From Data\n\nCausal Consistency: The values of exposure under comparisons correspond to well-defined interventions that, in turn, correspond to the versions of treatment in the data.(see: Chatton, Hernan & Robbins)\nPositivity: The probability of receiving every value of the exposure within all strata of co-variates is greater than zero\nExchangeablility:The conditional probability of receiving every value of an exposure level, though not decided by the investigators, depends only on the measured covariates (see: Chatton, Hernan & Robbins)"
  },
  {
    "objectID": "slides/07-slides.html#causal-consistency-assumption",
    "href": "slides/07-slides.html#causal-consistency-assumption",
    "title": "Introduction to Causal Inference",
    "section": "Causal Consistency assumption",
    "text": "Causal Consistency assumption\n\nThe values of exposure under comparisons correspond to well-defined interventions that, in turn, correspond to the versions of treatment in the data.\n\n\\begin{equation}\nY^{obs} = AY^{a=1} + (1-A)Y^{a=0}\n\\end{equation}"
  },
  {
    "objectID": "slides/07-slides.html#causal-consistency-gets-from-observations-to-counterfactuals",
    "href": "slides/07-slides.html#causal-consistency-gets-from-observations-to-counterfactuals",
    "title": "Introduction to Causal Inference",
    "section": "Causal Consistency gets from observations to counterfactuals",
    "text": "Causal Consistency gets from observations to counterfactuals\nFor individuals with exposure level A = 1:\n\\begin{equation}\n\\begin{split}\n(Y^{obs}|A = 1) &= 1 \\times A  \\times Y^{a=1} + (1-1) \\times Y^{a=0}\\\\\n& = 1 \\times Y^{a=1} + 0 \\times  Y^{a=0} \\\\\n& =  Y^{a=1}\n\\end{split}\n\\end{equation}\nFor individuals with exposure level A = 0:\n\\begin{equation}\n\\begin{split}\n(Y^{obs}|A = 0) &= 0 \\times A  \\times Y^{a=1} + (1-0) \\times Y^{a=0}\\\\\n& = 0 \\times Y^{a=1} + 1 \\times  Y^{a=0} \\\\\n& =  Y^{a=0}\n\\end{split}\n\\end{equation}\nWhich implies:\n\\begin{equation}\n\\begin{split}\nY_i &= Y_i^{a=1}~~~\\text{if}~ A_i = 1\\\\\nY_i &= Y_i^{a=0}~~~ \\text{if}~  A_i = 0\n\\end{split}\n\\end{equation}"
  },
  {
    "objectID": "slides/07-slides.html#positivity",
    "href": "slides/07-slides.html#positivity",
    "title": "Introduction to Causal Inference",
    "section": "Positivity",
    "text": "Positivity\n\nThe probability of receiving every value of the exposure within all strata of co-variates is greater than zero\n\n\\begin{equation}\n0 &lt; \\Pr(A=a|L)&lt;1, ~ \\forall a \\in A, ~ \\forall a \\in L\n\\end{equation}\n\nTwo types of positivity violations\n\nRandom non-positivity: the casual effect of aging with observations missing at ages 40-41 (we use parametric models as a work around.)\nDeterministic non-positivity: the causal effect of hysterectomy in biological males (assumption violated)."
  },
  {
    "objectID": "slides/07-slides.html#conditional-exchangeability",
    "href": "slides/07-slides.html#conditional-exchangeability",
    "title": "Introduction to Causal Inference",
    "section": "Conditional Exchangeability",
    "text": "Conditional Exchangeability\n\nThe conditional probability of receiving every value of an exposure level, though not decided by the investigators, depends only on the measured covariates\n\ndef: \\coprod means “independent of”, a|b translates to “a conditional on b”\n\\begin{equation}\nY^{a=1},Y^{a=0}\\coprod A|L\n\\end{equation}\nor equivalently\n\\begin{equation}\nA \\coprod Y^{a=1},Y^{a=0}|L\n\\end{equation}\nWhere L is the set of co-variates sufficient to ensure the independence of the counterfactual outcomes and the exposure."
  },
  {
    "objectID": "slides/07-slides.html#average-treatement-effect-in-observational-studies",
    "href": "slides/07-slides.html#average-treatement-effect-in-observational-studies",
    "title": "Introduction to Causal Inference",
    "section": "Average Treatement Effect in Observational Studies",
    "text": "Average Treatement Effect in Observational Studies\nWhere L is observed:\n\n\\begin{aligned}\nATE = E[Y^{a=1}|L = l] - E[Y^{a=0}|L = l] ~ \\text{for any value}~l\n\\end{aligned}"
  },
  {
    "objectID": "slides/07-slides.html#causal-graphs",
    "href": "slides/07-slides.html#causal-graphs",
    "title": "Introduction to Causal Inference",
    "section": "Causal Graphs",
    "text": "Causal Graphs\nPsychology starts with a question. “Does A cause Y?”\n\n\n\ncg-1"
  },
  {
    "objectID": "slides/07-slides.html#question",
    "href": "slides/07-slides.html#question",
    "title": "Introduction to Causal Inference",
    "section": "Question:",
    "text": "Question:\n\n\n\ncg-2"
  },
  {
    "objectID": "slides/07-slides.html#one-problem",
    "href": "slides/07-slides.html#one-problem",
    "title": "Introduction to Causal Inference",
    "section": "One Problem",
    "text": "One Problem\n\n\n\ncg-3"
  },
  {
    "objectID": "slides/07-slides.html#solution-collect-time-series-data",
    "href": "slides/07-slides.html#solution-collect-time-series-data",
    "title": "Introduction to Causal Inference",
    "section": "Solution: collect time-series data",
    "text": "Solution: collect time-series data\n\n\n\ncg-4"
  },
  {
    "objectID": "slides/07-slides.html#why-time-series-data-are-critical",
    "href": "slides/07-slides.html#why-time-series-data-are-critical",
    "title": "Introduction to Causal Inference",
    "section": "Why time series data are critical",
    "text": "Why time series data are critical\n\n\n\ncg-5"
  },
  {
    "objectID": "slides/07-slides.html#another-problem-common-causes-of-a-an-y-d-separation",
    "href": "slides/07-slides.html#another-problem-common-causes-of-a-an-y-d-separation",
    "title": "Introduction to Causal Inference",
    "section": "Another problem: common causes of A an Y (D-separation)",
    "text": "Another problem: common causes of A an Y (D-separation)\n\n\n\ncg-6"
  },
  {
    "objectID": "slides/07-slides.html#solution-condition-on-common-causes-to-ensure-d-separation",
    "href": "slides/07-slides.html#solution-condition-on-common-causes-to-ensure-d-separation",
    "title": "Introduction to Causal Inference",
    "section": "Solution: condition on common causes to ensure d-separation",
    "text": "Solution: condition on common causes to ensure d-separation\n\n\n\ncg-8"
  },
  {
    "objectID": "slides/07-slides.html#another-problem-conditioning-on-a-mediator",
    "href": "slides/07-slides.html#another-problem-conditioning-on-a-mediator",
    "title": "Introduction to Causal Inference",
    "section": "Another problem: conditioning on a mediator",
    "text": "Another problem: conditioning on a mediator\n\n\n\ncg-7"
  },
  {
    "objectID": "slides/07-slides.html#conditioning-on-a-mediator-is-a-common-problem",
    "href": "slides/07-slides.html#conditioning-on-a-mediator-is-a-common-problem",
    "title": "Introduction to Causal Inference",
    "section": "Conditioning on a mediator is a common problem",
    "text": "Conditioning on a mediator is a common problem\n\n\n\ncg-9"
  },
  {
    "objectID": "slides/07-slides.html#forshadowing-in-the-weeks-ahead-we-will-discuss-inferring-mediated-causal-effects",
    "href": "slides/07-slides.html#forshadowing-in-the-weeks-ahead-we-will-discuss-inferring-mediated-causal-effects",
    "title": "Introduction to Causal Inference",
    "section": "Forshadowing: in the weeks ahead we will discuss inferring mediated causal effects",
    "text": "Forshadowing: in the weeks ahead we will discuss inferring mediated causal effects"
  },
  {
    "objectID": "slides/07-slides.html#collider-bias",
    "href": "slides/07-slides.html#collider-bias",
    "title": "Introduction to Causal Inference",
    "section": "Collider bias",
    "text": "Collider bias\n\n\n\ncg-13"
  },
  {
    "objectID": "slides/07-slides.html#collider-bias-solution-do-not-condition-on-a-collider",
    "href": "slides/07-slides.html#collider-bias-solution-do-not-condition-on-a-collider",
    "title": "Introduction to Causal Inference",
    "section": "Collider bias: solution do not condition on a collider",
    "text": "Collider bias: solution do not condition on a collider\n\n\n\ncg-13"
  },
  {
    "objectID": "slides/07-slides.html#conditioning-on-descendants",
    "href": "slides/07-slides.html#conditioning-on-descendants",
    "title": "Introduction to Causal Inference",
    "section": "Conditioning on descendants",
    "text": "Conditioning on descendants\n\n\n\ncg-14"
  },
  {
    "objectID": "slides/07-slides.html#summary",
    "href": "slides/07-slides.html#summary",
    "title": "Introduction to Causal Inference",
    "section": "Summary",
    "text": "Summary\n\n\n\ncg-15"
  },
  {
    "objectID": "slides/07-slides.html#clear-advice-for-drawing-causal-graphs",
    "href": "slides/07-slides.html#clear-advice-for-drawing-causal-graphs",
    "title": "Introduction to Causal Inference",
    "section": "Clear Advice for Drawing Causal Graphs",
    "text": "Clear Advice for Drawing Causal Graphs\n\nEnsure that causes come before effects. Assign time indices to your variables.\nOrganize your variables chronologically.\nSimplify your graph by removing unnecessary nodes that don’t impact the assessment of bias between an exposure and an outcome.\nKeep in mind that Directed Acyclic Graphs (DAGs) are qualitative tools. They don’t represent non-linear associations or interactions.\nAvoid depicting interactions by crossing arrows.\nRemember that DAGs are distinct from graphs used in Structural Equation Modeling (SEM). Be cautious of SEM literature, as it often overlooks the assumptions needed for causal inference."
  },
  {
    "objectID": "slides/07-slides.html#summary-drawing-causal-graphs-dags",
    "href": "slides/07-slides.html#summary-drawing-causal-graphs-dags",
    "title": "Introduction to Causal Inference",
    "section": "Summary: Drawing Causal Graphs (DAGs)",
    "text": "Summary: Drawing Causal Graphs (DAGs)\n\nDirected Acyclic Graphs (DAGs) help visualize sources of bias.\nThere are five main sources of bias:\n\nTemporal order ambiguity: Uncertainty about whether causes precede effects. Solution: Collect time series data or clarify assumptions when unavailable.\nCommon causes of exposure and outcome: Address this by including common causes in your statistical model (e.g., using regression).\nConditioning on a mediator: Avoid this unless mediation is of interest.\nConditioning on a collider: Refrain from doing this.\nBias induced by conditioning on a confounder’s descendant: Draw your DAG and follow guidelines for points 1-4.\n\nImportant Note 1: In observational studies, it’s impossible to guarantee complete control for confounding. Always conduct sensitivity analyses. Techniques for sensitivity analyses will be discussed next week.\nImportant Note 2: Methods for computing causal effects for group comparisons will be covered in the following week’s lecture."
  },
  {
    "objectID": "slides/07-slides.html#what-have-we-learned",
    "href": "slides/07-slides.html#what-have-we-learned",
    "title": "Introduction to Causal Inference",
    "section": "What have we learned?",
    "text": "What have we learned?\n\nCause/Effect: a contrast between the world under different interventions, at most one of which is realised.\nConfounding: failure to condition on a common cause.\nCollider bias: conditioning on a common effect\n\n\nMetric equivalence: Factor loadings are similar across groups. Configural equivalence: The factor structure is the same across groups in a multi-group confirmatory factor analysis. Scalar equivalence: Values/Means are also equivalent across groups."
  },
  {
    "objectID": "content/course-outline.html#week-6---causal-inference-and-effect-modification",
    "href": "content/course-outline.html#week-6---causal-inference-and-effect-modification",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "(Hernan and Robins 2020) Chapters 4-5 link\n\n\n\n\n[Bulbulia (2024c)][link](https://www.cambridge.org/core/journals/evolutionary-human-sciences/article/methods-in-causal-inference-part-2-interaction-mediation-and-timevarying-treatments/D7FD95D3ED64FE0FBBEC37AC6CEAFBC1)\n(Tyler J. VanderWeele and Robins 2007) link\n(Tyler J. VanderWeele 2009) link\n\n\n\n\n\nEffect Modification: Definine your Causal Estimand\nDistinguishing Cultural Effect-Modification from the confused and conflated concepts of “Moderation”, “Mediation”, “Interaction.”\nDetour into Causal Mediation\n\n\n\n\n\nAnalysis step 1: data wrangling and descriptive tables/graphs"
  },
  {
    "objectID": "content/course-outline.html#week-7---in-class-test-25",
    "href": "content/course-outline.html#week-7---in-class-test-25",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Assessment covering key terms and concepts taught so far."
  },
  {
    "objectID": "content/course-outline.html#week-8---causal-inference-estimating-marginal-structural-models-inverse-probability-of-treatment-weighting-conditional-average-treatment-effects-iptw-when-groups-are-compared.",
    "href": "content/course-outline.html#week-8---causal-inference-estimating-marginal-structural-models-inverse-probability-of-treatment-weighting-conditional-average-treatment-effects-iptw-when-groups-are-compared.",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Workflow for causal question formulation, population statement, and causal diagram creation\nMarginal Structural Models: propensity scores and Inverse Probability of Treatment Weighting (IPTW)\nIPTW when estimating conditional causal effects\nEstimation techniques evaluating evidence for group-wise effect modification using R.\n\n\n\n\n\n(Greifer 2023) link\n(Tyler J. VanderWeele, Mathur, and Chen 2020) link\n\n\n\n\n(Bulbulia 2024a) link\n(Hoffman et al. 2023) link\n\n\n\n\n\n\nEstimation ATE; CATE"
  },
  {
    "objectID": "content/course-outline.html#week-9---hands-on-analysis",
    "href": "content/course-outline.html#week-9---hands-on-analysis",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Second assessment covering advanced topics in causal inference\nTopics include ATE, Effect-Modification, fundamental assumptions of causal inference, experiments, and real-world confounding\n\n\n\n\n\nPreparing your analysis: Hands On Study!"
  },
  {
    "objectID": "content/course-outline.html#week-10---hands-on-working-with-quarto-manuscript",
    "href": "content/course-outline.html#week-10---hands-on-working-with-quarto-manuscript",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "No readings, do your take-home assignment (see course details)."
  },
  {
    "objectID": "content/course-outline.html#lab-8",
    "href": "content/course-outline.html#lab-8",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Creating and managing Quarto documents for publication quality research workflows"
  },
  {
    "objectID": "content/course-outline.html#week-11---measurement-matters",
    "href": "content/course-outline.html#week-11---measurement-matters",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Factor analysis, confirmatory factor analysis (CFA), multigroup CFA, partial invariance\nWorked example on configural, metric, and scalar equivalence\n\n\n\n\n\n(Fischer and Karl 2019) link\n\n\n\n\n(Vijver et al. 2021) link\n(He and Vijver 2012) link\n(J. [et. al]. Harkness 2003) link\n\n\n\n\n\n\n\n\n\nUnderstanding causal assumptions of measurement theory\nGuidance on your final assessment.\n\n\n\n\n\n[Tyler J. VanderWeele (2022)][link](https://www.dropbox.com/scl/fi/mmyguc0hrci8wtyyfkv6w/tyler-vanderweele-contruct-measures.pdf?rlkey=o18fiyajdqqpyjgssyh6mz6qm&dl=0)\n[Bulbulia (2024d)][link](https://www.cambridge.org/core/journals/evolutionary-human-sciences/article/methods-in-causal-inference-part-3-measurement-error-and-external-validity-threats/4D35FFDECF32B2EFF7557EC26075175F)\n\n\n\n\n(J. A. Harkness, Van de Vijver, and Johnson 2003) link"
  },
  {
    "objectID": "content/course-outline.html#week-3-causal-diagrams-the-structures-of-confounding-bias",
    "href": "content/course-outline.html#week-3-causal-diagrams-the-structures-of-confounding-bias",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Confounding bias using causal diagrams\nApplication of regression and simulation in R\n\n\n\n\n\nPractical exercises in R: regression and ggdag\n\n\n\n\n\n(Bulbulia 2024b) link\nsee simplified reading\n\n\n\n\n(Suzuki, Shinozaki, and Yamamoto 2020) link\n(Neal 2020) Chapter 3 link\n(Hernan and Robins 2024) Chapter 6 link"
  },
  {
    "objectID": "content/course-outline.html#week-3---causal-diagrams-the-structures-of-confounding-bias",
    "href": "content/course-outline.html#week-3---causal-diagrams-the-structures-of-confounding-bias",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Confounding bias using causal diagrams\nApplication of regression and simulation in R\n\n\n\n\n\nPractical exercises in R: regression and ggdag\n\n\n\n\n\n(Bulbulia 2024b) link\nsee simplified reading\n\n\n\n\n[Bulbulia (2024d)][link](https://www.cambridge.org/core/journals/evolutionary-human-sciences/article/methods-in-causal-inference-part-3-measurement-error-and-external-validity-threats/4D35FFDECF32B2EFF7557EC26075175F)\n(Suzuki, Shinozaki, and Yamamoto 2020) link\n(Neal 2020) Chapter 3 link\n(Hernan and Robins 2020) Chapter 6 link"
  },
  {
    "objectID": "slides/01-slides.html",
    "href": "slides/01-slides.html",
    "title": "Seminar 1",
    "section": "",
    "text": "Understand the special problems that cultural research presents for measurement.\nUnderstand the deeper, and prior concept of confounding.\n\n\n\n\n\nMeasurement\nValidity\nA confounder"
  },
  {
    "objectID": "slides/extra-slides.html",
    "href": "slides/extra-slides.html",
    "title": "How to ask a Why Question",
    "section": "",
    "text": "Why why?"
  },
  {
    "objectID": "slides/extra-slides.html#why-questions",
    "href": "slides/extra-slides.html#why-questions",
    "title": "How to ask a Why Question",
    "section": "",
    "text": "Why why?"
  },
  {
    "objectID": "slides/extra-slides.html#test",
    "href": "slides/extra-slides.html#test",
    "title": "How to ask a Why Question",
    "section": "Test",
    "text": "Test\nAt the core:\n[ Y|A=1 Y_{0} ]"
  },
  {
    "objectID": "slides/extra-slides.html#dags",
    "href": "slides/extra-slides.html#dags",
    "title": "How to ask a Why Question",
    "section": "Dags",
    "text": "Dags\n\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ncoords &lt;- read.table(header = TRUE, text = \"\n  x y name\n  0 1 U\n  1 .1 L0\n  2 0 A0\n  3 0.1 Y0\n  4 0 A1\n  5 0.2 Y2\n\")\n\nd &lt;- dagify(\n  A0 ~ L0,\n  A1 ~ A0,\n  L0 ~ U,\n  Y0 ~ A0,\n  Y2 ~ A1 + L0,\n  Y2 ~ U + Y0,\n  coords = coords)\n\nggdag(d) +\n   theme_dag()"
  },
  {
    "objectID": "slides/extra-slides.html#code",
    "href": "slides/extra-slides.html#code",
    "title": "How to ask a Why Question",
    "section": "Code",
    "text": "Code\nlibrary(ggdag)\nlibrary(dplyr)\n\n\ncoords &lt;- read.table(header = TRUE, text = \"\n  x y name\n  0 1 U\n  1 0 L0\n  2 0 A0\n  3 0 Y0\n  4 0 A1\n  5 0 Y2\n\")\n\nd &lt;- dagify(\n  A1 ~ L0 + A0,\n  L0 ~ U,\n  Y0 ~ U,\n  Y2 ~ A1,\n  Y2 ~ U + Y0,\n  coords = coords)\n\nggdag(d) +\n   theme_dag()"
  },
  {
    "objectID": "slides/extra-slides.html#dag",
    "href": "slides/extra-slides.html#dag",
    "title": "How to ask a Why Question",
    "section": "DAG",
    "text": "DAG\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n# coordinates for graph\ncoords &lt;- read.table(header = TRUE, text = \"\n  x y name\n  0 -1 U\n  0 1 L\n  2 0 A0\n  3 0 Y0\n  4 0 A1\n  6 0 Y2\n\")\n\n# make into daggity object\nd &lt;- dagify(\n  L ~ U,\n  A0 ~  L + U,\n  Y0 ~  L + U,\n  A1 ~  L + U ,\n  Y2 ~  L + U,\n  exposure = \"A1\",\n  outcome = \"Y2\",\n  latent = \"U\",\n  coords = coords)\n\n# plot\nggdag(d,  text = TRUE, node = FALSE, \n      text_size = 9,\n      text_col = \"black\") +\n   theme_dag()"
  },
  {
    "objectID": "slides/extra-slides.html#check-confounding",
    "href": "slides/extra-slides.html#check-confounding",
    "title": "How to ask a Why Question",
    "section": "Check confounding",
    "text": "Check confounding\n\nggdag_adjustment_set(d) + theme_dag_blank()\n\nWarning in dag_adjustment_sets(., exposure = exposure, outcome = outcome, : Failed to close backdoor paths. Common reasons include:\n            * graph is not acyclic\n            * backdoor paths are not closeable with given set of variables\n            * necessary variables are unmeasured (latent)"
  },
  {
    "objectID": "slides/extra-slides.html#visualise-confounding",
    "href": "slides/extra-slides.html#visualise-confounding",
    "title": "How to ask a Why Question",
    "section": "Visualise confounding",
    "text": "Visualise confounding\n\nggdag_dseparated(d,\n  controlling_for = c(\"L\", \"A0\", \"Y0\"),\n  text = TRUE, collider_lines = FALSE\n) + theme_dag()"
  },
  {
    "objectID": "slides/extra-slides.html#test-dag",
    "href": "slides/extra-slides.html#test-dag",
    "title": "How to ask a Why Question",
    "section": "test dag",
    "text": "test dag\ninstall.packages(c(\"coda\",\"mvtnorm\",\"devtools\",\"loo\",\"dagitty\",\"shape\"))\ndevtools::install_github(\"rmcelreath/rethinking\")\nlibrary(rethinking)\ndrawdag(d, arrow = \"gray\",  Z=list(\"U\"))\n\n\nhttps://gist.github.com/andrewheiss/4ece621813a27dfdcaef7f1c2d773237"
  },
  {
    "objectID": "laboratory/Readme.html",
    "href": "laboratory/Readme.html",
    "title": "PSYC 434 Conducting Research Across Cultures: Trimester 1, 2025",
    "section": "",
    "text": "Structure is as follows:\n\n\n01-lab.R - introduction to R script: intsallation/basic commands/graphing"
  },
  {
    "objectID": "laboratory/Readme.html#labs",
    "href": "laboratory/Readme.html#labs",
    "title": "PSYC 434 Conducting Research Across Cultures: Trimester 1, 2025",
    "section": "",
    "text": "Structure is as follows:\n\n\n01-lab.R - introduction to R script: intsallation/basic commands/graphing"
  },
  {
    "objectID": "slides/07-slides.html",
    "href": "slides/07-slides.html",
    "title": "Introduction to Causal Inference",
    "section": "",
    "text": "Understand the definition of “causality” as it is used in the human and health sciences.\nUnderstand the assumptions required for consistently estimating causal effects.\nUnderstand how to use causal diagrammes to assess these assumptions.\n\n\n\n\n\nCause/Effect\nConfounder\nCollider"
  },
  {
    "objectID": "content/05-content.html#how-can-we-make-contrasts-between-counterfactual-potential-outcomes",
    "href": "content/05-content.html#how-can-we-make-contrasts-between-counterfactual-potential-outcomes",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "How can we make contrasts between counterfactual (potential) outcomes?",
    "text": "How can we make contrasts between counterfactual (potential) outcomes?\n\nFundamental Assumption 1: Causal Consistency\nCausal consistency means that the potential outcome corresponding to the exposure an individual actually receives is exactly what we observe. In other words, if individual i receives exposure a, then the potential outcome (or equivalently the counterfactual outcome under a given level of exposure A=a – that is Y_i(a) – is equivalent to the the observed outcome: Y_i \\mid A_i \\equiv a. Where the symbol \\equiv means “equivalent to”, when we assume that the causal consistency assumption is satisfied, we assume that:\n\n\\begin{aligned}\n\\underbrace{Y_i(1)}_{\\text{counterfactual}} &\\equiv \\underbrace{(Y_i \\mid A_i = 1)}_{\\text{observable}}, \\\\\n\\underbrace{Y_i(0)}_{\\text{counterfactual}} &\\equiv \\underbrace{(Y_i \\mid A_i = 0)}_{\\text{observable}}.\n\\end{aligned}\n\nNotice however that we cannot generally obtain individual causal effects because at any given time, each individual may only receive at most one leve of an exposure. Where the symbol \\implies means “implies,” at any given time, receiving one level of an exposure precludes receiving any other level of that exposure:\n\nY_i|A_i = 1 \\implies Y_i(0)|A_i = 1~ \\text{is counterfactual}\n Likewise:\n\nY_i|A_i = 0 \\implies Y_i(1)|A_i = 1~ \\text{is counterfactual}\n\nBecause of the laws of physics (above the atomic scale), an individual can experience only one exposure level at any moment. Consequently, we can observe only one of the two counterfactual outcomes needed to quantify a causal effect. This is the fundamental problem of causal inference. Counterfactual contrasts cannot be individually observed.\nHowever, because of the causal consistency assumption, we can nevertheless recover half of the missing counterfactual (or “potential”) outcomes needed to estimate average treatment effects. We may do this if two other assumptions are satisfied.\n\n\nFundamental Assumption 2: Exchangeability\nExchangeability justifies recovering unobserved counterfactuals from observed outcomes and averaging them. By accepting that Y_i(a) = Y_i if A_i = a, we can estimate population-level average potential outcomes. In an experiment where exposure groups are comparable, we define the Average Treatment Effect (ATE) as:\n\n\\begin{aligned}\n\\text{ATE} &= \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)] \\\\\n           &= \\mathbb{E}(Y \\mid A=1) \\;-\\; \\mathbb{E}(Y \\mid A=0).\n\\end{aligned}\n Because randomisation ensures that missing counterfactuals are exchangeable with those observed, we can still estimate \\mathbb{E}[Y(a)]. For instance, assume:\n\n\\underbrace{\\mathbb{E}[Y(1)\\mid A=1]}_{\\text{counterfactual}}  = \\textcolor{red}{\\underbrace{\\mathbb{E}[Y(1)\\mid A=0]}_{\\text{unobservable}}}  =  \\underbrace{(Y_i \\mid A_i = 1)}_{\\text{observed}}\n\nwhich lets us infer the average outcome if everyone were treated. Likewise, if\n\n\\underbrace{\\mathbb{E}[Y(0)\\mid A=0]}_{\\text{counterfactual}}  = \\textcolor{red}{\\underbrace{\\mathbb{E}[Y(1)\\mid A=0]}_{\\text{unobservable}}}  =  \\underbrace{\\mathbb{E}(Y \\mid A_i = 0)}_{\\text{observed}}\n\nthen we can infer the average outcome if everyone were given the control. The difference between these two quantities gives the ATE:\n\n\\text{ATE} = \\Big[\n\\overbrace{\\mathbb{E}[Y(1)\\mid A=1]}^{\\substack{\\text{by consistency:}\\\\ \\equiv \\text{ observed } \\; \\mathbb{E}[Y\\mid A=1]}}\n\\;+\\;\n\\overbrace{\\textcolor{red}{\\mathbb{E}[Y(1)\\mid A=0]}}^{\\substack{\\text{by exchangeability:}\\\\ \\text{unobservable, yet } \\; \\equiv \\mathbb{E}[Y\\mid A=1]}}\n\\Big]\n-\\,\n\\Big[\n\\overbrace{\\mathbb{E}[Y(0)\\mid A=0]}^{\\substack{\\text{by consistency:}\\\\ \\equiv  \\text{observed } \\; \\mathbb{E}[Y\\mid A=0]}}\n\\;+\\;\n\\overbrace{\\textcolor{red}{\\mathbb{E}[Y(0)\\mid A=1]}}^{\\substack{\\text{by exchangeability:}\\\\ \\text{unobservable, yet } \\; \\equiv \\mathbb{E}[Y\\mid A=0]}}\n\\Big]\n\nWe have it that \\mathbb{E}[Y\\mid A=1] and \\mathbb{E}[Y\\mid A=0] and \\mathbb{E}[Y(1)\\mid A=0] are observed. If both consistency and exchangeability are satisifed then we may use these observed quantities to identify contrasts of counterfactual quanities.\nThus, although individual-level counterfactuals are missing, the consistency assumptions and the exchangeability assumptions allow us to identify the average effect of treatment using observed data. Randomised controlled experiments allow us to meet these assumptions. Randomisation warrents the exchangeability assumption. Control warrents the consistency asumption.\n\n\nFundamental Assumption 3: Positivity\nThere is one further assumption, called positivity. It states that treatment assignments cannot be deterministic. That is, for every covariate pattern L = l, each individual has a non-zero probability of receiving ever treatment level to be compared:\n\nP(A = a \\mid L = l) &gt; 0.\n\nRandomised experiments achieve positivity by design – at least for the sample that is selected into the study. In observational settings violations occur if some subgroups never receive a particular treatment. If treatments occur but are rare, we may have sufficient data from which to obtain convincing causal inferences.\nPositivity is the only assumption that can be verified with data. We will consider how to assess this assumption using data when we develop our data analytic workflows in the second half of this course."
  },
  {
    "objectID": "content/05-content.html#average-treatment-effect-in-randomised-controlled-experiments",
    "href": "content/05-content.html#average-treatment-effect-in-randomised-controlled-experiments",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Average Treatment Effect in Randomised Controlled Experiments",
    "text": "Average Treatment Effect in Randomised Controlled Experiments\nBecause of randomisation, we can infer average treatment effects (ATE) from experiments despite the fact that some parts of the potential outcomes remain unobserved. In a schematic table:\n\n\n\n\n\\text{ATE} = \\Big[ \\underbrace{\\mathbb{E}[Y(1)\\mid A=1]}_{\\text{observed}} + \\textcolor{red}{\\underbrace{\\mathbb{E}[Y(1)\\mid A=0]}_{\\text{unobserved}}} \\Big]\n\\;-\\;\n\\Big[ \\underbrace{\\mathbb{E}[Y(0)\\mid A=0]}_{\\text{observed}} + \\textcolor{red}{\\underbrace{\\mathbb{E}[Y(0)\\mid A=1]}_{\\text{unobserved}}} \\Big].\n\n\n\nTable 1\n\n\n\nHere, \\mathbb{E}[Y(1)\\mid A=1] is observable; \\mathbb{E}[Y(1)\\mid A=0] and \\mathbb{E}[Y(0)\\mid A=1] remain unobserved. In experiments, although individual-level counterfactual outcomes are missing, randomisation ensures that the observed data can still identify what happens on average if the treatment is applied to the population. This estimand is called the Average Treatment Effect or the Marginal Effect of the treatment."
  },
  {
    "objectID": "content/05-content.html#challenges-with-observational-data",
    "href": "content/05-content.html#challenges-with-observational-data",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Challenges with Observational Data",
    "text": "Challenges with Observational Data\n\n1. Satisfying Causal Consistency is Difficult in Observational Settings\nBelow are some ways in which real-world complexities can violate causal consistency in observational studies. For example, causal consistency requires there is no interference between units (also called “SUTVA” or “Stable Unit Treatment Value.” Causal consistency also requires that each treatment level is well-defined and applied uniformly. If these conditions fail, then Y(a) may not reflect a consistent exposures across individuals. We are then comparing apples with oranges. Consider some examples:\n\nCultural differences: one group’s “second-language exposure” may differ qualitatively from another’s if cultural norms shape how, when, or by whom the second language is taught. For instance, a child in a bilingual community may receive diverse and immersive language experiences all of which are are coded as A=1. Yet the treatments might be quite different. We might be comparing apples with oranges.\nAge of acquisition: the developmental effect of learning a second language may vary by when the child is exposed. Comparing aquisition at, say, age two with aquisition at say, age twelve, might be comparing apples with oranges.\nLanguage variation: sign languages, highly tonal languages, or unwritten languages may demand different cognitive tasks than spoken, nontonal, or widely documented languages. Thus, lumping them together as “learning a second language” can obscure the fact that these distinct exposures might produce fundamentally different outcomes. Again, comparisons here would be of apples with oranges.\n\nThese sources of heterogeneity reveal why careful delineation of treatments is crucial. If the actual exposures differ across individuals, then consistency (Y_i(a) = Y_i \\mid A_i = a) may fail, because A=a is not the same phenomenon for everyone.\n\n\n2. Conditional Exchangeability (No Unmeasured Confounding) Is Difficult to Achieve\nIn theory, we can identify a causal effect from observational data if all confounders L are measured. Formally, we need the potential outcomes to be independent of treatment once we condition on L. One way to express this asssumption is: Y(a) \\coprod A \\mid L. If the potential outcomes are independent of treatment assignment, we can identify the Average Treatment Effect (ATE) as: \n\\text{ATE} \\;=\\; \\sum_{l}\n\\Bigl[\\mathbb{E}\\bigl(Y \\mid A=1, L=l\\bigr) \\;-\\; \\mathbb{E}\\bigl(Y \\mid A=0, L=l\\bigr)\\Bigr] \\;\\Pr(L=l).\n\nIn randomised experiments, conditioning is automatic because A is unrelated to potential outcomes by design. In observational studies, ensuring or approximating such conditional exchangeability is often difficult. For example, bilingualism research would need to consider:\n\nCultural histories: cultures that value language acquistion might also value knowledge acquistion. Associations might arise from Culture, not causation.\nPersonal values: families who place a high priority on bilingualism may also promote other developmental resources.\n\nIf important confounders go unmeasured or are poorly measured, these differences can bias causal effect estimates.\n\n\n3. The Positivity Assumption May Fail: Treatments Might Not Exist for All\nPositivity requires that each individual could, in principle, receive any exposure level. But in real-world observational settings, some groups have no access to bilingual education (or no reason to be monolingual), making certain treatment levels impossible for them. If a treatment level does not appear in the data for a given subgroup, any causal effect estimate for that subgroup is purely an extrapolation (Westreich and Cole 2010; Hernan and Robins 2023).\n\nWestreich, Daniel, and Stephen R. Cole. 2010. “Invited commentary: positivity in practice.” American Journal of Epidemiology 171 (6). https://doi.org/10.1093/aje/kwp436.\n\n———. 2023. Causal Inference. Chapman & Hall/CRC Monographs on Statistics &Applied Probab. Taylor & Francis. https://books.google.co.nz/books?id=\\_KnHIAAACAAJ."
  },
  {
    "objectID": "content/05-content.html#summary",
    "href": "content/05-content.html#summary",
    "title": "Causal Inference: Average Treatment Effects",
    "section": "Summary",
    "text": "Summary\nWe introduced the fundamental problem of causal inference by distinguishing correlation (associations in the data) from causation (contrasts between potential outcomes, of which only one can be observed for each individual).\nRandomised experiments address this problem by balancing confounding variables across treatment levels. Although individual causal effects are unobservable, random assignment allows us to infer average causal effects — also called marginal effects.\nIn observational data, inferring average treatment effects demands that we satisfy three assumptions that are automatically satisfied in (well-conducted) experiments: causal consistency, exchangeability, and positivity. These assumptions ensure that we can compare like-with-like (that the population-level treatment effect is consistent across individuals), that there are no unmeasured common causes of the exposure and outcomes that may lead to associations in the absence of causality, and that every exposure level is a real possibility for each subgroup."
  },
  {
    "objectID": "content/06-content.html#in-causal-inference-what-do-the-words-moderation-and-interaction-mean",
    "href": "content/06-content.html#in-causal-inference-what-do-the-words-moderation-and-interaction-mean",
    "title": "Causal Inference: Effect-Modification and Interaction",
    "section": "In Causal Inference, What do the words “Moderation” and “Interaction” mean?",
    "text": "In Causal Inference, What do the words “Moderation” and “Interaction” mean?\nWe here these words used freely in psychology. What do they mean? Causal inference allows us to be precise.\nLet’s set the term moderation to the side, and in its place consider “effect-modification.” Our task today is to use counterfactual notation to distinguish between two concepts:\n\nInteraction\nEffect-modification"
  },
  {
    "objectID": "content/06-content.html#baseline-table",
    "href": "content/06-content.html#baseline-table",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Baseline table",
    "text": "Baseline table\n\ntables$baseline_table\n\n\n\n\n\n\n\n\n\n\n\nExposure + Demographic Variables\nN = 14,4391\n\n\n\n\nAge\n\n\n\n\n    Mean (SD)\n50 (14)\n\n\n    Min, Max\n18, 94\n\n\n    Q1, Q3\n41, 61\n\n\nAlert Level Combined Lead\n\n\n\n\n    no_alert\n10,261 (71%)\n\n\n    early_covid\n1,617 (11%)\n\n\n    alert_level_1\n1,272 (8.8%)\n\n\n    alert_level_2\n366 (2.5%)\n\n\n    alert_level_2_5_3\n253 (1.8%)\n\n\n    alert_level_4\n670 (4.6%)\n\n\nBorn Nz\n11,398 (79%)\n\n\n    Unknown\n25\n\n\nEdu\n\n\n\n\n    Mean (SD)\n5.38 (2.72)\n\n\n    Min, Max\n0.00, 10.00\n\n\n    Q1, Q3\n3.00, 7.00\n\n\n    Unknown\n110\n\n\nEmployed\n11,466 (80%)\n\n\n    Unknown\n22\n\n\nEth Cat\n\n\n\n\n    euro\n11,835 (83%)\n\n\n    maori\n1,526 (11%)\n\n\n    pacific\n314 (2.2%)\n\n\n    asian\n655 (4.6%)\n\n\n    Unknown\n109\n\n\nHousehold Inc log\n\n\n\n\n    Mean (SD)\n11.41 (0.74)\n\n\n    Min, Max\n0.71, 14.40\n\n\n    Q1, Q3\n11.00, 11.92\n\n\n    Unknown\n655\n\n\nMale\n5,238 (36%)\n\n\nNeighbourhood Community\n\n\n\n\n    Mean (SD)\n4.23 (1.66)\n\n\n    Min, Max\n1.00, 7.00\n\n\n    Q1, Q3\n2.99, 5.95\n\n\n    Unknown\n73\n\n\nParent\n10,350 (72%)\n\n\n    Unknown\n8\n\n\nPartner\n10,661 (76%)\n\n\n    Unknown\n399\n\n\nReligion Religious\n5,218 (36%)\n\n\n    Unknown\n19\n\n\nRural Gch 2018 l\n\n\n\n\n    1\n8,822 (62%)\n\n\n    2\n2,784 (19%)\n\n\n    3\n1,740 (12%)\n\n\n    4\n820 (5.7%)\n\n\n    5\n170 (1.2%)\n\n\n    Unknown\n103\n\n\nSample Weights\n\n\n\n\n    Mean (SD)\n0.94 (1.22)\n\n\n    Min, Max\n0.30, 19.96\n\n\n    Q1, Q3\n0.40, 1.03\n\n\n\n1 n (%)"
  },
  {
    "objectID": "content/06-content.html#exposure-table",
    "href": "content/06-content.html#exposure-table",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Exposure table",
    "text": "Exposure table\n\n# tables$exposure_tables"
  },
  {
    "objectID": "content/06-test-lecture.html",
    "href": "content/06-test-lecture.html",
    "title": "Causal Inference: Effect Modification, Interaction, and Conditional Average Treatment Effects",
    "section": "",
    "text": "Note\n\n\n\nRequired Reading - (Hernan and Robins 2020) Chapters 4-5 link\nOptional Reading - (Tyler J. VanderWeele and Robins 2007) link - (Tyler J. VanderWeele 2009) link"
  },
  {
    "objectID": "content/06-test-lecture.html#if-you-learn-nothing-else-from-this-course",
    "href": "content/06-test-lecture.html#if-you-learn-nothing-else-from-this-course",
    "title": "Causal Inference: Effect Modification, Interaction, and Conditional Average Treatment Effects",
    "section": "If you learn nothing else from this course…",
    "text": "If you learn nothing else from this course…\nTo answer a psychological question scientifically, we must first state it precisely. Causal inference provides the framework for doing so."
  },
  {
    "objectID": "content/06-test-lecture.html#causal-inference-begins-by-stating-a-counterfactual-contrast",
    "href": "content/06-test-lecture.html#causal-inference-begins-by-stating-a-counterfactual-contrast",
    "title": "Causal Inference: Effect Modification, Interaction, and Conditional Average Treatment Effects",
    "section": "Causal inference begins by stating a counterfactual contrast",
    "text": "Causal inference begins by stating a counterfactual contrast\nIn the simplest case, we define a contrast between potential outcomes under two or more alternative conditions, often called “treatments” or “exposures” (denoted A). For an outcome Y, we might compare the potential outcome if everyone received treatment level a^* versus if everyone received treatment level a. The average difference in the population is the Average Treatment Effect (ATE):\n\n\\text{ATE} = \\mathbb{E}[Y(a^*) - Y(a)]\n\nUnderstanding causal inference involves comparing what actually happened with what could have happened (counterfactuals) on average, under different specified conditions, within a defined target population. A crucial part of this process involves identifying and accounting for factors (confounders) that could distort the comparison and lead to incorrect conclusions about the causal question asked."
  },
  {
    "objectID": "content/06-test-lecture.html#in-causal-inference-what-do-interaction-and-effect-modification-mean",
    "href": "content/06-test-lecture.html#in-causal-inference-what-do-interaction-and-effect-modification-mean",
    "title": "Causal Inference: Effect Modification, Interaction, and Conditional Average Treatment Effects",
    "section": "In Causal Inference, What do ‘Interaction’ and ‘Effect Modification’ mean?",
    "text": "In Causal Inference, What do ‘Interaction’ and ‘Effect Modification’ mean?\nWe often encounter terms like ‘moderation’ and ‘interaction’ used loosely in psychological science. Causal inference demands greater precision. We will set aside the ambiguous term ‘moderation’ and focus on clearly distinguishing two specific concepts using the counterfactual framework:\n\nInteraction\nEffect Modification"
  },
  {
    "objectID": "content/06-test-lecture.html#interaction-the-effect-of-joint-interventions",
    "href": "content/06-test-lecture.html#interaction-the-effect-of-joint-interventions",
    "title": "Causal Inference: Effect Modification, Interaction, and Conditional Average Treatment Effects",
    "section": "Interaction: The Effect of Joint Interventions",
    "text": "Interaction: The Effect of Joint Interventions\nIn the framework of causal inference, interaction specifically relates to evaluating a joint intervention. We are interested in the combined causal effect when we simultaneously intervene on two (or more) distinct treatments or exposures.\nLet these distinct treatments be A and B, and the outcome be Y.\nRecall the potential outcome under a single intervention setting A to level \\tilde{a} is Y(\\tilde{a}). We typically aim to identify this from observational data using assumptions such as consistency (Y_i(\\tilde{a}) = (Y_i |A_i = \\tilde{a}) for individual i) and conditional exchangeability (no unmeasured confounding) given a set of measured covariates L: Y(\\tilde{a}) \\perp\\kern-5pt\\perp A | L. The set L must be sufficient to block all non-causal (backdoor) paths between A and Y.\nWhen considering a joint intervention on both A and B, we extend the notation to Y(\\tilde{a}, \\tilde{b}). This represents the potential outcome if treatment A were set to \\tilde{a} and treatment B were set to \\tilde{b} for an individual or population.\nIdentification of Y(\\tilde{a}, \\tilde{b}) requires analogous assumptions for both interventions. In addition to needing L for the A \\to Y relationship (Y(\\tilde{a}) \\perp\\kern-5pt\\perp A | L), we also need a set of covariates Q sufficient to ensure conditional exchangeability for the B \\to Y relationship: Y(\\tilde{b}) \\perp\\kern-5pt\\perp B | Q. Consistency must also hold for B.\nThe sets of required covariates L and Q may overlap (L \\cap Q \\not\\equiv \\emptyset). Critically, identifying causal interaction requires sufficient data to control for all variables in the union of these sets, L \\cup Q.\n\nDefining Interaction with a Counterfactual Contrast\nLet’s use an educational example. Consider the effect of a new teaching method (treatment A: 1=New, 0=Old) on student test scores (outcome Y). Suppose we are also interested in the effect of providing extra tutoring (treatment B: 1=Yes, 0=No). We want to assess the individual and combined effects of A and B on test scores, perhaps on the difference scale (additive scale).\nUsing counterfactuals, we define causal interaction on the additive scale as occurring when the effect of intervening on both A and B simultaneously differs from the sum of the effects of intervening on each one individually (always relative to a common baseline, usually A=0, B=0). The interaction is non-zero if:\n\n\\bigg(\\underbrace{\\mathbb{E}[Y(1,1)]}_{\\text{joint intervention}} - \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither intervention}}\\bigg) - \\bigg[ \\bigg(\\underbrace{\\mathbb{E}[Y(1,0)]}_{\\text{only A intervention}} - \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither intervention}}\\bigg) + \\bigg(\\underbrace{\\mathbb{E}[Y(0,1)]}_{\\text{only B intervention}} - \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither intervention}} \\bigg)\\bigg] \\neq 0\n\nThis expression simplifies to the standard definition of additive interaction (see appendix):\n\n\\underbrace{\\mathbb{E}[Y(1,1)]}_{\\text{joint}} - \\underbrace{\\mathbb{E}[Y(1,0)]}_{\\text{only A}} - \\underbrace{\\mathbb{E}[Y(0,1)]}_{\\text{only B}} + \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{neither}} \\neq 0\n\nThis specific quantity is the causal estimand for additive interaction. It captures the extent to which the joint effect deviates from additivity. A positive value implies synergy (superadditivity); a negative value implies antagonism (subadditivity). Note the definition is symmetric regarding A and B: Y(\\tilde{a}, \\tilde{b}) \\equiv Y(\\tilde{b}, \\tilde{a}).\n(Other scales, like the multiplicative/ratio scale, can define interaction, potentially leading to different conclusions. The choice of scale should always be justified.)\n\nIdentification of Causal Interaction\nTo estimate this causal interaction from data, we must identify all four potential outcome means: \\mathbb{E}[Y(0,0)], \\mathbb{E}[Y(1,0)], \\mathbb{E}[Y(0,1)], and \\mathbb{E}[Y(1,1)]. This requires controlling for confounding factors for both the A \\to Y relationship and the B \\to Y relationship. The causal diagram in Figure 1 illustrates this requirement.\n\n\n\n\n\n\n\n\nFigure 1: Diagram illustrating causal interaction. Assessing the joint effect of two interventions, A (e.g., teaching method) and B (e.g., tutoring), on outcome Y (e.g., test score). L represents confounders of the A-Y relationship, and Q represents confounders of the B-Y relationship. Red arrows indicate biasing backdoor paths requiring adjustment. Assumes A and B are decided independently here.\n\n\n\n\n\nAs shown in Figure 2, identifying the causal interaction requires conditioning on (adjusting for) both sets of confounders, L_0 and Q_0, blocking the backdoor paths.\n\n\n\n\n\n\n\n\nFigure 2: Identification of causal interaction requires adjusting for all confounders of A-Y (L) and B-Y (Q). Boxes around L and Q indicate conditioning, closing backdoor paths.\n\n\n\n\n\nIn our education example: L (confounders of Teaching Method -&gt; Score): Prior student achievement, student motivation, socioeconomic status (SES), school resources, teacher experience/quality (if method not randomly assigned across teachers).\nQ (confounders of Tutoring -&gt; Score): Prior student achievement, student motivation, parental involvement/SES (influencing access/payment for tutoring), student time availability, specific learning needs.\nHere, L \\cap Q clearly includes prior achievement and student motivation. Therefore, L \\cup Q is a potentially large set of variables requiring measurement and adjustment. Failure to adequately control for L \\cup Q leads to biased estimates of causal interaction."
  },
  {
    "objectID": "content/06-test-lecture.html#effect-modification-variation-in-a-single-effect",
    "href": "content/06-test-lecture.html#effect-modification-variation-in-a-single-effect",
    "title": "Causal Inference: Effect Modification, Interaction, and Conditional Average Treatment Effects",
    "section": "Effect Modification: Variation in a Single Effect",
    "text": "Effect Modification: Variation in a Single Effect\nDistinct from interaction (which involves joint interventions), effect modification examines whether the causal effect of a single intervention (A) on an outcome (Y) differs across subgroups within a population. These subgroups are typically defined by baseline characteristics (G or X), which may or may not be modifiable themselves. Understanding effect modification helps identify for whom an intervention is most (or least) effective. We explore this potential variability using concepts like Heterogeneous Treatment Effects (HTE) and Conditional Average Treatment Effects (CATE).\n\nHeterogeneous Treatment Effects (HTE): The Phenomenon of Variability\nHeterogeneous Treatment Effects (HTE) refers to the phenomenon where the causal effect of an intervention (A on Y) is not constant across all individuals or subgroups. This is effect modification. The variability might stem from differences in observed characteristics (like age, sex, or prior conditions captured in measured covariates X) or unobserved factors (like genetics, unmeasured environmental factors, or personal history). HTE reflects the reality that interventions rarely affect everyone identically.\nIdentifying the full extent of HTE, especially variation due to unobserved factors, is challenging because individual causal effects are generally not point identifiable.\n\n\nConditional Average Treatment Effect (CATE): A Specific Measure \\tau(x)\nTo investigate HTE using observable data, we focus on estimating the Conditional Average Treatment Effect (CATE). CATE is a specific causal estimand: the average treatment effect conditional on observed covariates X taking a specific value or set of values x.\n\n\\tau(x) = \\text{CATE}(x) = \\mathbb{E}[Y(1) - Y(0) | X = x]\n\nEstimating \\text{CATE}(x) (denoted \\tau(x)) for different values of x allows us to quantify effect modification by the measured characteristics X. It is a primary tool for studying the broader HTE phenomenon, answering: How does the average effect of A differ for individuals with specific observed characteristics X=x?\n\n\nComparing Effect Heterogeneity Across Groups\nA direct way to assess effect modification by a categorical variable G (e.g., comparing effects between males and females, or across different study sites) is to compare the average treatment effect (ATE) estimated within each level of G. This involves comparing CATEs where the conditioning variable X is simply the group indicator G.\nWe define the causal estimand that compares two conditional average treatment effects between levels g and g' of variable G:\n\n{\\gamma} = \\overbrace{\\big( \\mathbb{E}[Y(a^*)|G=g] - \\mathbb{E}[Y(a)|G=g] \\big)}^{{\\delta_g=\\text{CATE}(G=g)}} - \\overbrace{\\big(\\mathbb{E}[Y(a^*)|G=g^{\\prime}]- \\mathbb{E}[Y(a)|G=g']\\big)}^{{\\delta_{g^{\\prime}}=\\text{CATE}(G=g')}}\n\nLet A be the treatment (e.g., a=0 to a^*=1), G the potential effect-modifier (e.g., sex: g=female, g'=male), and Y the outcome. The analysis assesses whether \\delta, the effect of A on Y, differs across levels of G.\nFor example, comparing the causal effect of an intervention for females (G=g_1) versus males (G=g_2):\n\nCausal effect within females (G=g_1): \n\\delta_{g_1} = \\mathbb{E}[Y(1)|G= g_1] - \\mathbb{E}[Y(0)|G = g_1]\n\nCausal effect within males (G=g_2): \n\\delta_{g_2} = \\mathbb{E}[Y(1)|G=g_2] - \\mathbb{E}[Y(0)|G=g_2]\n\nComparing causal effects across groups: \n{\\gamma} = {\\delta}_{g_1} - {\\delta}_{g_2}\n A non-zero estimate \\hat{\\gamma} indicates effect modification by G. The intervention’s effect differs between females and males. Note that \\hat{\\gamma} \\neq 0 can occur even if one group’s effect (\\hat{\\delta}_{g_1} or \\hat{\\delta}_{g_2}) is near zero.\n\n\nIdentification of Effect Modification\nTo identify group-specific effects (\\delta_g, \\delta_{g'}) and their difference (\\gamma), we need to ensure exchangeability for A holds within each stratum defined by G. That is, we must control for confounders (L) of the A \\to Y relationship, potentially allowing L itself to be associated with G. Crucially, since G defines the subpopulations of interest and is not (usually) the intervention itself, we do not necessarily need to control for factors (Q) that cause G, unless those factors also confound the A \\to Y relationship (i.e., if Q \\subseteq L).\nReferencing Figure 3: To estimate the A \\to Y effect within levels of G, we must adjust for L_0. Adjusting for Q (causes of G) is necessary only if Q is also in L_0.\n\n\n\n\n\n\n\n\nFigure 3: Imagine A is an experiment. How shall we investigate effect modification of A on Y by Z? Can you see why regression coefficients will not work?\n\n\n\n\n\nThe primary identification challenge for effect modification lies in adequately controlling for the A \\to Y confounders (L) within each subgroup defined by G or X. Next week, we turn our focus to statistical estimation."
  },
  {
    "objectID": "content/06-test-lecture.html#estimating-conditional-average-treatment-effects-hattaux",
    "href": "content/06-test-lecture.html#estimating-conditional-average-treatment-effects-hattaux",
    "title": "Causal Inference: Effect Modification, Interaction, and Conditional Average Treatment Effects",
    "section": "Estimating Conditional Average Treatment Effects: \\hat{\\tau}(X)",
    "text": "Estimating Conditional Average Treatment Effects: \\hat{\\tau}(X)\nIn the previous section, we defined the Conditional Average Treatment Effect (CATE), \\tau(x), as a key causal estimand for understanding effect modification by observed covariates X:\n\n\\tau(x) = \\mathbb{E}[Y(1) - Y(0) | X = x]\n\nThis represents the true average treatment effect within the specific subpopulation where individuals share the covariate profile X=x. It quantifies how the average effect of intervention A varies according to baseline characteristics X.\nOur focus now shifts to estimating this CATE function from observed data. We denote the statistical estimate, derived from our sample, as \\hat{\\tau}(X). For any individual i in our study with observed baseline covariates X_i, the value \\hat{\\tau}(X_i) represents our data-driven prediction of the average treatment effect for the subgroup of individuals possessing characteristics like X_i.\n\nClarifying “Individualised” Treatment Effects vs. Individual Effects\nYou might recall our earlier discussion highlighting that true individual causal effects—the effect of treatment for a single specific person, Y_i(1) - Y_i(0)—are generally impossible to identify from data. This might seem contradictory to the goal of estimating “individualised” treatment effects.\nThe essential distinction lies in the interpretation:\n\nIndividual Causal Effect (Unobservable): Y_i(1) - Y_i(0) is the precise effect for person i. We cannot observe both potential outcomes for the same person simultaneously.\nEstimated Conditional Average Treatment Effect (CATE estimate): \\hat{\\tau}(X_i) is an estimate of the average effect \\mathbb{E}[Y(1) - Y(0) | X = X_i] within the stratum, or subgroup, of the population who share the observable characteristics X_i.\n\nThe term “individualised” or “personalised” treatment effect, in this context, acknowledges that our prediction \\hat{\\tau}(X_i) is tailored to the specific covariate profile X_i of an individual. However, it crucially represents an estimated average effect for a subgroup, not the unique causal effect for that one person. It is our best estimate of the average response for the type of individual defined by X_i, based on the observed data and the chosen model.\n\n\nThe Multidimensional Nature of Individual Characteristics\nAn important reality is that individuals do not belong to just one group; they simultaneously belong to many overlapping groups defined by their multidimensional characteristics. For example, a student might be:\n\nFemale (gender)\n14 years old (age)\nFrom a low-income household (socioeconomic status)\nHigh-achieving in previous assessments (prior performance)\nAttending a rural school (location)\nHighly motivated (psychological trait)\n\nEach of these characteristics, and their combinations, might influence how the student responds to an intervention. The challenge becomes estimating treatment effects that account for this multidimensional reality.\nTraditional approaches using regression with manually specified interaction terms quickly become unwieldy when trying to model all relevant combinations (e.g., A*gender*age*income*prior_performance*location*motivation). With even a modest number of covariates, we face problems including:\n\nInsufficient data within many specific covariate combinations (strata).\nInflated risks of false positives due to multiple testing.\nDifficulties in model specification, complexity, and interpretation.\nAn inability to discover unexpected interaction patterns without pre-specifying them.\n\nModern methods like causal forests address this challenge by:\n\nConsidering the full covariate space simultaneously: Instead of requiring researchers to pre-specify interaction terms, causal forests automatically search for patterns of effect heterogeneity across the multidimensional covariate space (X).\nAdaptive complexity: The algorithm adapts to the data, splitting on variables and combinations that best differentiate treatment effects, while using regularisation techniques to avoid overfitting.\nLocal averaging: Causal forests effectively perform a weighted local averaging. For each individual i, the estimate \\hat{\\tau}(X_i) is derived by considering the outcomes of similar individuals (i.e., “neighbours” in the covariate space X).\nHonest estimation: By employing techniques like sample splitting (using separate parts of the data for building the tree structure and estimating the effects within the leaves) or out-of-bag estimation, causal forests aim to provide statistically valid (“honest”) estimates with appropriate confidence intervals.\n\nThe result is that for each individual i with covariates X_i, we obtain an estimate \\hat{\\tau}(X_i) reflecting the predicted treatment effect given their particular combination of characteristics. While still not a true individual causal effect, this approach acknowledges and leverages the multidimensional nature of individual differences.\n\n\nWhy Estimate CATEs (\\hat{\\tau}(X))?\nEstimating the CATE function \\hat{\\tau}(X) is the primary statistical approach for investigating the phenomenon of Heterogeneous Treatment Effects (HTE) using measured baseline variables X. By examining how the estimated effect \\hat{\\tau}(X) changes across different covariate profiles X, we aim to:\n\nExplore Effect Heterogeneity: Move beyond the single summary statistic of the Average Treatment Effect (ATE) to understand if, how, and potentially by which observed factors the intervention’s impact varies across the population.\nIdentify Potential Subgroups: Discover if specific subgroups, defined by their baseline characteristics X, experience substantially different benefits (or harms) from the intervention.\nInform Potential Targeting: Use the predictions \\hat{\\tau}(X_i) as a basis for exploring whether tailoring treatment decisions could lead to better overall outcomes. This involves evaluating whether strategies based on \\hat{\\tau}(X_i) outperform uniform treatment approaches, assessed using tools like RATE and Qini curves (discussed later).\n\n\n\nMethods for Estimation: Introducing grf\nWhile traditional regression models incorporating interaction terms (e.g., lm(Y ~ A * X1 + A * X2)) can estimate CATEs under specific assumptions (like linearity and correct model specification), they often struggle with complex scenarios involving many potential effect modifiers (X) or non-linear relationships.\nModern machine learning methods, particularly causal forests as implemented in the R package grf (Tibshirani et al. 2024), are specifically designed for estimating CATEs (\\hat{\\tau}(X)) flexibly and robustly. Causal forests adapt the random forest algorithm for causal inference. Instead of building trees simply to predict the outcome Y, they construct trees that explicitly maximise the heterogeneity in estimated treatment effects between leaves. This design allows them to:\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024. Grf: Generalized Random Forests. https://github.com/grf-labs/grf.\n\nHandle a large number of potential covariates (X) simultaneously.\nCapture complex, non-linear interactions between covariates and the treatment effect without pre-specification.\nAvoid strong parametric assumptions about the functional form of \\tau(X).\n\nThe output of a causal forest is typically a prediction \\hat{\\tau}(X_i) for each individual i in the dataset (ideally, an evaluation dataset distinct from the training data to prevent overfitting). We will demonstrate these methods after the mid-term break and clarify how they facilitate the development of treatment prioritisation rules. Such rules, often assuming fixed budgets, help determine policy regarding whether and when assigning treatment based on pre-treatment covariates is advantageous.\n\n\nSummary: Key Distinctions\nThis lecture has introduced several interconnected but distinct concepts:\n\nInteraction: Concerns the joint causal effect of two or more distinct interventions (A and B) on an outcome (Y). It asks if the effect of intervening on both differs from the sum of their individual intervention effects. The estimand involves joint potential outcomes like \\mathbb{E}[Y(a,b)]. Identification requires controlling confounders for all involved interventions (L \\cup Q).\nEffect Modification: Concerns how the causal effect of a single intervention (A) on an outcome (Y) varies across subgroups defined by a baseline characteristic (G or X). It asks if the effect, \\mathbb{E}[Y(1) - Y(0)], changes depending on the level of G or X.\nHTE is the general phenomenon of such effect variability.\nCATE (\\tau(x) = \\mathbb{E}[Y(1) - Y(0) | X=x]) is the specific causal estimand used to quantify this variability based on observed covariates X. Comparing CATEs across levels of G or X assesses effect modification by measured factors.\nEstimated Individualised Treatment Effects (\\hat{\\tau}(X_i)): Refers to our data-derived estimates of treatment effects tailored to individuals’ specific combinations of observed characteristics (X_i).\n\nThese are not true individual causal effects (Y_i(1) - Y_i(0)), which remain fundamentally unobservable. They estimate the CATE for the subgroup defined by X = X_i.\nModern methods like causal forests generate these predictions \\hat{\\tau}(X_i) by considering the multidimensional covariate profile X_i without requiring pre-specification of effect modifiers.\nThese estimates acknowledge that individuals exist at the intersection of many characteristics simultaneously.\nThey form the basis for evaluating whether personalised treatment strategies might outperform uniform approaches (using RATE, Qini curves, etc.).\n\n\nClearly distinguishing between these concepts is vital for precisely formulating research questions (defining causal estimands) and selecting valid methods for identification and estimation. In future lectures, we will explore the practical implementation of these methods in R using grf and related packages."
  },
  {
    "objectID": "content/06-test-lecture.html#course-review-so-far",
    "href": "content/06-test-lecture.html#course-review-so-far",
    "title": "Causal Inference: Effect Modification, Interaction, and Conditional Average Treatment Effects",
    "section": "Course Review So Far",
    "text": "Course Review So Far\nImagine you are trying to determine if something causes something else. For example, does a new teaching method (let’s call it ‘treatment A’) actually cause better test scores (the ‘outcome Y’)? Causal inference provides the set of tools researchers use to answer these kinds of questions rigorously.\n\nThe Core Idea: What If?\nAt its heart, causal inference compares what actually happened to what would have happened under different circumstances. This “what if” scenario is called a counterfactual.\nWhat would a student’s score (Y) be if they received the new teaching method (A=1)? We write this potential outcome as Y(1).\nWhat would that same student’s score (Y) be if they received the old method (A=0)? We write this potential outcome as Y(0).\nThe Average Treatment Effect (ATE) is the average difference between these “what if” scenarios across a whole population or group: \\text{ATE} = \\mathbb{E}[Y(1) - Y(0)]. This tells us, on average, how much the new teaching method changes the score.\n\n\nThis Lecture: Interaction vs. Effect Modification vs. Individualised Treatment Effects\n\nInteraction (Think: Teamwork Effects)\nWhat it is: Interaction is about whether the combined effect of two different interventions (say, treatment A and treatment B) is different from simply adding up their individual effects. Example: Does using the new teaching method (A) and having extra tutoring (B) improve scores (Y) more (or less) than you would expect by combining the improvement from only teaching method A and only extra tutoring B? How it’s defined: We use counterfactuals: Is the outcome with both A and B, \\mathbb{E}[Y(1, 1)], different from what you would predict based on having neither \\mathbb{E}[Y(0, 0)], only A \\mathbb{E}[Y(1, 0)], and only B \\mathbb{E}[Y(0, 1)]? Specifically, if [\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)]] \\neq 0, then there is interaction on the additive scale. Challenge: To study interaction properly, you need to control for confounding factors for both intervention A and intervention B (confounders L for A-Y and Q for B-Y).\n\n\nEffect Modification (Think: Different Effects for Different Groups)\nWhat it is: Effect modification is about whether the effect of one intervention (A) differs depending on some pre-existing characteristic (G or X) of the individuals. This characteristic is observed, not intervened upon. Example: Does the new teaching method (A) improve scores (Y) more for students who already have high prior grades (Group G=high) compared to students with low prior grades (Group G=low)? Here, G (prior grades) is the “effect modifier”. This phenomenon is called Heterogeneous Treatment Effects (HTE). How it’s defined: You compare the treatment effect (A’s effect on Y) across different groups defined by G or X. The Conditional Average Treatment Effect (CATE) quantifies this: \\tau(x) = \\mathbb{E}[Y(1) - Y(0) | X=x]. If \\tau(x) varies for different values of x, then X modifies the effect of A. Challenge: To study effect modification, you need to control for confounding factors (L) of the relationship between the single intervention A and the outcome Y, potentially within each subgroup G or X.\n\n\nEstimated Individualised Treatment Effects (Think: Personal Profiles Matter)\nWhat it is: In reality, people belong to many groups simultaneously based on multiple characteristics (age, gender, prior performance, motivation, etc.). “Individualised treatment effects” typically refer to the estimated CATE, \\hat{\\tau}(X_i), which predicts the average treatment response based on an individual’s complex combination of characteristics X_i.\nExample: We want to predict whether the new teaching method will help a specific student who is female, 14 years old, from a low-income background, with high prior achievement and high motivation. We use all this information (X_i) for the prediction \\hat{\\tau}(X_i).\nHow it’s estimated: Modern methods like causal forests process all characteristics (X) simultaneously to generate predictions \\hat{\\tau}(X_i) for each individual. These are not true individual causal effects (which remain unobservable) but are our best estimates of the average effect for people like individual i, given the available data.\nWhy it matters: These predictions \\hat{\\tau}(X_i) allow us to explore whether tailoring treatments based on individual profiles (X_i) could be more effective than treating everyone uniformly. This involves further evaluation (e.g., using RATE, Qini curves).\nIn Simple Terms:\n\nInteraction: Do two treatments work together synergistically or antagonistically? (Requires analysing effects of A, B, and A+B).\nEffect Modification (HTE/CATE): Does a single treatment’s effect change depending on who receives it (based on their characteristics X)? (Requires analysing effect of A within levels of X).\nEstimated Individualised Effects (\\hat{\\tau}(X_i)): How can we predict treatment responses based on the complex combination of an individual’s characteristics X_i? (Requires flexible models like causal forests trained on A, Y, and X).\n\nUnderstanding these distinctions helps formulate precise research questions, choose appropriate methods, and potentially develop more effective intervention strategies that acknowledge individual differences."
  },
  {
    "objectID": "content/06-test-lecture.html#appendix-simplification-of-additive-interaction-formula",
    "href": "content/06-test-lecture.html#appendix-simplification-of-additive-interaction-formula",
    "title": "Causal Inference: Effect Modification, Interaction, and Conditional Average Treatment Effects",
    "section": "Appendix: Simplification of Additive Interaction Formula",
    "text": "Appendix: Simplification of Additive Interaction Formula\nWe start with the definition of additive interaction based on comparing the joint effect relative to baseline versus the sum of individual effects relative to baseline:\n\n\\Big(\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)]\\Big) - \\Big[\\Big(\\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,0)]\\Big) + \\Big(\\mathbb{E}[Y(0,1)] - \\mathbb{E}[Y(0,0)]\\Big)\\Big]\n\nFirst, distribute the negative sign across the terms within the square brackets:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)] - \\Big(\\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,0)]\\Big) - \\Big(\\mathbb{E}[Y(0,1)] - \\mathbb{E}[Y(0,0)]\\Big)\n\nNow remove the parentheses, flipping the signs inside them where preceded by a minus sign:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(1,0)] + \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)]\n\nNext, combine the \\mathbb{E}[Y(0,0)] terms:\n\nWe have -\\mathbb{E}[Y(0,0)]\nThen +\\mathbb{E}[Y(0,0)] (these two cancel each other out)\nAnd another +\\mathbb{E}[Y(0,0)] remains.\n\nThe expression simplifies to:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)]\n\nThis is the standard definition of additive interaction, often called the interaction contrast. If this expression equals zero, there is no additive interaction; a non-zero value indicates an interaction effect.\nThis shows clearly that interaction is measured as the deviation of the joint effect from the sum of the separate effects, adjusted for the baseline."
  },
  {
    "objectID": "content/06-content-short.html",
    "href": "content/06-content-short.html",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "",
    "text": "Note\n\n\n\nRequired Reading\n\n(Hernan and Robins 2020) Chapters 4-5 link\n\nOptional Reading\n\n(Tyler J. VanderWeele and Robins 2007) link\n(Tyler J. VanderWeele 2009) link"
  },
  {
    "objectID": "content/06-content-short.html#if-you-learn-nothing-else-from-this-course",
    "href": "content/06-content-short.html#if-you-learn-nothing-else-from-this-course",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "If you learn nothing else from this course…",
    "text": "If you learn nothing else from this course…\nTo answer psychological questions properly, we first need to state them very clearly. Causal inference gives us the tools to do this."
  },
  {
    "objectID": "content/06-content-short.html#causal-inference-asks-what-if",
    "href": "content/06-content-short.html#causal-inference-asks-what-if",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Causal inference asks: “What if?”",
    "text": "Causal inference asks: “What if?”\nThe core idea is to compare what actually happened with what could have happened under different conditions (these “what ifs” are called counterfactuals).\nImagine an outcome we care about, like student test scores (Y). We might compare the score if everyone got a new teaching method (let’s call this condition a^*) versus if everyone got the old method (condition a).\nThe difference in the potential outcome (Y) under these two scenarios is the causal effect for one person: Y(a^*) - Y(a).\nSince we can’t see both scenarios for the same person, we often look at the average effect across a group. The Average Treatment Effect (ATE) is the average difference in potential outcomes across the whole population:\n\n\\text{ATE} = \\mathbb{E}[Y(a^*) - Y(a)]\n\nThis asks: “On average, how much would scores change if we switched everyone from the old method (a) to the new method (a^*)?”.\nA big challenge is dealing with confounders – other factors that mix up the relationship between the treatment (A) and the outcome (Y), potentially misleading us about the true causal effect. We need to account for these."
  },
  {
    "objectID": "content/06-content-short.html#what-do-interaction-and-effect-modification-mean-in-causal-inference",
    "href": "content/06-content-short.html#what-do-interaction-and-effect-modification-mean-in-causal-inference",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "What do ‘Interaction’ and ‘Effect Modification’ mean in Causal Inference?",
    "text": "What do ‘Interaction’ and ‘Effect Modification’ mean in Causal Inference?\nWords like ‘moderation’ and ‘interaction’ are often used loosely. Causal inference needs precise terms.\nWe’ll focus on two specific ideas:\n\nInteraction: About the effect of combining interventions.\nEffect Modification: About how the effect of one intervention changes for different types of people."
  },
  {
    "objectID": "content/06-content-short.html#interaction-the-effect-of-teamwork-or-lack-thereof",
    "href": "content/06-content-short.html#interaction-the-effect-of-teamwork-or-lack-thereof",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Interaction: The Effect of Teamwork (or Lack Thereof)",
    "text": "Interaction: The Effect of Teamwork (or Lack Thereof)\nInteraction in causal inference is about joint interventions. We look at what happens when we apply two or more different treatments at the same time.\nLet’s say we have two treatments, A and B, and an outcome Y.\n\nY(\\tilde{a}) is the potential outcome if we set treatment A to level \\tilde{a}.\nY(\\tilde{b}) is the potential outcome if we set treatment B to level \\tilde{b}.\nY(\\tilde{a}, \\tilde{b}) is the potential outcome if we set A to \\tilde{a} and B to \\tilde{b} simultaneously.\n\nTo figure out these effects from observational data, we usually need assumptions like:\n\nConsistency: The outcome we see for someone who got treatment \\tilde{a} is the same as their potential outcome Y(\\tilde{a}).\nConditional Exchangeability (No Unmeasured Confounding): We can make the groups receiving different treatments comparable by adjusting for measured confounders (L for A \\to Y, and Q for B \\to Y). The sets L and Q might overlap.\nPositivity: the exposures to be compared occur in all subgroups.\n\nTo study the interaction between A and B, we need to be able to estimate the effect of A and the effect of B, which means we need to adjust for all confounders in both L and Q (i.e., their union L \\cup Q).\n\nDefining Interaction: Does 1 + 1 = 2?\nLet’s use an education example:\n\nA: New teaching method (1=New, 0=Old)\nB: Extra tutoring (1=Yes, 0=No)\nY: Test score\n\nIs the boost in scores from getting both the new method and tutoring simply the sum of the boost from only the new method and the boost from only tutoring?\nWe define causal interaction on the additive scale (looking at differences) by comparing the effect of the joint intervention to the sum of the individual effects (all compared to getting neither):\n\n\\underbrace{(\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)])}_{\\text{Effect of Both}} \\quad \\text{vs} \\quad \\underbrace{(\\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,0)])}_{\\text{Effect of A only}} + \\underbrace{(\\mathbb{E}[Y(0,1)] - \\mathbb{E}[Y(0,0)])}_{\\text{Effect of B only}}\n\nInteraction exists if these are not equal. This simplifies to checking if the following is non-zero (see Appendix):\n\n\\underbrace{\\mathbb{E}[Y(1,1)]}_{\\text{Both}} - \\underbrace{\\mathbb{E}[Y(1,0)]}_{\\text{A only}} - \\underbrace{\\mathbb{E}[Y(0,1)]}_{\\text{B only}} + \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{Neither}} \\neq 0\n\n\nIf this is positive: Synergy (the combination is better than expected).\nIf this is negative: Antagonism (the combination is worse than expected).\n\n(We could also look at interaction on other scales, like ratios, which might give different answers. Always state the scale you’re using – we’ll come back to this in later lectures)\n\nFinding Causal Interaction in Data\nTo estimate this interaction, we need valid estimates for all four average potential outcomes:\n\\mathbb{E}[Y(0,0)], \\mathbb{E}[Y(1,0)], \\mathbb{E}[Y(0,1)], \\mathbb{E}[Y(1,1)]\nThis means we must control for confounders of both the A \\to Y link and the B \\to Y link.\nFigure 1 shows this. L are confounders for A \\to Y, and Q are confounders for B \\to Y. We need to block the backdoor paths (red arrows).\n\n\n\n\n\n\n\n\nFigure 1: Diagram illustrating causal interaction. Assessing the joint effect of two interventions, A (e.g., teaching method) and B (e.g., tutoring), on outcome Y (e.g., test score). L represents confounders of the A-Y relationship, and Q represents confounders of the B-Y relationship. Red arrows indicate biasing backdoor paths requiring adjustment. Assumes A and B are decided independently here.\n\n\n\n\n\nFigure 2 shows we need to condition on (adjust for) both L_0 and Q_0.\n\n\n\n\n\n\n\n\nFigure 2: Identification of causal interaction requires adjusting for all confounders of A-Y (L) and B-Y (Q). Boxes around L and Q indicate conditioning, closing backdoor paths.\n\n\n\n\n\nIn our education example:\n\nL (Confounders for Teaching Method \\to Score): Prior achievement, motivation, family background (SES), school quality, teacher differences (if not randomly assigned).\nQ (Confounders for Tutoring \\to Score): Prior achievement, motivation, family background (SES - paying for tutoring), student availability, specific learning needs.\n\nNotice that prior achievement and motivation are in both L and Q. We need to measure and adjust for all important factors in L and Q to get a reliable estimate of the interaction."
  },
  {
    "objectID": "content/06-content-short.html#effect-modification-different-effects-for-different-people",
    "href": "content/06-content-short.html#effect-modification-different-effects-for-different-people",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Effect Modification: Different Effects for Different People",
    "text": "Effect Modification: Different Effects for Different People\nUnlike interaction (about combining treatments), effect modification is about whether the causal effect of a single intervention (A) on an outcome (Y) is different for different subgroups in the population. These subgroups are defined by baseline characteristics (like age, sex, prior history - let’s call these G or X).\nEffect modification helps us understand who benefits most (or least) from an intervention. We explore this using ideas like Heterogeneous Treatment Effects (HTE) and Conditional Average Treatment Effects (CATE).\n\nHeterogeneous Treatment Effects (HTE): The Idea of Variation\nHeterogeneous Treatment Effects (HTE) just means that the effect of a treatment (A on Y) isn’t the same for everyone. The effect varies. This variation is effect modification.\nWhy does it vary?\n\nDifferences in things we can measure (like age, sex, baseline health - our X variables).\nDifferences in things we can’t easily measure (like genetics, unmeasured background factors).\n\nHTE is the reality; treatments rarely work identically for all.\n\n\nConditional Average Treatment Effect (CATE): Measuring Variation with Data \\tau(x)\nTo study HTE using data, we focus on the\nConditional Average Treatment Effect (CATE). CATE is a specific causal question (estimand): What is the average treatment effect for the subgroup of people who share specific measured characteristics X=x?\n\n\\tau(x) = \\text{CATE}(x) = \\mathbb{E}[Y(1) - Y(0) | X = x]\n\nHere, Y(1) is the potential outcome with treatment, Y(0) without. \\tau(x) tells us the average effect specifically for people with characteristics X=x. By looking at how \\tau(x) changes for different x, we quantify effect modification by the characteristics we measured in X.\n\n\nComparing Effects Across Defined Groups\nA simple way to check for effect modification by a category G (like comparing males vs females, or different locations) is to estimate the Average Treatment Effect (ATE) separately within each group. This is like comparing CATEs where X is just the group variable G.\nLet’s say A is the treatment (0=control, 1=treated) and G is the potential modifier (e.g., g=female, g'=male).\nWe compare:\n\nThe average effect for females (G=g_1): \\delta_{g_1} = \\mathbb{E}[Y(1) | G=g_1] - \\mathbb{E}[Y(0) | G=g_1]\nThe average effect for males (G=g_2): \\delta_{g_2} = \\mathbb{E}[Y(1) | G=g_2] - \\mathbb{E}[Y(0) | G=g_2]\n\nEffect modification by G exists if these are different: \\gamma = \\delta_{g_1} - \\delta_{g_2} \\neq 0.\nIf our estimate \\hat{\\gamma} is far from zero, it suggests the treatment effect differs between males and females.\n\nFinding Effect Modification in Data\nTo estimate these group-specific effects (\\delta_g) and their difference (\\gamma) correctly, we need to control for confounders (L) of the A \\to Y relationship within each group defined by G.\nNote: we don’t necessarily need to control for things that cause G itself, unless they also confound the A \\to Y relationship (i.e., are also in L).\nLook at Figure 3. To estimate the A \\to Y effect within levels of G, we need to adjust for the confounders L_0. But this will partially block the effect-modification of G on Y. Moreover, if we were identifying the causal effect of G on Y, after conditioning on L, we would find that a backdoor path opends from G to Y because L is a collider. We cannot interpret any coefficient for G in a regression model where L is in the model.\n\n\n\n\n\n\n\n\nFigure 3: How shall we investigate effect modification of A on Y by G? Can you see the problem?\n\n\n\n\n\nThus it is essential to understand that when we control for confounding along the the A \\to Y path, we do not identify the causal effects of effect-modifiers. Rather, we should consider effect-modifiers prognostic indicators. Moreover, we’re going to need to develop methods for clarifying prognostic indicators in multi-dimensional settings where"
  },
  {
    "objectID": "content/06-content-short.html#estimating-how-effects-vary-getting-hattaux-from-data",
    "href": "content/06-content-short.html#estimating-how-effects-vary-getting-hattaux-from-data",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Estimating How Effects Vary: Getting \\hat{\\tau}(x) from Data",
    "text": "Estimating How Effects Vary: Getting \\hat{\\tau}(x) from Data\nWe defined the Conditional Average Treatment Effect (CATE), \\tau(x), as the true average effect for a subgroup with specific features X=x:\n\n\\tau(x) = \\mathbb{E}[Y(1) - Y(0) | X = x]\n\nNow, we want to estimate this from our actual data. We call our estimate \\hat{\\tau}(x). For any person i in our study with features X_i, the value \\hat{\\tau}(X_i) is our data-based prediction of the average treatment effect for people like person i.\n\n“Personalised” Effects vs. True Individual Effects\nWait - didn’t we say we can’t know the true effect for one specific person, Y_i(1) - Y_i(0)? Yes, that’s still true.\nSo what does \\hat{\\tau}(X_i) mean?\n\nIndividual Causal Effect (Unknowable): Y_i(1) - Y_i(0). This is the true effect for person i. We can’t observe both Y_i(1) and Y_i(0).\nEstimated CATE (\\hat{\\tau}(X_i)) (What we calculate): This is our estimate of the average effect, \\mathbb{E}[Y(1) - Y(0)], for the subgroup of people who share the same measured characteristics X_i as person i.\n\nWhen people talk about “personalised” or “individualised” treatment effects in this context, they usually mean \\hat{\\tau}(x). It’s “personalised” because the prediction uses person i’s specific characteristics X_i = x. But remember, it’s an estimated average effect for a group, not the unique effect for that single individual.\n\n\nPeople Have Many Characteristics\nPeople aren’t just in one group; they have many features at once. A student might be:\n\nFemale\n21 years old\nFrom a low-income family\nDid well on previous tests\nGoes to a rural school\nHighly motivated\n\nAll these factors (X_i) together might influence how they respond to a new teaching method.\nTrying to figure this out with traditional regression by manually adding interaction terms (like A*gender*age*income*...) becomes impossible very quickly:\n\nToo many combinations, not enough data in each specific combo.\nHigh risk of finding “effects” just by chance (false positives).\nHard to know which interactions to even include.\nCan’t easily discover unexpected patterns.\n\nThus, while simple linear regression with interaction terms (lm(Y ~ A * X1 + A * X2)) can estimate CATEs if the model is simple and correct, it often fails when things get complex (many X variables, non-linear effects).\nCausal forests (using the grf package in R) (Tibshirani et al. 2024) are a powerful, flexible alternative designed for this task. They build decision trees that specifically aim to find groups with different treatment effects.\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024. Grf: Generalized Random Forests. https://github.com/grf-labs/grf.\nWe’ll learn how to use grf after the mid-term break. It will allow us to get the \\hat{\\tau}(x) predictions and then think about how to use them, for instance, to prioritise who gets a treatment if resources are limited.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\nLet’s revisit the centeral ideas:\n\nInteraction:\n\nThink: Teamwork effect.\nWhat: Effect of two or more different interventions (A and B) applied together.\nQuestion: Is the joint effect \\mathbb{E}[Y(a,b)] different from the sum of individual effects?\nNeeds: Control confounders for all interventions involved (L \\cup Q).\n\n\n\nEffect Modification / HTE / CATE:\n\nThink: Different effects for different groups.\nWhat: Effect of a single intervention (A) varies depending on people’s baseline characteristics (G or X).\nQuestion (HTE): Does the effect vary? (The phenomenon).\nQuestion (CATE \\tau(x)): What is the average effect for a specific subgroup with features X=x? (The measure).\nNeeds: Control confounders for the single intervention (L) within subgroups.\n\n\n\nEstimated “Individualised” Treatment Effects (\\hat{\\tau}(x)):\n\nThink: Personal profile prediction.\nWhat: Our estimate of the average treatment effect for the subgroup of people sharing characteristics X_i.\nHow: Calculated using models (like causal forests) that use the person’s full profile X_i.\nImportant: This is not the true effect for that single person (which is unknowable). It’s an average for people like them.\nUse: Explore HTE, identify subgroups, potentially inform targeted treatment strategies.\n\nKeeping these concepts distinct helps us ask clear research questions and choose the right methods."
  },
  {
    "objectID": "content/06-content-short.html#course-review-so-far-a-quick-recap",
    "href": "content/06-content-short.html#course-review-so-far-a-quick-recap",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Course Review So Far: A Quick Recap",
    "text": "Course Review So Far: A Quick Recap\nLet’s quickly review the main ideas of causal inference we’ve covered.\n\nThe Big Question: Does A cause Y?\nCausal inference helps us answer if something (like a teaching method, A) causes a change in something else (like test scores, Y).\n\n\nCore Idea: “What If?” (Counterfactuals)\nWe compare what actually happened to what would have happened in a different scenario.\n\nY(1): Score if the student had received the new method.\nY(0): Score if the student had received the old method.\n\nThe Average Treatment Effect (ATE) = \\mathbb{E}[Y(1) - Y(0)] is the average difference across the whole group.\n\n\nThis Lecture Clarified Concepts of Interaction vs. Effect Modification vs. Individual Predictions\n\nInteraction (Think: Teamwork Effects)\n\nAbout: Combining two different interventions (A and B).\nQuestion: Does using both A and B together give a result different from just adding up their separate effects? (e.g., new teaching method + tutoring).\nNeeds: Analyse effects of A alone, B alone, and A+B together. Control confounders for both A and B.\n\n\n\nEffect Modification (Think: Different Effects for Different Groups)\n\nAbout: How the effect of one intervention (A) changes based on people’s characteristics (X, like prior grades).\nQuestion: Does the teaching method (A) work better for high-achieving students (X=high) than low-achieving students (X=low)?\n\nHTE: The idea that effects differ.\nCATE \\tau(x): The average effect for the specific group with characteristics X=x.\n\nNeeds: Analyse effect of A within different groups (levels of X). Control confounders for A.\n\n\n\nEstimated Individualised Effects (\\hat{\\tau}(X_i)) (Think: Personal Profile Prediction)\n\nAbout: Using a person’s whole profile of characteristics (X_i - age, gender, background, etc.) to predict their likely response to treatment A.\nHow: Modern methods (like causal forests) take all of X_i and estimate \\hat{\\tau}(X_i).\nResult: this \\hat{\\tau}(X_i) is not the true unknowable effect for person i. It is the estimated average effect for people similar to person i (sharing characteristics X_i).\nUse: helps explore if tailoring treatment based on these profiles (X_i) could be beneficial.\n\n\n\n\nSimple Summary:\n\nInteraction: Do A and B work together well/badly?\nEffect Modification: Does A’s effect depend on who you are (based on X)?\n\\hat{\\tau}(X_i): Can we predict A’s average effect for someone based on their specific profile X_i?\n\nUnderstanding these differences is key to doing good causal research!"
  },
  {
    "objectID": "content/06-content-short.html#appendix-simplification-of-additive-interaction-formula",
    "href": "content/06-content-short.html#appendix-simplification-of-additive-interaction-formula",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Appendix: Simplification of Additive Interaction Formula",
    "text": "Appendix: Simplification of Additive Interaction Formula\nWe start with the definition of additive interaction based on comparing the joint effect relative to baseline versus the sum of individual effects relative to baseline:\n\n\\Big(\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)]\\Big) - \\Big[\\Big(\\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,0)]\\Big) + \\Big(\\mathbb{E}[Y(0,1)] - \\mathbb{E}[Y(0,0)]\\Big)\\Big]\n\nFirst, distribute the negative sign across the terms within the square brackets:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)] - \\Big(\\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,0)]\\Big) - \\Big(\\mathbb{E}[Y(0,1)] - \\mathbb{E}[Y(0,0)]\\Big)\n\nNow remove the parentheses, flipping the signs inside them where preceded by a minus sign:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(1,0)] + \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)]\n\nNext, combine the \\mathbb{E}[Y(0,0)] terms:\n\nWe have -\\mathbb{E}[Y(0,0)]\nThen +\\mathbb{E}[Y(0,0)] (these two cancel each other out)\nAnd another +\\mathbb{E}[Y(0,0)] remains.\n\nThe expression simplifies to:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)]\n\nThis is the standard definition of additive interaction, often called the interaction contrast. If this expression equals zero, there is no additive interaction; a non-zero value indicates an interaction effect.\nThis shows clearly that interaction is measured as the deviation of the joint effect from the sum of the separate effects, adjusted for the baseline."
  },
  {
    "objectID": "content/06-content.html#if-you-learn-nothing-else-from-this-course",
    "href": "content/06-content.html#if-you-learn-nothing-else-from-this-course",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "If you learn nothing else from this course…",
    "text": "If you learn nothing else from this course…\nTo answer psychological questions properly, we first need to state them very clearly. Causal inference gives us the tools to do this."
  },
  {
    "objectID": "content/06-content.html#causal-inference-asks-what-if",
    "href": "content/06-content.html#causal-inference-asks-what-if",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Causal inference asks: “What if?”",
    "text": "Causal inference asks: “What if?”\nThe core idea is to compare what actually happened with what could have happened under different conditions (these “what ifs” are called counterfactuals).\nImagine an outcome we care about, like student test scores (Y). We might compare the score if everyone got a new teaching method (let’s call this condition a^*) versus if everyone got the old method (condition a).\nThe difference in the potential outcome (Y) under these two scenarios is the causal effect for one person: Y(a^*) - Y(a).\nSince we can’t see both scenarios for the same person, we often look at the average effect across a group. The Average Treatment Effect (ATE) is the average difference in potential outcomes across the whole population:\n\n\\text{ATE} = \\mathbb{E}[Y(a^*) - Y(a)]\n\nThis asks: “On average, how much would scores change if we switched everyone from the old method (a) to the new method (a^*)?”.\nA big challenge is dealing with confounders – other factors that mix up the relationship between the treatment (A) and the outcome (Y), potentially misleading us about the true causal effect. We need to account for these."
  },
  {
    "objectID": "content/06-content.html#what-do-interaction-and-effect-modification-mean-in-causal-inference",
    "href": "content/06-content.html#what-do-interaction-and-effect-modification-mean-in-causal-inference",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "What do ‘Interaction’ and ‘Effect Modification’ mean in Causal Inference?",
    "text": "What do ‘Interaction’ and ‘Effect Modification’ mean in Causal Inference?\nWords like ‘moderation’ and ‘interaction’ are often used loosely. Causal inference needs precise terms.\nWe’ll focus on two specific ideas:\n\nInteraction: About the effect of combining interventions.\nEffect Modification: About how the effect of one intervention changes for different types of people."
  },
  {
    "objectID": "content/06-content.html#interaction-the-effect-of-teamwork-or-lack-thereof",
    "href": "content/06-content.html#interaction-the-effect-of-teamwork-or-lack-thereof",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Interaction: The Effect of Teamwork (or Lack Thereof)",
    "text": "Interaction: The Effect of Teamwork (or Lack Thereof)\nInteraction in causal inference is about joint interventions. We look at what happens when we apply two or more different treatments at the same time.\nLet’s say we have two treatments, A and B, and an outcome Y.\n\nY(\\tilde{a}) is the potential outcome if we set treatment A to level \\tilde{a}.\nY(\\tilde{b}) is the potential outcome if we set treatment B to level \\tilde{b}.\nY(\\tilde{a}, \\tilde{b}) is the potential outcome if we set A to \\tilde{a} and B to \\tilde{b} simultaneously.\n\nTo figure out these effects from observational data, we usually need assumptions like:\n\nConsistency: The outcome we see for someone who got treatment \\tilde{a} is the same as their potential outcome Y(\\tilde{a}).\nConditional Exchangeability (No Unmeasured Confounding): We can make the groups receiving different treatments comparable by adjusting for measured confounders (L for A \\to Y, and Q for B \\to Y). The sets L and Q might overlap.\nPositivity: the exposures to be compared occur in all subgroups.\n\nTo study the interaction between A and B, we need to be able to estimate the effect of A and the effect of B, which means we need to adjust for all confounders in both L and Q (i.e., their union L \\cup Q).\n\nDefining Interaction: Does 1 + 1 = 2?\nLet’s use an education example:\n\nA: New teaching method (1=New, 0=Old)\nB: Extra tutoring (1=Yes, 0=No)\nY: Test score\n\nIs the boost in scores from getting both the new method and tutoring simply the sum of the boost from only the new method and the boost from only tutoring?\nWe define causal interaction on the additive scale (looking at differences) by comparing the effect of the joint intervention to the sum of the individual effects (all compared to getting neither):\n\n\\underbrace{(\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)])}_{\\text{Effect of Both}} \\quad \\text{vs} \\quad \\underbrace{(\\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,0)])}_{\\text{Effect of A only}} + \\underbrace{(\\mathbb{E}[Y(0,1)] - \\mathbb{E}[Y(0,0)])}_{\\text{Effect of B only}}\n\nInteraction exists if these are not equal. This simplifies to checking if the following is non-zero (see Appendix):\n\n\\underbrace{\\mathbb{E}[Y(1,1)]}_{\\text{Both}} - \\underbrace{\\mathbb{E}[Y(1,0)]}_{\\text{A only}} - \\underbrace{\\mathbb{E}[Y(0,1)]}_{\\text{B only}} + \\underbrace{\\mathbb{E}[Y(0,0)]}_{\\text{Neither}} \\neq 0\n\n\nIf this is positive: Synergy (the combination is better than expected).\nIf this is negative: Antagonism (the combination is worse than expected).\n\n(We could also look at interaction on other scales, like ratios, which might give different answers. Always state the scale you’re using – we’ll come back to this in later lectures)\n\nFinding Causal Interaction in Data\nTo estimate this interaction, we need valid estimates for all four average potential outcomes:\n\\mathbb{E}[Y(0,0)], \\mathbb{E}[Y(1,0)], \\mathbb{E}[Y(0,1)], \\mathbb{E}[Y(1,1)]\nThis means we must control for confounders of both the A \\to Y link and the B \\to Y link.\nFigure 1 shows this. L_A are confounders for A \\to Y, and L_B are confounders for B \\to Y. We need to block the backdoor paths (red arrows).\n\n\n\n\n\n\n\n\nFigure 1: Diagram illustrating causal interaction. Assessing the joint effect of two interventions, A (e.g., teaching method) and B (e.g., tutoring), on outcome Y (e.g., test score). L_A represents confounders of the A-Y relationship, and L_B represents confounders of the B-Y relationship. Red arrows indicate biasing backdoor paths requiring adjustment. Assumes A and B are decided independently here.\n\n\n\n\n\nFigure 2 shows we need to condition on (adjust for) both L_0 and Q_0.\n\n\n\n\n\n\n\n\nFigure 2: Identification of causal interaction requires adjusting for all confounders of A-Y (L) and B-Y (Q). Boxes around L_A and L_B indicate conditioning, closing backdoor paths.\n\n\n\n\n\nIn our education example:\n\nL_A (Confounders for Teaching Method \\to Score): Prior achievement, motivation, family background (SES), school quality, teacher differences (if not randomly assigned).\nL_B (Confounders for Tutoring \\to Score): Prior achievement, motivation, family background (SES - paying for tutoring), student availability, specific learning needs.\n\nNotice that prior achievement and motivation are in both L_A and L_B. We need to measure and adjust for all important factors in \\boxed{L_A} and \\boxed{L_B} to get a reliable estimate for interaction."
  },
  {
    "objectID": "content/06-content.html#effect-modification-different-effects-for-different-people",
    "href": "content/06-content.html#effect-modification-different-effects-for-different-people",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Effect Modification: Different Effects for Different People",
    "text": "Effect Modification: Different Effects for Different People\nUnlike interaction (about combining treatments), effect modification is about whether the causal effect of a single intervention (A) on an outcome (Y) is different for different subgroups in the population. These subgroups are defined by baseline characteristics (like age, sex, prior history - let’s call these G or X).\nEffect modification helps us understand who benefits most (or least) from an intervention. We explore this using ideas like Heterogeneous Treatment Effects (HTE) and Conditional Average Treatment Effects (CATE).\n\nHeterogeneous Treatment Effects (HTE): The Idea of Variation\nHeterogeneous Treatment Effects (HTE) just means that the effect of a treatment (A on Y) isn’t the same for everyone. The effect varies. This variation is effect modification.\nWhy does it vary?\n\nDifferences in things we can measure (like age, sex, baseline health - our X variables).\nDifferences in things we can’t easily measure (like genetics, unmeasured background factors).\n\nHTE is the reality; treatments rarely work identically for all.\n\n\nConditional Average Treatment Effect (CATE): Measuring Variation with Data \\tau(x)\nTo study HTE using data, we focus on the\nConditional Average Treatment Effect (CATE). CATE is a specific causal question (estimand): What is the average treatment effect for the subgroup of people who share specific measured characteristics X=x?\n\n\\tau(x) = \\text{CATE}(x) = \\mathbb{E}[Y(1) - Y(0) | X = x]\n\nHere, Y(1) is the potential outcome with treatment, Y(0) without. \\tau(x) tells us the average effect specifically for people with characteristics X=x. By looking at how \\tau(x) changes for different x, we quantify effect modification by the characteristics we measured in X.\n\n\nComparing Effects Across Defined Groups\nA simple way to check for effect modification by a category G (like comparing males vs females, or different locations) is to estimate the Average Treatment Effect (ATE) separately within each group. This is like comparing CATEs where X is just the group variable G.\nLet’s say A is the treatment (0=control, 1=treated) and G is the potential modifier (e.g., g=female, g'=male).\nWe compare:\n\nThe average effect for females (G=g_1): \\delta_{g_1} = \\mathbb{E}[Y(1) | G=g_1] - \\mathbb{E}[Y(0) | G=g_1]\nThe average effect for males (G=g_2): \\delta_{g_2} = \\mathbb{E}[Y(1) | G=g_2] - \\mathbb{E}[Y(0) | G=g_2]\n\nEffect modification by G exists if these are different: \\gamma = \\delta_{g_1} - \\delta_{g_2} \\neq 0.\nIf our estimate \\hat{\\gamma} is far from zero, it suggests the treatment effect differs between males and females.\n\nFinding Effect Modification in Data\nTo estimate these group-specific effects (\\delta_g) and their difference (\\gamma) correctly, we need to control for confounders (L) of the A \\to Y relationship within each group defined by G. Note that we are not estimating the causal effect of G. As such, we do not need to control for things that cause G itself, unless they also confound the A \\to Y relationship (i.e., are also in L).\nLook at Figure 3. To estimate the A \\to Y effect within levels of G, we need to adjust for the confounders L_0. But this will partially block the effect-modification of G on Y because L_0 is a mediator for that path. Moreover, if we were identifying the causal effect of G on Y, after conditioning on L, we would find that a backdoor path opens from G \\to Y because \\boxed{L} is a collider. G does not have a causal interpretation in this model. However we would be wrong to thing that G is not and an effect modifier of the effect of A on Y. (See Appendix C).\n\n\n\n\n\n\n\n\nFigure 3: How shall we investigate effect modification of A on Y by G? Can you see the problem?\n\n\n\n\n\nThus it is essential to understand that when we control for confounding along the the A \\to Y path, we do not identify the causal effects of effect-modifiers.\nTo clarify:\n\nIf the statistical model correctly identifies causal effect modification (by appropriately handling confounders and avoiding collider bias), then it has prognostic value regarding the differential outcomes expected under intervention A=1 vs A=0 depending on G=g.\nIf the statistical model contains interaction terms that are artifacts of bias (like conditioning on a collider) or reflect a different target (like conditioning on a mediator when the total effect was intended), its causal prognostic value is compromised or needs careful interpretation. It might still predict Y well given A, G, and L in an observational setting, but it wouldn’t accurately predict the results of intervening on A differently for different G groups. (See Appendix B).\n\nThe choice of variables fundamentally determines which causal question (if any) the statistical model is estimating. As with the average treatment effect, we interpret evidence for effect modification in the context of our assumptions about the causal relationships that obtain in the world. This is because the statistical interaction we observe is highly sensitive to model choices. Any interpretation as causal effect modification, and therefore any reliable prognostic value for intervention effects within different segments of the population, depends entirely on whether and how our statistical model accounts for the causal structure."
  },
  {
    "objectID": "content/06-content.html#estimating-how-effects-vary-getting-hattaux-from-data",
    "href": "content/06-content.html#estimating-how-effects-vary-getting-hattaux-from-data",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Estimating How Effects Vary: Getting \\hat{\\tau}(x) from Data",
    "text": "Estimating How Effects Vary: Getting \\hat{\\tau}(x) from Data\nWe defined the Conditional Average Treatment Effect (CATE), \\tau(x), as the true average effect for a subgroup with specific features X=x:\n\n\\tau(x) = \\mathbb{E}[Y(1) - Y(0) | X = x]\n\nNow, we want to estimate this from our actual data. We call our estimate \\hat{\\tau}(x). For any person i in our study with features X_i, the value \\hat{\\tau}(X_i) is our data-based prediction of the average treatment effect for people like person i.\n\n“Personalised” Effects vs. True Individual Effects\nWait - didn’t we say we can’t know the true effect for one specific person, Y_i(1) - Y_i(0)? Yes, that’s still true.\nSo what does \\hat{\\tau}(X_i) mean?\n\nIndividual Causal Effect (Unknowable): Y_i(1) - Y_i(0). This is the true effect for person i. We can’t observe both Y_i(1) and Y_i(0).\nEstimated CATE (\\hat{\\tau}(X_i)) (What we calculate): This is our estimate of the average effect, \\mathbb{E}[Y(1) - Y(0)], for the subgroup of people who share the same measured characteristics X_i as person i.\n\nWhen people talk about “personalised” or “individualised” treatment effects in this context, they usually mean \\hat{\\tau}(x). It’s “personalised” because the prediction uses person i’s specific characteristics X_i = x. But remember, it’s an estimated average effect for a group, not the unique effect for that single individual.\n\n\nPeople Have Many Characteristics\nPeople aren’t just in one group; they have many features at once. A student might be:\n\nFemale\n21 years old\nFrom a low-income family\nDid well on previous tests\nGoes to a rural school\nHighly motivated\n\nAll these factors (X_i) together might influence how they respond to a new teaching method.\nTrying to figure this out with traditional regression by manually adding interaction terms (like A*gender*age*income*...) becomes impossible very quickly:\n\nToo many combinations, not enough data in each specific combo.\nHigh risk of finding “effects” just by chance (false positives).\nHard to know which interactions to even include.\nCan’t easily discover unexpected patterns.\n\nThus, while simple linear regression with interaction terms (lm(Y ~ A * X1 + A * X2)) can estimate CATEs if the model is simple and correct, it often fails when things get complex (many X variables, non-linear effects).\nCausal forests (using the grf package in R) (Tibshirani et al. 2024) are a powerful, flexible alternative designed for this task. They build decision trees that specifically aim to find groups with different treatment effects.\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024. Grf: Generalized Random Forests. https://github.com/grf-labs/grf.\nWe’ll learn how to use grf after the mid-term break. It will allow us to get the \\hat{\\tau}(x) predictions and then think about how to use them, for instance, to prioritise who gets a treatment if resources are limited.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\nLet’s revisit the core concepts:\n\nInteraction:\n\nThink: Teamwork effect.\nWhat: Effect of two or more different interventions (A and B) applied together.\nQuestion: Is the joint effect \\mathbb{E}[Y(a,b)] different from the sum of individual effects?\nNeeds: Control confounders for all interventions involved (L \\cup Q).\n\n\n\nEffect Modification / HTE / CATE:\n\nThink: Different effects for different groups.\nWhat: Effect of a single intervention (A) varies depending on people’s baseline characteristics (G or X).\nQuestion (HTE): Does the effect vary? (The phenomenon).\nQuestion (CATE \\tau(x)): What is the average effect for a specific subgroup with features X=x? (The measure).\nNeeds: Control confounders for the single intervention (L) within subgroups.\n\n\n\nEstimated “Individualised” Treatment Effects (\\hat{\\tau}(x)):\n\nThink: Personal profile prediction.\nWhat: Our estimate of the average treatment effect for the subgroup of people sharing characteristics X_i.\nHow: Calculated using models (like causal forests) that use the person’s full profile X_i.\nImportant: This is not the true effect for that single person (which is unknowable). It’s an average for people like them.\nUse: Explore HTE, identify subgroups, potentially inform targeted treatment strategies.\n\nKeeping these concepts distinct helps us ask clear research questions and choose the right methods."
  },
  {
    "objectID": "content/06-content.html#course-review-so-far-a-quick-recap",
    "href": "content/06-content.html#course-review-so-far-a-quick-recap",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Course Review So Far: A Quick Recap",
    "text": "Course Review So Far: A Quick Recap\nLet’s quickly review the main ideas of causal inference we’ve covered.\n\nThe Big Question: Does A cause Y?\nCausal inference helps us answer if something (like a teaching method, A) causes a change in something else (like test scores, Y).\n\n\nCore Idea: “What If?” (Counterfactuals)\nWe compare what actually happened to what would have happened in a different scenario.\n\nY(1): Score if the student had received the new method.\nY(0): Score if the student had received the old method.\n\nThe Average Treatment Effect (ATE) = \\mathbb{E}[Y(1) - Y(0)] is the average difference across the whole group.\n\n\nThis Lecture Clarified Concepts of Interaction vs. Effect Modification vs. Individual Predictions\n\nInteraction (Think: Teamwork Effects)\n\nAbout: Combining two different interventions (A and B).\nQuestion: Does using both A and B together give a result different from just adding up their separate effects? (e.g., new teaching method + tutoring).\nNeeds: Analyse effects of A alone, B alone, and A+B together. Control confounders for both A and B.\n\n\n\nEffect Modification (Think: Different Effects for Different Groups)\n\nAbout: How the effect of one intervention (A) changes based on people’s characteristics (X, like prior grades).\nQuestion: Does the teaching method (A) work better for high-achieving students (X=high) than low-achieving students (X=low)?\n\nHTE: The idea that effects differ.\nCATE \\tau(x): The average effect for the specific group with characteristics X=x.\n\nNeeds: Analyse effect of A within different groups (levels of X). Control confounders for A.\n\n\n\nEstimated Individualised Effects (\\hat{\\tau}(X_i)) (Think: Personal Profile Prediction)\n\nAbout: Using a person’s whole profile of characteristics (X_i - age, gender, background, etc.) to predict their likely response to treatment A.\nHow: Modern methods (like causal forests) take all of X_i and estimate \\hat{\\tau}(X_i).\nResult: this \\hat{\\tau}(X_i) is not the true unknowable effect for person i. It is the estimated average effect for people similar to person i (sharing characteristics X_i).\nUse: helps explore if tailoring treatment based on these profiles (X_i) could be beneficial.\n\n\n\n\nSimple Summary:\n\nInteraction: Do A and B work together well/badly?\nEffect Modification: Does A’s effect depend on who you are (based on X)?\n\\hat{\\tau}(X_i): Can we predict A’s average effect for someone based on their specific profile X_i?\n\nUnderstanding these differences is key to doing good causal research!"
  },
  {
    "objectID": "content/06-content.html#appendix-simplification-of-additive-interaction-formula",
    "href": "content/06-content.html#appendix-simplification-of-additive-interaction-formula",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Appendix: Simplification of Additive Interaction Formula",
    "text": "Appendix: Simplification of Additive Interaction Formula\nWe start with the definition of additive interaction based on comparing the joint effect relative to baseline versus the sum of individual effects relative to baseline:\n\n\\Big(\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)]\\Big) - \\Big[\\Big(\\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,0)]\\Big) + \\Big(\\mathbb{E}[Y(0,1)] - \\mathbb{E}[Y(0,0)]\\Big)\\Big]\n\nFirst, distribute the negative sign across the terms within the square brackets:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)] - \\Big(\\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,0)]\\Big) - \\Big(\\mathbb{E}[Y(0,1)] - \\mathbb{E}[Y(0,0)]\\Big)\n\nNow remove the parentheses, flipping the signs inside them where preceded by a minus sign:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(1,0)] + \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)]\n\nNext, combine the \\mathbb{E}[Y(0,0)] terms:\n\nWe have -\\mathbb{E}[Y(0,0)]\nThen +\\mathbb{E}[Y(0,0)] (these two cancel each other out)\nAnd another +\\mathbb{E}[Y(0,0)] remains.\n\nThe expression simplifies to:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)]\n\nThis is the standard definition of additive interaction, often called the interaction contrast. If this expression equals zero, there is no additive interaction; a non-zero value indicates an interaction effect.\nThis shows clearly that interaction is measured as the deviation of the joint effect from the sum of the separate effects, adjusted for the baseline."
  },
  {
    "objectID": "content/06-content.html#appendix-b",
    "href": "content/06-content.html#appendix-b",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Appendix B",
    "text": "Appendix B\n\nEvidence for effect-modification is relative to inclusion of other variables in the model\nThe ‘sharp-null hypothesis’ states there is no effect of the exposure on the outcome for any unit in the target population. Unless the ‘sharp-null hypothesis’ is false, there may be effect-modification. For any study worth conducting, we cannot evaluate whether the sharp-null hypothesis is false. If we could the experiment would be otiose. Therefore, we must assume the possibility of effect-modification. Notably, whether a variable is an effect-modifier also depends on which other variables are included in the model. That is, just as for the concept of a ‘confounder’, where a variable is an ‘effect-modifier’ cannot be stated without reference to an assumed causal order and an explicit statement about which other variables will be included in the model (Tyler J. VanderWeele 2012).\n\nVanderWeele, Tyler J. 2012. “Confounding and Effect Modification: Distribution and Measure.” Epidemiologic Methods 1 (1): 55–82. https://doi.org/10.1515/2161-962X.1004.\nAs illustrated in Figure 4, the marginal association between A and Y is unbiased. Here, exposure A is unconditionally associated with Y. Recall our convention G denotes effect-modification with conditioning and Z indicates effect-modification without conditioning.\n\n\n\n\n\n\n\n\nFigure 4: Consider a randomised experiment. There is no confounding. Here, the marginal association between A and Y provides an unbiased estimate for the causal effect of A on Y. Does the conditional association of A on Y vary within levels of G? The causal diagram allows for a classification of G as an effect modifier of A on Y by proxy. G modifies A’s effect on Y in virtue of G’s relationship to Z, which, according to this graph, is a direct effect modifier for the effect of A on Y.\n\n\n\n\n\nFigure 5 presents the same a randomised experiment as in the previous causal diagram. We again assume that there is no confounding of the marginal association between the exposure, A, and the outcome, Y. However, suppose we were to adjust for Z and ask, does the conditional association of A on Y vary within levels of G, after adjusting for Z? That is, does G remain an effect-modifier of the exposure on the outcome? Tyler J. VanderWeele and Robins (2007b) proved that for effect-modification to occur, at least one other arrow besides the treatment must enter into the outcome. According to Figure 5 the only arrow into Y other than A arrives from Z. Because Y is independent of G conditional on Z we may infer that G is no longer an effect modifier for the effect of A on Y. Viewed another way, G no longer co-varies with Y conditional on Z and so cannot act as an effect-modifier.\n\n\n\n\n\n\n\n\nFigure 5: Conditioning on Z renders G independent of Y. G is no longer an effect modifier after conditioning on Z because G is independent of Y. Although Z is an unconditional effect modifier, G is not.\n\n\n\n\n\nFigure 6 presents the same a randomised experiment as in the previous graph. We assume a true effect of A \\rightarrow Y. If we do not condition on B, then G will not modify the effect of A  \\rightarrow Y because G will not be associated with Y. However, if we were to condition on B, then both B (an effect modifier by proxy) and G may become effect-modifiers for the causal effect of A on Y. In this setting, both B and G are conditional effect-modifiers.\nNote that casual graphs help us to evaluate classifications of conditional and unconditional effect modifiers. They may also help to clarify conditions in which conditioning on unconditional effect-modifiers may remove conditional effect-modification. However we cannot not tell from a causal diagram whether the ancestors of an unconditional effect-modifier will be conditional effect-modifiers for the effect of the exposure on the outcome; see: Tyler J. VanderWeele and Robins (2007b), also Suzuki et al. (2013). Causal diagrams express non-parametric relations. I have adopted an off-label colouring convention to denote instances of effect-modification to highlight possible pathways for effect-modification, which may be relative to other variables in a model.\n\nVanderWeele, Tyler J., and James M. Robins. 2007b. “Four types of effect modification: a classification based on directed acyclic graphs.” Epidemiology (Cambridge, Mass.) 18 (5): 561–68. https://doi.org/10.1097/EDE.0b013e318127181b.\n\n\n\n\n\n\n\n\nFigure 6: Blue path denotes effect-modification for G by conditioning on B. Both B and G are conditional effect modifiers.\n\n\n\n\n\nFigure 7 reveals the relativity of effect-modification. If investigators do not condition on B, then G cannot be a conditional effect-modifier because G would then be independent of Z because B is a collider. However, as we observed in Figure 6, conditioning on B, a collider, may open a path for effect-modification of G by Z. Both B and G are conditional effect modifiers.\n\n\n\n\n\n\n\n\nFigure 7: Blue path denotes effect-modification. Here G is not an effect modifier because B, a common effect (collider) of G and Z, is not conditioned on. Any conditional effect modification for G would require conditioning on B, and not-conditioning on G. Otherwise G will be d-separated from Y.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Blue path denotes effect-modification. Neither, G nor B are unconditional effect-modifiers for the effect of A on Y after Z is conditioned upon. If investigators condition on Z, the causal diagram implies they will not find evidence for effect-modification by B or G, which are conditionally independent of Y once Z is conditioned upon.\n\n\n\n\n\nFigure 8 considers the implications of conditioning on Z, which is the only unconditional effect-modifier on the graph. If Z is measured, conditioning on Z will remove effect-modification for B and G because B,G\\coprod Y |Z. This examples again reveals the context dependency of effect-modification. Here, causal diagrams are useful for clarifying features of dependent and independent effect modification. For further discussion, see: Suzuki et al. (2013); Tyler J. VanderWeele (2009).\n\nSuzuki, Etsuji, Toshiharu Mitsuhashi, Toshihide Tsuda, and Eiji Yamamoto. 2013. “A Counterfactual Approach to Bias and Effect Modification in Terms of Response Types.” BMC Medical Research Methodology 13 (1): 1–17.\n\nVanderWeele, Tyler J. 2009. “On the Distinction Between Interaction and Effect Modification.” Epidemiology, 863–71.\n\n\nPackages\n\nreport::cite_packages()\n\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 0.3.3.3 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Firke S (2024). _janitor: Simple Tools for Examining and Cleaning Dirty Data_. R package version 2.2.1, &lt;https://CRAN.R-project.org/package=janitor&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 &lt;https://doi.org/10.21105/joss.02445&gt;.\n  - Müller K (2020). _here: A Simpler Way to Find Your Files_. R package version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Rich B (2023). _table1: Tables of Descriptive Statistics in HTML_. R package version 1.4.3, &lt;https://CRAN.R-project.org/package=table1&gt;.\n  - Sjoberg D, Whiting K, Curry M, Lavery J, Larmarange J (2021). \"Reproducible Summary Tables with the gtsummary Package.\" _The R Journal_, *13*, 570-580. doi:10.32614/RJ-2021-053 &lt;https://doi.org/10.32614/RJ-2021-053&gt;, &lt;https://doi.org/10.32614/RJ-2021-053&gt;.\n  - Sjolander A, Dahlqwist E (2021). _stdReg: Regression Standardization_. R package version 3.4.1, &lt;https://CRAN.R-project.org/package=stdReg&gt;.\n  - Waring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, Ellis S (2022). _skimr: Compact and Flexible Summaries of Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=skimr&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;."
  },
  {
    "objectID": "content/06-content.html#appendix-a-simplification-of-additive-interaction-formula",
    "href": "content/06-content.html#appendix-a-simplification-of-additive-interaction-formula",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Appendix A: Simplification of Additive Interaction Formula",
    "text": "Appendix A: Simplification of Additive Interaction Formula\nWe start with the definition of additive interaction based on comparing the joint effect relative to baseline versus the sum of individual effects relative to baseline:\n\n\\Big(\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)]\\Big) - \\Big[\\Big(\\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,0)]\\Big) + \\Big(\\mathbb{E}[Y(0,1)] - \\mathbb{E}[Y(0,0)]\\Big)\\Big]\n\nFirst, distribute the negative sign across the terms within the square brackets:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)] - \\Big(\\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,0)]\\Big) - \\Big(\\mathbb{E}[Y(0,1)] - \\mathbb{E}[Y(0,0)]\\Big)\n\nNow remove the parentheses, flipping the signs inside them where preceded by a minus sign:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(1,0)] + \\mathbb{E}[Y(0,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)]\n\nNext, combine the \\mathbb{E}[Y(0,0)] terms:\n\nWe have -\\mathbb{E}[Y(0,0)]\nThen +\\mathbb{E}[Y(0,0)] (these two cancel each other out)\nAnd another +\\mathbb{E}[Y(0,0)] remains.\n\nThe expression simplifies to:\n\n\\mathbb{E}[Y(1,1)] - \\mathbb{E}[Y(1,0)] - \\mathbb{E}[Y(0,1)] + \\mathbb{E}[Y(0,0)]\n\nThis is the standard definition of additive interaction, often called the interaction contrast. If this expression equals zero, there is no additive interaction; a non-zero value indicates an interaction effect.\nThis shows clearly that interaction is the deviation of the joint effect from the sum of the separate effects, adjusted for the baseline."
  },
  {
    "objectID": "content/07-quiz.html#last-years-test",
    "href": "content/07-quiz.html#last-years-test",
    "title": "In-Class Test",
    "section": "Last Year’s Test",
    "text": "Last Year’s Test"
  },
  {
    "objectID": "content/06-content.html#appendix-b-evidence-for-effect-modification-is-relative-to-inclusion-of-other-variables-in-the-model",
    "href": "content/06-content.html#appendix-b-evidence-for-effect-modification-is-relative-to-inclusion-of-other-variables-in-the-model",
    "title": "Causal Inference: Understanding How Effects Differ",
    "section": "Appendix B: Evidence for effect-modification is relative to inclusion of other variables in the model",
    "text": "Appendix B: Evidence for effect-modification is relative to inclusion of other variables in the model\nThe ‘sharp-null hypothesis’ states there is no effect of the exposure on the outcome for any unit in the target population. Unless the ‘sharp-null hypothesis’ is false, there may be effect-modification. For any study worth conducting, we cannot evaluate whether the sharp-null hypothesis is false. If we could the experiment would be otiose. Therefore, we must assume the possibility of effect-modification. Notably, whether a variable is an effect-modifier also depends on which other variables are included in the model. That is, just as for the concept of a ‘confounder’, where a variable is an ‘effect-modifier’ cannot be stated without reference to an assumed causal order and an explicit statement about which other variables will be included in the model (Tyler J. VanderWeele 2012).\n\nVanderWeele, Tyler J. 2012. “Confounding and Effect Modification: Distribution and Measure.” Epidemiologic Methods 1 (1): 55–82. https://doi.org/10.1515/2161-962X.1004.\nAs illustrated in Figure 4, the marginal association between A and Y is unbiased. Here, exposure A is unconditionally associated with Y. Recall our convention G denotes effect-modification with conditioning and Z indicates effect-modification without conditioning.\n\n\n\n\n\n\n\n\nFigure 4: Consider a randomised experiment. There is no confounding. Here, the marginal association between A and Y provides an unbiased estimate for the causal effect of A on Y. Does the conditional association of A on Y vary within levels of G? The causal diagram allows for a classification of G as an effect modifier of A on Y by proxy. G modifies A’s effect on Y in virtue of G’s relationship to Z, which, according to this graph, is a direct effect modifier for the effect of A on Y.\n\n\n\n\n\nFigure 5 presents the same a randomised experiment as in the previous causal diagram. We again assume that there is no confounding of the marginal association between the exposure, A, and the outcome, Y. However, suppose we were to adjust for Z and ask, does the conditional association of A on Y vary within levels of G, after adjusting for Z? That is, does G remain an effect-modifier of the exposure on the outcome? Tyler J. VanderWeele and Robins (2007b) proved that for effect-modification to occur, at least one other arrow besides the treatment must enter into the outcome. According to Figure 5 the only arrow into Y other than A arrives from Z. Because Y is independent of G conditional on Z we may infer that G is no longer an effect modifier for the effect of A on Y. Viewed another way, G no longer co-varies with Y conditional on Z and so cannot act as an effect-modifier.\n\n\n\n\n\n\n\n\nFigure 5: Conditioning on Z renders G independent of Y. G is no longer an effect modifier after conditioning on Z because G is independent of Y. Although Z is an unconditional effect modifier, G is not.\n\n\n\n\n\nFigure 6 presents the same a randomised experiment as in the previous graph. We assume a true effect of A \\rightarrow Y. If we do not condition on B, then G will not modify the effect of A  \\rightarrow Y because G will not be associated with Y. However, if we were to condition on B, then both B (an effect modifier by proxy) and G may become effect-modifiers for the causal effect of A on Y. In this setting, both B and G are conditional effect-modifiers.\nNote that casual graphs help us to evaluate classifications of conditional and unconditional effect modifiers. They may also help to clarify conditions in which conditioning on unconditional effect-modifiers may remove conditional effect-modification. However we cannot not tell from a causal diagram whether the ancestors of an unconditional effect-modifier will be conditional effect-modifiers for the effect of the exposure on the outcome; see: Tyler J. VanderWeele and Robins (2007b), also Suzuki et al. (2013). Causal diagrams express non-parametric relations. I have adopted an off-label colouring convention to denote instances of effect-modification to highlight possible pathways for effect-modification, which may be relative to other variables in a model.\n\nVanderWeele, Tyler J., and James M. Robins. 2007b. “Four types of effect modification: a classification based on directed acyclic graphs.” Epidemiology (Cambridge, Mass.) 18 (5): 561–68. https://doi.org/10.1097/EDE.0b013e318127181b.\n\n\n\n\n\n\n\n\nFigure 6: Blue path denotes effect-modification for G by conditioning on B. Both B and G are conditional effect modifiers.\n\n\n\n\n\nFigure 7 reveals the relativity of effect-modification. If investigators do not condition on B, then G cannot be a conditional effect-modifier because G would then be independent of Z because B is a collider. However, as we observed in Figure 6, conditioning on B, a collider, may open a path for effect-modification of G by Z. Both B and G are conditional effect modifiers.\n\n\n\n\n\n\n\n\nFigure 7: Blue path denotes effect-modification. Here G is not an effect modifier because B, a common effect (collider) of G and Z, is not conditioned on. Any conditional effect modification for G would require conditioning on B, and not-conditioning on G. Otherwise G will be d-separated from Y.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Blue path denotes effect-modification. Neither, G nor B are unconditional effect-modifiers for the effect of A on Y after Z is conditioned upon. If investigators condition on Z, the causal diagram implies they will not find evidence for effect-modification by B or G, which are conditionally independent of Y once Z is conditioned upon.\n\n\n\n\n\nFigure 8 considers the implications of conditioning on Z, which is the only unconditional effect-modifier on the graph. If Z is measured, conditioning on Z will remove effect-modification for B and G because B,G\\coprod Y |Z. This examples again reveals the context dependency of effect-modification. Here, causal diagrams are useful for clarifying features of dependent and independent effect modification. For further discussion, see: Suzuki et al. (2013); Tyler J. VanderWeele (2009).\n\nSuzuki, Etsuji, Toshiharu Mitsuhashi, Toshihide Tsuda, and Eiji Yamamoto. 2013. “A Counterfactual Approach to Bias and Effect Modification in Terms of Response Types.” BMC Medical Research Methodology 13 (1): 1–17.\n\nVanderWeele, Tyler J. 2009. “On the Distinction Between Interaction and Effect Modification.” Epidemiology, 863–71.\n\nAppendix C: Futher Clarification on Effect Modification Without Statistical Evidence for It.\nAgain, look at Figure 3. Suppose the investigator model the effect of A on Y. Suppose G is not associated with Y conditional on L. Figure 3 provides structural clarification for why concluding there is no effect modification by G is incorrect.\nTo clarify: - We have a DAG structure: G \\to L; L \\to A, L \\to Y - We’re interested in the conditional average treatment effect (CATE): \\tau(g) = E[Y(1) - Y(0)| G = g] - To identify the effect of A on Y, we must condition on \\boxed{L} - G is (partially or fully) d-separated from Y conditional on \\boxed{L}\nEven though G is d-separated from Y given L, this doesn’t mean G can’t modify the effect of A on Y. The CATE for a specific value of G can be expressed as:\n\\tau(g) = E[Y(1) - Y(0)| G = g] = E[E[Y(1) - Y(0)| L, G = g]| G = g]\nSince G is d-separated from Y given L:\nτ(g) = E[E[Y(1) - Y(0)| L]| G = g]\nThis means τ(g) is a weighted average of the L-specific treatment effects, where the weights are determined by the distribution of L given G=g.\nIf two conditions are met: 1. The effect of A on Y varies across levels of L 2. The distribution of L varies with G (which it does, since G \\to L)\nThen \\tau(g) will vary with g, indicating effect modification by G.\nThe investigators are would be wrong to equate d-separation with absence of effect modification. Although G doesn’t directly affect Y after conditioning on L, G can still modify the effect of A on Y through its influence on the distribution of L.\n\n\nPackages\n\nreport::cite_packages()\n\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 0.3.3.3 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Firke S (2024). _janitor: Simple Tools for Examining and Cleaning Dirty Data_. R package version 2.2.1, &lt;https://CRAN.R-project.org/package=janitor&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Lüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 &lt;https://doi.org/10.21105/joss.02445&gt;.\n  - Müller K (2020). _here: A Simpler Way to Find Your Files_. R package version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Rich B (2023). _table1: Tables of Descriptive Statistics in HTML_. R package version 1.4.3, &lt;https://CRAN.R-project.org/package=table1&gt;.\n  - Sjoberg D, Whiting K, Curry M, Lavery J, Larmarange J (2021). \"Reproducible Summary Tables with the gtsummary Package.\" _The R Journal_, *13*, 570-580. doi:10.32614/RJ-2021-053 &lt;https://doi.org/10.32614/RJ-2021-053&gt;, &lt;https://doi.org/10.32614/RJ-2021-053&gt;.\n  - Sjolander A, Dahlqwist E (2021). _stdReg: Regression Standardization_. R package version 3.4.1, &lt;https://CRAN.R-project.org/package=stdReg&gt;.\n  - Waring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, Ellis S (2022). _skimr: Compact and Flexible Summaries of Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=skimr&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;."
  },
  {
    "objectID": "content/08-content.html#appendix",
    "href": "content/08-content.html#appendix",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "content/08-content.html#approach",
    "href": "content/08-content.html#approach",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Approach",
    "text": "Approach\n\n#|echo: true\n\n# boilerplate package\nif (!require(boilerplate, quietly = TRUE)) {\n  # install devtools if necessary\n  if (!require(devtools, quietly = TRUE)) {\n    install.packages(\"devtools\")\n  }\n  devtools::install_github(\"go-bayes/boilerplate\")\n}\n\n\n# set boilerplate path (set to your own machine)\nmaster_path &lt;- \"/Users/joseph/GIT/templates/boilerplate_data\"\n\n# set directory database path\nhere_data_path = here::here(\"data\")\n\n# # import\n# master_unified_db &lt;- boilerplate_import(data_path = master_path)\n\n# boilerplate_save(master_unified_db, output_file = \"unified_db\", data_path  = here_data_path, create_backup = FALSE)\n\nunified_db &lt;- boilerplate_import(data_path = here_data_path)\n\ncat(unified_db$appendix$explain$grf)\n\n\nIn this appendix we show how to estimate causal effects with the `grf` R package (Generalised Random Forests), following a standard workflow. We begin with the overall average treatment effect (ATE), investigate whether effects vary (heterogeneity), and --where useful -- estimate individualised effects and derive simple, actionable treatment rules.\n\n### Step 1 Estimating the Average Treatment Effect (ATE)\n\nThe ATE answers a simple question: on average, how much does treatment change the outcome? In a well‑randomised trial (or an observational study with adequate control), we estimate it with the difference in mean outcomes between treated and control units. Formally,\n\n$${\\rm ATE}=E\\,[Y(1)-Y(0)],$$\n\nwhere $Y(1)$ and $Y(0)$ denote potential outcomes under treatment and control.\n`grf::average_treatment_effect()` computes the ATE, using doubly‑robust estimation for precision. This step gives us a baseline—say, the new therapy lifts well‑being by 0.25 points (1–7 scale).\n\nWe start here because the ATE tells us whether the treatment works **on average** if everyone were treated versus no‑one. Yet a zero (or noisy) ATE does not rule out helpful or harmful effects for sub‑groups, and a non‑zero ATE does not guarantee benefits for all. Either way, we proceed to heterogeneity.\n\n### Step 2 Assessing Heterogeneous Treatment Effects (HTE)\n\nTreatments seldom work equally for everyone. We define the conditional average treatment effect (CATE) for covariate profile $x$ as\n$$\\tau(x)=E\\,[Y(1)-Y(0)\\mid X=x].$$\nIf $\\tau(x)$ is constant, effects are homogeneous; if it varies, we have heterogeneity.\n\nClassical regression tackles heterogeneity via interaction terms, but that approach demands strong functional‐form assumptions. Real‑world heterogeneity can be non‑linear and high‑dimensional; guessing the correct model risks misspecification or over‑fitting [@Sadique2022]. Thus we turn to causal forests, which let the data speak.\n\n\n### Step 3 Estimating Individualised Effects with Causal Forests\n\nA causal forest is an ensemble of 'honest' causal trees grown to capture **differences** in treatment effects rather than outcome levels [@grf2024]. Each tree splits the data to maximise treated‑versus‑control contrasts within leaves, and the forest averages those leaf‑level estimates to give\n$$\\hat\\tau(x)$$\nfor every individual.\n\n Advantages:\n\n- **Flexibility** – no need to pre‑specify interactions or non‑linearities.\n- **Orthogonalisation** – residualising outcomes and treatment probabilities focuses the forest on causal signal [@wager2018].\n- **Per‑person estimates** – we obtain $\\hat\\tau(x_i)$ for every $i$.\n\n### Step 4 Honest Estimation and Out‑of‑Bag Validation\n\nOver‑fitting lurks in flexible models. `grf` combats this with 'honest'' trees: one half‑sample picks splits, the other half estimates effects, ensuring each leaf estimate is out‑of‑sample. Out‑of‑bag (OOB) predictions—averaging over trees that did **not** train on an observation—provide further unbiased validation and standard errors [@grf2024].\n\n### Step 5 Handling Missing Data and Basic Validation\n\nMissing covariate values? No drama. `grf` uses the MIA (Missing Incorporated in Attributes) rule: it treats 'missing' as a legitimate split category, so we keep cases rather than impute or drop them [@grf2024]. Hyper‑parameters rarely need fine‑tuning, though cross‑validation can refine minimum leaf size, tree depth, and the like.\n\n### Step 6 Testing for Treatment‑Effect Heterogeneity\n\nDo the estimated $\\hat\\tau(x)$ really differ, or is it just noise? The RATE framework answers this. We rank individuals by $\\hat\\tau$ and ask whether treating the top scorers improves outcomes relative to random assignment. The Targeting Operator Characteristic (TOC) curve plots this gain over treatment fractions $q$. Two scalar summaries are\n\n* **AUTOC** – area under the TOC; accentuates extreme responders.\n* **Qini** – a weighted area (weights grow with $q$); favours broader gains [@yadlowsky2021evaluating].\n\nUnder $H_0$ (no heterogeneity) both metrics equal zero; `grf::rank_average_treatment_effect()` supplies estimates, standard errors, and $t$‑tests. Rejecting $H_0$ signals actionable heterogeneity.\n\n### Step 7 Quantifying Policy Value with RATE Metrics\n\nBeyond significance, AUTOC and Qini quantify the **magnitude** of improvement from targeting. A large AUTOC means a prioritisation rule sharply distinguishes high‑benefit individuals; a large Qini means gains persist over wider coverage. Policymakers can weigh these gains against costs—because budgets rarely stretch to treating everyone (as any Kiwi running a lab grant knows too well).\n\n### Step 8 Plotting Qini Curves for Policy Insight\n\nA single number is handy, but a Qini curve reveals where returns plateau. `grf` can plot cumulative gain versus spend; @fig‑example‑qini below shows an analysis of religious‑service attendance and agreeableness. The dashed curve (model‑based targeting) lies above the solid ATE line until roughly 50% coverage—beyond that, extra treatment yields little extra benefit.\n\n```{r, results='asis'}\n#| label: fig-example-qini\n#| fig-cap: \"Example Qini Curve.\"\n#| eval: true\n#| echo: false\n#| fig-width: 12\n#| fig-height: 12\nmodels_binary_batch_example$model_t2_agreeableness_z$policy_tree\n\n```\n\n\nIn the above illustration, the Qini curve rises sharply at the beginning, indicating that the first several percent of individuals treated (those with the highest predicted $  au(x)$) yield a large gains to agreeableness. By contrast, the dashed line is roughly a 35-degree line starting at (0,0) – it represents a policy that doesn’t use the heterogeneity (just treating people arbitrarily), which yields, on average, a linear accumulation of the overall ATE. At $x=1.0$ (100% treated), both the model-based policy and the random policy coincide – at that point, everyone is treated, so the total gain is just the ATE. The fact that the Qini curve is above the solid line (forming a 'bulge') indicates that targeting is effective: we achieve more outcome gain for the same treatment fraction [@grf2024]. The area between the solid curve and dashed line up to a certain point corresponds to the RATE metrics discussed (QINI would be the total area between them from 0 to 1).\n\nIn our analysis, we report budget-constrained thresholds for treating the top 20% or 50% of recipients. Policymakers often have limits on how many people can be treated (due to cost or other constraints). The Qini curve allows us to zoom in on specific treatment fractions. For instance, if we can only treat 20% of the population, we look at $q = 0.2$ on the x-axis. The y-value of the Qini curve at $0.2$ tells us the expected gain from treating the top 20% (as ranked by the model) compared to treating 20% randomly.\n\nThe Qini curve allows us to visualise and explain how the effectiveness of the treatment allocation changes as we expand who gets treated. For an applied researcher, this is very useful. We do more than not that 'heterogeneity exists.'  We are able to advise whether, 'if we have a budget to treat X% of people, here's what outcome we can expect.' For example, suppose an education intervention has an average effect of 5 test score points. The Qini analysis might show that focusing on the top 50% of the population (maybe those with certain demographic profiles) could yield an average effect of .15 SD units for those expected to benefit, meaning we would save resources on those unlikely to gain. At 100% (treating everyone), we're back to average treatment effect, which in this example is about half this amount (0.07 SD units). So the policy insight is: if only 50% can be afforded, target that group to maximise impact.\n\nTo summarise, Qini curves allow us to identify and communicate the value of targeted policies at specific coverage levels. They show whether the benefit of the treatment is concentrated in a small segment of the population or more widely distributed. When making policy recommendations, one could use the Qini curve to decide, for instance, 'we should implement the program for the top 50% most likely to benefit; beyond 50%, the returns diminish to roughly the average effect, so including more people isn’t cost-effective.' This level of analysis goes beyond saying 'there is heterogeneity'; instead, we quantifies *how much improvement* is possible and for what percentage of a target population.\n\n### Step 9: Avoiding Overfitting in Policy Evaluation (Sample Splitting for RATE/Qini)\n\nAn important statistical caution: when we use the same data to both train a model (such the causal forest) and evaluate its performance (like computing how well targeting does), we can get overly optimistic results. Even though the causal forest provides OOB estimates that are ostensibly out-of-sample for each individual, there is still a subtle form of overfitting possible if we are not careful. The forest was constructed to maximise heterogeneity in the training data, so using those same estimates to evaluate the policy can bias the evaluation upward (we might overstate the value of targeting because we are implicitly reusing the data).\n\nTo ensure valid inference, the best practice is to use explicit sample splitting or cross-fitting for the evaluation stage. This means, for example, after training the causal forest on the whole dataset (or a portion of it), we assess the RATE or Qini metrics on a fresh hold-out set that was not used in training. Here we use a separate hold-out set in which we train {{traning_proportion}} of the data, and use the 'unseen' remainder as the validation set.\n\nWhy do we do this? Even though the forest’s OOB predictions are not directly from a model that saw that observation's outcome, there remains correlation – each observation’s prediction is an average from many trees, and while any given tree didn't see that observation, it saw many others including some that are also used in evaluating the policy. There is also the fact that OOB predictions are a form of cross-validation but not a true independent test, especially since the forest structure was influenced by all data. To be rigorous, researchers often do a double sample split: one split to train the forest, and a second split (completely independent) to compute the policy metrics like Qini or evaluate a specific targeting rule. This double-splitting ensures that when we say 'targeting the top 20% yields X improvement,' that claim is verified on data that played no part in determining who was top 20%. Thus, our inference (confidence intervals, p-values for heterogeneity) remains valid and not overly optimistic.\n\nIn plain terms, our workflow is as follows:\n\n1. Split the data into two parts: A and B.\n2. Use part A to train the causal forest and get $\\hat{ au}(x)$.\n3. Use part B to evaluate heterogeneity: calculate the actual outcomes if we apply the policy (treat those with highest $\\hat{\\tau}$ in B, compare to others in B, etc.). Compute RATsE/Qini metrics and perform the heterogeneity test on part B.\n\nBy doing this, when we report 'p = 0.01 for heterogeneity' or 'Qini = 0.10 at 20% spending', we know these numbers are honest assessments of how the model would perform on new data, not just the data we fit it to.\n\nIn short, even with OOB estimates, further sample splitting is used for final evaluation to ensure our conclusions about heterogeneous effects and the benefits of targeting are reliable. This extra step is crucial for rigour: it prevents us from convincing ourselves that a complicated model is useful when in fact it might be explaining noise.\n\n### Step 10: Interpretable Treatment Rules with Policy Trees\n\nFinally, once we have evidence of heterogeneity and an effective way to target treatment, we might want to translate the complex model into a simple decision rule. Causal forests, while powerful, are 'black box' in nature – they may involve hundreds of trees and intricate splits that are not easy to interpret. For practical decision-making (especially in policy contexts), stakeholders often prefer a simple rule (for example, a checklist or a flowchart) that determines treatment eligibility. This is where policy trees come in.\n\nA policy tree is essentially a simple decision tree that assigns treatment or control based on a few covariates splits, optimised to yield good outcomes. The idea is to summarise the individual treatment effects $\\hat{\\tau}(x)$ (or directly use the data) into a set of if-then rules that approximate the optimal targeting. For instance, a policy tree might look like: 'If age &lt; 25 and baseline severity is high, give treatment; if age ≥ 25 and baseline severity is low, do not treat,' etc. These rules are much easier to communicate than a black-box forest.\n\n`grf` integrates with the `policytree` package to derive an optimal decision tree given the causal forest's estimates [@policytree_package_2024]. Our procedure is as follows: use the forest's estimates or doubly robust scores as input to a decision tree algorithm that maximises the expected outcome (welfare) under that tree policy. In other words, we finds splits that best separate who should be treated vs not to improve the overall result. The result is a shallow tree (often depth 1, 2, or 3) that is much easier to interpret than the full forest. Here we use a decision tree of depth = 2.\n\n**Why use policy trees?** Apart from interpretability, policy trees can enforce fairness or simplicity constraints and avoid overfitting by limiting complexity. A shallow tree might capture the broad strokes of heterogeneity (e.g., young vs old, or high risk vs low risk) in a way that practitioners can double-check with domain knowledge. As an example from `grf` documentation: “Deciding who to assign the program based on a complicated black-box CATE function may be undesirable if policymakers want transparent criteria. Likewise, it may be problematic for participants to learn they were denied a beneficial program solely because a black-box algorithm predicted low benefit. In such settings, we want an interpretable policy, for example, a shallow decision tree that says ‘Alice is assigned treatment because she is under 25 and lives in a disadvantaged neighborhood' see: [https://grf-labs.github.io/grf/articles/policy_learning.html](https://grf-labs.github.io/grf/articles/policy_learning.html). This nicely illustrates how a policy tree can provide a rationale in human terms.\n\n**Training and validation for policy trees:** As when construsting causal forests and evaluating heterogeneity in them using RATE AUTOC or Qini curves, when creating policy trees we must be careful to avoid overfitting. It's tempting to use the same data that suggested heterogeneity to also choose the best splits for the policy tree, but that can lead to optimistic results. The optimal tree is chosen to fit the training data well – if we do not validate it, we might pick a tree that works by chance quirks of the data. Therefore, we use cross-validation to select the tree's complexity (depth) and sample splitting to evaluate its performance, using {{train_proportion_decision_tree}} to train the tree and the remainder to valid it.\n\n```{r, results='asis'}\n#| label: fig-example-decision-tree\n#| fig-cap: \"Example Decision Tree.\"\n#| eval: true\n#| include: true\n#| echo: false\n#| fig-width: 10\n#| fig-height: 14\nmodels_binary_batch_example$model_t2_agreeableness_z$combined_plot\n```\n\n**Example**: Recall, @fig-example-qini shows reliable treatment heterogeneity for the effect of religious service attendance on agreeableness. @fig-example-decision-tree presents a policy tree analysis in which simple treatment rules are evaluated indicating who we should 'treat' with religious service if we hope to optimise agreeableness. Here we find that three variables predict treatment benefits: weekly hours commuting, weekly hours doing housework, and household income.  Specifically:\n\n- Participants who commute less than about 5.8 hours per week and have a household income above NZD 28,600 experience greater gains in agreeableness when attending religious services. However among those with very low income and lower commuting demands we would not expect added gains for agreeableness from religious service attendance.\n- Meanwhile, those with comparable higher commuting hours who household working hours are less than 16 hours per week are not expected to benefit in Agreeableness. However, those who are heavier commuters but and also doing more housework are expected to gain in agreeableness.\n\nThis analysis holds both theoretical and practical interest. Rather than picking out a familiar category grouping such as gender or ethnicity, policy trees reveal that effect vary by resource availability.\n\nOverall, policy trees condense the insights from causal forests into actionable guidelines. We emphasise that deriving these guidelines includes rigorous validation: we use one part of the data to learn the policy and another to test it. This ensures that the simple rules we recommend (e.g. 'treat those with lower time demands; do not treat incomes at a certain level') are supported by evidence rather than being artifacts of noise.\n\nAlthough estimating heterogeneous treatment effects and deriving actionable policy rules can be both theoretically and practically important, two considerations should temper our enthusiasm.\n\nFirst, the factors highlighted in policy trees are predictors of treatment-effect variability, but these predictors do not themselves have a causal interpretation. Returning to the example above, we should not infer that intervening to set someone’s travel and housework times to different values would change the variability in response. To understand the causal effects of joint interventions, we would need a different analysis. Decision trees are important because they draw attention to segments of a population likely to benefit, but they do not clarify the effects of changing a population structure.\n\nSecond, decisions about prioritising treatment rules are often ethical and political. Even if we set aside uncertainties in the modelling process, few would argue that fairness and justice should be determined by optimisation rules alone. Such questions are typically resolved through democratic processes that involve stakeholder consultations, reflection on social values, a reckoning with historical inequities, and considerations beyond the scope of statistical analyses."
  },
  {
    "objectID": "content/08-content.html#heterogeneous-treatment-effect-hte-analysis-with-grf",
    "href": "content/08-content.html#heterogeneous-treatment-effect-hte-analysis-with-grf",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Heterogeneous-Treatment-Effect (HTE) Analysis with grf",
    "text": "Heterogeneous-Treatment-Effect (HTE) Analysis with grf\nWe will work step-by-step:\n\nStart with the overall average treatment effect (ATE).\n\nAsk whether effects vary across people.\n\nEstimate personalised effects with a causal forest.\n\nCheck whether targeting the largest effects is worthwhile.\n\nTranslate the black-box model into an easy-to-explain policy tree.\n\n\n\n1 Average Treatment Effect (ATE)\nQuestion If everyone were treated instead of no-one, by how much would the outcome change on average?\n\n\\text{ATE}=E\\!\\bigl[Y(1)-Y(0)\\bigr].\n\n\nEstimated with grf::average_treatment_effect(), which is doubly robust: the estimate stays consistent if either the treatment model or the outcome model is correct.\n\nDetails on doubly-robust estimation in the lab.\n\n\n\n\n2 Do Effects Vary? — Formal Test of Heterogeneity\nDefine the conditional average treatment effect (CATE)\n\n\\tau(x)=E\\!\\bigl[Y(1)-Y(0)\\,\\big|\\,X=x\\bigr].\n\n\nIf begin:math:text(x)end:math:text is constant, there is no heterogeneity.\n\nClassical interaction models impose simple forms; causal forests (Wager and Athey 2018) discover complex, nonlinear patterns.\n\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–42. https://doi.org/10.1080/01621459.2017.1319839.\nTesting begin:math:textH_0!:(x)end:math:text constant will rely on the RATE statistics below.\n\n\n\n3 Causal Forests for Personalised Estimates\nA causal forest is an ensemble of “honest” causal trees that split the data to maximise the treated-vs-control contrast.\nFor each person begin:math:textiend:math:text:\n\n\\widehat{\\tau}(x_i)\n\nis our best estimate of their individual treatment effect.\n\n\n\n\n\n\n\nStrength\nExplanation\n\n\n\n\nFlexibility\nNo need to guess the correct functional form.\n\n\nHonesty\nEach tree uses one part of the data to choose splits and the other to estimate.\n\n\nValid SEs\nOut-of-bag predictions supply standard errors automatically.\n\n\n\n\n\n\n4 Built-in Protection Against Over-fitting\n\nHonest splitting (split half / estimate half).\n\nOut-of-bag (OOB) prediction: each unit’s begin:math:text(x)end:math:text is computed by trees that did not use that unit for splitting.\n\nThese yield nearly unbiased estimates and valid uncertainty without exhaustive hyper-tuning.\n\n\n\n5 Missing-Data Handling\ngrf uses Missing Incorporated in Attributes (MIA):\n\n“Missing” can itself be the best split.\n\nNo observations are dropped or crudely imputed.\n\n\n\n\n6 Is the Heterogeneity Actionable? — TOC & RATE\nRank people by begin:math:text(x)end:math:text and draw the Targeting Operator Characteristic (TOC) curve: cumulative gain from treating the top fraction begin:math:textqend:math:text.\n\n\n\n\n\n\n\nStatistic\nWhat it rewards\n\n\n\n\nRATE AUTOC\nSharp gains among the very top responders (area under the full TOC).\n\n\nRATE Qini\nSteady gains as coverage increases (weighted area, more weight on larger begin:math:textqend:math:text).\n\n\n\nUnder begin:math:textH_0{:}(x)end:math:text constant, both statistics = 0.\nCompute with grf::rank_average_treatment_effect().\n\nQuick guide\n- AUTOC: “How well can we rank the very best group?”\n- Qini: “If we have a moderate budget, does targeting still help?”\n\n\n\n\n7 Valid Inference for RATE / Qini\nOOB predictions are out-of-sample per tree, but not per forest.\nWe therefore make an explicit train/test split:\n\nTrain set Fit the causal forest; obtain begin:math:text(x)end:math:text.\n\nTest set Calculate RATE AUTOC and RATE Qini; test begin:math:textH_0end:math:text.\n\nThis guards against optimistic bias (Tibshirani et al. 2024).\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024. Grf: Generalized Random Forests. https://github.com/grf-labs/grf.\n\n\n\n\n\n\nFigure 1: RATE AUTOC: Hours Socialising → Sense of Meaning\n\n\n\n\n\n\n8 Visualising Policy Value: the Qini Curve\n\n\n\n\n\n\nFigure 2: Qini Curve: Hours Socialising → Social Belonging\n\n\n\nReading Figure 2\n- 20 % coverage: targeting adds 0.08 units (95 % CI 0.04 – 0.12).\n- 50 % coverage: gain rises to 0.13 units (95 % CI 0.07 – 0.19).\n\n\n\n9 From Black Box to Simple Rules: Policy Trees\nThe policytree algorithm (Sverdrup et al. 2024) converts begin:math:text(x)end:math:text into a shallow decision tree.\n\nSverdrup, Erik, Ayush Kanodia, Zhengyuan Zhou, Susan Athey, and Stefan Wager. 2024. Policytree: Policy Learning via Doubly Robust Empirical Welfare Maximization over Trees. https://CRAN.R-project.org/package=policytree.\n\n\n\n\n\n\nFigure 3: Decision tree for Social Belonging\n\n\n\nInterpretation (example)\n\nSelf-Esteem ≤ –0.925 → check Neuroticism.\n\nNeuroticism ≤ 0.642 → control; else treat.\n\nSelf-Esteem &gt; –0.925 → check Social Belonging.\n\nBelonging ≤ 0.776 → treat; else control.\n\n\nCaveat Splits identify who benefits, not what to change.\n\n\n\n\n\n\n\nFigure 4: Predicted treatment assignment (out-of-sample)\n\n\n\n\n\n\n10 Ethical and Practical Considerations\nStatistical optimality rarely equals social acceptability.\nEquity, cost, and legitimacy must be decided in democratic fora.\n\n\n\nSummary Checklist\n\n\n\n\n\n\n\n\n\nStage\nTool\nMain output\nGuard-rail\n\n\n\n\n1 ATE\naverage_treatment_effect\nbegin:math:textend:math:text\nDoubly robust\n\n\n2–3 CATE\ncausal_forest\nbegin:math:text(x)end:math:text\nHonest trees\n\n\n6 HTE test\nrank_average_treatment_effect\nRATE AUTOC/Qini, p\nTrain/test split\n\n\n7 Visualise\nQini curve\nGain vs begin:math:textqend:math:text\nSame test fold\n\n\n9 Policy tree\npolicy_tree\nDecision rule\nCross-validation\n\n\n\nTake-away The workflow answers three questions:\n\nIs heterogeneity substantial?\n\nDoes targeting pay off at realistic budgets?\n\nCan we express a good policy in a few, defensible rules?"
  },
  {
    "objectID": "content/08-content.html#lab-data-preparation-and-analysis-scripts",
    "href": "content/08-content.html#lab-data-preparation-and-analysis-scripts",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "Lab: Data Preparation and Analysis Scripts",
    "text": "Lab: Data Preparation and Analysis Scripts\n\nLink to data dictionary\n\n\n\n\n\n\nNote\n\n\n\nFor information about the variables in the synthetic data, download the New Zealand Attitudes and Values Data Dictionary here under “Primary Resources”\nhttps://osf.io/75snb/"
  },
  {
    "objectID": "content/08-content.html#heterogeneous-treatment-effect-hte-analysis-with-causal-forests",
    "href": "content/08-content.html#heterogeneous-treatment-effect-hte-analysis-with-causal-forests",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Heterogeneous-Treatment-Effect (HTE) Analysis with causal forests",
    "text": "Heterogeneous-Treatment-Effect (HTE) Analysis with causal forests\nWe will work step-by-step through our workflow:\n\nStart with the overall average treatment effect (ATE).\n\nAsk whether effects vary across people.\n\nEstimate subgroup effects with a causal forest.\n\nCheck whether targeting the largest effects is worthwhile.\n\nTranslate the black-box model into an easy-to-explain policy tree.\n\n\n\n1 Average Treatment Effect (ATE)\nQuestion If everyone were treated instead of no-one, by how much would the outcome change on average?\n\n\\text{ATE}=E\\!\\bigl[Y(1)-Y(0)\\bigr].\n\n\nEstimated with grf::average_treatment_effect(), which is doubly robust: the estimate stays consistent if either the treatment model or the outcome model is correct.\n\nDetails on doubly-robust estimation in the lab.\n\n\n\n\n2 Do Effects Vary? — Formal Test of Heterogeneity\nDefine the conditional average treatment effect (CATE)\n\n\\tau(x)=E\\!\\bigl[Y(1)-Y(0)\\,\\big|\\,X=x\\bigr].\n\n\nIf \\tau(x) is constant, there is no heterogeneity.\n\nClassical interaction models impose simple forms; causal forests (Wager and Athey 2018) discover complex, nonlinear patterns.\n\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–42. https://doi.org/10.1080/01621459.2017.1319839.\nTesting H_0\\!:\\tau(x) constant will rely on the RATE statistics below.\n\n\n\n3 Causal Forests for Personalised Estimates\nA causal forest is an ensemble of “honest” causal trees that split the data to maximise the treated-vs-control contrast.\nFor each person i:\n\n\\widehat{\\tau}(x_i)\n\nis our best estimate of their individual treatment effect.\n\n\n\n\n\n\n\nStrength\nExplanation\n\n\n\n\nFlexibility\nNo need to guess the correct functional form.\n\n\nHonesty\nEach tree uses one part of the data to choose splits and the other to estimate.\n\n\nValid SEs\nOut-of-bag predictions supply standard errors automatically.\n\n\n\n\n\n\n4 Built-in Protection Against Over-fitting\n\nHonest splitting (split half / estimate half).\n\nOut-of-bag (OOB) prediction: each unit’s \\widehat{\\tau}(x) is computed by trees that did not use that unit for splitting.\n\nThese yield nearly unbiased estimates and valid uncertainty without exhaustive hyper-tuning.\n\n\n\n5 Missing-Data Handling\ngrf uses Missing Incorporated in Attributes (MIA):\n\n“Missing” can itself be the best split.\n\nNo observations are dropped or crudely imputed.\n\n\n\n\n6 Is the Heterogeneity Actionable? — TOC & RATE\nRank people by \\widehat{\\tau}(x) and draw the Targeting Operator Characteristic (TOC) curve: cumulative gain from treating the top fraction q.\n\n\n\n\n\n\n\nStatistic\nWhat it rewards\n\n\n\n\nRATE AUTOC\nSharp gains among the very top responders (area under the full TOC).\n\n\nRATE Qini\nSteady gains as coverage increases (weighted area, more weight on larger q).\n\n\n\nUnder H_0{:}\\tau(x) constant, both statistics = 0.\nCompute with grf::rank_average_treatment_effect().\n\nQuick guide\n- AUTOC: “How well can we rank the very best group?”\n- Qini: “If we have a moderate budget, does targeting still help?”\n\n\n\n\n7 Valid Inference for RATE / Qini\nOOB predictions are out-of-sample per tree, but not per forest.\nWe therefore make an explicit train/test split:\n\nTrain set Fit the causal forest; obtain \\widehat{\\tau}(x).\n\nTest set Calculate RATE AUTOC and RATE Qini; test H_0.\n\nThis guards against optimistic bias (Tibshirani et al. 2024).\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024. Grf: Generalized Random Forests. https://github.com/grf-labs/grf.\n\n\n\n\n\n\n\nFigure 1: RATE AUTOC: Hours Socialising → Sense of Meaning\n\n\n\n\n\nFigure 1 Shows a RATE AUTOC Curve.\n\n\n8 Visualising Policy Value: the Qini Curve\n\n\n\n\n\n\n\n\nFigure 2: Qini Curve: Hours Socialising → Social Belonging\n\n\n\n\n\nReading Figure 2\n- 20 % coverage: targeting adds 0.08 units (95 % CI 0.04 – 0.12).\n- 50 % coverage: gain rises to 0.13 units (95 % CI 0.07 – 0.19).\n\n\n\n9 From Black Box to Simple Rules: Policy Trees\nThe policytree algorithm (Sverdrup et al. 2024) converts \\widehat{\\tau}(x) into a shallow decision tree.\n\nSverdrup, Erik, Ayush Kanodia, Zhengyuan Zhou, Susan Athey, and Stefan Wager. 2024. Policytree: Policy Learning via Doubly Robust Empirical Welfare Maximization over Trees. https://CRAN.R-project.org/package=policytree.\n\n\n\n\n\n\n\nFigure 3: Decision tree for Social Belonging\n\n\n\n\nInterpretation (example)\n\nSelf-Esteem ≤ –0.925 → check Neuroticism.\n\nNeuroticism ≤ 0.642 → control; else treat.\n\nSelf-Esteem &gt; –0.925 → check Social Belonging.\n\nBelonging ≤ 0.776 → treat; else control.\n\n\nCaveat Splits identify who benefits, not what to change.\n\n\n\n\n\n\n\n\nFigure 4: Predicted treatment assignment (out-of-sample)\n\n\n\n\n\n\n\n10 Ethical and Practical Considerations\nStatistical optimality rarely equals social acceptability.\nEquity, cost, and legitimacy must be decided in democratic fora.\n\n\n\nSummary Checklist\n\n\n\n\n\n\n\n\n\nStage\nTool\nMain output\nGuard-rail\n\n\n\n\n1 ATE\naverage_treatment_effect\n\\widehat{\\text{ATE}}\nDoubly robust\n\n\n2–3 CATE\ncausal_forest\n\\widehat{\\tau}(x)\nHonest trees\n\n\n6 HTE test\nrank_average_treatment_effect\nRATE AUTOC/Qini, p\nTrain/test split\n\n\n7 Visualise\nQini curve\nGain vs q\nSame test fold\n\n\n9 Policy tree\npolicy_tree\nDecision rule\nCross-validation\n\n\n\nTake-away The workflow answers three questions:\n\nIs heterogeneity substantial?\n\nDoes targeting pay off at realistic budgets?\n\nCan we express a good policy in a few, defensible rules?"
  },
  {
    "objectID": "content/08-content.html#why-worry-about-heterogeneity",
    "href": "content/08-content.html#why-worry-about-heterogeneity",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Why worry about heterogeneity?",
    "text": "Why worry about heterogeneity?\nRelying on the average treatment effect (ATE) is a bit like handing out size-nine shoes to an entire student body: on average they might fit, but watch the tall students hobble and the small ones trip. In the same way, a one-hour boost in weekly community socialising could send some students’ sense of belonging soaring while leaving others cold—or even wishing they’d stayed home with the cat. Spotting that spread, measuring how big it really is, and deciding whether it is worth tailoring the ‘shoe size’ are the three practical goals of HTE analysis."
  },
  {
    "objectID": "content/08-content.html#start-with-the-average-treatment-effect-ate",
    "href": "content/08-content.html#start-with-the-average-treatment-effect-ate",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "1 Start with the average treatment effect (ATE)",
    "text": "1 Start with the average treatment effect (ATE)\nWe begin with the most straightforward (and secretly impossible) counterfactual: *run two parallel universes—one where everyone gets the treatment, another where no-one does—and compare the final scores. The resulting difference is the average treatment effect:\n\n\\text{ATE}=E\\!\\bigl[Y(1)-Y(0)\\bigr].\n\nThis gives us the average response – the shoe size…"
  },
  {
    "objectID": "content/08-content.html#do-effects-differ-across-people",
    "href": "content/08-content.html#do-effects-differ-across-people",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "2 Do effects differ across people?",
    "text": "2 Do effects differ across people?\nVariation is captured by the conditional average treatment effect (CATE),\n\n\\tau(x)=E\\!\\bigl[Y(1)-Y(0)\\mid X=x\\bigr],\n\nwhere X gathers pre-treatment covariates – age, baseline wellbeing, personality, etc… Normally these will be our baseline confounders.\nIf \\tau(x) turns out to be flat, there is no heterogeneity worth targeting.\nPeople differ in countless, overlapping ways—think of age, baseline wellbeing, personality traits, study habits, and more. A linear interaction model tests whether the treatment works differently along one straight dimension, such as gender, by fitting a straight line. But real‐world data often twist and turn. If the true relationship bends like a garden hose, a straight line will miss the curve. Causal forests fix this by letting the data place splits wherever the shape changes, so they can follow any bends that appear (Wager and Athey 2018). Straight-line models are fine for simple patterns, but causal forests can trace the curves that simple lines overlook.\n\n3. From straight lines to trees\nTraditional ‘parametric’ models (like simple regression) guess a single functional shape – often a straight line – before seeing the data. A non-parametric model, by contrast, lets the data decide the shape. A regression tree is the simplest non-parametric learner we will use.\n\nRegression tree\n\nIdea: split the covariate space by asking yes/no questions— ‘Age ≤ 20?’, ‘Baseline wellbeing &gt; 0.3?’ — until each terminal leaf is fairly homogeneous. Inside a leaf the predicted outcome is just the sample mean, so the tree builds a piece-wise constant surface instead of a global line.\nAnalogy: think of tiling a garden with stepping-stones: each stone is flat, but taken together they follow the ground’s contours.\n\nRegression forest\nA single tree is quick and interpretable but unstable: small changes in the data can move the splits and shift predictions. A random forest grows many trees on bootstrap samples and averages their outputs. Averaging cancels much of the noise (Breiman 2001).\nCausal Forests\nTo estimate treatment effects rather than outcomes, each tree plays a two-step ‘honest’ game (Wager and Athey 2018):\n\nuse one half of its sample to choose splits that separate treated from control units;\n\nuse the other half to compute treatment-control differences within every leaf.\n\nFor a new individual with covariates x_i each tree supplies a noisy leaf-level effect; the forest reports the average, written\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. https://doi.org/10.1023/A:1010933404324.\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–42. https://doi.org/10.1080/01621459.2017.1319839.\n\n  \\widehat{\\tau}(x)=E[Y(1)-Y(0)\\mid X=x].\n\nBecause the noisy estimates point in many directions, their average is markedly less variable – the wisdom of trees is a wisdom of crowds."
  },
  {
    "objectID": "content/08-content.html#estimate-person-specific-effects-with-a-causal-forest",
    "href": "content/08-content.html#estimate-person-specific-effects-with-a-causal-forest",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "3 Estimate person-specific effects with a causal forest",
    "text": "3 Estimate person-specific effects with a causal forest\nA causal forest is an ensemble of “honest” trees. Each tree uses one subset of the data to decide where to split and a different subset to estimate treatment effects within the leaves. For student i the forest returns the personalised estimate\n\n\\widehat{\\tau}(x_i).\n\nBecause every tree that predicts for i avoided using i to determine its splits, the resulting estimates have low bias and valid standard errors without extensive hyper-parameter tuning."
  },
  {
    "objectID": "content/08-content.html#built-in-protection-against-over-fitting",
    "href": "content/08-content.html#built-in-protection-against-over-fitting",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "4 Built-in protection against over-fitting",
    "text": "4 Built-in protection against over-fitting\nHonesty already separates model selection from estimation, but the forest adds a second safeguard: out-of-bag (OOB) prediction. Each \\widehat{\\tau}(x_i) is averaged only over trees that never used i in their split phase. Together, honesty and OOB prediction deliver reliable uncertainty estimates even in high-dimensional settings."
  },
  {
    "objectID": "content/08-content.html#handling-missing-data",
    "href": "content/08-content.html#handling-missing-data",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "5 Handling missing data",
    "text": "5 Handling missing data\nThe grf package adopts Missing Incorporated in Attributes (MIA) splitting. ‘Missing’ can itself become a branch, so cases are neither discarded nor randomly imputed. This pragmatic approach keeps all observations in play while preserving the forest’s interpretability."
  },
  {
    "objectID": "content/08-content.html#is-the-heterogeneity-actionable-rate-statistics",
    "href": "content/08-content.html#is-the-heterogeneity-actionable-rate-statistics",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "6 Is the heterogeneity actionable? — RATE statistics",
    "text": "6 Is the heterogeneity actionable? — RATE statistics\nOnce we have a personalised score \\widehat{\\tau}(x) for every unit, the practical question is whether targeting high scorers delivers a benefit large enough to justify the extra effort. The tool of choice is the Targeting-Operator Characteristic (TOC) curve:\n\nG(q)=\\frac{1}{n}\\sum_{i=1}^{\\lfloor qn\\rfloor}\\widehat{\\tau}_{(i)}, \\qquad 0\\le q\\le1,\n\nwhere \\widehat{\\tau}_{(1)}\\ge\\widehat{\\tau}_{(2)}\\ge\\cdots are the estimated effects sorted from largest to smallest. The horizontal axis q is the fraction of the population we would treat; the vertical axis G(q) is the cumulative gain we expect from treating that top slice.\nTwo integrals of the TOC curve summarise how lucrative targeting could be:\n\nRATE AUTOC (Area Under the TOC) puts equal weight on every q. This answers: If benefits are concentrated among the very best prospects, how much can we harvest by cherry-picking them?\nRATE Qini applies heavier weight to the mid-range of q. This is the go-to metric when investigators face a fixed, moderate-sized budget—say, “we can afford to treat 40 % of individuals; will targeting help?” (Yadlowsky et al. 2021). We will evaluate the curve at treatment of 20% and 50% of the population.\n\n\nYadlowsky, Steve, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager. 2021. “Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects.” arXiv Preprint arXiv:2111.07966. https://doi.org/10.48550/arXiv.2111.07966.\nTo quantify the economic or policy value of heterogeneity, rank units by \\widehat{\\tau}(x) and draw a Targeting-Operator Characteristic (TOC) curve that plots cumulative gain against the fraction q of the population treated."
  },
  {
    "objectID": "content/08-content.html#valid-inference-through-a-traintest-split",
    "href": "content/08-content.html#valid-inference-through-a-traintest-split",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "7 Valid inference through a train/test split",
    "text": "7 Valid inference through a train/test split\nAlthough OOB predictions are “out-of-sample” for individual trees, the full forest still reuses information. A simple remedy is to cut the data in half: train the forest on one fold and test RATE/Qini on the other. This explicit split blocks optimistic bias and yields honest p-values (Tibshirani et al. 2024).\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024. Grf: Generalized Random Forests. https://github.com/grf-labs/grf.\n\n\n\n\n\n\n\nFigure 1: RATE AUTOC: Hours Socialising → Sense of Meaning\n\n\n\n\nFigure 1 depicts a typical RATE AUTOC curve. A steep initial rise indicates that a small, correctly targeted programme could deliver large gains."
  },
  {
    "objectID": "content/08-content.html#visualising-policy-value-the-qini-curve",
    "href": "content/08-content.html#visualising-policy-value-the-qini-curve",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "8 Visualising policy value: the Qini curve",
    "text": "8 Visualising policy value: the Qini curve\nA Qini curve displays cumulative benefit on the vertical axis and treatment coverage on the horizontal. As with the AUTOC curve we are using a held-out test fold to validate the reponse curve.\n\n\n\n\n\n\n\nFigure 2: Qini Curve: Hours Socialising → Social Belonging\n\n\n\n\nFigure 2: we find that focussing on the top 20 % of individuals nets a gain of 0.08 units (95 % CI 0.04–0.12). Widening the net to 50 % bumps the haul to 0.13 units (95 % CI 0.07–0.19). After that the curve flattens – once you’ve treated everyone who offers a decent return, there are no more ‘big fish’ left to catch."
  },
  {
    "objectID": "content/08-content.html#from-black-box-to-simple-rules-policy-trees",
    "href": "content/08-content.html#from-black-box-to-simple-rules-policy-trees",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "9 From black box to simple rules: policy trees",
    "text": "9 From black box to simple rules: policy trees\nThe policytree algorithm serves that need by pruning the forest’s many (x) estimates into a decision tree whose depth you choose; the algorithm picks splits that maximise predicted benefit at that depth (Sverdrup et al. 2024). In this course we stop at depth = 2 for a practical balance:\n\nSverdrup, Erik, Ayush Kanodia, Zhengyuan Zhou, Susan Athey, and Stefan Wager. 2024. Policytree: Policy Learning via Doubly Robust Empirical Welfare Maximization over Trees. https://CRAN.R-project.org/package=policytree.\n\nevery rule uses at most two yes/no questions, so even politicians can follow the logic;\neach leaf still holds enough cases to give a stable effect estimate;\ndeeper trees add substantial computational complexity.\n\n\n\n\n\n\n\n\nFigure 3: Decision tree for Social Belonging\n\n\n\n\nFindings for Effect of Hour Socialising on Social Belonging:\nParticipants are first split by Self Esteem at -0.925 (original scale: 3.958). For those with Self Esteem &lt;= this threshold, the next split is by Neuroticism at 0.642 (original scale: 4.228). Within that subgroup, individuals with Neuroticism &lt;= the threshold are recommended control, while those with Neuroticism &gt; the threshold are recommended treated.\nFor participants with Self Esteem &gt; -0.925 (original scale: 3.958), the second split is by Social Belonging at 0.776 (original scale: 5.972). In this subgroup, individuals with Social Belonging &lt;= the threshold are recommended treated, while those with Social Belonging &gt; the threshold are recommended control.\nPolicy Rule\n\nIf self-esteem ≤ −0.93 and neuroticism ≤ 0.64, do not recommend extra socialising; otherwise, recommend it unless current belonging &gt; 0.78.*\n\n\n\n\n\n\n\nFigure 4: Predicted treatment assignment (predictions out of training sample)"
  },
  {
    "objectID": "content/08-content.html#ethical-and-practical-considerations",
    "href": "content/08-content.html#ethical-and-practical-considerations",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "10 Ethical and practical considerations",
    "text": "10 Ethical and practical considerations\nStatistical optimality rarely aligns perfectly with social acceptability. Cost, fairness, and transparency must all be weighed before implementing a targeted intervention."
  },
  {
    "objectID": "content/08-content.html#summary-and-next-steps",
    "href": "content/08-content.html#summary-and-next-steps",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Summary and next steps",
    "text": "Summary and next steps\nOur workflow answers three questions in sequence:\n\nIs there substantial heterogeneity? Reject H_0{:}\\tau(x) constant if RATE AUTOC or RATE Qini is positive and significant.\n\nDoes targeting pay at realistic budgets? Inspect the slope of the Qini curve around plausible coverage levels.\nCan we express the targeting rule in a few defensible steps? fit and validate a shallow policy tree.\n\nIn the lab section you will reproduce each stage on a simulated dataset."
  },
  {
    "objectID": "content/08-content.html#in-sum-a-regression-tree-chops-the-data-into-locally-flat-chunks-a-regression-forest-averages-many-such-trees-to-smooth-away-chance-idiosyncrasies-a-causal-forest-adds-honesty-so-that-its-averaged-differences-though-never-directly-observable-for-any-one-person-give-our-best-data-driven-forecast-of-individual-treatment-effects.",
    "href": "content/08-content.html#in-sum-a-regression-tree-chops-the-data-into-locally-flat-chunks-a-regression-forest-averages-many-such-trees-to-smooth-away-chance-idiosyncrasies-a-causal-forest-adds-honesty-so-that-its-averaged-differences-though-never-directly-observable-for-any-one-person-give-our-best-data-driven-forecast-of-individual-treatment-effects.",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "In sum, a regression tree chops the data into locally flat chunks; a regression forest averages many such trees to smooth away chance idiosyncrasies; a causal forest adds honesty so that its averaged differences, though never directly observable for any one person, give our best data-driven forecast of individual treatment effects.",
    "text": "In sum, a regression tree chops the data into locally flat chunks; a regression forest averages many such trees to smooth away chance idiosyncrasies; a causal forest adds honesty so that its averaged differences, though never directly observable for any one person, give our best data-driven forecast of individual treatment effects.\n\n4 Built-in protection against over-fitting\nHonesty already separates model selection from estimation, but the forest adds a second safeguard: out-of-bag (OOB) prediction. Each \\widehat{\\tau}(x_i) is averaged only over trees that never used i in their split phase. Together, honesty and OOB prediction deliver reliable uncertainty estimates even in high-dimensional settings.\n\n\n\n5 Handling missing data\nThe grf package adopts Missing Incorporated in Attributes (MIA) splitting. ‘Missing’ can itself become a branch, so cases are neither discarded nor randomly imputed. This pragmatic approach keeps all observations in play while preserving the forest’s interpretability.\n\n\n\n6 Is the heterogeneity actionable? — RATE statistics\nOnce we have a personalised score \\widehat{\\tau}(x) for every unit, the practical question is whether targeting high scorers delivers a benefit large enough to justify the extra effort. The tool of choice is the Targeting-Operator Characteristic (TOC) curve:\n\nG(q)=\\frac{1}{n}\\sum_{i=1}^{\\lfloor qn\\rfloor}\\widehat{\\tau}_{(i)}, \\qquad 0\\le q\\le1,\n\nwhere \\widehat{\\tau}_{(1)}\\ge\\widehat{\\tau}_{(2)}\\ge\\cdots are the estimated effects sorted from largest to smallest. The horizontal axis q is the fraction of the population we would treat; the vertical axis G(q) is the cumulative gain we expect from treating that top slice.\nTwo integrals of the TOC curve summarise how lucrative targeting could be:\n\nRATE AUTOC (Area Under the TOC) puts equal weight on every q. This answers: If benefits are concentrated among the very best prospects, how much can we harvest by cherry-picking them?\nRATE Qini applies heavier weight to the mid-range of q. This is the go-to metric when investigators face a fixed, moderate-sized budget—say, “we can afford to treat 40 % of individuals; will targeting help?” (Yadlowsky et al. 2021). We will evaluate the curve at treatment of 20% and 50% of the population.\n\n\nYadlowsky, Steve, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager. 2021. “Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects.” arXiv Preprint arXiv:2111.07966. https://doi.org/10.48550/arXiv.2111.07966.\nTo quantify the economic or policy value of heterogeneity, rank units by \\widehat{\\tau}(x) and draw a Targeting-Operator Characteristic (TOC) curve that plots cumulative gain against the fraction q of the population treated.\n\n\n\n7 RATE AUTOC EXAMPLE\nAlthough OOB predictions are ‘out-of-sample’ for individual trees, the full forest still reuses information. A simple remedy is to cut the data in half: train the forest on one fold and test RATE/Qini on the other. This explicit split blocks optimistic bias and yields honest test statistics (p-values) (Tibshirani et al. 2024).\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024. Grf: Generalized Random Forests. https://github.com/grf-labs/grf.\n\n\n\n\n\n\n\nFigure 1: RATE AUTOC: Hours Socialising → Sense of Meaning\n\n\n\n\nFigure 1 depicts a typical RATE AUTOC curve with sample splitting. A steep initial rise indicates that a small, correctly targeted programme could deliver large gains. Note that the curve begins dipping below zero past about 30%. At that point we might be doing worse than the ATE by targeting the CATE.\n\n\n\n8 Visualising policy value: the Qini curve\nA Qini curve displays cumulative benefit on the vertical axis and treatment coverage on the horizontal. As with the AUTOC curve we are using a held-out test fold to validate the reponse curve.\n\n\n\n\n\n\n\nFigure 2: Qini Curve: Hours Socialising → Social Belonging\n\n\n\n\nFigure 2: we find that focussing on the top 20 % of individuals nets a gain of 0.08 units (95 % CI 0.04–0.12). Widening the net to 50 % bumps the haul to 0.13 units (95 % CI 0.07–0.19). After that the curve flattens – once you’ve treated everyone who offers a decent return, there are no more ‘big fish’ left to catch.\n\n\n\n9 From ‘a black box’ to simple rules: policy trees\nThe causal forest hands us a personalised CATE for every individual, mapping a high-dimensional covariate vector X to a number \\widehat{\\tau}(X). Helpful as that forecast is, it stops short of telling us what to do: the function itself is too tangled — thousands of overlapping splits – to translate directly into a policy.\nThe policytree algorithm bridges that gap by collapsing the forest’s many \\widehat{\\tau}(X) values into a single, shallow decision tree whose depth you choose; each split is chosen to maximise expected benefit (Sverdrup et al. 2024). In this course we cap the depth at 2 for a practical balance:\n\nSverdrup, Erik, Ayush Kanodia, Zhengyuan Zhou, Susan Athey, and Stefan Wager. 2024. Policytree: Policy Learning via Doubly Robust Empirical Welfare Maximization over Trees. https://CRAN.R-project.org/package=policytree.\n\nat most two yes/no questions per rule, so the logic fits on a slide you can present to policy-makers;\neach leaf still contains enough observations to yield a stable effect estimate;\n\ndeeper trees increase complexity faster than they improve payoff.\n\n\n\n\n\n\n\n\nFigure 3: Decision tree for Social Belonging\n\n\n\n\nPolicy Tree Findings for Effect of Hour Socialising on Social Belonging:\nParticipants are first split by Self Esteem at -0.925 (original scale: 3.958). For those with Self Esteem &lt;= this threshold, the next split is by Neuroticism at 0.642 (original scale: 4.228). Within that subgroup, individuals with Neuroticism &lt;= the threshold are recommended control, while those with Neuroticism &gt; the threshold are recommended treated.\nFor participants with Self Esteem &gt; -0.925 (original scale: 3.958), the second split is by Social Belonging at 0.776 (original scale: 5.972). In this subgroup, individuals with Social Belonging &lt;= the threshold are recommended treated, while those with Social Belonging &gt; the threshold are recommended control.\nPolicy Rule\n\nIf self-esteem ≤ −0.93 and neuroticism ≤ 0.64, do not recommend extra socialising; otherwise, recommend it unless current belonging &gt; 0.78.*\n\n\n\n\n\n\n\nFigure 4: Predicted treatment assignment (predictions out of training sample)\n\n\n\n\n\n\n10 Ethical and practical considerations\nStatistical optimality rarely lines up with social acceptability. A rule that maximises expected health gains might still be unaffordable for a public agency, unfair to a protected group, or opaque to those asked to trust it. Typically these trade-offs lie beyond the statistician’s remit (see the caveats in Lecture 6).\nYet the very same CATE machinery that powers targeting also helps science move past a one-size-fits-all mindset. By mapping treatment effects across a high-dimensional covariate space, we can test whether our favourite categories – gender, age group, clinical severity – actually capture the differences that matter. Sometimes they do; often they don’t, revealing that nature is not carved at the joints of our folk classifications. Discovering where the forest sees meaningful splits can generate fresh psychological hypotheses about who responds, why, and under what circumstances, even when no policy decision is on the table.\n\n\n\n\n\n\n\nSummary/next steps\nOur workflow answers three questions in sequence:\n\nIs there substantial heterogeneity? Reject H_0{:}\\tau(x) constant if RATE AUTOC or RATE Qini is positive and statistically reliable\n\nDoes targeting pay at realistic budgets? Inspect the slope of the Qini curve around plausible coverage levels.\nCan we express the targeting rule in a few defensible steps? fit and validate a shallow policy tree.\n\nIn the lab section you will reproduce each stage on a simulated dataset."
  },
  {
    "objectID": "content/08-content.html#rate-autoc-example",
    "href": "content/08-content.html#rate-autoc-example",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "7 RATE AUTOC EXAMPLE",
    "text": "7 RATE AUTOC EXAMPLE\nAlthough OOB predictions are ‘out-of-sample’ for individual trees, the full forest still reuses information. A simple remedy is to cut the data in half: train the forest on one fold and test RATE/Qini on the other. This explicit split blocks optimistic bias and yields honest test statistics (p-values) (Tibshirani et al. 2024).\n\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024. Grf: Generalized Random Forests. https://github.com/grf-labs/grf.\n\n\n\n\n\n\n\nFigure 1: RATE AUTOC: Hours Socialising → Sense of Meaning\n\n\n\n\nFigure 1 depicts a typical RATE AUTOC curve with sample splitting. A steep initial rise indicates that a small, correctly targeted programme could deliver large gains. Note that the curve begins dipping below zero past about 30%. At that point we might be doing worse than the ATE by targeting the CATE."
  },
  {
    "objectID": "content/08-content.html#reading-fig-qini-example-cherry-picking-the-top-20-of-students-nets-a-gain-of-0.08-units-95-ci-0.040.12.-widening-the-net-to-50-bumps-the-haul-to-0.13-units-95-ci-0.070.19.-after-that-the-curve-flattens-once-youve-treated-everyone-who-offers-a-decent-return-there-are-no-more-big-fish-left-to-catch.",
    "href": "content/08-content.html#reading-fig-qini-example-cherry-picking-the-top-20-of-students-nets-a-gain-of-0.08-units-95-ci-0.040.12.-widening-the-net-to-50-bumps-the-haul-to-0.13-units-95-ci-0.070.19.-after-that-the-curve-flattens-once-youve-treated-everyone-who-offers-a-decent-return-there-are-no-more-big-fish-left-to-catch.",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Reading Figure 2: cherry-picking the top 20 % of students nets a gain of 0.08 units (95 % CI 0.04–0.12). Widening the net to 50 % bumps the haul to 0.13 units (95 % CI 0.07–0.19). After that the curve flattens – once you’ve treated everyone who offers a decent return, there are no more big fish left to catch.",
    "text": "Reading Figure 2: cherry-picking the top 20 % of students nets a gain of 0.08 units (95 % CI 0.04–0.12). Widening the net to 50 % bumps the haul to 0.13 units (95 % CI 0.07–0.19). After that the curve flattens – once you’ve treated everyone who offers a decent return, there are no more big fish left to catch."
  },
  {
    "objectID": "content/08-content.html#fig-qini-example-we-find-that-focussing-on-the-top-20-of-individuals-nets-a-gain-of-0.08-units-95-ci-0.040.12.-widening-the-net-to-50-bumps-the-haul-to-0.13-units-95-ci-0.070.19.-after-that-the-curve-flattens-once-youve-treated-everyone-who-offers-a-decent-return-there-are-no-more-big-fish-left-to-catch.",
    "href": "content/08-content.html#fig-qini-example-we-find-that-focussing-on-the-top-20-of-individuals-nets-a-gain-of-0.08-units-95-ci-0.040.12.-widening-the-net-to-50-bumps-the-haul-to-0.13-units-95-ci-0.070.19.-after-that-the-curve-flattens-once-youve-treated-everyone-who-offers-a-decent-return-there-are-no-more-big-fish-left-to-catch.",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "Figure 2: we find that focussing on the top 20 % of individuals nets a gain of 0.08 units (95 % CI 0.04–0.12). Widening the net to 50 % bumps the haul to 0.13 units (95 % CI 0.07–0.19). After that the curve flattens – once you’ve treated everyone who offers a decent return, there are no more big fish left to catch.",
    "text": "Figure 2: we find that focussing on the top 20 % of individuals nets a gain of 0.08 units (95 % CI 0.04–0.12). Widening the net to 50 % bumps the haul to 0.13 units (95 % CI 0.07–0.19). After that the curve flattens – once you’ve treated everyone who offers a decent return, there are no more big fish left to catch."
  },
  {
    "objectID": "content/08-content.html#homework-prepare-a-fresh-set-of-analysis-scripts-with-a-different-exposure",
    "href": "content/08-content.html#homework-prepare-a-fresh-set-of-analysis-scripts-with-a-different-exposure",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "HOMEWORK: Prepare A Fresh Set of Analysis Scripts With A Different Exposure",
    "text": "HOMEWORK: Prepare A Fresh Set of Analysis Scripts With A Different Exposure\n\nE.g. Ask: what are the effects of a shift in religious service religion_church on multi-dimensional well-being.\nConsider what variables you need for confounding control at baseline.\nThink about how to make the exposure variable binary.\nYou may consider different outcome(s) as well as a different exposure.\n\n\nPackages\n\nreport::cite_packages()\n\n  - Bulbulia J (2024). _boilerplate_. doi:10.5281/zenodo.13370825 &lt;https://doi.org/10.5281/zenodo.13370825&gt;, R package version 1.0.4, &lt;https://go-bayes.github.io/biolerplate/&gt;.\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 1.0.21 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont &lt;https://doi.org/10.32614/CRAN.package.extrafont&gt;, R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Müller K (2020). _here: A Simpler Way to Find Your Files_. doi:10.32614/CRAN.package.here &lt;https://doi.org/10.32614/CRAN.package.here&gt;, R package version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. doi:10.32614/CRAN.package.tibble &lt;https://doi.org/10.32614/CRAN.package.tibble&gt;, R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. doi:10.32614/CRAN.package.patchwork &lt;https://doi.org/10.32614/CRAN.package.patchwork&gt;, R package version 1.3.0, &lt;https://CRAN.R-project.org/package=patchwork&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. doi:10.32614/CRAN.package.forcats &lt;https://doi.org/10.32614/CRAN.package.forcats&gt;, R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. doi:10.32614/CRAN.package.stringr &lt;https://doi.org/10.32614/CRAN.package.stringr&gt;, R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. doi:10.32614/CRAN.package.dplyr &lt;https://doi.org/10.32614/CRAN.package.dplyr&gt;, R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. doi:10.32614/CRAN.package.purrr &lt;https://doi.org/10.32614/CRAN.package.purrr&gt;, R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. doi:10.32614/CRAN.package.readr &lt;https://doi.org/10.32614/CRAN.package.readr&gt;, R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. doi:10.32614/CRAN.package.tidyr &lt;https://doi.org/10.32614/CRAN.package.tidyr&gt;, R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;.\n  - Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. doi:10.32614/CRAN.package.kableExtra &lt;https://doi.org/10.32614/CRAN.package.kableExtra&gt;, R package version 1.4.0, &lt;https://CRAN.R-project.org/package=kableExtra&gt;."
  },
  {
    "objectID": "content/08-content.html#from-a-black-box-to-simple-rules-policy-trees",
    "href": "content/08-content.html#from-a-black-box-to-simple-rules-policy-trees",
    "title": "Causal Inference: Estimation of ATE and CATE",
    "section": "9 From ‘a black box’ to simple rules: policy trees",
    "text": "9 From ‘a black box’ to simple rules: policy trees\nThe causal forest hands us a personalised CATE for every individual, mapping a high-dimensional covariate vector X to a number \\widehat{\\tau}(X). Helpful as that forecast is, it stops short of telling us what to do: the function itself is too tangled — thousands of overlapping splits – to translate directly into a policy.\nThe policytree algorithm bridges that gap by collapsing the forest’s many \\widehat{\\tau}(X) values into a single, shallow decision tree whose depth you choose; each split is chosen to maximise expected benefit (Sverdrup et al. 2024). In this course we cap the depth at 2 for a practical balance:\n\nSverdrup, Erik, Ayush Kanodia, Zhengyuan Zhou, Susan Athey, and Stefan Wager. 2024. Policytree: Policy Learning via Doubly Robust Empirical Welfare Maximization over Trees. https://CRAN.R-project.org/package=policytree.\n\nat most two yes/no questions per rule, so the logic fits on a slide you can present to policy-makers;\neach leaf still contains enough observations to yield a stable effect estimate;\n\ndeeper trees increase complexity faster than they improve payoff.\n\n\n\n\n\n\n\n\nFigure 3: Decision tree for Social Belonging\n\n\n\n\nPolicy Tree Findings for Effect of Hour Socialising on Social Belonging:\nParticipants are first split by Self Esteem at -0.925 (original scale: 3.958). For those with Self Esteem &lt;= this threshold, the next split is by Neuroticism at 0.642 (original scale: 4.228). Within that subgroup, individuals with Neuroticism &lt;= the threshold are recommended control, while those with Neuroticism &gt; the threshold are recommended treated.\nFor participants with Self Esteem &gt; -0.925 (original scale: 3.958), the second split is by Social Belonging at 0.776 (original scale: 5.972). In this subgroup, individuals with Social Belonging &lt;= the threshold are recommended treated, while those with Social Belonging &gt; the threshold are recommended control.\nPolicy Rule\n\nIf self-esteem ≤ −0.93 and neuroticism ≤ 0.64, do not recommend extra socialising; otherwise, recommend it unless current belonging &gt; 0.78.*\n\n\n\n\n\n\n\nFigure 4: Predicted treatment assignment (predictions out of training sample)"
  },
  {
    "objectID": "content/08-content.html#heterogeneous-treatment-effect-analysis-with-causal-forests",
    "href": "content/08-content.html#heterogeneous-treatment-effect-analysis-with-causal-forests",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "Heterogeneous-Treatment-Effect Analysis with causal forests",
    "text": "Heterogeneous-Treatment-Effect Analysis with causal forests\n\nWhy worry about heterogeneity?\nRelying on the average treatment effect (ATE) is a bit like handing out size-nine shoes to an entire student body: on average they might fit, but watch the tall students hobble and the small ones trip.\nToday we will focus on the causal question: “What would be the effects on multi-dimensional well-being if everyone spent at least one hour a week socialising with their community?”\nNote that a one-hour boost in weekly community socialising could send some students’ sense of belonging soaring while leaving others desolated. Spotting that spread, measuring how big it really is, and deciding whether it is worth tailoring an exposure to individual ‘shoe sizes’ are the three practical goals of Heterogeneous Treatment Effects analysis.\n\n\n\n1 Start with estimating the average treatment effect (ATE)\nAssume the Three Fundamental Assumptions of Causal Inference here are met. Suppose we wish to estimate the average treatment effect for socialising with one’s community.\nWe begin with the most straightforward (and secretly impossible) counterfactual: *run two parallel universes—one where everyone gets the treatment, another where no-one does—and compare the final scores. The resulting difference is the average treatment effect:\n\n\\text{ATE}=E\\!\\bigl[Y(1)-Y(0)\\bigr].\n\nThis gives us the average response – the shoe size… You’ve seen this before.\n\n\n\n2 Do effects differ across people?\nVariation is captured by the conditional average treatment effect (CATE),\n\n\\tau(x)=E\\!\\bigl[Y(1)-Y(0)\\mid X=x\\bigr],\n\nwhere X gathers pre-treatment covariates – age, baseline wellbeing, personality, whatever we have measured and included in our model. Normally these will be our baseline confounders.\nIf \\tau(x) turns out to be flat, we say there is no evidence for heterogeneity worth targeting.\nPeople differ in countless, overlapping ways. Think of age, baseline wellbeing, personality traits, study habits, and more.\nA linear interaction model tests whether the treatment works differently along one straight dimension, such as gender, by fitting a straight line.\nBut real‐world data often twist and turn. If the true relationship bends like a garden hose, a straight line will miss the curve.\nRegression forests fix this by letting the data place splits wherever the shape changes, so they can follow any bends that appear (Wager and Athey 2018).\nStraight-line models are fine for simple patterns, but regression forests can trace the curves that simple lines overlook.\nCausal forests are based on regression forests, where the splitting attempts to maximise differences in causal effect estimates. What this means will soon be clear.\n\n\n3. From straight lines to trees\nTraditional ‘parametric’ models (like simple regression) guess a single functional shape – often a straight line – before seeing the data. A non-parametric model, by contrast, lets the data decide the shape. A regression tree is the simplest non-parametric learner we will use.\n\nRegression tree\n\nIdea: split the covariate space by asking yes/no questions— ‘Age ≤ 20?’, ‘Baseline wellbeing &gt; 0.3?’ — until each terminal leaf is fairly homogeneous. Inside a leaf the predicted outcome is just the sample mean, so the tree builds a piece-wise constant surface instead of a global line.\nAnalogy: think of tiling a garden with stepping-stones: each stone is flat, but taken together they follow the ground’s contours.\n\nRegression forest\nA single tree is quick and interpretable but unstable: small changes in the data can move the splits and shift predictions. A random forest grows many trees on bootstrap samples and averages their outputs. Averaging cancels much of the noise (Breiman 2001).\nCausal Forests\nTo estimate treatment effects rather than outcomes, each tree plays a two-step ‘honest’ game (Wager and Athey 2018):\n\nuse one half of its sample to choose splits that separate treated from control units;\n\nuse the other half to compute treatment-control differences within every leaf.\n\nFor a new individual with covariates x_i each tree supplies a noisy leaf-level effect; the forest reports the average, written\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. https://doi.org/10.1023/A:1010933404324.\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–42. https://doi.org/10.1080/01621459.2017.1319839.\n\n  \\widehat{\\tau}(x)=E[Y(1)-Y(0)\\mid X=x].\n\nBecause the noisy estimates point in many directions, their average is markedly less variable – the wisdom of trees is a wisdom of crowds.\nStraight‑line models suit simple patterns; regression forests flex to any bends; causal forests add a third dimension – variation in treatment responses. We’ll see this in action/\nFor more about causal forests see (~18mins in…)\n\n\n\n\n\n4 Building Honest Trees: Avoiding Over-Fitting\nSample splitting meanings partitioning your data into training and testing sets. This avoids overfittign the model to observations (remember we seek to estimate parameters for an entire population under two different exposures, at most, only one of which is observed on any individual.) Sample splitting is a feature of estimation in cauasal forests – we separate model selection from estimation. Moreover, the forest adds a second safeguard: out-of-bag (OOB) prediction. Each \\widehat{\\tau}(x_i) is averaged only over trees that never used i in their split phase. Together, honesty and OOB prediction deliver reliable uncertainty estimates even in high-dimensional settings (i.e. settings with many covariates.)\n\n\n\n5 Handling missing data\nThe grf package adopts Missing Incorporated in Attributes (MIA) splitting. ‘Missing’ can itself become a branch, so cases are neither discarded nor randomly imputed. This pragmatic approach keeps all observations in play while preserving the forest’s interpretability.\n\n\n\n6 Is the heterogeneity actionable? — RATE statistics\nOnce we have a personalised score \\widehat{\\tau}(x) for every unit, the practical question is whether targeting high scorers delivers a benefit large enough to justify the extra effort. The tool of choice is the Targeting-Operator Characteristic (TOC) curve:\n\nG(q)=\\frac{1}{n}\\sum_{i=1}^{\\lfloor qn\\rfloor}\\widehat{\\tau}_{(i)}, \\qquad 0\\le q\\le1,\n\nwhere \\widehat{\\tau}_{(1)}\\ge\\widehat{\\tau}_{(2)}\\ge\\cdots are the estimated effects sorted from largest to smallest. The horizontal axis q is the fraction of the population we would treat; the vertical axis G(q) is the cumulative gain we expect from treating that top slice.\nTwo integrals of the TOC curve summarise how lucrative targeting could be:\n\nRATE AUTOC (Area Under the TOC) puts equal weight on every q. This answers: If benefits are concentrated among the very best prospects, how much can we harvest by cherry-picking them?\nRATE Qini applies heavier weight to the mid-range of q. This is the go-to metric when investigators face a fixed, moderate-sized budget—say, “we can afford to treat 40 % of individuals; will targeting help?” (Yadlowsky et al. 2021). We will evaluate the curve at treatment of 20% and 50% of the population.\n\n\nYadlowsky, Steve, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager. 2021. “Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects.” arXiv Preprint arXiv:2111.07966. https://doi.org/10.48550/arXiv.2111.07966.\nTo quantify the economic or policy value of heterogeneity, rank units by \\widehat{\\tau}(x) and draw a Targeting-Operator Characteristic (TOC) curve that plots cumulative gain against the fraction q of the population treated.\n\n\n\n7 RATE AUTOC EXAMPLE\nAlthough OOB predictions are ‘out-of-sample’ for individual trees, the full forest still reuses information. A simple remedy when estimating the RATE AUTOC and Qini is to split the data, training the forest on one fold and testing RATE/Qini on the other. Again, this explicit splitting blocks optimistic bias and yields honest test statistics (such as confidence intervals) (Tibshirani et al. 2024).\n\n\n\n\n\n\n\nFigure 1: RATE AUTOC: Hours Socialising → Sense of Meaning\n\n\n\n\nFigure 1 depicts a typical RATE AUTOC curve with sample splitting. A steep initial rise indicates that a small, correctly targeted programme could deliver large gains. Note that the curve begins dipping below zero past about 30% of the sample. At that point we might be doing worse than the ATE by targeting the CATE – at least for some.\nRemember – figuring out who will benefit from a treatment is a difficult statistical problem (Tibshirani et al. 2024).\nTibshirani, Julie, Susan Athey, Erik Sverdrup, and Stefan Wager. 2024. Grf: Generalized Random Forests. https://github.com/grf-labs/grf.\n\n\n\n\n8 Visualising policy value: the Qini curve\nA Qini curve displays cumulative benefit on the vertical axis and treatment coverage (% of the population treated) on the horizontal. As with the AUTOC curve we are using a held-out test fold to validate the response curve.\n\n\n\n\n\n\n\nFigure 2: Qini Curve: Hours Socialising → Social Belonging\n\n\n\n\nFigure 2: we find that focussing on the top 20 % of individuals nets a gain of 0.08 units (95 % CI 0.04–0.12). Widening the net to 50 % bumps the haul to 0.13 units (95 % CI 0.07–0.19). After that the curve flattens – once we’ve treated everyone who offers a decent return, there are no more ‘big fish’ left to catch.\n\n\n\n9 From ‘a black box’ to simple rules: policy trees\nThe causal forest hands us a personalised CATE for every individual, mapping a high-dimensional covariate vector X to a number \\widehat{\\tau}(X). Helpful as that forecast may be, it stops short of telling us what to do: the function itself is too tangled — thousands of overlapping splits – to translate directly into a policy.\nThe policytree algorithm bridges that gap by collapsing the forest’s many \\widehat{\\tau}(X) values into a single, shallow decision tree whose depth you choose; each split is chosen to maximise expected benefit (Sverdrup et al. 2024). In this course we cap the depth at two for a practical balance, specifially:\n\nSverdrup, Erik, Ayush Kanodia, Zhengyuan Zhou, Susan Athey, and Stefan Wager. 2024. Policytree: Policy Learning via Doubly Robust Empirical Welfare Maximization over Trees. https://CRAN.R-project.org/package=policytree.\n\nAt most three yes/no questions per rule, so the logic fits on a slide you can present to policy-makers\nEach leaf still contains enough observations to yield a stable effect estimate;\n\nDeeper trees increase computational complexity faster than they improve payoffs.\n\n\n\n\n\n\n\n\nFigure 3: Decision tree for Social Belonging\n\n\n\n\nPolicy Tree Findings for Effect of Hour Socialising on Social Belonging:\nParticipants are first split by Self Esteem at -0.925 (original scale: 3.958). For those with Self Esteem &lt;= this threshold, the next split is by Neuroticism at 0.642 (original scale: 4.228). Within that subgroup, individuals with Neuroticism &lt;= the threshold are recommended control, while those with Neuroticism &gt; the threshold are recommended treated.\nFor participants with Self Esteem &gt; -0.925 (original scale: 3.958), the second split is by Social Belonging at 0.776 (original scale: 5.972). In this subgroup, individuals with Social Belonging &lt;= the threshold are recommended treated, while those with Social Belonging &gt; the threshold are recommended control.\n\n\n\n\n\n\nFigure 4: Predicted treatment assignment (predictions out of training sample)\n\n\n\n\n\n\n10 Ethical and practical considerations\nThere is no guarantee that statistical optimality will line up with social optimality. A rule that maximises expected health gains might still be unaffordable for a public agency, unfair to a protected group, or opaque to those asked to trust it. We all have our notions of fairness, and we can’t be expected to ignore them. Moreover, the estimation of CATE is always senstive to which variables we include in our model (see the caveats in Lecture 6).\nSo, we should not consider CATE an absolute guide to practice. We should be cautious.\nYet the very same CATE machinery that powers targeting also helps science move past a one-size-fits-all mindset. By mapping treatment effects across a high-dimensional covariate space, we can test whether our favourite categories – gender, age group, clinical severity – actually capture the differences that matter. Sometimes they do; often they don’t, revealing that nature is not carved at the joints of our folk classifications. Discovering where the forest finds meaningful splits can generate fresh psychological hypotheses about who responds, why, and under what circumstances, even when no policy decision is on the table. Over the next several weeks, we shall return to this point with examples.\n\n\n\n\n\n\n\nSummary/next steps\nOur workflow answers three questions in sequence:\n\nIs there substantial heterogeneity? Reject H_0{:}\\tau(x) constant if RATE AUTOC or RATE Qini is positive and statistically reliable\n\nDoes targeting pay at realistic budgets? Inspect the slope of the Qini curve around plausible coverage levels.\nCan we express the targeting rule in a few defensible steps? fit and validate a shallow policy tree.\n\nIn the lab section you will reproduce each stage on a simulated dataset."
  },
  {
    "objectID": "content/08-content.html#homework-prepare-a-fresh-set-of-analysis-scripts-using-a-different-exposure",
    "href": "content/08-content.html#homework-prepare-a-fresh-set-of-analysis-scripts-using-a-different-exposure",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "HOMEWORK: Prepare a fresh set of analysis scripts using a different exposure",
    "text": "HOMEWORK: Prepare a fresh set of analysis scripts using a different exposure\n\nE.g. Ask: what are the effects of a shift in religious service religion_church on multi-dimensional well-being.\nConsider what variables you need for confounding control at baseline.\nThink about how to make the exposure variable binary.\nYou may consider different outcome(s) as well as a different exposure.\n\n\nPackages\n\nreport::cite_packages()\n\n  - Bulbulia J (2024). _boilerplate_. doi:10.5281/zenodo.13370825 &lt;https://doi.org/10.5281/zenodo.13370825&gt;, R package version 1.0.4, &lt;https://go-bayes.github.io/biolerplate/&gt;.\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 &lt;https://doi.org/10.5281/zenodo.10907724&gt;, R package version 1.0.37 Functions to obtain MARGinal Observational Treatment-effects from observational data., &lt;https://go-bayes.github.io/margot/&gt;.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont &lt;https://doi.org/10.32614/CRAN.package.extrafont&gt;, R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Müller K (2020). _here: A Simpler Way to Find Your Files_. doi:10.32614/CRAN.package.here &lt;https://doi.org/10.32614/CRAN.package.here&gt;, R package version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. doi:10.32614/CRAN.package.tibble &lt;https://doi.org/10.32614/CRAN.package.tibble&gt;, R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. doi:10.32614/CRAN.package.patchwork &lt;https://doi.org/10.32614/CRAN.package.patchwork&gt;, R package version 1.3.0, &lt;https://CRAN.R-project.org/package=patchwork&gt;.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. doi:10.32614/CRAN.package.forcats &lt;https://doi.org/10.32614/CRAN.package.forcats&gt;, R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. doi:10.32614/CRAN.package.stringr &lt;https://doi.org/10.32614/CRAN.package.stringr&gt;, R package version 1.5.1, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. doi:10.32614/CRAN.package.dplyr &lt;https://doi.org/10.32614/CRAN.package.dplyr&gt;, R package version 1.1.4, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. doi:10.32614/CRAN.package.purrr &lt;https://doi.org/10.32614/CRAN.package.purrr&gt;, R package version 1.0.4, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. doi:10.32614/CRAN.package.readr &lt;https://doi.org/10.32614/CRAN.package.readr&gt;, R package version 2.1.5, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. doi:10.32614/CRAN.package.tidyr &lt;https://doi.org/10.32614/CRAN.package.tidyr&gt;, R package version 1.3.1, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;.\n  - Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. doi:10.32614/CRAN.package.kableExtra &lt;https://doi.org/10.32614/CRAN.package.kableExtra&gt;, R package version 1.4.0, &lt;https://CRAN.R-project.org/package=kableExtra&gt;."
  },
  {
    "objectID": "content/09-content.html#laboratory",
    "href": "content/09-content.html#laboratory",
    "title": "Causal inference: a step by step guide",
    "section": "LABORATORY",
    "text": "LABORATORY\n\nScripts for a full analysis\n\nDownload full lab scripts 0\n\n\nDownload full lab scripts 1\n\n\nDownload full lab scripts 2\n\n\nDownload full lab scripts 3"
  },
  {
    "objectID": "content/09-content.html#code-review",
    "href": "content/09-content.html#code-review",
    "title": "Causal inference: a step by step guide",
    "section": "Code Review",
    "text": "Code Review\n\nScript 0 Setup"
  },
  {
    "objectID": "content/09-content.html#script-0-setup",
    "href": "content/09-content.html#script-0-setup",
    "title": "Causal inference: a step by step guide",
    "section": "Script 0: Setup",
    "text": "Script 0: Setup\nRun in full. No need to change anything.\nThis script prepares your data. Run it once – running it twice would be like turning the ignition off just to start your car again: unnecessary/silly.\n\n\nCode\n# for students: reproducibility is like following a recipe; each step ensures the same result\n# restart fresh session if needed\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# load packages ----------------------------------------------------------\n# install and load 'margot' from GitHub if missing\nif (!requireNamespace(\"margot\", quietly = TRUE)) {\n  message(\"installing 'margot' from GitHub\")\n  devtools::install_github(\"go-bayes/margot\", upgrade = \"never\")\n}\nlibrary(margot)\n\n# install and load other packages from CRAN if missing\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\nif (!requireNamespace(\"qs\", quietly = TRUE)) {\n  install.packages(\"qs\")\n}\nlibrary(qs)\n\nif (!requireNamespace(\"here\", quietly = TRUE)) {\n  install.packages(\"here\")\n}\nlibrary(here)\n\n# create data directory if it doesn't exist -----------------------------\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\n# define file paths ------------------------------------------------------\n# use here() to build paths relative to your project root\ndata_dir &lt;- here::here(\"data\")\n\n# download synthetic data ------------------------------------------------\n# specify the url for the data file\nurl &lt;- \"https://www.dropbox.com/scl/fi/ru0ecayju04ja8ky1mhel/df_nz_long.qs?rlkey=prpk9a5v4vcg1ilhkgf357dhd&dl=1\"\n\n# download to a temporary file for safety\ntmp_file &lt;- tempfile(fileext = \".qs\")\ndownload.file(url, tmp_file, mode = \"wb\")\n\n# read the data into R using qread\ndf_nz_long &lt;- qread(tmp_file)\n\n# inspect the data -------------------------------------------------------\n# view the first few rows to check it loaded correctly\nprint(head(df_nz_long))\n# list column names so you know what variables are available\nprint(colnames(df_nz_long))\n\n# save a copy of the data ------------------------------------------------\n# save the dataset to your data directory for future use\nhere_save_qs(df_nz_long, \"df_nz_long\", data_dir)\n\n# takeaway: clear setup saves time later."
  },
  {
    "objectID": "content/09-content.html#script-01-initial-data-wrangling",
    "href": "content/09-content.html#script-01-initial-data-wrangling",
    "title": "Causal inference: a step by step guide",
    "section": "Script 01 Initial Data Wrangling",
    "text": "Script 01 Initial Data Wrangling\nKey checkpoints in the wrangle-1 script are as follows;\n\nFirst — pick your exposure*\nDecide which variable is the “cause”. Here it is extraversion. Verify it exists at baseline (t0_) and at the exposure wave (t1_)\nname_exposure &lt;- \"hours_community\"\nhere_save(name_exposure, \"name_exposure\")\n\n\nSecond – choose a binary cut-point\nCausal inference requires a contrast between two conditions. The splits give us the conditions of a hypothetical experiment.\nmargot_plot_categorical(df,\n                        col_name       = name_exposure,\n                        custom_breaks  = c(0, 1))\nAsk: which break meaningfully separate groups into two exposure groups? Is the split *theoretically motivated**? adjust custom_breaks until the answer is “yes”. then commit:\ncreate_ordered_variable(df,\n                        var_name       = name_exposure,\n                        custom_breaks  = c(0, 1))\n\n\nThird — select outcomes that fit your interests\nThis example considers multi-dimensional wellbeing outcomes. You can select from these outcomes if you like. Or you can pick different outcomes. Discuss with us if you are confused*.\noutcome_vars &lt;- c(\"lifesat\", \"pwi\", \"self_esteem\")\nhere_save(outcome_vars, \"outcome_vars\")\nEverything downstream—eligibility, transitions… depends on this trio.\n\n\nFouth — locking in the wave structure.\nDefine one baseline wave, one or more exposure waves, and one outcome wave:\nbaseline_wave  &lt;- \"2018\"\nexposure_waves &lt;- \"2019\"\noutcome_wave   &lt;- \"2020\"\nNote waves run from October –&gt; September of the year folloowing the wave.\n\n\nFifth — save as you go.\nUse margot::here_save() (objects) and here_save_qs() (large objects) right after you create something you’ll cite in your report. The convention keeps analysis, tables, and manuscript in sync.\nhere_save(dat_long_final,          \"dat_long_final\")\nhere_save(percent_missing_baseline,  \"percent_missing_baseline\")\n\n\nTake-away: Think about the hypothetical experiment you wish to perform with observational data.\nDecide early (exposure, cut-point, outcomes, waves), checkpoint everything with here_save(), and the rest of the pipeline flows."
  },
  {
    "objectID": "content/09-content.html#script-2-what-script-2-does-while-you-sip-your-coffee",
    "href": "content/09-content.html#script-2-what-script-2-does-while-you-sip-your-coffee",
    "title": "Causal inference: a step by step guide",
    "section": "Script 2: What script 2 does while you sip your coffee",
    "text": "Script 2: What script 2 does while you sip your coffee\nPurpose\nConvert the tidy long data into a grf-ready wide matrix and correct for dropout bias with inverse-probability-of-censoring weighting (IPCW).\n\nStep 1 – go wide automatically\nmargot_wide_machine() reshapes each wave into t0_, t1_, t2_ columns. You gave it all the names in script 1, so no new choices here. Inspect df_wide just to be sure the columns look sensible (baseline before exposure before outcome).\ncolnames(df_wide)\nnaniar::vis_miss(df_wide)\n\n\nStep 2 – Encode + scale for algorithms\nmargot_process_longitudinal_data_wider() turns factors into dummies, scales continuous variables, and appends loss-to-follow-up flags. The result is df_wide_encoded.\nQuick sanity check: the exposure binaries should be 0/1 and their NAs must match the *_lost* indicators.\ntable(df_wide_encoded$t0_not_lost_following_wave)\n\n\nStep 3 – Build IPCW weights*\nTwo probability forests estimate the chance of being observed at t1 and t2. margot_adjust_weights() then trims extreme values and normalises the weights. these appear as t0_adjusted_weights and t1_adjusted_weights. The weigths should not be too extreme (e.g. values &gt; 10)\nhist(df_wide_encoded$t1_adjusted_weights)\nWhy? observations that resemble those who dropped out are up-weighted, so estimates stay unbiased when data vanish.\n\n\nStep 4 – drop the censored\nRows with *_lost_following_wave == 1 are removed, leaving df_grf. This dataset feeds the causal forest and should now have no missing values in any t1_ or t2_ column. Missingness is permitted in t0_ columns\nnaniar::vis_miss(df_grf)   # should be a blank canvas\n\n\nStep 5 – checkpoint everything\nEach major object (df_wide, df_wide_encoded, df_grf, the weight vectors) is saved with here_save() or here_save_qs(). this keeps the analysis reproducible and the manuscript tables in sync.\n\n\nTakeaway: what you must do\n\nRun the script without edits.\n\nskim the weight histograms; if you see huge spikes, ask for help.\n\nconfirm df_grf has the expected number of rows and no missing cells.\n\nScript 2 is a conveyor belt: long \\to wide \\to weighted \\to clean. yYur only job is to eyeball the parcels before they roll into the modelling bay."
  },
  {
    "objectID": "content/09-content.html#script-3-the-analysis",
    "href": "content/09-content.html#script-3-the-analysis",
    "title": "Causal inference: a step by step guide",
    "section": "Script 3 The Analysis",
    "text": "Script 3 The Analysis\n\n1 Reproducibility scaffold\nset.seed(123)                 # lock randomness  \npacman::p_load(margot, qs …)  # load every tool once  \nNo choices here—just run and move on.\n\n\n2 Pull in prepared objects\ndf_grf           &lt;- here_read(\"df_grf\")     # wide, weighted data  \nname_exposure    &lt;- here_read(\"name_exposure\")  \nIf anything is NULL, return to scripts 1–2 and debug.\n\n\n3 Sanity-check with a toy forest\nFit one outcome on ¼ of the data to confirm the pipeline before burning CPU hours.\ncf_out &lt;- margot_causal_forest(\n  data         = df_grf[sample(nrow(df_grf), nrow(df_grf)/4), ],\n  outcome_vars = \"t2_kessler_latent_depression_z\",\n  covariates   = X,           # baseline covariates E\n  W            = W,           # exposure\n  weights      = weights\n)\nmargot_plot_policy_combo(cf_out, max_depth = 1L)\nSuccess = a plot appears without errors.\n\n\n4 Full model batches + ATE tables\nLoop over the five outcome domains, saving each model to models_example_2/.\nmodels_binary &lt;- margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_z,\n  covariates   = X, W = W, weights = weights,\n  save_models  = TRUE, save_data = TRUE\n)\nhere_save_qs(models_binary_health, \"models_binary\")\nThen create ATE plots and interpretations with margot_plot(). Save as these go straight into the manuscript.\n\n\n5 Heterogeneity hunt (CATE)\nEvidence phase\n\nRATE AUTOC & Qini curves via margot_rate() and margot_inspect_qini() flag outcomes with meaningful heterogeneity.\n\nAction phase\n\nFlip outcomes you want to minimise (e.g. depression) so positive CATE = good.\n\nflipped &lt;- margot_flip_forests(models_binary,\n                                   flip_outcomes =\n                                   c(\"t2_kessler_latent_depression_z\"), \n                                   recalc_policy = TRUE)\n\nFit policy trees (margot_policy(), max_depth = 2L) to discover actionable rules.\nDepth-1 trees are often trivial; depth-2 gives interpretable nuance.\n\nCheck that every split variable exists in the original data and that leaves map sensibly onto subgroups (no cells with &lt; 30 observations).\n\n\n6 Report subgroup analyses (optional)\nWhen theory predicts effect modification—age bands, income thirds, etc.—use margot_planned_subgroups_batch(). The helper auto-generates tables and mini-plots; just pass the subset definitions you crafted:\nplanned_subset_results &lt;- margot_planned_subgroups_batch(\n  domain_models = list(models_binary, …),\n  X             = X,\n  subset_types  = list(wealth = subsets_standard_wealth, …)\n)\nIf you do subgroup analyses, save the markdown tables/ plots using the protocols for the main analysis (For you to do, not done in the script.)\n\n\nTakeaway\nRun the script in order, eyes on three checkpoints: the toy forest plot, convincing Qini/RATE curves/interpretations, and sensible policy-tree leaves. Everything else is auto-saved for the manuscript."
  },
  {
    "objectID": "content/09-content.html#key-concepts-for-lecture",
    "href": "content/09-content.html#key-concepts-for-lecture",
    "title": "Causal inference: a step by step guide",
    "section": "Key Concepts for Lecture",
    "text": "Key Concepts for Lecture\n\nSensitivity Analysis: E-values\nWorkflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity Analysis using E-values\n\nThe minimum strength of association on the risk ratio scale that an unmeasured confounder would need to have with both the exposure and the outcome, conditional on the measured covariates, to fully explain away a specific exposure-outcome association\n\nSee: Mathur et al. (2018); Linden, Mathur, and VanderWeele (2020); Tyler J. VanderWeele and Ding (2017).\n\nMathur, Maya B, Peng Ding, Corinne A Riddell, and Tyler J VanderWeele. 2018. “Website and r Package for Computing E-Values.” Epidemiology (Cambridge, Mass.) 29 (5): e45.\nFor example, suppose that the lower bound of the the E-value was 1.3 with the lower bound of the confidence interval = 1.12, we might then write:\n\nWith an observed risk ratio of RR=1.3, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 1.3-fold each (or 30%), above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 1.12-fold (or 12%) each could do so, but weaker joint confounder associations could not.\n\nThe equations are as follows (for risk ratios)\n\nE-value_{RR} = RR + \\sqrt{RR \\times (RR - 1)}\n\n\nE-value_{LCL} = LCL + \\sqrt{LCL \\times (LCL - 1)}\n\nHere is an R function that will calculate E-values\n\n\nCode\n# evalue for risk ratio\ncalculate_e_value &lt;- function(rr, lcl) {\n  e_value_rr = rr + sqrt(rr*(rr - 1))\n  e_value_lcl = lcl + sqrt(lcl*(lcl - 1))\n  \n  list(e_value_rr = e_value_rr, e_value_lcl = e_value_lcl)\n}\n\n# e.g. smoking causes cancer\n# finding   RR = 10.73 (95% CI: 8.02, 14.36)\nevalue_computed &lt;- calculate_e_value(10.73, 8.02)\n\n#print\nevalue_computed\n\n\n$e_value_rr\n[1] 20.94777\n\n$e_value_lcl\n[1] 15.52336\n\n\nWe write:\n\nWith an observed risk ratio of RR=10.7, an unmeasured confounder that was associated 20.9477737-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of e_value_rr$e_value_lcl-fold each could do so, but weaker joint confounder associations could not.\n\nNote that in this class, most of the outcomes will be (standardised) continuous outcomes. Here’s a function and LaTeX code to describe the approximation.\nThis function takes a linear regression coefficient estimate (est), its standard error (se), the standard deviation of the outcome (sd), a contrast of interest in the exposure (delta, which defaults to 1), and a “true” standardized mean difference (true, which defaults to 0). It calculates the odds ratio using the formula from Chinn (2000) and VanderWeele (2017), and then uses this to calculate the E-value.\n\n\nCode\n#evalue for ols\ncompute_evalue_ols &lt;- function(est, se, delta = 1, true = 0) {\n  # rescale estimate and SE to get a contrast of size delta\n  est &lt;- est / delta\n  se &lt;- se / delta\n\n  # compute transformed odds ratio and ci's\n  odds_ratio &lt;- exp(0.91 * est)\n  lo &lt;- exp(0.91 * est - 1.78 * se)\n  hi &lt;- exp(0.91 * est + 1.78 * se)\n\n  # compute E-Values based on the RR values\n  evalue_point_estimate &lt;- odds_ratio * sqrt(odds_ratio + 1)\n  evalue_lower_ci &lt;- lo * sqrt(lo + 1)\n\n  # return the e-values\n  return(list(EValue_PointEstimate = evalue_point_estimate,\n              EValue_LowerCI = evalue_lower_ci))\n}\n\n\n# example:\n# suppose we have an estimate of 0.5, a standard error of 0.1, and a standard deviation of 1.\n# this would correspond to a half a standard deviation increase in the outcome per unit increase in the exposure.\nresults &lt;- compute_evalue_ols(est = 0.5, se = 0.1, delta = 1)\npoint_round &lt;- round(results$EValue_PointEstimate, 3)\nci_round &lt;- round(results$EValue_LowerCI, 3)\n\n# print results\nprint(point_round)\n\n\n[1] 2.53\n\n\nCode\nprint(ci_round)\n\n\n[1] 2.009\n\n\nWe write:\n\nWith an observed risk ratio of 2.53, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 2.53-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 2.009-fold each could do so, but weaker joint confounder associations could not.\n\nNote the E-values package will do the computational work for us and this is what we use in the margot package to obtain E-values for sensitivity analysis."
  },
  {
    "objectID": "content/09-content.html#part-2",
    "href": "content/09-content.html#part-2",
    "title": "Causal inference: a step by step guide",
    "section": "Part 2:",
    "text": "Part 2:\n\nSensitivity Analysis: E-values\nWorkflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity Analysis using E-values\n\nThe minimum strength of association on the risk ratio scale that an unmeasured confounder would need to have with both the exposure and the outcome, conditional on the measured covariates, to fully explain away a specific exposure-outcome association\n\nSee: Mathur et al. (2018); Linden, Mathur, and VanderWeele (2020); Tyler J. VanderWeele and Ding (2017).\n\nMathur, Maya B, Peng Ding, Corinne A Riddell, and Tyler J VanderWeele. 2018. “Website and r Package for Computing E-Values.” Epidemiology (Cambridge, Mass.) 29 (5): e45.\nFor example, suppose that the lower bound of the the E-value was 1.3 with the lower bound of the confidence interval = 1.12, we might then write:\n\nWith an observed risk ratio of RR=1.3, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 1.3-fold each (or 30%), above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 1.12-fold (or 12%) each could do so, but weaker joint confounder associations could not.\n\nThe equations are as follows (for risk ratios)\n\nE-value_{RR} = RR + \\sqrt{RR \\times (RR - 1)}\n\n\nE-value_{LCL} = LCL + \\sqrt{LCL \\times (LCL - 1)}\n\nHere is an R function that will calculate E-values\n\n\nCode\n# evalue for risk ratio\ncalculate_e_value &lt;- function(rr, lcl) {\n  e_value_rr = rr + sqrt(rr*(rr - 1))\n  e_value_lcl = lcl + sqrt(lcl*(lcl - 1))\n  \n  list(e_value_rr = e_value_rr, e_value_lcl = e_value_lcl)\n}\n\n# e.g. smoking causes cancer\n# finding   RR = 10.73 (95% CI: 8.02, 14.36)\nevalue_computed &lt;- calculate_e_value(10.73, 8.02)\n\n#print\nevalue_computed\n\n\n$e_value_rr\n[1] 20.94777\n\n$e_value_lcl\n[1] 15.52336\n\n\nWe write:\n\nWith an observed risk ratio of RR=10.7, an unmeasured confounder that was associated 20.9477737-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of e_value_rr$e_value_lcl-fold each could do so, but weaker joint confounder associations could not.\n\nNote that in this class, most of the outcomes will be (standardised) continuous outcomes. Here’s a function and LaTeX code to describe the approximation.\nThis function takes a linear regression coefficient estimate (est), its standard error (se), the standard deviation of the outcome (sd), a contrast of interest in the exposure (delta, which defaults to 1), and a “true” standardized mean difference (true, which defaults to 0). It calculates the odds ratio using the formula from Chinn (2000) and VanderWeele (2017), and then uses this to calculate the E-value.\n\n\nCode\n#evalue for ols\ncompute_evalue_ols &lt;- function(est, se, delta = 1, true = 0) {\n  # rescale estimate and SE to get a contrast of size delta\n  est &lt;- est / delta\n  se &lt;- se / delta\n\n  # compute transformed odds ratio and ci's\n  odds_ratio &lt;- exp(0.91 * est)\n  lo &lt;- exp(0.91 * est - 1.78 * se)\n  hi &lt;- exp(0.91 * est + 1.78 * se)\n\n  # compute E-Values based on the RR values\n  evalue_point_estimate &lt;- odds_ratio * sqrt(odds_ratio + 1)\n  evalue_lower_ci &lt;- lo * sqrt(lo + 1)\n\n  # return the e-values\n  return(list(EValue_PointEstimate = evalue_point_estimate,\n              EValue_LowerCI = evalue_lower_ci))\n}\n\n\n# example:\n# suppose we have an estimate of 0.5, a standard error of 0.1, and a standard deviation of 1.\n# this would correspond to a half a standard deviation increase in the outcome per unit increase in the exposure.\nresults &lt;- compute_evalue_ols(est = 0.5, se = 0.1, delta = 1)\npoint_round &lt;- round(results$EValue_PointEstimate, 3)\nci_round &lt;- round(results$EValue_LowerCI, 3)\n\n# print results\nprint(point_round)\n\n\n[1] 2.53\n\n\nCode\nprint(ci_round)\n\n\n[1] 2.009\n\n\nWe write:\n\nWith an observed risk ratio of 2.53, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 2.53-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 2.009-fold each could do so, but weaker joint confounder associations could not.\n\nNote the E-values package will do the computational work for us and this is what we use in the margot package to obtain E-values for sensitivity analysis."
  },
  {
    "objectID": "content/09-content.html#step-2-answer-your-question",
    "href": "content/09-content.html#step-2-answer-your-question",
    "title": "Causal inference: a step by step guide",
    "section": "STEP 2: Answer Your Question",
    "text": "STEP 2: Answer Your Question\n\nObtain longitudinal data\nNote that causal inference from observational data turns on the appropriate temporal ordering of the key variables involved in the study.\nRecall we have defined.\n\nA: Our exposure or treatment variable, denoted as A. Here we consider the example of ‘Church attendance’.\nY: The outcome variable we are interested in, represented by Y, is psychological distress. We operationalise this variable through the ‘Kessler-6’ distress scale.\nL: The confounding variables, collectively referred to as L, represent factors that can independently influence both A and Y. For example, socio-economic status could be a confounder that impacts both the likelihood of church attendance and the levels of psychological distress.\n\nGiven the importance of temporal ordering, we must now define time:\n\nt \\in T: Let t denote within a multiwave panel study with T measurement intervals.\n\nWhere t/\\text{{exposure}} denotes the measurement interval for the exposure. Longitudinal data collection provides us the ability to establish a causal model such that:\nt_{confounders} &lt; t_{exposure}&lt; t_{outcome}\nTo minimise the posibility of time-varying confounding and obtain the clearest effect estimates, we should acquire the most recent values of \\mathbf{L} preceding A and the latest values of A before Y.\nNote in Figure 1, We use the prefixes “t0, t1, and t2” to denote temporal ordering. We include in the set of baseline confounders the pre-exposure measurement of A and Y. This allows for more substantial confounding control. For unmeasured confounder to affect both the exposure and the outcome, it would need to do so independently of the pre-exposure confounders. Additionally, including the baseline exposure gives us an effect estimate for the incidence exposure, rather than the prevelance of the exposure. This helps us to assess the expected change in the outcome were we to initate a change in the exposure.\n\n\nInclude the measured exposure with baseline covariates\nControlling for prior exposure enables the interpretation of the effect estimate as a change in the exposure in a manner akin to a randomised trial. We propose that the effect estimate with prior control for the exposure estimates the “incidence exposure” rather than the “prevalence exposure” (Danaei, Tavakkoli, and Hernán 2012). It is crucial to estimate the incidence exposure because if the effects of an exposure are harmful in the short term such that these effects are not subsequently measured, a failure to adjust for prior exposure will yield the illusion that the exposure is beneficial. Furthermore, this approach aids in controlling for unmeasured confounding. For such a confounder to explain away the observed exposure-outcome association, it would need to do so independently of the prior level of the exposure and outcome.\n\nDanaei, Goodarz, Mohammad Tavakkoli, and Miguel A. Hernán. 2012. “Bias in observational studies of prevalent users: lessons for comparative effectiveness research from a meta-analysis of statins.” American Journal of Epidemiology 175 (4): 250–62. https://doi.org/10.1093/aje/kwr301.\n\n\nState the eligibility criteria for participation\nThis step is invaluable for assessing whether we are answering the causal question that we have asked.\n\nConsider:\n\nGeneralisability: we cannot evaluate inferences to a target group from the source population if we do not describe the source population\nEligibility criteria will help us to ensure whether we have correctly evaluated potential measurement bias/error in our instruments.\n\nFor example, the New Zealand Attitudes and Values Study is a National Probability study of New Zealanders. The details provided in the supplementary materials describe how individuals were randomly selected from the country’s electoral roll. From these invitations there was typically less than 15% response rate. How might this process of recruitment affect generalisability and transportability of our results?\n\nAside: discuss per protocol effects/ intention to treat effects\n\n\n\n\nDetermine how missing data will be handled\n\nAs we will consider in the upcoming weeks, loss to follow up and non-response opens sources for bias. We must develop a strategy for handling missing data.\n\n\n\nState a statistical model\nThe models we have considered in this course are G-computation, Inverse Probability of Treatement Weighting, and Doubly-Robust estimation.\n\n\nReporting\nConsider the following ideas about how to report one’s model:\n\nEstimator: Doubly robust where possible.\nPropensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations.\nWeightIt Package: Explicitly mention the use of the ‘WeightIt’ package in R, including any specific options or parameters used in the propensity score estimation process.\nMethod Variations: Report if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as ‘ebal’, ‘energy’, and ‘ps’.\nContinuous Exposures: Highlight that for continuous exposures, only the ‘energy’ option was used for propensity score estimation. (not relevant for your report)\nBinary Exposure Justify your cutpoints by referencing theory/existing knowledge in advance of conducting the analysis.\nSubgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\nCovariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting.\nWeighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit.\nOutcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation.\nSubgroup Interaction: Address whether the subgroup was included separately as an interaction in the outcome model, and if the model successfully converged.\nMachine Learning Using lmtp If using the lmtp package, do a stratified analysis. (see today’s lab)\nModel coefficients: note that the model coefficients should not be interpreted, as they are not meaningful in this context.\nConfidence intervals and standard errors: Describe the methods used to derive confidence intervals and standard errors, noting the use of the ‘clarify’ package in R for simulation based inference.\n\n\n\nExample of how to report a doubly robust method in your report\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\nStep 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\nStep 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\nStep 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\nStep 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\nStep 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\n\nInference\nConsider the following ideas about what to discuss in one’s findings: Consider the following ideas about what to discuss in one’s findings. The order of exposition might be different.\n\nSummary of results: What did you find?\nInterpretation of E-values: Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\nCausal Effect Interpretation: What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\nComparison of Subgroups: Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\nUncertainty and Confidence Intervals: Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\nGeneralisability and Transportability: Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\nAssumptions and Limitations: Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\nTheoretical Relevance: How are these findings relevant to existing theories.\nReplication and Future Research: Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\nReal-world Implications: Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research."
  },
  {
    "objectID": "content/09-content.html#script-0-is-here",
    "href": "content/09-content.html#script-0-is-here",
    "title": "Causal inference: a step by step guide",
    "section": "Script 0 is here",
    "text": "Script 0 is here\nRun in full. No need to change anything.\nThis script prepares your data. Run it once – running it twice would be like turning the ignition off just to start your car again: unnecessary/silly.\n\n\nCode\n# for students: reproducibility is like following a recipe; each step ensures the same result\n# restart fresh session if needed\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n\n# check margot ------------------------------------------------------------\n\n# install and load 'margot' from GitHub if missing\nif (!requireNamespace(\"margot\", quietly = TRUE)) {\n  message(\"installing 'margot' from GitHub\")\n  devtools::install_github(\"go-bayes/margot\", upgrade = \"never\")\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.35\") {\n  stop(\"please install margot &gt;= 1.0.35 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\nlibrary(\"margot\")\n\n\n# load packages ----------------------------------------------------------\n\n\n# install and load other packages from CRAN if missing\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\nif (!requireNamespace(\"qs\", quietly = TRUE)) {\n  install.packages(\"qs\")\n}\nlibrary(qs)\n\nif (!requireNamespace(\"here\", quietly = TRUE)) {\n  install.packages(\"here\")\n}\nlibrary(here)\n\n\n\n# create data directory if it doesn't exist -----------------------------\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\n# define file paths ------------------------------------------------------\n# use here() to build paths relative to your project root\ndata_dir &lt;- here::here(\"data\")\n\n# download synthetic data ------------------------------------------------\n# specify the url for the data file\nurl &lt;- \"https://www.dropbox.com/scl/fi/ru0ecayju04ja8ky1mhel/df_nz_long.qs?rlkey=prpk9a5v4vcg1ilhkgf357dhd&dl=1\"\n\n# download to a temporary file for safety\ntmp_file &lt;- tempfile(fileext = \".qs\")\ndownload.file(url, tmp_file, mode = \"wb\")\n\n# read the data into R using qread\ndf_nz_long &lt;- qread(tmp_file)\n\n# inspect the data -------------------------------------------------------\n# view the first few rows to check it loaded correctly\nprint(head(df_nz_long))\n\n# list column names so you know what variables are available\nprint(colnames(df_nz_long))\n\n# save a copy of the data ------------------------------------------------\n# save the dataset to your data directory for future use\nhere_save_qs(df_nz_long, \"df_nz_long\", data_dir)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+"
  },
  {
    "objectID": "content/09-content.html#script-1-is-here",
    "href": "content/09-content.html#script-1-is-here",
    "title": "Causal inference: a step by step guide",
    "section": "Script 1 is here:",
    "text": "Script 1 is here:\n\n\nCode\n# script 1 workflow lecture 10\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# install and load 'margot' from GitHub if missing\nif (!requireNamespace(\"margot\", quietly = TRUE)) {\n  message(\"installing 'margot' from GitHub\")\n  devtools::install_github(\"go-bayes/margot\", upgrade = \"never\")\n}\n\nif (packageVersion(\"margot\") &lt; \"1.0.36\") {\n  stop(\"please install margot &gt;= 1.0.36 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\nlibrary(\"margot\")\n\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf,             # machine learning forests\n  kableExtra,      # tables\n  ggplot2,         # graphs\n  doParallel       # parallel processing\n  grf,             # causal forests\n  janitor          # variables names\n  stringr          # variable names\n  patchwork        # graphs\n  table1           # tables\n)\n\n# check margot version ------------------------------------------------------\nif (packageVersion(\"margot\") &lt; \"1.0.35\") {\n  stop(\"please install margot &gt;= 1.0.35 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n\n# check margot version ------------------------------------------------------\nif (packageVersion(\"margot\") &lt; \"1.0.35\") {\n  stop(\"please install margot &gt;= 1.0.35 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\nlibrary(\"margot\")\n\n# install and load other packages from CRAN if missing\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\n\n\n# create directories --------------------------------------------------------\n# create data directory if it doesn't exist\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\nif (!dir.exists(\"save_directory\")) {\n  dir.create(\"save_directory\")  # first time only: make a folder named 'data'\n}\n\n# set up data directory structure\ndata_dir    &lt;- here::here(\"data\")\npush_mods &lt;- here::here(\"save_directory\") \n\n# load data -----------------------------------------------------------------\ndf_nz_long &lt;- margot::here_read_qs(\"df_nz_long\", data_dir)\n\n# initial data prep ---------------------------------------------------------\n# prepare intial data\n# define labels for rural classification\nrural_labels &lt;- c(\n  \"High Urban Accessibility\", \n  \"Medium Urban Accessibility\",\n  \"Low Urban Accessibility\", \n  \"Remote\", \n  \"Very Remote\"\n)\n\ndat_prep &lt;- df_nz_long |&gt;\n  arrange(id, wave) |&gt;\n  margot::remove_numeric_attributes() |&gt;\n  mutate(\n    # cap extreme values\n    alcohol_intensity = pmin(alcohol_intensity, 15),\n    # flag heavy drinkers: freq ≥3 → 1, ≤2 → 0, else NA\n    heavy_drinker = case_when(\n      alcohol_frequency &gt;= 3 ~ 1,\n      alcohol_frequency &lt;= 2 ~ 0,\n      TRUE                  ~ NA_real_\n    ),\n    # map freq categories to weekly counts\n    alcohol_frequency_weekly = recode(\n      alcohol_frequency,\n      `0` = 0, `1` = 0.25,\n      `2` = 1, `3` = 2.5,\n      `4` = 4.5,\n      .default = NA_real_\n    ),\n    # relabel rural factor\n    rural_gch_2018_l = factor(\n      rural_gch_2018_l,\n      levels = 1:5,\n      labels = rural_labels,\n      ordered = TRUE\n    )\n  ) |&gt;\n  droplevels()\n\n\n\n# view variable names -----------------------------------------------------\nprint(colnames(df_nz_long)) \n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# define study variables ----------------------------------------------------\n# ** key decision 1: define your exposure variable **\nname_exposure &lt;- \"extraversion\"\n\n# exposure variable labels\nvar_labels_exposure &lt;- list(\n  \"extraversion\" = \"Extraversion\",\n  \"extraversion_binary\" = \"Extraversion (binary)\"\n)\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n\n# +--------------------------+\n# | OPTIONALLY MODIFY SECTION|\n# +--------------------------+\n\n# **  define your study waves **\nbaseline_wave      &lt;- \"2018\"        # baseline measurement\nexposure_waves     &lt;- c(\"2019\")     # when exposure is measured\noutcome_wave       &lt;- \"2020\"        # when outcomes are measured\nall_waves          &lt;- c(baseline_wave, exposure_waves, outcome_wave)\n\n# **  define baseline covariates **\n# these are demographics, traits, etc. measured at baseline\n\nbaseline_vars &lt;- c(\n  # demographics\n  \"age\", \"born_nz_binary\", \"education_level_coarsen\",\n  \"employed_binary\", \"eth_cat\", \"male_binary\",\n  \"not_heterosexual_binary\", \"parent_binary\", \"partner_binary\",\n  \"rural_gch_2018_l\", \"sample_frame_opt_in_binary\",\n  \n  # personality traits (excluding exposure)\n  \"agreeableness\", \"conscientiousness\", \"neuroticism\", \"openness\",\n  \n  # health and lifestyle\n  \"alcohol_frequency\", \"alcohol_intensity\", \"hlth_disability_binary\",\n  \"log_hours_children\", \"log_hours_commute\", \"log_hours_exercise\",\n  \"log_hours_housework\", \"log_household_inc\",\n  \"short_form_health\", \"smoker_binary\",\n  \n  # social and psychological\n  \"belong\", \"nz_dep2018\", \"nzsei_13_l\",\n  \"political_conservative\", \"religion_identification_level\"\n)\n\n# +--------------------------+\n# |END OPTIONALLY MODIFY SEC.|\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n# ** key decision 3: define outcome variables **\noutcome_vars &lt;- c(\n  # health outcomes\n  # \"alcohol_frequency_weekly\", \"alcohol_intensity\",\n  # \"hlth_bmi\", \n  \"log_hours_exercise\", \n  # \"hlth_sleep_hours\", \n  # \"short_form_health\",\n  \n  # psychological outcomes\n  # \"hlth_fatigue\", \n  \"kessler_latent_anxiety\", \n  \"kessler_latent_depression\", \n  \"rumination\",\n  \n  # wellbeing outcomes\n  \"bodysat\", \n  #\"forgiveness\", \"gratitude\", \n  \"lifesat\", \"meaning_purpose\", \"meaning_sense\", \n  # \"perfectionism\", \n  \"pwi\", \n  #\"self_control\", \n  \"self_esteem\", \n  #\"sexual_satisfaction\",\n  \n  # social outcomes\n  \"belong\", \"neighbourhood_community\", \"support\"\n)\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# after selecting your exposure/ baseline / outcome variables do not modify this\n# code\n\n# make binary variable (UNLESS YOUR EXPOSURE IS A BINARY VARIABLE)\nexposure_var_binary = paste0(name_exposure, \"_binary\")\n\n# make exposure variable list (we will keep both the continuous and binary variable)\nexposure_var  &lt;- c(name_exposure, paste0(name_exposure, \"_binary\"))\n\n# sort for easier reference\nbaseline_vars &lt;- sort(baseline_vars)\noutcome_vars &lt;- sort(outcome_vars)\n\n# save key variables --------------------------------------------------------\nmargot::here_save(name_exposure, \"name_exposure\")\nmargot::here_save(var_labels_exposure,\"var_labels_exposure\")\nmargot::here_save(baseline_vars,\"baseline_vars\")\nmargot::here_save(exposure_var, \"exposure_var\")\nmargot::here_save(exposure_var_binary, \"exposure_var_binary\")\nmargot::here_save(outcome_vars, \"outcome_vars\")\nmargot::here_save(baseline_wave, \"baseline_wave\")\nmargot::here_save(exposure_waves, \"exposure_waves\")\nmargot::here_save(outcome_wave, \"outcome_wave\")\nmargot::here_save(all_waves,\"all_waves\")\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# | OPTIONALLY MODIFY SECTION|\n# +--------------------------+\n\n# select eligible participants ----------------------------------------------\n# only include participants who have exposure data at baseline\n\n# You might require tighter conditions \n# for example, if you are interested in the effects of hours of childcare, \n# you might want to select only those who were parents at baseline. \n# talk to me if you think you might night tighter eligibility criteria.\n\nids_baseline &lt;- dat_prep |&gt; \n  filter(wave == baseline_wave, !is.na(!!sym(name_exposure))) |&gt; \n  pull(id)\n\n# filter data to include only eligible participants and relevant waves\ndat_long_1 &lt;- dat_prep |&gt; \n  filter(id %in% ids_baseline, wave %in% all_waves) |&gt; \n  droplevels()\n\n# +--------------------------+\n# |END OPTIONALLY MODIFY SEC.|\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n# plot distribution to help with cutpoint decision\n# get exposure wave to inspect exposure variable distribution\ndat_long_exposure &lt;- dat_long_1 |&gt; filter(wave %in% exposure_waves)\n\n# make graph \ngraph_cut &lt;- margot::margot_plot_categorical(\n  dat_long_exposure,\n  col_name         = name_exposure,\n  sd_multipliers = c(-1, 1), # select to suit\n  # either use n_divisions for equal-sized groups:\n  # n_divisions      = 2,\n  # or use custom_breaks for specific values:\n  custom_breaks    = c(1, 4),  # ** adjust as needed **\n  cutpoint_inclusive = \"upper\",\n  show_mean        = TRUE,\n  show_sd          = TRUE\n)\nprint(graph_cut)\n\n# save your graph\nmargot::here_save(graph_cut, \"graph_cut\", push_mods)\n\n# create binary exposure variable based on chosen cutpoint\ndat_long_2 &lt;- margot::create_ordered_variable(\n  dat_long_1,\n  var_name           = name_exposure,\n  custom_breaks      = c(1, 4),  # ** -- adjust based on your decision above -- **\n  cutpoint_inclusive = \"upper\"\n)\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# process binary variables and log-transform --------------------------------\n# convert binary factors to 0/1 format\ndat_long_3 &lt;- margot::margot_process_binary_vars(dat_long_2)\n\n# log-transform hours and income variables: tables for analysis (only logged versions of vars)\ndat_long_final &lt;- margot::margot_log_transform_vars(\n  dat_long_3,\n  vars            = c(starts_with(\"hours_\"), \"household_inc\"),\n  prefix          = \"log_\",\n  keep_original   = FALSE,\n  exceptions = exposure_var # omit original variables\n) |&gt; \n  # select only variables needed for analysis\n  select(all_of(c(baseline_vars, exposure_var, outcome_vars, \"id\", \"wave\", \"year_measured\", \"sample_weights\"))) |&gt; \n  droplevels()\n\n\n# check missing data --------------------------------------------------------\n# this is crucial to understand potential biases\nmissing_summary &lt;- naniar::miss_var_summary(dat_long_final)\nprint(missing_summary)\nmargot::here_save(missing_summary, \"missing_summary\", push_mods)\n\n# visualise missing data pattern\n# ** -- takes a while to render ** \nvis_miss &lt;- naniar::vis_miss(dat_long_final, warn_large_data = FALSE)\nprint(vis_miss)\nmargot::here_save(vis_miss, \"vis_miss\", push_mods)\n\n# calculate percentage of missing data at baseline\ndat_baseline_pct &lt;- dat_long_final |&gt; filter(wave == baseline_wave)\npercent_missing_baseline &lt;- naniar::pct_miss(dat_baseline_pct)\nmargot::here_save(percent_missing_baseline, \"percent_missing_baseline\", push_mods)\n\n# save prepared dataset for next stage --------------------------------------\nmargot::here_save(dat_long_final, \"dat_long_final\", push_mods)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n# create transition matrices to check positivity ----------------------------\n# this helps assess whether there are sufficient observations in all exposure states\ndt_positivity &lt;- dat_long_final |&gt;\n  filter(wave %in% c(baseline_wave, exposure_waves)) |&gt;\n  select(!!sym(name_exposure), id, wave) |&gt;\n  mutate(exposure = round(as.numeric(!!sym(name_exposure)), 0)) |&gt;\n  # create binary exposure based on cutpoint\n  mutate(exposure_binary = ifelse(exposure &gt;= 4, 1, 0)) |&gt; ## *-- modify this --* \n  mutate(wave = as.numeric(wave) -1 )\n\n# create transition tables\ntransition_tables &lt;- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"wave\",\n  table_name = \"transition_table\"\n)\nprint(transition_tables$tables[[1]])\nmargot::here_save(transition_tables, \"transition_tables\", push_mods)\n\n# create binary transition tables\ntransition_tables_binary &lt;- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure_binary\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"wave\",\n  table_name = \"transition_table_binary\"\n)\nprint(transition_tables_binary$tables[[1]])\nmargot::here_save(transition_tables_binary, \"transition_tables_binary\", push_mods)\n\n# create tables -----------------------------------------------------------\n# baseline variable labels\nvar_labels_baseline &lt;- list(\n  # demographics\n  \"age\" = \"Age\",\n  \"born_nz_binary\" = \"Born in NZ\",\n  \"education_level_coarsen\" = \"Education Level\",\n  \"employed_binary\" = \"Employed\",\n  \"eth_cat\" = \"Ethnicity\",\n  \"male_binary\" = \"Male\",\n  \"not_heterosexual_binary\" = \"Non-heterosexual\",\n  \"parent_binary\" = \"Parent\",\n  \"partner_binary\" = \"Has Partner\",\n  \"rural_gch_2018_l\" = \"Rural Classification\",\n  \"sample_frame_opt_in_binary\" = \"Sample Frame Opt-In\",\n  \n  # economic & social status\n  \"household_inc\" = \"Household Income\",\n  \"log_household_inc\" = \"Log Household Income\",\n  \"nz_dep2018\" = \"NZ Deprivation Index\",\n  \"nzsei_13_l\" = \"Occupational Prestige Index\",\n  \"household_inc\" = \"Household Income\",\n\n  \n  # personality traits\n  \"agreeableness\" = \"Agreeableness\",\n  \"conscientiousness\" = \"Conscientiousness\",\n  \"neuroticism\" = \"Neuroticism\",\n  \"openness\" = \"Openness\",\n  \n  # beliefs & attitudes\n  \"political_conservative\" = \"Political Conservatism\",\n  \"religion_identification_level\" = \"Religious Identification\",\n  \n  # health behaviors\n  \"alcohol_frequency\" = \"Alcohol Frequency\",\n  \"alcohol_intensity\" = \"Alcohol Intensity\",\n  \"hlth_disability_binary\" = \"Disability Status\",\n  \"smoker_binary\" = \"Smoker\",\n  \"hours_exercise\" = \"Hours of Exercise\",\n  \n  \n  # time use\n  \"hours_children\" = \"Hours with Children\",\n  \"hours_commute\" = \"Hours Commuting\",\n  \"hours_exercise\" = \"Hours Exercising\",\n  \"hours_housework\" = \"Hours on Housework\",\n  \"log_hours_children\" = \"Log Hours with Children\",\n  \"log_hours_commute\" = \"Log Hours Commuting\",\n  \"log_hours_exercise\" = \"Log Hours Exercising\",\n  \"log_hours_housework\" = \"Log Hours on Housework\"\n)\nhere_save(var_labels_baseline, \"var_labels_baseline\")\n\n# outcome variable labels, organized by domain\n# reivew your outcomes make sure they appear on the list below\n# comment out what you do not need\noutcome_vars\n\n# get names\nvar_labels_outcomes &lt;- list(\n  # \"alcohol_frequency_weekly\" = \"Alcohol Frequency (weekly)\",\n  # \"alcohol_intensity\" = \"Alcohol Intensity\",\n  # \"hlth_bmi\" = \"Body Mass Index\",\n  # \"hlth_sleep_hours\" = \"Sleep\",\n  \"log_hours_exercise\" = \"Hours of Exercise (log)\",\n # \"short_form_health\" = \"Short Form Health\",\n  \"hlth_fatigue\" = \"Fatigue\",\n  \"kessler_latent_anxiety\" = \"Anxiety\",\n  \"kessler_latent_depression\" = \"Depression\",\n # \"rumination\" = \"Rumination\",\n  \"bodysat\" = \"Body Satisfaction\",\n # \"forgiveness\" = \"Forgiveness\",\n # \"perfectionism\" = \"Perfectionism\",\n # \"self_control\" = \"Self Control\",\n  \"self_esteem\" = \"Self Esteem\",\n  \"sexual_satisfaction\" = \"Sexual Satisfaction\",\n # \"gratitude\" = \"Gratitude\",\n  \"lifesat\" = \"Life Satisfaction\",\n  \"meaning_purpose\" = \"Meaning: Purpose\",\n  \"meaning_sense\" = \"Meaning: Sense\",\n  \"pwi = Personal Well-being Index\",\n  \"belong\" = \"Social Belonging\",\n  \"neighbourhood_community\" = \"Neighbourhood Community\",\n  \"support\" = \"Social Support\"\n)\n\n# save for manuscript\nhere_save(var_labels_outcomes, \"var_labels_outcomes\")\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n# tables ------------------------------------------------------------------\n# create baseline characteristics table\ndat_baseline = dat_long_final |&gt;\n  filter(wave %in% c(baseline_wave)) |&gt;\n  mutate(\n    male_binary = factor(male_binary),\n    parent_binary = factor(parent_binary),\n    smoker_binary = factor(smoker_binary),\n    born_nz_binary = factor(born_nz_binary),\n    employed_binary = factor(employed_binary),\n    not_heterosexual_binary = factor(not_heterosexual_binary),\n    sample_frame_opt_in_binary = factor(sample_frame_opt_in_binary)\n  )\n\n\n# save sample weights from baseline wave\n# save sample weights\nt0_sample_weights &lt;- dat_baseline$sample_weights\nhere_save(t0_sample_weights, \"t0_sample_weights\")\n\n\n\nbaseline_table &lt;- margot::margot_make_tables(\n  data = dat_baseline,\n  vars = baseline_vars,\n  by = \"wave\",\n  labels = var_labels_baseline,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(baseline_table)\nmargot::here_save(baseline_table, \"baseline_table\", push_mods)\n\n# create exposure table by wave\nexposure_table &lt;- margot::margot_make_tables(\n  data = dat_long_final |&gt; filter(wave %in% c(baseline_wave, exposure_waves)),\n  vars = exposure_var,\n  by = \"wave\",\n  labels = var_labels_exposure,\n  factor_vars = exposure_var_binary,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(exposure_table)\nmargot::here_save(exposure_table, \"exposure_table\", push_mods)\n\n# create outcomes table by wave\noutcomes_table &lt;- margot::margot_make_tables(\n  data = dat_long_final |&gt; filter(wave %in% c(baseline_wave, outcome_wave)),\n  vars = outcome_vars,\n  by = \"wave\",\n  labels = var_labels_outcomes,\n  format = \"markdown\"\n)\nprint(outcomes_table)\nmargot::here_save(outcomes_table, \"outcomes_table\", push_mods)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n\n\n# note: completed data preparation step -------------------------------------\n# you're now ready for the next steps:\n# 1. creating wide-format dataset for analysis \n# 2. applying causal inference methods\n# 3. conducting sensitivity analyses\n\n# key decisions summary:\n# exposure variable: extraversion\n# study waves: baseline (2018), exposure (2019), outcome (2020)\n# baseline covariates: demographics, traits, health measures (excluding exposure)\n# outcomes: health, psychological, wellbeing, and social variables\n# binary cutpoint for exposure: here, 4 on the extraversion scale\n# label names for tables\n\n\n\n\n# THIS IS FOR INTEREST ONLY ----------------------------------------------------\n# uncomment to view random chang in individuals\n# visualise individual changes in exposure over time ------------------------\n# useful for understanding exposure dynamics\n# individual_plot &lt;- margot_plot_individual_responses(\n#   dat_long_1,\n#   y_vars = name_exposure,\n#   id_col = \"id\",\n#   waves = c(2018:2019),\n#   random_draws = 56,  # number of randomly selected individuals to show\n#   theme = theme_classic(),\n#   scale_range = c(1, 7),  # range of the exposure variable\n#   full_response_scale = TRUE,\n#   seed = 123\n# )\n# print(individual_plot)"
  },
  {
    "objectID": "content/09-content.html#script-2-is-here",
    "href": "content/09-content.html#script-2-is-here",
    "title": "Causal inference: a step by step guide",
    "section": "Script 2 is here:",
    "text": "Script 2 is here:\n\n\nCode\n# script 2: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# libraries ---------------------------------------------------------------\nif (!requireNamespace(\"margot\", quietly = TRUE)) {\n  message(\"installing 'margot' from GitHub\")\n  devtools::install_github(\"go-bayes/margot\", upgrade = \"never\")\n}\nlibrary(margot)\n\n# install and load other packages from CRAN if missing\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\n# install and load other packages from CRAN if missing\nif (!requireNamespace(\"grf\", quietly = TRUE)) {\n  install.packages(\"grf\")\n}\nlibrary(grf)\n\n\n# required version of margot ----------------------------------------------\n\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.36\") {\n  stop(\"please install margot &gt;= 1.0.36 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\nlibrary(\"margot\")\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf,             # machine learning forests\n  kableExtra,      # tables\n  ggplot2,         # graphs\n  doParallel,      # parallel processing\n  grf,             # causal forests\n  janitor,         # variables names\n  stringr,         # variable names\n  patchwork,       # graphs\n  table1           # tables\n)\n\n# save paths -------------------------------------------------------------------\npush_mods &lt;- here::here(\"save_directory\") \n\n# read data\ndat_long_final &lt;- margot::here_read(\"dat_long_final\")\n\n# read baseline sample weights\nt0_sample_weights &lt;- margot::here_read(\"t0_sample_weights\")\n\n# read exposure\nname_exposure &lt;- margot::here_read(\"name_exposure\")\nname_exposure_binary = paste0(name_exposure, \"_binary\")\nname_exposure_continuous = name_exposure\n\n# read variables\nbaseline_vars &lt;- margot::here_read(\"baseline_vars\")\nexposure_var &lt;- margot::here_read(\"exposure_var\")\noutcome_vars &lt;- margot::here_read(\"outcome_vars\")\nbaseline_wave &lt;- margot::here_read(\"baseline_wave\")\nexposure_waves &lt;- margot::here_read(\"exposure_waves\")\noutcome_wave &lt;- margot::here_read(\"outcome_wave\")\n\n# define continuous columns to keep\ncontinuous_columns_keep &lt;- c(\"t0_sample_weights\")\n\n# define ordinal columns that we will expand into binary variables\nordinal_columns &lt;- c(\"t0_education_level_coarsen\",\n                     \"t0_eth_cat\",\n                     \"t0_rural_gch_2018_l\")\n\n\n\n#check\nname_exposure_binary\nname_exposure_continuous\n\n# define wide variable names\nt0_name_exposure_binary &lt;- paste0(\"t0_\", name_exposure_binary)\nt0_name_exposure_binary\n\n# make exposure names (continuous not genreally used)\nt1_name_exposure_binary &lt;- paste0(\"t1_\", name_exposure_binary)\nt1_name_exposure_binary\n\n# treatments (continuous verion)\nt0_name_exposure &lt;- paste0(\"t0_\", name_exposure_continuous)\nt1_name_exposure &lt;- paste0(\"t1_\", name_exposure_continuous)\nt0_name_exposure_continuous &lt;- paste0(\"t0_\", name_exposure)\nt1_name_exposure_continuous &lt;- paste0(\"t1_\", name_exposure)\n\n# raw outcomes\n# read health outcomes\noutcome_vars &lt;- here_read(\"outcome_vars\")\nt2_outcome_z &lt;- paste0(\"t2_\", outcome_vars, \"_z\")\n\n# view\nt2_outcome_z\n\n# check\nstr(dat_long_final)\n\n# check\nnaniar::gg_miss_var(dat_long_final)\n\n# impute data --------------------------------------------------------------\n# ordinal use\nordinal_columns &lt;- c(\n  \"t0_education_level_coarsen\",\n  \"t0_eth_cat\",\n  \"t0_rural_gch_2018_l\",\n  \"t0_gen_cohort\"\n)\n\n# define cols we will not standardise\ncontinuous_columns_keep &lt;- c(\"t0_sample_weights\")\n\n# remove sample weights\ndat_long_final_2 &lt;- dat_long_final |&gt; select(-sample_weights)\n\n# prepare data for analysis ----------------------\ndat_long_final_2 &lt;- margot::remove_numeric_attributes(dat_long_final_2)\n# wide data\ndf_wide &lt;- margot_wide_machine(\n  dat_long_final,\n  id = \"id\",\n  wave = \"wave\",\n  baseline_vars,\n  exposure_var = exposure_var,\n  outcome_vars,\n  confounder_vars = NULL,\n  imputation_method = \"none\",\n  include_exposure_var_baseline = TRUE,\n  include_outcome_vars_baseline = TRUE,\n  extend_baseline = FALSE,\n  include_na_indicators = FALSE\n)\n\n# check\ncolnames(df_wide)\n\n# return sample weights\ndf_wide$t0_sample_weights &lt;-  t0_sample_weights\n\n#df_wide &lt;- margot::here_read(\"df_wide\")\nnaniar::vis_miss(df_wide, warn_large_data = FALSE)\n\n# order data with missingness assigned to work with grf and lmtp\n# if any outcome is censored all are censored\n# create version for model reports\n\n# check\ncolnames(df_wide)\n\n\n# made data wide in correct format\n# ** ignore warning *** \ndf_wide_encoded  &lt;- margot::margot_process_longitudinal_data_wider(\n  df_wide,\n  ordinal_columns = ordinal_columns,\n  continuous_columns_keep = continuous_columns_keep,\n  not_lost_in_following_wave = \"not_lost_following_wave\",\n  lost_in_following_wave = \"lost_following_wave\",\n  remove_selected_columns = TRUE,\n  exposure_var = exposure_var,\n  scale_continuous = TRUE,\n  censored_if_any_lost = FALSE\n)\n\n# check\ncolnames(df_wide_encoded)\n\n# check\ntable(df_wide_encoded$t0_not_lost_following_wave)\n\n# make the binary variable numeric\ndf_wide_encoded[[t0_name_exposure_binary]] &lt;-\n  as.numeric(df_wide_encoded[[t0_name_exposure_binary]]) - 1\ndf_wide_encoded[[t1_name_exposure_binary]] &lt;-\n  as.numeric(df_wide_encoded[[t1_name_exposure_binary]]) - 1\n\n# view\ndf_wide_encoded[[t0_name_exposure_binary]]\ndf_wide_encoded[[t1_name_exposure_binary]]\n\n# 1. ensure both binaries only take values 0 or 1 (ignore NA)\nstopifnot(all(df_wide_encoded[[t0_name_exposure_binary]][!is.na(df_wide_encoded[[t0_name_exposure_binary]])] %in% 0:1),\n          all(df_wide_encoded[[t1_name_exposure_binary]][!is.na(df_wide_encoded[[t1_name_exposure_binary]])] %in% 0:1))\n\n# 2. ensure NA‐patterns match between t1_exposure and t0_lost flag\n# count n-as in t1 exposure\nn_na_t1 &lt;- sum(is.na(df_wide_encoded[[t1_name_exposure_binary]]))\n\n# count how many were lost at t0\nn_lost_t0 &lt;- sum(df_wide_encoded$t0_lost_following_wave == 1, na.rm = TRUE)\n\n# print them for inspection\nmessage(\"NAs in \", t1_name_exposure_binary, \": \", n_na_t1)\nmessage(\"t0_lost_following_wave == 1: \", n_lost_t0)\n\n# stop if they don’t match\nstopifnot(n_na_t1 == n_lost_t0)\n\n# 3. ensure if t1 is non‐NA then subject was not lost at t0\nstopifnot(all(is.na(df_wide_encoded[[t1_name_exposure_binary]]) |\n                df_wide_encoded[[\"t0_not_lost_following_wave\"]] == 1))\n\n# view\nhead(df_wide_encoded)\n\n#naniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\nnaniar::gg_miss_var(df_wide_encoded)\n\n\n# predict attrition and create censoring weights --------------------------\n# step 1: prepare baseline covariates\n# select all t0_ variables except the exposure binary and any _lost indicators, then sort their names\nt0_var_names &lt;- df_wide_encoded |&gt;\n  select(-all_of(t0_name_exposure_binary)) |&gt;\n  select(starts_with(\"t0_\"),-ends_with(\"_lost\"),-ends_with(\"lost_following_wave\"), -ends_with(\"_weights\")) |&gt;\n  colnames() |&gt;\n  sort()\n\n# get unique values (to be safe)\nE &lt;- unique(t0_var_names)\n\n# view\nprint(E)\n\n# save baseline covariates\nmargot::here_save(E, \"E\")\n\n# view\nprint(E)\n\n# step 2: calculate weights for t0\nD_0 &lt;- as.factor(df_wide_encoded$t0_lost_following_wave)\n\n# get co-variates\ncen_0 &lt;- df_wide_encoded[, E]\n\n# probability forest for censoring\n# this will take time\ncen_forest_0 &lt;- probability_forest(cen_0, D_0)\n\n# get predictions\npredictions_grf_0 &lt;- predict(cen_forest_0, newdata = cen_0, type = \"response\")\n\n# get propensity scores\npscore_0 &lt;- predictions_grf_0$pred[, 2]\n\n# use margot_adjust_weights for t0\nt0_weights &lt;- margot_adjust_weights(\n  pscore = pscore_0,\n  trim = TRUE,\n  normalize = TRUE,\n  # lower trimming\n  lower_percentile = 0.00,\n  # upper trimming\n  upper_percentile = 0.99,\n  censoring_indicator = df_wide_encoded$t0_lost_following_wave,\n  sample_weights = df_wide_encoded$t0_sample_weights\n)\n\n# view\nhist(t0_weights$adjusted_weights)\n\n# give weights\ndf_wide_encoded$t0_adjusted_weights &lt;- t0_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\n\n# remove lost next wave (censored)\ndf_wide_encoded_1 &lt;- df_wide_encoded %&gt;%\n  filter(t0_lost_following_wave == 0) %&gt;%\n  droplevels()\n\n# step 4: calculate weights for t1\nE_and_exposure &lt;- c(E, t1_name_exposure_continuous)\nD_1 &lt;- as.factor(df_wide_encoded_1$t1_lost_following_wave)\ncen_1 &lt;- df_wide_encoded_1[, E_and_exposure]\n\n# probability forest for censoring\n#  *** this will take time ***\ncen_forest_1 &lt;- probability_forest(cen_1, D_1, sample.weights = df_wide_encoded_1$t0_adjusted_weights)\n\n# predict forest\npredictions_grf_1 &lt;- predict(cen_forest_1, newdata = cen_1, type = \"response\")\n\n# get propensity score\npscore_1 &lt;- predictions_grf_1$pred[, 2]\n\n# check\nhist(pscore_1)\n\n# use margot_adjust_weights for t1\nt1_weights &lt;- margot_adjust_weights(\n  pscore = pscore_1,\n  trim = TRUE,\n  normalize = TRUE,\n  lower_percentile = 0.00,\n  # upper trimming\n  upper_percentile = 0.99,\n  censoring_indicator = df_wide_encoded_1$t1_lost_following_wave,\n  sample_weights = df_wide_encoded_1$t0_adjusted_weights # combine with weights\n)\n\n# add weights -- these will be the weights we use\ndf_wide_encoded_1$t1_adjusted_weights &lt;- t1_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded_1, warn_large_data = FALSE)\n\n# save\nhere_save(df_wide_encoded_1, \"df_wide_encoded_1\")\n\n# check names\ncolnames(df_wide_encoded_1)\n\n# check\ndf_wide_encoded_1[[t1_name_exposure_binary]]\n\n# step 5: prepare final dataset\nnrow(df_wide_encoded_1)\ntable(df_wide_encoded_1$t1_lost_following_wave)\n\n# arrange\ndf_grf &lt;- df_wide_encoded_1 |&gt;\n  filter(t1_lost_following_wave == 0) |&gt;\n  select(\n    where(is.factor),\n    ends_with(\"_binary\"),\n    ends_with(\"_lost_following_wave\"),\n    ends_with(\"_z\"),\n    ends_with(\"_weights\"),\n    starts_with(\"t0_\"),\n    starts_with(\"t1_\"),\n    starts_with(\"t2_\"),\n  ) |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\")) |&gt;\n  relocate(starts_with(\"t1_\"), .before = starts_with(\"t2_\")) |&gt;\n  relocate(\"t0_not_lost_following_wave\", .before = starts_with(\"t1_\")) |&gt;\n  relocate(all_of(t1_name_exposure_binary), .before = starts_with(\"t2_\")) |&gt;\n  droplevels()\n\n# save final data\nmargot::here_save(df_grf, \"df_grf\")\ndf_grf &lt;- margot::here_read(\"df_grf\")\n\n# check final dataset\ncolnames(df_grf)\n\n# visualise missing\n# should have no missing in t1 and t2 variables\n# handled by IPCW\n# make final missing data graph\nmissing_final_data_plot &lt;- naniar::vis_miss(df_grf, warn_large_data = FALSE)\nmissing_final_data_plot\n\n# save plot\nmargot_save_png(missing_final_data_plot, prefix = \"missing_final_data\")\n\n#checks\ncolnames(df_grf)\nstr(df_grf)\n\n# check exposures\ntable(df_grf[[t1_name_exposure_binary]])\n\n# check\nhist(df_grf$t1_adjusted_weights)\n\n# calculate summary statistics\nt0_weight_summary &lt;- summary(df_wide_encoded)\n\n# check\nglimpse(df_grf$t1_adjusted_weights)\n\n# visualise weight distributions\nhist(df_grf$t1_adjusted_weights, main = \"t0_stabalised weights\", xlab = \"Weight\")\n\n# check n\nn_observed_grf &lt;- nrow(df_grf)\n\n# view\nn_observed_grf\n\n# save\nmargot::here_save(n_observed_grf, \"n_observed_grf\")\n\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n\n# this is just for your interest ------------------------------------------\n# not used in final manuscript\n# FOR INTEREESTS\n# inspect propensity scores -----------------------------------------------\n# get data\n# df_grf &lt;- here_read('df_grf')\n# \n# # assign weights var name\n# weights_var_name = \"t0_adjusted_weights\"\n# \n# # baseline covariates  # E already exists and is defined\n# E\n# \n# # must be a data frame, no NA in exposure\n# \n# # df_grf is a data frame - we must process this data frame in several steps\n# # user to specify which columns are outcomes, default to 'starts_with(\"t2_\")'\n# df_propensity_org &lt;- df_grf |&gt; select(!starts_with(\"t2_\"))\n# \n# # Remove NAs and print message that this has been done\n# df_propensity &lt;- df_propensity_org |&gt; drop_na() |&gt; droplevels()\n# \n# # E_propensity_names\n# # first run model for baseline propensity if this is selected.  The default should be to not select it.\n# propensity_model_and_plots &lt;- margot_propensity_model_and_plots(\n#   df_propensity = df_propensity,\n#   exposure_variable = t1_name_exposure_binary,\n#   baseline_vars = E,\n#   weights_var_name = weights_var_name,\n#   estimand = \"ATE\",\n#   method = \"ebal\",\n#   focal = NULL\n# )\n# \n# # visualise\n# summary(propensity_model_and_plots$match_propensity)\n# \n# # key plot\n# propensity_model_and_plots$love_plot\n# \n# # other plots\n# propensity_model_and_plots$summary_plot\n# propensity_model_and_plots$balance_table\n# propensity_model_and_plots$diagnostics\n# \n# \n# # check size\n# size_bytes &lt;- object.size(propensity_model_and_plots)\n# print(size_bytes, units = \"auto\") # Mb\n# \n# # use qs to save only if you have space\n# here_save_qs(propensity_model_and_plots,\n#              \"propensity_model_and_plots\",\n#              push_mods)"
  },
  {
    "objectID": "content/09-content.html#script-3-is-here",
    "href": "content/09-content.html#script-3-is-here",
    "title": "Causal inference: a step by step guide",
    "section": "Script 3 is here:",
    "text": "Script 3 is here:\n\n\nCode\n# script 3: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session\n\nrstudioapi::restartSession()\n\n\n\n# reproducibility ---------------------------------------------------------\n\n\nset.seed(123)\n\n\n# essential library ---------------------------------------------------------\n\n\n\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.36\") {\n  stop(\"please install margot &gt;= 1.0.36 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n# check package version\npackageVersion(pkg = \"margot\")\n\n\n\n# load libraries ----------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  margot,          # off-cran causal workflow tools\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf, ranger,     # machine learning forests\n  doParallel,      # parallel processing,\n  kableExtra,\n  ggplot2 ,        # graphs\n  rlang ,          # functions for base types/Core R/ 'Tidyverse'\n  purrr ,          # functional programming tools.\n  patchwork,      # nice graph placement\n  janitor,         # nice labels\n  glue            # format/ interpolate a string\n)\n\n\n\n# directory path configuration -----------------------------------------------\n# save path (customise for your own computer) ----------------------------\npush_mods &lt;- here::here(\"save_directory\") \n\n# read original data (for plots) ------------------------------------------\noriginal_df &lt;- margot::here_read(\"df_wide\", push_mods)\n\n# plot title --------------------------------------------------------------\ntitle_binary = \"Effects of {{name_exposure}} on {{name_outcomes}}\"\nfilename_prefix = \"grf_extraversion_wb\"\n\n# for manuscript later\nmargot::here_save(title_binary,\"title_binary\")\n\n# import names ------------------------------------------------------------\nname_exposure &lt;- margot::here_read(\"name_exposure\")\nname_exposure\n\n# make exposure names\nt1_name_exposure_binary &lt;- paste0(\"t1_\", name_exposure, \"_binary\")\n\n# check exposure name\nt1_name_exposure_binary\n\n# read outcome vars\noutcome_vars &lt;- margot::here_read(\"outcome_vars\")\n\n# read and sort outcome variables -----------------------------------------\n# we do this by domain: health, psych, present, life, social\nread_and_sort &lt;- function(key) {\n  raw  &lt;- margot::here_read(key, push_mods)\n  vars &lt;- paste0(\"t2_\", raw, \"_z\")\n  sort(vars)\n}\nt2_outcome_z  &lt;- read_and_sort(\"outcome_vars\")\n\n# view\nt2_outcome_z\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# define names for titles -------------------------------------------------\n\nnice_exposure_name = \"Extraversion\"\nnice_outcome_name = \"Wellbeing\"\ntitle = \"Effect of {{nice_exposure_name}} on {{nice_outcome_name}}\"\n\n# save for final rport\nhere_save(title, \"title\")\n\n# combine outcomes ---------------------------------------------------------\n# check outcome vars and make labels for graphs/tables\noutcome_vars\n\n\nlabel_mapping_all &lt;- list(\n  #\"t2_alcohol_frequency_weekly_z\" = \"Alcohol Frequency\",\n  #\"t2_alcohol_intensity_weekly_z\" = \"Alcohol Intensity\",\n  #\"t2_hlth_bmi_z\" = \"BMI\",\n  #\"t2_hlth_sleep_hours_z\" = \"Sleep\",\n  \"t2_log_hours_exercise_z\" = \"Hours of Exercise (log)\",\n  #\"t2_short_form_health_z\" = \"Short Form Health\"\n  \"t2_hlth_fatigue_z\" = \"Fatigue\",\n  \"t2_kessler_latent_anxiety_z\" = \"Anxiety\",\n  \"t2_kessler_latent_depression_z\" = \"Depression\",\n  \"t2_rumination_z\" = \"Rumination\",\n  \"t2_bodysat_z\" = \"Body Satisfaction\",\n  \"t2_foregiveness_z\" = \"Forgiveness\",\n  \"t2_perfectionism_z\" = \"Perfectionism\", \n  \"t2_self_esteem_z\" = \"Self Esteem\",\n  # \"t2_self_control_z\" = \"Self Control\",\n  # \"t2_sexual_satisfaction_z\" = \"Sexual Satisfaction\".\n  \"t2_gratitude_z\" = \"Gratitude\",\n  \"t2_lifesat_z\" = \"Life Satisfaction\",\n  \"t2_meaning_purpose_z\" = \"Meaning: Purpose\",\n  \"t2_meaning_sense_z\" = \"Meaning: Sense\",\n  \"t2_pwi_z\" = \"Personal Well-being Index\",\n  \"t2_belong_z\" = \"Social Belonging\",\n  \"t2_neighbourhood_community_z\" = \"Neighbourhood Community\",\n  \"t2_support_z\" = \"Social Support\"\n)\n\n\n# save\nhere_save(label_mapping_all, \"label_mapping_all\")\n\n# check\nlabel_mapping_all\n\n\n# make options -------------------------------------------------------------\n# titles\ntitle = \"ATE Effects of {{nice_name_exposure}} on {{nice_name_outcome}}\"\nsubtitle = \"\"\nfilename_prefix = \"final_report\"\n\n\n# settings\nx_offset = -.5\nx_lim_lo = -.5\nx_lim_hi = .5\n\n\n# defaults for ate plots\nbase_defaults_binary &lt;- list(\n  type = \"RD\",\n  title = title_binary,\n  e_val_bound_threshold = 1.2,\n  colors = c(\n    \"positive\" = \"#E69F00\",\n    \"not reliable\" = \"grey50\",\n    \"negative\" = \"#56B4E9\"\n  ),\n  x_offset = x_offset,\n  # will be set based on type\n  x_lim_lo = x_lim_lo,\n  # will be set based on type\n  x_lim_hi = x_lim_hi,\n  text_size = 4,\n  linewidth = 0.5,\n  estimate_scale = 1,\n  base_size = 18,\n  point_size = 2,\n  title_size = 19,\n  subtitle_size = 16,\n  legend_text_size = 10,\n  legend_title_size = 10,\n  include_coefficients = FALSE\n)\n\n# health graph options\noutcomes_options_all &lt;- margot_plot_create_options(\n  title = subtitle,\n  base_defaults = base_defaults_binary,\n  subtitle = subtitle,\n  filename_prefix = filename_prefix\n)\n\n\n# policy tree graph settings ----------------------------------------------\n\ndecision_tree_defaults &lt;- list(\n  span_ratio       = .3,\n  text_size        = 3.8,\n  y_padding        = 0.25,\n  edge_label_offset = .002,\n  border_size      = .05\n)\npolicy_tree_defaults &lt;- list(\n  point_alpha       = .5,\n  title_size        = 12,\n  subtitle_size     = 12,\n  axis_title_size   = 12,\n  legend_title_size = 12,\n  split_line_color  = \"red\",\n  split_line_alpha  = .8,\n  split_label_color = \"red\",\n  list(split_label_nudge_factor = 0.007)\n)\n\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +----------------------------------------------+\n# |       DO NOT ALTER  (except where noted)     |\n# +----------------------------------------------+\n\n# load GRF data and prepare inputs ----------------------------------------\ndf_grf &lt;- margot::here_read('df_grf', push_mods)\nE      &lt;- margot::here_read('E',      push_mods)\n# check exposure binary\nstopifnot(all(df_grf[[t1_name_exposure_binary]][!is.na(df_grf[[t1_name_exposure_binary]])] %in% 0:1))\n# set exposure and weights\nW       &lt;- as.vector(df_grf[[t1_name_exposure_binary]]) # note it is the processed weights for attrition \"t1\"\nweights &lt;- df_grf$t1_adjusted_weights\nhist(weights) # quick check for extreme weights\n# select covariates and drop numeric attributes\nX &lt;- margot::remove_numeric_attributes(df_grf[E])\n\n\n# set model defaults -----------------------------------------------------\ngrf_defaults &lt;- list(seed = 123, stabilize.splits = TRUE, num.trees = 2000)\n\n\n# example: fit causal forest on a toy subset ------------------------------\n# first, create a smaller test sample\nn   &lt;- nrow(X)\ntoy &lt;- sample(seq_len(n), floor(n / 4))\n# define toy data\ntoy_data     &lt;- df_grf[toy, ]\nX_toy        &lt;- X[toy, ]\nW_toy        &lt;- W[toy]\nweights_toy  &lt;- weights[toy]\n\n# fit the model\ncf_out &lt;- margot_causal_forest(\n  data         = toy_data,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  outcome_vars = \"t2_kessler_latent_depression_z\", # select variable in your outcome_variable set\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  covariates   = X_toy,\n  W            = W_toy,\n  weights      = weights_toy,\n  save_data    = TRUE,\n  save_models  = TRUE\n)\n\n# inspect Qini curve ------------------------------------------------------\nqini_tbl &lt;- margot::margot_inspect_qini(cf_out, propensity_bounds = c(0.01, 0.97))\n# show\nprint(qini_tbl)\n\n# plot policy-combo trees --------------------------------------------------\ncombo1 &lt;- margot_plot_policy_combo(\n  result_object    = cf_out,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_name       = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  max_depth        = 1L,          # depth-1 tree\n  decision_tree_args = list(text_size = 4),\n  policy_tree_args   = list(point_alpha = 0.7)\n)\n\n# show\ncombo1$combined_plot\n\n# you can repeat for depth-2 ----------------------------------------------\ncombo2 &lt;- margot_plot_policy_combo(\n  result_object    = cf_out,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_name       = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  max_depth        = 2L,\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  original_df        = original_df,\n  label_mapping      = label_mapping_all\n)\n\n# show\ncombo2$combined_plot\n\n# batch plotting ----------------------------------------------------------\nmodels_batch_1L &lt;- margot_policy(\n  cf_out,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names        = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df        = original_df,\n  label_mapping      = label_mapping_all,\n  max_depth          = 1L\n)\n\n# view first model's plots\nmodels_batch_1L[[1]][[3]]  # combo plot\nmodels_batch_1L[[1]][[4]]  # qini plot\n\nmodels_batch_2L &lt;- margot_policy(\n  cf_out,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names        = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df        = original_df,\n  label_mapping      = label_mapping_all,\n  max_depth          = 2L)\n# view first model's plots\nmodels_batch_2L[[1]][[3]]  # combo plot\nmodels_batch_2L[[1]][[4]]  # qini plot\n\n# 2. flip the selected outcomes (and regen trees)\n# use -- when the outcome is undesirable and we want to minimise it \n# (assuming the exposure is something we'd prescribe)\ncf_out_f &lt;- margot_flip_forests(\n  model_results = cf_out,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  flip_outcomes = c(\"t2_kessler_latent_depression_z\"),\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  recalc_policy = TRUE\n)\n\n# where there are very low or high propensity scores (prob of exposure) we might consider trimming\nmargot::margot_inspect_qini(cf_out_f, propensity_bounds = c(0.01, 0.97))\n\n\n# if we had extreme scores (not used here)\n# cf_out_flipped_trimmed &lt;- margot_rescue_qini(model_results      = cf_out_f,\n#                                              propensity_bounds  = c(0.05, 0.95)) \n\n# flipped batch model\nmodels_batch_flipped_2L &lt;- margot_policy(\n  cf_out_f,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names = c(\"model_t2_kessler_latent_depression_z\"),\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth     = 2L\n)\n\n# flipped\n# interpretation: exposure minimising depression\nmodels_batch_flipped_2L[[1]][[3]]\n\n# not flipped: exposure as maximizing depression\nmodels_batch_2L[[1]][[3]]\n\n\n# full model -------------------------------------------------------------\n\n# ** uncomment to run full model**\n\n# # health models -----------------------------------------------------------\nmodels_binary &lt;- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n# save model\nmargot::here_save_qs(models_binary, \"models_binary\", push_mods)\n\n\n# read results ------------------------------------------------------------\n\n# if you save models you do not need to re-run them\nmodels_binary &lt;- margot::here_read_qs(\"models_binary\", push_mods)\n\n\n\n\n# make graphs -------------------------------------------------------------\n# make ate plots ----------------------------------------------------------\n# health plots ------------------------------------------------------------\nbinary_results &lt;- margot_plot(\n  models_binary$combined_table,\n  options = outcomes_options_all,\n  label_mapping = label_mapping_all,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  order = \"evaluebound_asc\",\n  original_df = original_df,\n  e_val_bound_threshold = 1.2\n)\n\n# view\nbinary_results$transformed_table |&gt; rename(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\") |&gt;\n  kbl(format = 'markdown')\n\n# check\nbinary_results$plot\n\n# interpretation\ncat(binary_results$interpretation)\n\n# nice table\ntables_list &lt;- list(\n  Wellbeing = binary_results$transformed_table\n)\n\n# make markdown tables (to be imported into the manuscript)\nmargot_bind_tables_markdown &lt;- margot_bind_tables(\n  tables_list = tables_list,\n  #list(all_models$combined_table),\n  sort_E_val_bound = \"desc\",\n  e_val_bound_threshold = 1.2,\n  # ← choose threshold\n  highlight_color = NULL,\n  bold = TRUE,\n  rename_cols = TRUE,\n  col_renames = list(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\"),\n  rename_ate = TRUE,\n  threshold_col = \"E_Val_bound\",\n  output_format = \"markdown\",\n  kbl_args = list(\n    booktabs = TRUE,\n    caption = NULL,\n    align = NULL\n  )\n)\n\n# view markdown table\nmargot_bind_tables_markdown\n\n# save for publication\nhere_save(margot_bind_tables_markdown, \"margot_bind_tables_markdown\")\n\n\n# count models by category\ncat(\"Number of original models:\\n\", length(models_binary$results), \"\\n\")\n\n\n\n# evaluate models ---------------------------------------------------------\n# trim models if extreme propensity scores dominate\n# diag_tbl_98 &lt;- margot_inspect_qini(models_binary,\n#                                        propensity_bounds = c(0.01, 0.99))\n\n\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n# flipping models: outcomes we want to minimise given the exposure --------\n# standard negative outcomes/  not used in this study\n\n# +--------------------------+\n# |    MODIFY THIS           |\n# +--------------------------+\nflip_outcomes_standard = c(\n  #\"t2_alcohol_frequency_weekly_z\",\n  #\"t2_alcohol_intensity_z\",\n  #\"t2_hlth_bmi_z\",\n  #\"t2_hlth_fatigue_z\",\n  \"t2_kessler_latent_anxiety_z\", #  ← select\n  \"t2_kessler_latent_depression_z\",#  ← select\n  \"t2_rumination_z\" #  ← select\n  #\"t2_perfectionism_z\" # the exposure variable was not investigated\n)\n# +--------------------------+\n# |   END MODIFY             |\n# +--------------------------+\n\n\n# we will investigate losses to these outcomes\n# usual flipped names for positive interventions\n# commented out for this study\n\n# WHICH OUTCOMES -- if any ARE UNDESIREABLE? \n\n# +--------------------------+\n# |    MODIFY THIS           |\n# +--------------------------+\nflipped_names &lt;- c(\n  # v\"Alcohol Frequency\",\n  #  \"Alcohol Intensity\",\n  #  \"BMI\",\n  #  \"Fatigue\",\n  \"Anxiety\", \"Depression\", \"Rumination\"  #  ← select\n  # \"Perfectionism\")\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n)\n\n\n# NOT IF THE EXPOSURE IS NEGATIVE, FOCUS ON WHICH OUTCOMES, if any, ARE POSITIVE AND FLIP THESE?\nflip_outcomes &lt;- flip_outcomes_standard #c( setdiff(t2_outcomes_all, flip_outcomes_standard) )\n\n# check\nflip_outcomes\n\n# checks\n# neg_check &lt;- vapply(all_models$results[ paste0(\"model_\", flip_outcomes) ],\n#                     \\(x) mean(x$tau_hat, na.rm = TRUE) &lt; 0, logical(1))\n# stopifnot(all(neg_check))   # every chosen outcome has a negative mean cate\n\n# get labels\nflipped_names &lt;- margot_get_labels(flip_outcomes, label_mapping_all)\n\n# check\nflipped_names\n\n# save for publication\nhere_save(flipped_names, \"flipped_names\")\n\n\n\n# flip negatively oriented outcomes --------------------------------------\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n\n# flip models using margot's function\n\n#  *** this will take some time ***\n\n# ** give it time **\n# ** once run/ comment out **\nmodels_binary_flipped_all &lt;- margot_flip_forests(models_binary,\n                                                 flip_outcomes = flip_outcomes,\n                                                 recalc_policy = TRUE)\n\n# save\nhere_save_qs(models_binary_flipped_all, \"models_binary_flipped_all\", push_mods)\n\n# read back if needed\nmodels_binary_flipped_all &lt;- here_read_qs(\"models_binary_flipped_all\", push_mods)\n\n\n# where there are very low or high propensity scores (prob of exposure) we might consider trimming\n# margot::margot_inspect_qini(models_binary_flipped_all, propensity_bounds = c(0.05, 0.95))\n# \n# \n# # if we had extreme scores (not used here)\n# models_binary_flipped_all_t &lt;- margot_rescue_qini(model_results      = models_binary_flipped_all,\n#                                              propensity_bounds  = c(0.05, 0.95))\n\n\n\n# omnibus heterogeneity tests --------------------------------------------\n# test for treatment effect heterogeneity across all outcomes\n# result_ominbus_hetero_all &lt;- margot::margot_omnibus_hetero_test(models_binary_flipped_all, \n#                                                                 label_mapping = label_mapping_all)\n# \n# # view results table\n# result_ominbus_hetero_all$summary_table |&gt; kbl(\"markdown\")\n# \n# # view test interpretation\n# cat(result_ominbus_hetero_all$brief_interpretation)\n\n# rate test analysis -----------------------------------------------------\n\n# create rate analysis table\nrate_table_all &lt;- margot_rate(\n  models = models_binary_flipped_all,\n  policy = \"treat_best\",  # or \"withold_best\" but don't attempt fitting curves or policytrees\n  label_mapping = label_mapping_all\n)\n\n# view rate tables\nrate_table_all$rate_autoc |&gt; kbl(\"markdown\")\nrate_table_all$rate_qini |&gt; kbl(\"markdown\")\n\n# generate interpretation\nrate_interpretation_all &lt;- margot_interpret_rate(\n  rate_table_all, \n  flipped_outcomes = flipped_names\n)\n\n# view interpretations\ncat(rate_interpretation_all$autoc_results)\ncat(rate_interpretation_all$qini_results)\n\n# compare rate and qini -- see grf documentation\ncat(rate_interpretation_all$comparison)\n\n# check out model names for different ways of thinking about heterogeneity\nrate_interpretation_all$either_model_names\nrate_interpretation_all$qini_model_names\nrate_interpretation_all$both_model_names\nrate_interpretation_all$autoc_model_names\n\n# autoc plots ------------------------------------------------------------\n# generate batch rate plots for models with significant heterogeneity\nbatch_rate_autoc_plots &lt;- margot_plot_rate_batch(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  # just use rate autoc for rate plots\n  model_names = rate_interpretation_all$autoc_model_names\n)\n\n# extract individual plots from the batch result\nautoc_plots &lt;- batch_rate_autoc_plots\n\n# determine number of columns based on number of plots\nnum_cols &lt;- ifelse(length(autoc_plots) &gt; 3, 2, 1)\n\n# combine plots using patchwork\nlibrary(patchwork)\n\n# only proceed if there are plots to combine\nif (length(autoc_plots) &gt; 0) {\n  # initialize with first plot\n  combined_autoc_plot &lt;- autoc_plots[[1]]\n  \n  # add remaining plots if any\n  if (length(autoc_plots) &gt; 1) {\n    for (i in 2:length(autoc_plots)) {\n      combined_autoc_plot &lt;- combined_autoc_plot + autoc_plots[[i]]\n    }\n  }\n  \n  # apply the dynamic layout\n  combined_autoc_plot &lt;- combined_autoc_plot +\n    plot_layout(ncol = num_cols) &\n    plot_annotation(\n      title = \"AUTOC Model Plots\",\n      subtitle = paste0(length(autoc_plots), \" models with significant heterogeneity\"),\n      tag_levels = \"A\"\n    )\n  \n  # view the combined plot\n  print(combined_autoc_plot)\n  \n  # save the combined plot if needed\n  width &lt;- ifelse(num_cols == 1, 8, 12)\n  height &lt;- 6 * ceiling(length(autoc_plots) / num_cols)\n  \n  ggsave(\n    here::here(push_mods, \"combined_autoc_plots.pdf\"),\n    combined_autoc_plot,\n    width = width,\n    height = height\n  )\n} else {\n  # handle case with no plots\n  message(\"No AUTOC plots available\")\n}\n\nmodels_batch_qini_2L_test &lt;- margot_plot_policy_combo(\n  models_binary_flipped_all,\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_name =  \"model_t2_log_hours_exercise_z\",\n  max_depth  = 2L,\n  # ← new argument\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\nrate_interpretation_all$qini_model_names\n\n# qini --------------------------------------------------------------------\n# run the margot_policy function\nmodels_batch_qini_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$qini_model_names,\n  max_depth  = 2L,\n  # ← new argument\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\n\n# extract the plots from the results\nplots &lt;- lapply(seq_along(models_batch_qini_2L), function(i) {\n  models_batch_qini_2L[[i]][[4]]  # extract the 4th element (plot) from each model\n})\n\n# name the plots\nnames(plots) &lt;- rate_interpretation_all$qini_model_names\n\n# determine number of columns based on number of plots\nnum_cols &lt;- ifelse(length(plots) &gt; 3, 2, 1)\n\n# load the patchwork library for combining plots\nlibrary(patchwork)\n\n# check if there are any plots to combine\nif (length(plots) == 0) {\n  message(\"no plots available to combine\")\n  NULL  # removed return since this isn't in a function\n} else {\n  # create combined plot\n  combined_plot &lt;- plots[[1]]\n  \n  # only run the loop if there are at least 2 plots\n  if (length(plots) &gt; 1) {\n    for (i in 2:length(plots)) {\n      combined_plot &lt;- combined_plot + plots[[i]]\n    }\n  }\n  \n  # apply the dynamic layout\n  combined_plot &lt;- combined_plot + plot_layout(ncol = num_cols)\n  # add titles and annotations\n  combined_plot &lt;- combined_plot &\n    plot_annotation(\n      title = \"Qini Model Plots\",\n      subtitle = paste0(length(plots), \n                        ifelse(length(plots) == 1, \" model \", \" models \"), \n                        \"arranged in \", num_cols, \n                        ifelse(num_cols == 1, \" column\", \" columns\")),\n      tag_levels = \"A\"  # adds a, b, c, etc. to the plots\n    )\n  # view\n  combined_plot\n  # save (optional)\n  width &lt;- ifelse(num_cols == 1, 8, 12)\n  height &lt;- 6 * ceiling(length(plots)/num_cols)  # height per row * number of rows\n  # save\n  ggsave(here::here(push_mods, \"combined_qini_plots.pdf\"),\n         combined_plot,\n         width = width, height = height)\n  \n  combined_plot  # removed return since this isn't in a function\n}\n\n# interpretation ----------------------------------------------------------\n# interpret qini curves\ninterpretation_qini_curves_2L &lt;- margot_interpret_qini(\n  models_batch_qini_2L,\n  model_names = rate_interpretation_all$qini_model_names,\n  label_mapping = label_mapping_all\n)\ninterpretation_qini_curves_2L\n\n# view qini interpretation\ncat(interpretation_qini_curves_2L$qini_explanation)\n\n# view summary table\ninterpretation_qini_curves_2L$summary_table |&gt; kbl(\"markdown\")\n\n\n\n# policy tree analysis ---------------------------------------------------\n# make policy trees\n# 1 l decision trees are generally very bad\nplots_policy_trees_1L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names,\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 1L\n)\n\n# get number of models\nn_models &lt;- length(rate_interpretation_all$either_model_names)\n\n# # use purrr to map through and print each model\n# purrr::map(1:n_models_1L, function(i) {\n#   # print model name as a header\n#   cat(\"# model\", i, \"\\n\")\n#   # print the corresponding model plot\n#   print(plots_policy_trees_1L[[i]][[3]])\n#   # add spacing between models\n#   cat(\"\\n\\n\")\n# })\n\nmodel_outputs &lt;- purrr::map(1:n_models, ~plots_policy_trees_1L[[.x]][[3]])\n\n# name the list elements by model number\nnames(model_outputs) &lt;- paste0(\"model_\", 1:n_models)\n\n\nmodel_outputs$model_1\n\n# policy tree analysis ---------------------------------------------------\n# make policy trees\n# *** 2l is much more persuasive ***\nplots_policy_trees_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names,\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 2L\n)\n\nn_models &lt;- length(rate_interpretation_all$either_model_names)\n\nmodel_outputs_2L &lt;- purrr::map(1:n_models, ~plots_policy_trees_2L[[.x]][[3]])\n\nmodel_outputs_2L[[1]]\n\ninterpret_plots_policy_trees_2L &lt;- margot_interpret_policy_batch(\n  models_binary_flipped_all, model_names = rate_interpretation_all$either_model_names)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_2L)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n# you can investigate policy trees for all outcomes, mindful that the rate and qini are not reliable. \n# still, with appropriate caution, this may help to clarify psychologically interesting questions\n\nall_plots_policy_trees_1L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # model_names = rate_interpretation_all$either_model_names, # use all\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 1L\n)\nn_models &lt;- length(models_binary_flipped_all$results)\nn_models\n\n\nmodel_outputs_1L_all &lt;- purrr::map(1:n_models, ~all_plots_policy_trees_1L[[.x]][[3]])\n\n# view\nmodel_outputs_1L_all[[1]] # ← not convincing \nmodel_outputs_1L_all[[2]] # ←  \nmodel_outputs_1L_all[[3]] # ← not convincing  \nmodel_outputs_1L_all[[4]] # ← not convincing \nmodel_outputs_1L_all[[5]] # ← not convincing \nmodel_outputs_1L_all[[6]] # ← not convincing  \nmodel_outputs_1L_all[[7]] # ← not convincing  \nmodel_outputs_1L_all[[8]] #\nmodel_outputs_1L_all[[9]] # ← not convincing  \nmodel_outputs_1L_all[[10]] # ← not convincing  \nmodel_outputs_1L_all[[11]] # ← not convincing  \nmodel_outputs_1L_all[[12]] # ← not convincing  \nmodel_outputs_1L_all[[13]] # ← not convincing  \n\n\n\n# interpretation\ninterpret_plots_policy_trees_1L_all &lt;- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 1)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_1L_all)\n\n\n# ALL model 1L\nall_plots_policy_trees_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # model_names = rate_interpretation_all$either_model_names, # use all\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 2L\n)\nn_models &lt;- length(models_binary_flipped_all$results)\nn_models\n\n\nmodel_outputs_2L_all &lt;- purrr::map(1:n_models, ~all_plots_policy_trees_2L[[.x]][[3]])\n\n# view\nmodel_outputs_2L_all[[1]] # ← \nmodel_outputs_2L_all[[2]] # ← \nmodel_outputs_2L_all[[3]] # \nmodel_outputs_2L_all[[4]] # \nmodel_outputs_2L_all[[5]] # ← \nmodel_outputs_2L_all[[6]] # ←\nmodel_outputs_2L_all[[7]] # ← \nmodel_outputs_2L_all[[8]] #\nmodel_outputs_2L_all[[9]] # ←\nmodel_outputs_2L_all[[10]] # \nmodel_outputs_2L_all[[11]] #  \nmodel_outputs_2L_all[[12]] # ← not convincing \nmodel_outputs_2L_all[[13]] # \n\n\n# interpretation\ninterpret_plots_policy_trees_2L_all &lt;- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 2)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_2L_all)\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n\n\n\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n#############################################################################\n# theoretical comparisons ---------------------------------------------------\n# individual theoretical comparisons (if relevant)\n# need to get values for wealth if wealth is compared\n\n# step 1 get information for wealth for conditonal comparisons\nhead(df_grf$t0_log_household_inc_z)\n\n# get mean on original data scale\nlog_mean_inc &lt;- mean(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# get sd on original data scale\nlog_sd_inc &lt;- sd(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# function to get back to data scale\nmargot_back_transform_log_z(\n  log_mean = log_mean_inc,\n  log_sd = log_sd_inc,\n  z_scores = c(-1, 0, 1),\n  label = \"data_scale\"\n)\n\n# define complex conditions for subsetting\ncomplex_condition_political &lt;- X[, \"t0_political_conservative_z\"] &gt; -1 &\n  X[, \"t0_political_conservative_z\"] &lt; 1\n\ncomplex_condition_wealth &lt;- X[, \"t0_log_household_inc_z\"] &gt; -1 &\n  X[, \"t0_log_household_inc_z\"] &lt; 1\n\ncomplex_condition_age &lt;- X[, \"t0_age_z\"] &gt; -1 &\n  X[, \"t0_age_z\"] &lt; 1\n\n# # if we have specific groups to compare\n# complex_condition_age_under_neg_1_sd  &lt;- X[, \"t0_age_z\"] &lt; -1\n# complex_condition_age_gr_eq_neg_1_sd  &lt;- X[, \"t0_age_z\"] &gt; -1\n\n# check ages to get number\nmean(original_df$t0_age) - sd(original_df$t0_age)\nmean(original_df$t0_age) + sd(original_df$t0_age)\n\n\n# wealth subsets\nsubsets_standard_wealth &lt;- list(\n  Poor = list(\n    var = \"t0_log_household_inc_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those HShold income &lt; -1 SD (NZD ~41k)\",\n    label = \"Poor\"  # label remains as is, but could be changed if desired\n  ),\n  MiddleIncome = list(subset_condition = complex_condition_wealth, description = \"Effects among those HS_hold income within +/-1SD (&gt; NZD 41k &lt; NZD 191k)\"),\n  Rich = list(\n    var = \"t0_log_household_inc_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those HS_hold income &gt; +1 SD (NZD 191k)\",\n    label = \"Rich\"\n  )\n)\n\n# political subsets\nsubsets_standard_political &lt;- list(\n  Liberal = list(\n    var = \"t0_political_conservative_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those &lt; -1 SD in political conservativism\",\n    label = \"Liberal\"\n  ),\n  Centrist = list(\n    var = \"t0_political_conservative_z\",\n    # operator = \"&lt;\",\n    subset_condition = complex_condition_political,\n    description = \"Effects among those &gt; -1 SD and &lt; +1 in political conservativism\",\n    label = \"Centrist\"\n  ),\n  Conservative = list(\n    var = \"t0_political_conservative_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those &gt; +1 SD in political conservativism\",\n    label = \"Conservative\"\n  )\n)\n\n\n# political subsets\nsubsets_standard_age &lt;- list(\n  Younger = list(\n    var = \"t0_age_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those &lt; under 35 years old\",\n    label = \"Age &lt; 35\"\n  ),\n  Middle = list(\n    var = \"t0_age_z\",\n    # operator = \"&lt;\",\n    subset_condition = complex_condition_age,\n    description = \"Effects among those 35-62\",\n    label = \"Age 35-62\"\n  ),\n  Older = list(\n    var = \"t0_age_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those &gt; 62\",\n    label = \"Age &gt; 62\"\n  )\n)\n\n\n# gender subsets\nsubsets_standard_gender &lt;- list(\n  Female = list(\n    var = \"t0_male_binary\",\n    value = 0,\n    description = \"Females\"\n  ),\n  Male = list(\n    var = \"t0_male_binary\",\n    value = 1,\n    description = \"Males\"\n  )\n)\n\n# ethnicity subsets\nsubsets_standard_ethnicity &lt;- list(\n  Asian = list(\n    var = \"t0_eth_cat_asian_binary\",\n    value = 1,\n    description = \"Asians\"\n  ),\n  Euro = list(\n    var = \"t0_eth_cat_euro_binary\",\n    value = 1,\n    description = \"Europeans (Pakeha)\"\n  ),\n  Pacific = list(\n    var = \"t0_eth_cat_pacific_binary\",\n    value = 1,\n    description = \"Pacific Peoples\"\n  ),\n  Maori = list(\n    var = \"t0_eth_cat_maori_binary\",\n    value = 1,\n    description = \"Māori\"\n  )\n)\n\n\n# batch planned subgroup analysis -----------------------------------------\n# set up lists of models, names, and subtitles\ndomain_models &lt;- list(\n  models_binary # HERE WE USE THE ORIGINAL MODELS\n)\n\n\n# set up domain names\ndomain_names &lt;- c(\"wellbeing\")\n\n# set up subtitles\nsubtitles &lt;- \"\"\n\n# set up subset types in a list\nsubset_types &lt;- list(\n  wealth = subsets_standard_wealth,\n  ethnicity = subsets_standard_ethnicity,\n  political = subsets_standard_political,\n  gender = subsets_standard_gender,\n  cohort = subsets_standard_age\n)\n\n\n# run model\nplanned_subset_results &lt;- margot_planned_subgroups_batch(\n  domain_models = domain_models,\n  X = X,\n  base_defaults = base_defaults_binary,\n  subset_types = subset_types,\n  original_df = original_df,\n  domain_names = domain_names,\n  subtitles = subtitles\n)\n\n\n# results\ncat(planned_subset_results$wellbeing$wealth$explanation)\ncat(planned_subset_results$wellbeing$ethnicity$explanation)\ncat(planned_subset_results$wellbeing$political$explanation)\ncat(planned_subset_results$wellbeing$gender$explanation)\ncat(planned_subset_results$wellbeing$cohort$explanation)\n\n\n\n# cohort subgroups --------------------------------------------------------\n\n# plots -------------------------------------------------------------------\n# results plots\n# health\nplots_subgroup_wealth&lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$wealth$results$Poor$plot,\n    planned_subset_results$wellbeing$wealth$results$MiddleIncome$plot,\n    planned_subset_results$wellbeing$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Wealth\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nplots_subgroup_wealth\n\n# plots\nplots_subgroup_ethnicity &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$ethnicity$results$Asian$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Euro$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Pacific$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Maori$plot\n    \n  ),\n  ncol = 2\n) +\n  patchwork::plot_annotation(title = \"Ethnicity\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_ethnicity)\n\n# plots\nplots_subgroup_political &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$political$results$Liberal$plot,\n    planned_subset_results$wellbeing$political$results$Centrist$plot,\n    planned_subset_results$wellbeing$political$results$Conservative$plot  \n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Political Orientation\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_political)\n\n# plots\nplots_subgroup_gender &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$gender$results$Female$plot,\n    planned_subset_results$wellbeing$gender$results$Male$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Gender\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_gender)\n\n# plots\nplots_subgroup_cohort &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$cohort$results$`Age &lt; 35`$plot,\n    planned_subset_results$wellbeing$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$wellbeing$cohort$results$`Age &gt; 62`$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Age Cohorts\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_cohort)\n\n\n\n# plot options: showcased ---------------------------------------------\n# default\nmargot_plot_decision_tree(models_binary, \"model_t2_support_z\", )\n# tighten branches for easier viewing in single graphs\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .30,\n  text_size = 3.8,\n  border_size = .1,\n  #  title = \"none\",\n  original_df = original_df\n)\n# colour decision node\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .3,\n  text_size = 4,\n  title = \"New Title\",\n  non_leaf_fill =  \"violet\",\n  original_df = original_df\n)\n# make new title\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .2,\n  text_size = 3,\n  title = \"New Title\",\n  non_leaf_fill =  \"white\",\n  original_df = original_df\n)\n\n# remove title\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  text_size = 5,\n  title = 'none',\n  # set title to none\n  original_df = original_df\n)\n\n\n# adjust only the alpha\nmargot::margot_plot_policy_tree(models_binary_social, \"model_t2_support_z\", point_alpha = .1)"
  },
  {
    "objectID": "content/09-content.html#script-0-synthetic-data-fetch",
    "href": "content/09-content.html#script-0-synthetic-data-fetch",
    "title": "Causal inference: a step by step guide",
    "section": "Script 0: Synthetic Data Fetch",
    "text": "Script 0: Synthetic Data Fetch\nRun in full. No need to change anything.\nThis script prepares your data. Run it once – running it twice would be like turning the ignition off just to start your car again.\n\n\nCode\n# for students: reproducibility is like following a recipe; each step ensures the same result\n# restart fresh session if needed\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# essential library ---------------------------------------------------------\n# install and load 'margot' from GitHub if missing\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.37\") {\n  stop(\"please install margot &gt;= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n# load packages ----------------------------------------------------------\n# install and load other packages from CRAN if missing\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\nif (!requireNamespace(\"qs\", quietly = TRUE)) {\n  install.packages(\"qs\")\n}\nlibrary(qs)\n\nif (!requireNamespace(\"here\", quietly = TRUE)) {\n  install.packages(\"here\")\n}\nlibrary(here)\n\n\n\n# create data directory if it doesn't exist -----------------------------\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\n# define file paths ------------------------------------------------------\n# use here() to build paths relative to your project root\ndata_dir &lt;- here::here(\"data\")\n\n# download synthetic data ------------------------------------------------\n# specify the url for the data file\nurl &lt;- \"https://www.dropbox.com/scl/fi/ru0ecayju04ja8ky1mhel/df_nz_long.qs?rlkey=prpk9a5v4vcg1ilhkgf357dhd&dl=1\"\n\n# download to a temporary file for safety\ntmp_file &lt;- tempfile(fileext = \".qs\")\ndownload.file(url, tmp_file, mode = \"wb\")\n\n# read the data into R using qread\ndf_nz_long &lt;- qread(tmp_file)\n\n# inspect the data -------------------------------------------------------\n# view the first few rows to check it loaded correctly\nprint(head(df_nz_long))\n\n# list column names so you know what variables are available\nprint(colnames(df_nz_long))\n\n# save a copy of the data ------------------------------------------------\n# save the dataset to your data directory for future use\nhere_save_qs(df_nz_long, \"df_nz_long\", data_dir)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+"
  },
  {
    "objectID": "content/09-content.html#script-1-initial-data-wrangling-is-here",
    "href": "content/09-content.html#script-1-initial-data-wrangling-is-here",
    "title": "Causal inference: a step by step guide",
    "section": "Script 1: Initial Data Wrangling is HERE",
    "text": "Script 1: Initial Data Wrangling is HERE\n\n\nCode\n# script 1 workflow lecture 10\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# essential library ---------------------------------------------------------\n# install and load 'margot' from GitHub if missing\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.37\") {\n  stop(\"please install margot &gt;= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf,             # machine learning forests\n  kableExtra,      # tables\n  ggplot2,         # graphs\n  doParallel,       # parallel processing\n  grf,             # causal forests\n  janitor,          # variables names\n  stringr,          # variable names\n  patchwork,        # graphs\n  table1           # tables\n)\n\n\n# create directories --------------------------------------------------------\n# create data directory if it doesn't exist\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\nif (!dir.exists(\"save_directory\")) {\n  dir.create(\"save_directory\")  # first time only: make a folder named 'data'\n}\n\n# set up data directory structure\ndata_dir    &lt;- here::here(\"data\")\npush_mods &lt;- here::here(\"save_directory\") \n\n# load data -----------------------------------------------------------------\ndf_nz_long &lt;- margot::here_read_qs(\"df_nz_long\", data_dir)\n\n# initial data prep ---------------------------------------------------------\n# prepare intial data\n# define labels for rural classification\nrural_labels &lt;- c(\n  \"High Urban Accessibility\", \n  \"Medium Urban Accessibility\",\n  \"Low Urban Accessibility\", \n  \"Remote\", \n  \"Very Remote\"\n)\n\ndat_prep &lt;- df_nz_long |&gt;\n  arrange(id, wave) |&gt;\n  margot::remove_numeric_attributes() |&gt;\n  mutate(\n    # cap extreme values\n    alcohol_intensity = pmin(alcohol_intensity, 15),\n    # flag heavy drinkers: freq ≥3 → 1, ≤2 → 0, else NA\n    heavy_drinker = case_when(\n      alcohol_frequency &gt;= 3 ~ 1,\n      alcohol_frequency &lt;= 2 ~ 0,\n      TRUE                  ~ NA_real_\n    ),\n    # map freq categories to weekly counts\n    alcohol_frequency_weekly = recode(\n      alcohol_frequency,\n      `0` = 0, `1` = 0.25,\n      `2` = 1, `3` = 2.5,\n      `4` = 4.5,\n      .default = NA_real_\n    ),\n    # relabel rural factor\n    rural_gch_2018_l = factor(\n      rural_gch_2018_l,\n      levels = 1:5,\n      labels = rural_labels,\n      ordered = TRUE\n    )\n  ) |&gt;\n  droplevels()\n\n\n\n# view variable names -----------------------------------------------------\nprint(colnames(df_nz_long)) \n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# define study variables ----------------------------------------------------\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n\n# ** key decision 1: define your exposure variable **\nname_exposure &lt;- \"extraversion\"\n\n# exposure variable labels\nvar_labels_exposure &lt;- list(\n  \"extraversion\" = \"Extraversion\",\n  \"extraversion_binary\" = \"Extraversion (binary)\"\n)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n\n# +--------------------------+\n# | OPTIONALLY MODIFY SECTION|\n# +--------------------------+\n\n# **  define your study waves **\nbaseline_wave      &lt;- \"2018\"        # baseline measurement\nexposure_waves     &lt;- c(\"2019\")     # when exposure is measured\noutcome_wave       &lt;- \"2020\"        # when outcomes are measured\nall_waves          &lt;- c(baseline_wave, exposure_waves, outcome_wave)\n\n# **  define baseline covariates **\n# these are demographics, traits, etc. measured at baseline\n\nbaseline_vars &lt;- c(\n  # demographics\n  \"age\", \"born_nz_binary\", \"education_level_coarsen\",\n  \"employed_binary\", \"eth_cat\", \"male_binary\",\n  \"not_heterosexual_binary\", \"parent_binary\", \"partner_binary\",\n  \"rural_gch_2018_l\", \"sample_frame_opt_in_binary\",\n  \n  # personality traits (excluding exposure)\n  \"agreeableness\", \"conscientiousness\", \"neuroticism\", \"openness\",\n  \n  # health and lifestyle\n  \"alcohol_frequency\", \"alcohol_intensity\", \"hlth_disability_binary\",\n  \"log_hours_children\", \"log_hours_commute\", \"log_hours_exercise\",\n  \"log_hours_housework\", \"log_household_inc\",\n  \"short_form_health\", \"smoker_binary\",\n  \n  # social and psychological\n  \"belong\", \"nz_dep2018\", \"nzsei_13_l\",\n  \"political_conservative\", \"religion_identification_level\"\n)\n\n# +--------------------------+\n# |END OPTIONALLY MODIFY SEC.|\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n\n# ** key decision 3: define outcome variables **\n# here, we are focussing on a subset of wellbeing outcomes\n# chose outcomes relevant to * your * study. Might be all/some/none/exactly \n# these:\noutcome_vars &lt;- c(\n  # health outcomes\n  # \"alcohol_frequency_weekly\", \"alcohol_intensity\",\n  # \"hlth_bmi\", \n  \"log_hours_exercise\", \n  # \"hlth_sleep_hours\", \n  # \"short_form_health\",\n  \n  # psychological outcomes\n  # \"hlth_fatigue\", \n  \"kessler_latent_anxiety\", \n  \"kessler_latent_depression\", \n  \"rumination\",\n  \n  # well-being outcomes\n  # \"bodysat\", \n  #\"forgiveness\", \"gratitude\", \n  \"lifesat\", \"meaning_purpose\", \"meaning_sense\", \n  # \"perfectionism\", \n  \"pwi\", \n  #\"self_control\", \n  \"self_esteem\", \n  #\"sexual_satisfaction\",\n  \n  # social outcomes\n  \"belong\", \"neighbourhood_community\", \"support\"\n)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# after selecting your exposure/ baseline / outcome variables do not modify this\n# code\n\n# make binary variable (UNLESS YOUR EXPOSURE IS A BINARY VARIABLE)\nexposure_var_binary = paste0(name_exposure, \"_binary\")\n\n# make exposure variable list (we will keep both the continuous and binary variable)\nexposure_var  &lt;- c(name_exposure, paste0(name_exposure, \"_binary\"))\n\n# sort for easier reference\nbaseline_vars &lt;- sort(baseline_vars)\noutcome_vars &lt;- sort(outcome_vars)\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# save key variables --------------------------------------------------------\nmargot::here_save(name_exposure, \"name_exposure\")\nmargot::here_save(var_labels_exposure,\"var_labels_exposure\")\nmargot::here_save(baseline_vars,\"baseline_vars\")\nmargot::here_save(exposure_var, \"exposure_var\")\nmargot::here_save(exposure_var_binary, \"exposure_var_binary\")\nmargot::here_save(outcome_vars, \"outcome_vars\")\nmargot::here_save(baseline_wave, \"baseline_wave\")\nmargot::here_save(exposure_waves, \"exposure_waves\")\nmargot::here_save(outcome_wave, \"outcome_wave\")\nmargot::here_save(all_waves,\"all_waves\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# | OPTIONALLY MODIFY SECTION|\n# +--------------------------+\n\n# select eligible participants ----------------------------------------------\n# only include participants who have exposure data at baseline\n\n# You might require tighter conditions \n# for example, if you are interested in the effects of hours of childcare, \n# you might want to select only those who were parents at baseline. \n# talk to me if you think you might night tighter eligibility criteria.\n\nids_baseline &lt;- dat_prep |&gt; \n  # allow missing exposure at baseline\n  filter(wave == baseline_wave, !is.na(!!sym(name_exposure))) |&gt; \n  # option: do not allow missing exposure at baseline\n  filter(wave == baseline_wave, !is.na(!!sym(name_exposure))) |&gt; \n  pull(id)\n\n# filter data to include only eligible participants and relevant waves\ndat_long_1 &lt;- dat_prep |&gt; \n  filter(id %in% ids_baseline, wave %in% all_waves) |&gt; \n  droplevels()\n\n# +--------------------------+\n# |END OPTIONALLY MODIFY SEC.|\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n# plot distribution to help with cutpoint decision\n# get exposure wave to inspect exposure variable distribution\ndat_long_exposure &lt;- dat_long_1 |&gt; filter(wave %in% exposure_waves)\n\n# make graph \ngraph_cut &lt;- margot::margot_plot_categorical(\n  dat_long_exposure,\n  col_name         = name_exposure,\n  sd_multipliers = c(-1, 1), # select to suit\n  # either use n_divisions for equal-sized groups:\n  # n_divisions      = 2,\n  # or use custom_breaks for specific values:\n  custom_breaks    = c(1, 4),  # ** adjust as needed **\n  cutpoint_inclusive = \"upper\",\n  show_mean        = TRUE,\n  show_sd          = TRUE\n)\nprint(graph_cut)\n\n# save your graph\nmargot::here_save(graph_cut, \"graph_cut\", push_mods)\n\n# create binary exposure variable based on chosen cutpoint\ndat_long_2 &lt;- margot::create_ordered_variable(\n  dat_long_1,\n  var_name           = name_exposure,\n  custom_breaks      = c(1, 4),  # ** -- adjust based on your decision above -- **\n  cutpoint_inclusive = \"upper\"\n)\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# process binary variables and log-transform --------------------------------\n# convert binary factors to 0/1 format\ndat_long_3 &lt;- margot::margot_process_binary_vars(dat_long_2)\n\n# log-transform hours and income variables: tables for analysis (only logged versions of vars)\ndat_long_final &lt;- margot::margot_log_transform_vars(\n  dat_long_3,\n  vars            = c(starts_with(\"hours_\"), \"household_inc\"),\n  prefix          = \"log_\",\n  keep_original   = FALSE,\n  exceptions = exposure_var # omit original variables\n) |&gt; \n  # select only variables needed for analysis\n  select(all_of(c(baseline_vars, exposure_var, outcome_vars, \"id\", \"wave\", \"year_measured\", \"sample_weights\"))) |&gt; \n  droplevels()\n\n\n# check missing data --------------------------------------------------------\n# this is crucial to understand potential biases\nmissing_summary &lt;- naniar::miss_var_summary(dat_long_final)\nprint(missing_summary)\nmargot::here_save(missing_summary, \"missing_summary\", push_mods)\n\n# visualise missing data pattern\n# ** -- takes a while to render ** \nvis_miss &lt;- naniar::vis_miss(dat_long_final, warn_large_data = FALSE)\nprint(vis_miss)\nmargot::here_save(vis_miss, \"vis_miss\", push_mods)\n\n# calculate percentage of missing data at baseline\ndat_baseline_pct &lt;- dat_long_final |&gt; filter(wave == baseline_wave)\npercent_missing_baseline &lt;- naniar::pct_miss(dat_baseline_pct)\nmargot::here_save(percent_missing_baseline, \"percent_missing_baseline\", push_mods)\n\n# save prepared dataset for next stage --------------------------------------\nmargot::here_save(dat_long_final, \"dat_long_final\", push_mods)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n# create transition matrices to check positivity ----------------------------\n# this helps assess whether there are sufficient observations in all exposure states\ndt_positivity &lt;- dat_long_final |&gt;\n  filter(wave %in% c(baseline_wave, exposure_waves)) |&gt;\n  select(!!sym(name_exposure), id, wave) |&gt;\n  mutate(exposure = round(as.numeric(!!sym(name_exposure)), 0)) |&gt;\n  # create binary exposure based on cutpoint\n  mutate(exposure_binary = ifelse(exposure &gt;= 4, 1, 0)) |&gt; ## *-- modify this --* \n  mutate(wave = as.numeric(wave) -1 )\n\n# create transition tables\ntransition_tables &lt;- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"wave\",\n  table_name = \"transition_table\"\n)\nprint(transition_tables$tables[[1]])\nmargot::here_save(transition_tables, \"transition_tables\", push_mods)\n\n# create binary transition tables\ntransition_tables_binary &lt;- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure_binary\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"wave\",\n  table_name = \"transition_table_binary\"\n)\nprint(transition_tables_binary$tables[[1]])\nmargot::here_save(transition_tables_binary, \"transition_tables_binary\", push_mods)\n\n# create tables -----------------------------------------------------------\n# baseline variable labels\nvar_labels_baseline &lt;- list(\n  # demographics\n  \"age\" = \"Age\",\n  \"born_nz_binary\" = \"Born in NZ\",\n  \"education_level_coarsen\" = \"Education Level\",\n  \"employed_binary\" = \"Employed\",\n  \"eth_cat\" = \"Ethnicity\",\n  \"male_binary\" = \"Male\",\n  \"not_heterosexual_binary\" = \"Non-heterosexual\",\n  \"parent_binary\" = \"Parent\",\n  \"partner_binary\" = \"Has Partner\",\n  \"rural_gch_2018_l\" = \"Rural Classification\",\n  \"sample_frame_opt_in_binary\" = \"Sample Frame Opt-In\",\n  \n  # economic & social status\n  \"household_inc\" = \"Household Income\",\n  \"log_household_inc\" = \"Log Household Income\",\n  \"nz_dep2018\" = \"NZ Deprivation Index\",\n  \"nzsei_13_l\" = \"Occupational Prestige Index\",\n  \"household_inc\" = \"Household Income\",\n\n  \n  # personality traits\n  \"agreeableness\" = \"Agreeableness\",\n  \"conscientiousness\" = \"Conscientiousness\",\n  \"neuroticism\" = \"Neuroticism\",\n  \"openness\" = \"Openness\",\n  \n  # beliefs & attitudes\n  \"political_conservative\" = \"Political Conservatism\",\n  \"religion_identification_level\" = \"Religious Identification\",\n  \n  # health behaviors\n  \"alcohol_frequency\" = \"Alcohol Frequency\",\n  \"alcohol_intensity\" = \"Alcohol Intensity\",\n  \"hlth_disability_binary\" = \"Disability Status\",\n  \"smoker_binary\" = \"Smoker\",\n  \"hours_exercise\" = \"Hours of Exercise\",\n  \n  \n  # time use\n  \"hours_children\" = \"Hours with Children\",\n  \"hours_commute\" = \"Hours Commuting\",\n  \"hours_exercise\" = \"Hours Exercising\",\n  \"hours_housework\" = \"Hours on Housework\",\n  \"log_hours_children\" = \"Log Hours with Children\",\n  \"log_hours_commute\" = \"Log Hours Commuting\",\n  \"log_hours_exercise\" = \"Log Hours Exercising\",\n  \"log_hours_housework\" = \"Log Hours on Housework\"\n)\nhere_save(var_labels_baseline, \"var_labels_baseline\")\n\n# outcome variable labels, organized by domain\n# reivew your outcomes make sure they appear on the list below\n# comment out what you do not need\noutcome_vars\n\n# get names\nvar_labels_outcomes &lt;- list(\n  # \"alcohol_frequency_weekly\" = \"Alcohol Frequency (weekly)\",\n  # \"alcohol_intensity\" = \"Alcohol Intensity\",\n  # \"hlth_bmi\" = \"Body Mass Index\",\n  # \"hlth_sleep_hours\" = \"Sleep\",\n  \"log_hours_exercise\" = \"Hours of Exercise (log)\",\n # \"short_form_health\" = \"Short Form Health\",\n  \"hlth_fatigue\" = \"Fatigue\",\n  \"kessler_latent_anxiety\" = \"Anxiety\",\n  \"kessler_latent_depression\" = \"Depression\",\n # \"rumination\" = \"Rumination\",\n  \"bodysat\" = \"Body Satisfaction\",\n # \"forgiveness\" = \"Forgiveness\",\n # \"perfectionism\" = \"Perfectionism\",\n # \"self_control\" = \"Self Control\",\n  \"self_esteem\" = \"Self Esteem\",\n  \"sexual_satisfaction\" = \"Sexual Satisfaction\",\n # \"gratitude\" = \"Gratitude\",\n  \"lifesat\" = \"Life Satisfaction\",\n  \"meaning_purpose\" = \"Meaning: Purpose\",\n  \"meaning_sense\" = \"Meaning: Sense\",\n  \"pwi = Personal Well-being Index\",\n  \"belong\" = \"Social Belonging\",\n  \"neighbourhood_community\" = \"Neighbourhood Community\",\n  \"support\" = \"Social Support\"\n)\n\n# save for manuscript\nhere_save(var_labels_outcomes, \"var_labels_outcomes\")\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n# tables ------------------------------------------------------------------\n# create baseline characteristics table\ndat_baseline = dat_long_final |&gt;\n  filter(wave %in% c(baseline_wave)) |&gt;\n  mutate(\n    male_binary = factor(male_binary),\n    parent_binary = factor(parent_binary),\n    smoker_binary = factor(smoker_binary),\n    born_nz_binary = factor(born_nz_binary),\n    employed_binary = factor(employed_binary),\n    not_heterosexual_binary = factor(not_heterosexual_binary),\n    sample_frame_opt_in_binary = factor(sample_frame_opt_in_binary)\n  )\n\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n\n# save sample weights from baseline wave\n# save sample weights\nt0_sample_weights &lt;- dat_baseline$sample_weights\nhere_save(t0_sample_weights, \"t0_sample_weights\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# make baseline table -----------------------------------------------------\n\nbaseline_table &lt;- margot::margot_make_tables(\n  data = dat_baseline,\n  vars = baseline_vars,\n  by = \"wave\",\n  labels = var_labels_baseline,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(baseline_table)\nmargot::here_save(baseline_table, \"baseline_table\", push_mods)\n\n# create exposure table by wave\nexposure_table &lt;- margot::margot_make_tables(\n  data = dat_long_final |&gt; filter(wave %in% c(baseline_wave, exposure_waves)),\n  vars = exposure_var,\n  by = \"wave\",\n  labels = var_labels_exposure,\n  factor_vars = exposure_var_binary,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(exposure_table)\nmargot::here_save(exposure_table, \"exposure_table\", push_mods)\n\n# create outcomes table by wave\noutcomes_table &lt;- margot::margot_make_tables(\n  data = dat_long_final |&gt; filter(wave %in% c(baseline_wave, outcome_wave)),\n  vars = outcome_vars,\n  by = \"wave\",\n  labels = var_labels_outcomes,\n  format = \"markdown\"\n)\nprint(outcomes_table)\nmargot::here_save(outcomes_table, \"outcomes_table\", push_mods)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n\n\n# note: completed data preparation step -------------------------------------\n# you're now ready for the next steps:\n# 1. creating wide-format dataset for analysis \n# 2. applying causal inference methods\n# 3. conducting sensitivity analyses\n\n# key decisions summary:\n# exposure variable: extraversion\n# study waves: baseline (2018), exposure (2019), outcome (2020)\n# baseline covariates: demographics, traits, health measures (excluding exposure)\n# outcomes: health, psychological, wellbeing, and social variables\n# binary cutpoint for exposure: here, 4 on the extraversion scale\n# label names for tables\n\n\n\n\n# THIS IS FOR INTEREST ONLY ----------------------------------------------------\n# uncomment to view random chang in individuals\n# visualise individual changes in exposure over time ------------------------\n# useful for understanding exposure dynamics\n# individual_plot &lt;- margot_plot_individual_responses(\n#   dat_long_1,\n#   y_vars = name_exposure,\n#   id_col = \"id\",\n#   waves = c(2018:2019),\n#   random_draws = 56,  # number of randomly selected individuals to show\n#   theme = theme_classic(),\n#   scale_range = c(1, 7),  # range of the exposure variable\n#   full_response_scale = TRUE,\n#   seed = 123\n# )\n# print(individual_plot)"
  },
  {
    "objectID": "content/09-content.html#script-2-make-wide-data-format-with-censoring-weights-is-here",
    "href": "content/09-content.html#script-2-make-wide-data-format-with-censoring-weights-is-here",
    "title": "Causal inference: a step by step guide",
    "section": "Script 2: Make Wide Data Format With Censoring Weights is HERE",
    "text": "Script 2: Make Wide Data Format With Censoring Weights is HERE\n\n\nCode\n# script 2: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# libraries ---------------------------------------------------------------\n# essential library ---------------------------------------------------------\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.37\") {\n  stop(\"please install margot &gt;= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\nlibrary(margot)\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf,             # machine learning forests\n  kableExtra,      # tables\n  ggplot2,         # graphs\n  doParallel,      # parallel processing\n  grf,             # causal forests\n  janitor,         # variables names\n  stringr,         # variable names\n  patchwork,       # graphs\n  table1           # tables\n)\n\n# save paths -------------------------------------------------------------------\npush_mods &lt;- here::here(\"save_directory\") \n\n# read data\ndat_long_final &lt;- margot::here_read(\"dat_long_final\")\n\n# read baseline sample weights\nt0_sample_weights &lt;- margot::here_read(\"t0_sample_weights\")\n\n# read exposure\nname_exposure &lt;- margot::here_read(\"name_exposure\")\nname_exposure_binary = paste0(name_exposure, \"_binary\")\nname_exposure_continuous = name_exposure\n\n# read variables\nbaseline_vars &lt;- margot::here_read(\"baseline_vars\")\nexposure_var &lt;- margot::here_read(\"exposure_var\")\noutcome_vars &lt;- margot::here_read(\"outcome_vars\")\nbaseline_wave &lt;- margot::here_read(\"baseline_wave\")\nexposure_waves &lt;- margot::here_read(\"exposure_waves\")\noutcome_wave &lt;- margot::here_read(\"outcome_wave\")\n\n# define continuous columns to keep\ncontinuous_columns_keep &lt;- c(\"t0_sample_weights\")\n\n# define ordinal columns that we will expand into binary variables\nordinal_columns &lt;- c(\"t0_education_level_coarsen\",\n                     \"t0_eth_cat\",\n                     \"t0_rural_gch_2018_l\")\n\n# check is this the exposure variable that you want? \nname_exposure_binary\nname_exposure_continuous\n\n# define wide variable names\nt0_name_exposure_binary &lt;- paste0(\"t0_\", name_exposure_binary)\nt0_name_exposure_binary\n\n# make exposure names (continuous not genreally used)\nt1_name_exposure_binary &lt;- paste0(\"t1_\", name_exposure_binary)\nt1_name_exposure_binary\n\n# treatments (continuous verion)\nt0_name_exposure &lt;- paste0(\"t0_\", name_exposure_continuous)\nt1_name_exposure &lt;- paste0(\"t1_\", name_exposure_continuous)\nt0_name_exposure_continuous &lt;- paste0(\"t0_\", name_exposure)\nt1_name_exposure_continuous &lt;- paste0(\"t1_\", name_exposure)\n\n# raw outcomes\n# read health outcomes\noutcome_vars &lt;- here_read(\"outcome_vars\")\nt2_outcome_z &lt;- paste0(\"t2_\", outcome_vars, \"_z\")\n\n# view\nt2_outcome_z\n\n# check\nstr(dat_long_final)\n\n# check\nnaniar::gg_miss_var(dat_long_final)\n\n# impute data --------------------------------------------------------------\n# ordinal use\nordinal_columns &lt;- c(\n  \"t0_education_level_coarsen\",\n  \"t0_eth_cat\",\n  \"t0_rural_gch_2018_l\",\n  \"t0_gen_cohort\"\n)\n\n# define cols we will not standardise\ncontinuous_columns_keep &lt;- c(\"t0_sample_weights\")\n\n# remove sample weights\ndat_long_final_2 &lt;- dat_long_final |&gt; select(-sample_weights)\n\n# prepare data for analysis ----------------------\ndat_long_final_2 &lt;- margot::remove_numeric_attributes(dat_long_final_2)\n# wide data\ndf_wide &lt;- margot_wide_machine(\n  dat_long_final,\n  id = \"id\",\n  wave = \"wave\",\n  baseline_vars,\n  exposure_var = exposure_var,\n  outcome_vars,\n  confounder_vars = NULL,\n  imputation_method = \"none\",\n  include_exposure_var_baseline = TRUE,\n  include_outcome_vars_baseline = TRUE,\n  extend_baseline = FALSE,\n  include_na_indicators = FALSE\n)\n\n# check\ncolnames(df_wide)\n\n# return sample weights\ndf_wide$t0_sample_weights &lt;-  t0_sample_weights\n\n# save\nmargot::here_save(df_wide, \"df_wide\")\n\n\n#df_wide &lt;- margot::here_read(\"df_wide\")\nnaniar::vis_miss(df_wide, warn_large_data = FALSE)\n\n\n# order data with missingness assigned to work with grf and lmtp\n# if any outcome is censored all are censored\n# create version for model reports\n\n# check\ncolnames(df_wide)\n\n\n# made data wide in correct format\n# ** ignore warning *** \ndf_wide_encoded  &lt;- margot::margot_process_longitudinal_data_wider(\n  df_wide,\n  ordinal_columns = ordinal_columns,\n  continuous_columns_keep = continuous_columns_keep,\n  not_lost_in_following_wave = \"not_lost_following_wave\",\n  lost_in_following_wave = \"lost_following_wave\",\n  remove_selected_columns = TRUE,\n  exposure_var = exposure_var,\n  scale_continuous = TRUE,\n  censored_if_any_lost = FALSE\n)\n\n# check\ncolnames(df_wide_encoded)\n\n# check\ntable(df_wide_encoded$t0_not_lost_following_wave)\n\n# make the binary variable numeric\ndf_wide_encoded[[t0_name_exposure_binary]] &lt;-\n  as.numeric(df_wide_encoded[[t0_name_exposure_binary]]) - 1\ndf_wide_encoded[[t1_name_exposure_binary]] &lt;-\n  as.numeric(df_wide_encoded[[t1_name_exposure_binary]]) - 1\n\n# view\ndf_wide_encoded[[t0_name_exposure_binary]]\ndf_wide_encoded[[t1_name_exposure_binary]]\n\n# 1. ensure both binaries only take values 0 or 1 (ignore NA)\nstopifnot(all(df_wide_encoded[[t0_name_exposure_binary]][!is.na(df_wide_encoded[[t0_name_exposure_binary]])] %in% 0:1),\n          all(df_wide_encoded[[t1_name_exposure_binary]][!is.na(df_wide_encoded[[t1_name_exposure_binary]])] %in% 0:1))\n\n# 2. ensure NA‐patterns match between t1_exposure and t0_lost flag\n# count n-as in t1 exposure\nn_na_t1 &lt;- sum(is.na(df_wide_encoded[[t1_name_exposure_binary]]))\n\n# count how many were lost at t0\nn_lost_t0 &lt;- sum(df_wide_encoded$t0_lost_following_wave == 1, na.rm = TRUE)\n\n# print them for inspection\nmessage(\"NAs in \", t1_name_exposure_binary, \": \", n_na_t1)\nmessage(\"t0_lost_following_wave == 1: \", n_lost_t0)\n\n# stop if they don’t match\nstopifnot(n_na_t1 == n_lost_t0)\n\n# 3. ensure if t1 is non‐NA then subject was not lost at t0\nstopifnot(all(is.na(df_wide_encoded[[t1_name_exposure_binary]]) |\n                df_wide_encoded[[\"t0_not_lost_following_wave\"]] == 1))\n\n# view\nhead(df_wide_encoded)\n\n#naniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\nnaniar::gg_miss_var(df_wide_encoded)\n\n\n# predict attrition and create censoring weights --------------------------\n# step 1: prepare baseline covariates\n# select all t0_ variables except the exposure binary and any _lost indicators, then sort their names\nt0_var_names &lt;- df_wide_encoded |&gt;\n  select(-all_of(t0_name_exposure_binary)) |&gt;\n  select(starts_with(\"t0_\"),-ends_with(\"_lost\"),-ends_with(\"lost_following_wave\"), -ends_with(\"_weights\")) |&gt;\n  colnames() |&gt;\n  sort()\n\n# get unique values (to be safe)\nE &lt;- unique(t0_var_names)\n\n# view\nprint(E)\n\n# save baseline covariates\nmargot::here_save(E, \"E\")\n\n# view\nprint(E)\n\n# step 2: calculate weights for t0\nD_0 &lt;- as.factor(df_wide_encoded$t0_lost_following_wave)\n\n# get co-variates\ncen_0 &lt;- df_wide_encoded[, E]\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# probability forest for censoring\n# this will take time\ncen_forest_0 &lt;- probability_forest(cen_0, D_0)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# get predictions\npredictions_grf_0 &lt;- predict(cen_forest_0, newdata = cen_0, type = \"response\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n\n# get propensity scores\npscore_0 &lt;- predictions_grf_0$pred[, 2]\n\n# use margot_adjust_weights for t0\nt0_weights &lt;- margot_adjust_weights(\n  pscore = pscore_0,\n  trim = TRUE,\n  normalize = TRUE,\n  # lower trimming\n  lower_percentile = 0.00,\n  # upper trimming\n  upper_percentile = 0.99,\n  censoring_indicator = df_wide_encoded$t0_lost_following_wave,\n  sample_weights = df_wide_encoded$t0_sample_weights\n)\n\n# view\nhist(t0_weights$adjusted_weights)\n\n# give weights\ndf_wide_encoded$t0_adjusted_weights &lt;- t0_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\n\n# remove lost next wave (censored)\ndf_wide_encoded_1 &lt;- df_wide_encoded %&gt;%\n  filter(t0_lost_following_wave == 0) %&gt;%\n  droplevels()\n\n# step 4: calculate weights for t1\nE_and_exposure &lt;- c(E, t1_name_exposure_continuous)\nD_1 &lt;- as.factor(df_wide_encoded_1$t1_lost_following_wave)\ncen_1 &lt;- df_wide_encoded_1[, E_and_exposure]\n\n# probability forest for censoring\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\ncen_forest_1 &lt;- probability_forest(cen_1, D_1, sample.weights = df_wide_encoded_1$t0_adjusted_weights)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# predict forest\n\npredictions_grf_1 &lt;- predict(cen_forest_1, newdata = cen_1, type = \"response\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# get propensity score\npscore_1 &lt;- predictions_grf_1$pred[, 2]\n\n# check\nhist(pscore_1)\n\n# use margot_adjust_weights for t1\n# we will use these weights for inference in our models\nt1_weights &lt;- margot_adjust_weights(\n  pscore = pscore_1,\n  trim = TRUE,\n  normalize = TRUE,\n  lower_percentile = 0.00,\n  # upper trimming\n  upper_percentile = 0.99,\n  censoring_indicator = df_wide_encoded_1$t1_lost_following_wave,\n  sample_weights = df_wide_encoded_1$t0_adjusted_weights # combine with weights\n)\n\n# add weights -- these will be the weights we use\ndf_wide_encoded_1$t1_adjusted_weights &lt;- t1_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded_1, warn_large_data = FALSE)\n\n# save\nhere_save(df_wide_encoded_1, \"df_wide_encoded_1\")\n\n# check names\ncolnames(df_wide_encoded_1)\n\n# check\ndf_wide_encoded_1[[t1_name_exposure_binary]]\n\n# step 5: prepare final dataset\nnrow(df_wide_encoded_1)\ntable(df_wide_encoded_1$t1_lost_following_wave)\n\n# arrange\ndf_grf &lt;- df_wide_encoded_1 |&gt;\n  filter(t1_lost_following_wave == 0) |&gt;\n  select(\n    where(is.factor),\n    ends_with(\"_binary\"),\n    ends_with(\"_lost_following_wave\"),\n    ends_with(\"_z\"),\n    ends_with(\"_weights\"),\n    starts_with(\"t0_\"),\n    starts_with(\"t1_\"),\n    starts_with(\"t2_\"),\n  ) |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\")) |&gt;\n  relocate(starts_with(\"t1_\"), .before = starts_with(\"t2_\")) |&gt;\n  relocate(\"t0_not_lost_following_wave\", .before = starts_with(\"t1_\")) |&gt;\n  relocate(all_of(t1_name_exposure_binary), .before = starts_with(\"t2_\")) |&gt;\n  droplevels()\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# make sure to do this\n# save final data\nmargot::here_save(df_grf, \"df_grf\")\ndf_grf &lt;- margot::here_read(\"df_grf\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# check final dataset\ncolnames(df_grf)\n\n# visualise missing\n# should have no missing in t1 and t2 variables\n# handled by IPCW\n# make final missing data graph\nmissing_final_data_plot &lt;- naniar::vis_miss(df_grf, warn_large_data = FALSE)\nmissing_final_data_plot\n\n# save plot\nmargot_save_png(missing_final_data_plot, prefix = \"missing_final_data\")\n\n# checks\ncolnames(df_grf)\nstr(df_grf)\n\n# check exposures\ntable(df_grf[[t1_name_exposure_binary]])\n\n# check\nhist(df_grf$t1_adjusted_weights)\n\n# calculate summary statistics\nt0_weight_summary &lt;- summary(df_wide_encoded)\n\n# check\nglimpse(df_grf$t1_adjusted_weights)\n\n# visualise weight distributions\nhist(df_grf$t1_adjusted_weights, main = \"t0_stabalised weights\", xlab = \"Weight\")\n\n# check n\nn_observed_grf &lt;- nrow(df_grf)\n\n# view\nn_observed_grf\n\n# save\nmargot::here_save(n_observed_grf, \"n_observed_grf\")\n\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n\n# this is just for your interest ------------------------------------------\n# not used in final manuscript\n# FOR INTEREESTS\n# inspect propensity scores -----------------------------------------------\n# get data\n# df_grf &lt;- here_read('df_grf')\n# \n# # assign weights var name\n# weights_var_name = \"t0_adjusted_weights\"\n# \n# # baseline covariates  # E already exists and is defined\n# E\n# \n# # must be a data frame, no NA in exposure\n# \n# # df_grf is a data frame - we must process this data frame in several steps\n# # user to specify which columns are outcomes, default to 'starts_with(\"t2_\")'\n# df_propensity_org &lt;- df_grf |&gt; select(!starts_with(\"t2_\"))\n# \n# # Remove NAs and print message that this has been done\n# df_propensity &lt;- df_propensity_org |&gt; drop_na() |&gt; droplevels()\n# \n# # E_propensity_names\n# # first run model for baseline propensity if this is selected.  The default should be to not select it.\n# propensity_model_and_plots &lt;- margot_propensity_model_and_plots(\n#   df_propensity = df_propensity,\n#   exposure_variable = t1_name_exposure_binary,\n#   baseline_vars = E,\n#   weights_var_name = weights_var_name,\n#   estimand = \"ATE\",\n#   method = \"ebal\",\n#   focal = NULL\n# )\n# \n# # visualise\n# summary(propensity_model_and_plots$match_propensity)\n# \n# # key plot\n# propensity_model_and_plots$love_plot\n# \n# # other plots\n# propensity_model_and_plots$summary_plot\n# propensity_model_and_plots$balance_table\n# propensity_model_and_plots$diagnostics\n# \n# \n# # check size\n# size_bytes &lt;- object.size(propensity_model_and_plots)\n# print(size_bytes, units = \"auto\") # Mb\n# \n# # use qs to save only if you have space\n# here_save_qs(propensity_model_and_plots,\n#              \"propensity_model_and_plots\",\n#              push_mods)"
  },
  {
    "objectID": "content/09-content.html#script-3-models-graphs-is-here",
    "href": "content/09-content.html#script-3-models-graphs-is-here",
    "title": "Causal inference: a step by step guide",
    "section": "Script 3: Models & Graphs is HERE",
    "text": "Script 3: Models & Graphs is HERE\n\n\nCode\n# script 3: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session\n\nrstudioapi::restartSession()\n\n\n\n# reproducibility ---------------------------------------------------------\n\n\nset.seed(123)\n\n\n# essential library ---------------------------------------------------------\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.37\") {\n  stop(\"please install margot &gt;= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n# check package version\npackageVersion(pkg = \"margot\")\n\n\n\n# load libraries ----------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf, ranger,     # machine learning forests\n  doParallel,      # parallel processing,\n  kableExtra,\n  ggplot2 ,        # graphs\n  rlang ,          # functions for base types/Core R/ 'Tidyverse'\n  purrr ,          # functional programming tools.\n  patchwork,      # nice graph placement\n  janitor,         # nice labels\n  glue            # format/ interpolate a string\n)\n\n\n\n# directory path configuration -----------------------------------------------\n# save path (customise for your own computer) ----------------------------\npush_mods &lt;- here::here(\"save_directory\") \n\n# read original data (for plots) ------------------------------------------\noriginal_df &lt;- margot::here_read(\"df_wide\", push_mods)\n\n# plot title --------------------------------------------------------------\ntitle_binary = \"Effects of {{name_exposure}} on {{name_outcomes}}\"\nfilename_prefix = \"grf_extraversion_wb\"\n\n# for manuscript later\nmargot::here_save(title_binary,\"title_binary\")\n\n# import names ------------------------------------------------------------\nname_exposure &lt;- margot::here_read(\"name_exposure\")\nname_exposure\n\n# make exposure names\nt1_name_exposure_binary &lt;- paste0(\"t1_\", name_exposure, \"_binary\")\n\n# check exposure name\nt1_name_exposure_binary\n\n# read outcome vars\noutcome_vars &lt;- margot::here_read(\"outcome_vars\")\n\n# read and sort outcome variables -----------------------------------------\n# we do this by domain: health, psych, present, life, social\nread_and_sort &lt;- function(key) {\n  raw  &lt;- margot::here_read(key, push_mods)\n  vars &lt;- paste0(\"t2_\", raw, \"_z\")\n  sort(vars)\n}\nt2_outcome_z  &lt;- read_and_sort(\"outcome_vars\")\n\n# view\nt2_outcome_z\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# define names for titles -------------------------------------------------\n\nnice_exposure_name = \"Extraversion\"\nnice_outcome_name = \"Wellbeing\"\ntitle = \"Effect of {{nice_exposure_name}} on {{nice_outcome_name}}\"\n\n# save for final rport\nhere_save(title, \"title\")\n\n# combine outcomes ---------------------------------------------------------\n# check outcome vars and make labels for graphs/tables\noutcome_vars\n\n\nlabel_mapping_all &lt;- list(\n  #\"t2_alcohol_frequency_weekly_z\" = \"Alcohol Frequency\",\n  #\"t2_alcohol_intensity_weekly_z\" = \"Alcohol Intensity\",\n  #\"t2_hlth_bmi_z\" = \"BMI\",\n  #\"t2_hlth_sleep_hours_z\" = \"Sleep\",\n  \"t2_log_hours_exercise_z\" = \"Hours of Exercise (log)\",\n  #\"t2_short_form_health_z\" = \"Short Form Health\"\n  \"t2_hlth_fatigue_z\" = \"Fatigue\",\n  \"t2_kessler_latent_anxiety_z\" = \"Anxiety\",\n  \"t2_kessler_latent_depression_z\" = \"Depression\",\n  \"t2_rumination_z\" = \"Rumination\",\n  # \"t2_bodysat_z\" = \"Body Satisfaction\",\n  \"t2_foregiveness_z\" = \"Forgiveness\",\n  \"t2_perfectionism_z\" = \"Perfectionism\", \n  \"t2_self_esteem_z\" = \"Self Esteem\",\n  # \"t2_self_control_z\" = \"Self Control\",\n  # \"t2_sexual_satisfaction_z\" = \"Sexual Satisfaction\".\n  \"t2_gratitude_z\" = \"Gratitude\",\n  \"t2_lifesat_z\" = \"Life Satisfaction\",\n  \"t2_meaning_purpose_z\" = \"Meaning: Purpose\",\n  \"t2_meaning_sense_z\" = \"Meaning: Sense\",\n  \"t2_pwi_z\" = \"Personal Well-being Index\",\n  \"t2_belong_z\" = \"Social Belonging\",\n  \"t2_neighbourhood_community_z\" = \"Neighbourhood Community\",\n  \"t2_support_z\" = \"Social Support\"\n)\n\n\n# save\nhere_save(label_mapping_all, \"label_mapping_all\")\n\n# check\nlabel_mapping_all\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# select options that make sense fo your study/results\n# might need to be tweaked after the analysis\n\n# make options -------------------------------------------------------------\n# titles\ntitle = \"ATE Effects of {{nice_name_exposure}} on {{nice_name_outcome}}\"\nsubtitle = \"\"\nfilename_prefix = \"final_report\"\n\n\n# settings\nx_offset = -.5\nx_lim_lo = -.5\nx_lim_hi = .5\n\n\n# defaults for ate plots\nbase_defaults_binary &lt;- list(\n  type = \"RD\",\n  title = title_binary,\n  e_val_bound_threshold = 1.2,\n  colors = c(\n    \"positive\" = \"#E69F00\",\n    \"not reliable\" = \"grey50\",\n    \"negative\" = \"#56B4E9\"\n  ),\n  x_offset = x_offset,\n  # will be set based on type\n  x_lim_lo = x_lim_lo,\n  # will be set based on type\n  x_lim_hi = x_lim_hi,\n  text_size = 4,\n  linewidth = 0.5,\n  estimate_scale = 1,\n  base_size = 18,\n  point_size = 2,\n  title_size = 19,\n  subtitle_size = 16,\n  legend_text_size = 10,\n  legend_title_size = 10,\n  include_coefficients = FALSE\n)\n\n# health graph options\noutcomes_options_all &lt;- margot_plot_create_options(\n  title = subtitle,\n  base_defaults = base_defaults_binary,\n  subtitle = subtitle,\n  filename_prefix = filename_prefix\n)\n\n\n# policy tree graph settings ----------------------------------------------\ndecision_tree_defaults &lt;- list(\n  span_ratio       = .3,\n  text_size        = 3.8,\n  y_padding        = 0.25,\n  edge_label_offset = .002,\n  border_size      = .05\n)\npolicy_tree_defaults &lt;- list(\n  point_alpha       = .5,\n  title_size        = 12,\n  subtitle_size     = 12,\n  axis_title_size   = 12,\n  legend_title_size = 12,\n  split_line_color  = \"red\",\n  split_line_alpha  = .8,\n  split_label_color = \"red\",\n  list(split_label_nudge_factor = 0.007)\n)\n\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +----------------------------------------------+\n# |       DO NOT ALTER  (except where noted)     |\n# +----------------------------------------------+\n\n# load GRF data and prepare inputs ----------------------------------------\ndf_grf &lt;- margot::here_read('df_grf', push_mods)\nE      &lt;- margot::here_read('E',      push_mods)\n# check exposure binary\nstopifnot(all(df_grf[[t1_name_exposure_binary]][!is.na(df_grf[[t1_name_exposure_binary]])] %in% 0:1))\n# set exposure and weights\nW       &lt;- as.vector(df_grf[[t1_name_exposure_binary]]) # note it is the processed weights for attrition \"t1\"\nweights &lt;- df_grf$t1_adjusted_weights\nhist(weights) # quick check for extreme weights\n# select covariates and drop numeric attributes\nX &lt;- margot::remove_numeric_attributes(df_grf[E])\n\n\n# set model defaults -----------------------------------------------------\ngrf_defaults &lt;- list(seed = 123, stabilize.splits = TRUE, num.trees = 2000)\n\n\n# example: fit causal forest on a toy subset ------------------------------\n# first, create a smaller test sample\nn   &lt;- nrow(X)\ntoy &lt;- sample(seq_len(n), floor(n / 4))\n# define toy data\ntoy_data     &lt;- df_grf[toy, ]\nX_toy        &lt;- X[toy, ]\nW_toy        &lt;- W[toy]\nweights_toy  &lt;- weights[toy]\n\n# fit the model\ncf_out &lt;- margot_causal_forest(\n  data         = toy_data,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  outcome_vars = \"t2_kessler_latent_depression_z\", # select variable in your outcome_variable set\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  covariates   = X_toy,\n  W            = W_toy,\n  weights      = weights_toy,\n  save_data    = TRUE,\n  save_models  = TRUE\n)\n\n# inspect propensities ------------------------------------------------------\nqini_tbl &lt;- margot::margot_inspect_qini(cf_out, propensity_bounds = c(0.01, 0.97))\n\n# show\nprint(qini_tbl)\n\n# plot policy-combo trees --------------------------------------------------\ncombo1 &lt;- margot_plot_policy_combo(\n  result_object    = cf_out,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_name       = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  max_depth        = 1L,          # depth-1 tree\n  decision_tree_args = list(text_size = 4),\n  policy_tree_args   = list(point_alpha = 0.7),\n  original_df        = original_df,\n  label_mapping      = label_mapping_all\n)\n\n# show\ncombo1$combined_plot\n\n# you can repeat for depth-2 ----------------------------------------------\ncombo2 &lt;- margot_plot_policy_combo(\n  result_object    = cf_out,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_name       = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  max_depth        = 2L,\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  original_df        = original_df,\n  label_mapping      = label_mapping_all\n)\n\n# show\ncombo2$combined_plot\n\n# batch plotting ----------------------------------------------------------\nmodels_batch_1L &lt;- margot_policy(\n  cf_out,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names        = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df        = original_df,\n  label_mapping      = label_mapping_all,\n  max_depth          = 1L\n)\n\n# view first model's plots\nmodels_batch_1L[[1]][[3]]  # combo plot\nmodels_batch_1L[[1]][[4]]  # qini plot\n\n# sub plots\nmodels_batch_1L[[1]][[1]]  # predictions of policy tree\nmodels_batch_1L[[1]][[2]]  # policy tree\n\n# qini interpretations at different spends\n# negative is bad\nmodels_batch_1L[[1]][[5]]  \n\n# 2L tree\nmodels_batch_2L &lt;- margot_policy(\n  cf_out,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names        = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df        = original_df,\n  label_mapping      = label_mapping_all,\n  max_depth          = 2L)\n# view first model's plots\nmodels_batch_2L[[1]][[3]]  # combo plot\nmodels_batch_2L[[1]][[4]]  # qini plot - not convincing\n\n# 2. flip the selected outcomes (and regen trees)\n# use -- when the outcome is undesirable and we want to minimise it \n# (assuming the exposure is something we'd prescribe)\n\n# select models whose outcomes are undesirable  when the intervention is meant to be 'good'\n# such variables will be specific to your study \n\n# +--------------------------+\n# |    MODIFY THIS           |\n# +--------------------------+\n\nflip_outcomes_test = c(\"t2_kessler_latent_depression_z\")\n\n# function to get the labels from the models (labels were defined above)\nflipped_names_test &lt;- margot_get_labels(flip_outcomes_test, label_mapping_all)\n\n# +--------------------------+\n# |   END MODIFY             |\n# +--------------------------+\n\n# run flip forests\ncf_out_f &lt;- margot_flip_forests(\n  model_results = cf_out,\n  flip_outcomes = flip_outcomes_test,\n  recalc_policy = TRUE\n)\n\n# where there are very low or high propensity scores (prob of exposure) \n# we might consider trimming\nmargot::margot_inspect_qini(cf_out_f, propensity_bounds = c(0.01, 0.97))\n\n\n# if we had extreme scores (not used here)\n# cf_out_flipped_trimmed &lt;- margot_rescue_qini(model_results      = cf_out_f,\n#                                              propensity_bounds  = c(0.05, 0.95)) \n\n# flipped batch model\nmodels_batch_flipped_2L &lt;- margot_policy(\n  cf_out_f,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names = c(\"model_t2_kessler_latent_depression_z\"),\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth     = 2L\n)\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# flipped\n# interpretation: exposure minimising depression\nmodels_batch_flipped_2L[[1]][[3]]\n\n\n# *** NOTE DIFFERENCES IN INTERPRETATION\n\n# not flipped: exposure as maximizing depression\nmodels_batch_2L[[1]][[3]]\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n# interpretation example \n\n# test interpretations ----------------------------------------------------\n\n\n# policy tree interpretation: search depth = 1\ninterpret_model_policy_test_1L &lt;- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 1)\ncat(interpret_model_policy_test_1L)\n\n\n# policy tree interpretation: search depth = 2\ninterpret_model_policy_test_2L &lt;- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 2)\ncat(interpret_model_policy_test_2L)\n\n\n\n# interpret rate ----------------------------------------------------------\n\n# create rate analysis table\nrate_table_all_test &lt;- margot_rate(\n  models = cf_out_f,\n  policy = \"treat_best\",  # or \"withold_best\" but don't attempt fitting curves or policytrees\n  label_mapping = label_mapping_all\n)\n\n# view rate tables\nrate_table_all_test$rate_autoc |&gt; kbl(\"markdown\")\nrate_table_all_test$rate_qini |&gt; kbl(\"markdown\")\n\n\n# generate interpretation\nrate_interpretation_all &lt;- margot_interpret_rate(\n  rate_table_all_test, \n  flipped_outcomes = flipped_names_test\n)\n\n\n# ** uncomment to run full model**\n\n# causal forest model -----------------------------------------------------------\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\nmodels_binary &lt;- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# save model\nmargot::here_save_qs(models_binary, \"models_binary\", push_mods)\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# read results ------------------------------------------------------------\n# if you save models you do not need to re-run them\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# reading models takes time\n# if you want to check the size of an object use\n# margot::margot_size(object)\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\nmodels_binary &lt;- margot::here_read_qs(\"models_binary\", push_mods)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# count models by category\n# just a check\ncat(\"Number of original models:\\n\", length(models_binary$results), \"\\n\")\n\n\n# make ate plots ----------------------------------------------------------\nbinary_results &lt;- margot_plot(\n  models_binary$combined_table,\n  options = outcomes_options_all,\n  label_mapping = label_mapping_all,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  order = \"evaluebound_asc\",\n  original_df = original_df,\n  e_val_bound_threshold = 1.2\n)\n\n# view\nbinary_results$transformed_table |&gt; rename(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\") |&gt;\n  kbl(format = 'markdown')\n\n# check\nbinary_results$plot\n\n# interpretation\ncat(binary_results$interpretation)\n\n# nice table\ntables_list &lt;- list(\n  Wellbeing = binary_results$transformed_table\n)\n\n# make markdown tables (to be imported into the manuscript)\nmargot_bind_tables_markdown &lt;- margot_bind_tables(\n  tables_list = tables_list,\n  #list(all_models$combined_table),\n  sort_E_val_bound = \"desc\",\n  e_val_bound_threshold = 1.2,\n  # ← choose threshold\n  highlight_color = NULL,\n  bold = TRUE,\n  rename_cols = TRUE,\n  col_renames = list(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\"),\n  rename_ate = TRUE,\n  threshold_col = \"E_Val_bound\",\n  output_format = \"markdown\",\n  kbl_args = list(\n    booktabs = TRUE,\n    caption = NULL,\n    align = NULL\n  )\n)\n\n# view markdown table\nmargot_bind_tables_markdown\n\n# save for publication\nhere_save(margot_bind_tables_markdown, \"margot_bind_tables_markdown\")\n\n\n# evaluate models ---------------------------------------------------------\n# trim models if extreme propensity scores dominate\n# diag_tbl_98 &lt;- margot_inspect_qini(models_binary,\n#                                        propensity_bounds = c(0.01, 0.99))\n\n\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n# flipping models: outcomes we want to minimise given the exposure --------\n# standard negative outcomes/  not used in this study\n\n# +--------------------------+\n# |    MODIFY THIS           |\n# +--------------------------+\nflip_outcomes_standard = c(\n  #\"t2_alcohol_frequency_weekly_z\",\n  #\"t2_alcohol_intensity_z\",\n  #\"t2_hlth_bmi_z\",\n  #\"t2_hlth_fatigue_z\",\n  \"t2_kessler_latent_anxiety_z\", #  ← select\n  \"t2_kessler_latent_depression_z\",#  ← select\n  \"t2_rumination_z\" #  ← select\n  #\"t2_perfectionism_z\" # the exposure variable was not investigated\n)\n\n\n# we will investigate losses to these outcomes\n# usual flipped names for positive interventions\n# commented out for this study\n\n# WHICH OUTCOMES -- if any ARE UNDESIREABLE? \n\n# NOT IF THE EXPOSURE IS NEGATIVE, FOCUS ON WHICH OUTCOMES, if any, ARE POSITIVE AND FLIP THESE?\nflip_outcomes &lt;- flip_outcomes_standard #c( setdiff(t2_outcomes_all, flip_outcomes_standard) )\n\n# check\nflip_outcomes\n\n\n# +--------------------------+\n# |   END MODIFY             |\n# +--------------------------+\n\n\n# checks for when exposure is *damaging** \n# neg_check &lt;- vapply(all_models$results[ paste0(\"model_\", flip_outcomes) ],\n#                     \\(x) mean(x$tau_hat, na.rm = TRUE) &lt; 0, logical(1))\n# stopifnot(all(neg_check))   # every chosen outcome has a negative mean cate\n\n# get labels\nflipped_names &lt;- margot_get_labels(flip_outcomes, label_mapping_all)\n\n# check\nflipped_names\n\n# save for publication\nhere_save(flipped_names, \"flipped_names\")\n\n\n\n# flip negatively oriented outcomes --------------------------------------\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n\n# flip models using margot's function\n\n#  *** this will take some time ***\n\n# ** give it time **\n# ** once run/ comment out **\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\nmodels_binary_flipped_all &lt;- margot_flip_forests(models_binary,\n                                                 flip_outcomes = flip_outcomes,\n                                                 recalc_policy = TRUE)\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# save\nhere_save_qs(models_binary_flipped_all, \"models_binary_flipped_all\", push_mods)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# read back if needed\nmodels_binary_flipped_all &lt;- here_read_qs(\"models_binary_flipped_all\", push_mods)\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# where there are very low or high propensity scores (prob of exposure) we might consider trimming\n# margot::margot_inspect_qini(models_binary_flipped_all, propensity_bounds = c(0.05, 0.95))\n# \n# \n# # if we had extreme scores (not used here)\n# models_binary_flipped_all_t &lt;- margot_rescue_qini(model_results  = models_binary_flipped_all,\n#                                              propensity_bounds  = c(0.05, 0.95))\n\n\n\n# omnibus heterogeneity tests --------------------------------------------\n# test for treatment effect heterogeneity across all outcomes\nresult_ominbus_hetero_all &lt;- margot::margot_omnibus_hetero_test(models_binary_flipped_all,\n                                                                label_mapping = label_mapping_all)\n\n# view results table\nresult_ominbus_hetero_all$summary_table |&gt; kbl(\"markdown\")\n\n# view test interpretation\ncat(result_ominbus_hetero_all$brief_interpretation)\n\n# rate test analysis -----------------------------------------------------\n\n# create rate analysis table\nrate_table_all &lt;- margot_rate(\n  models = models_binary_flipped_all,\n  policy = \"treat_best\",  # or \"withold_best\" but don't attempt fitting curves or policytrees\n  label_mapping = label_mapping_all\n)\n\n# view rate tables\nrate_table_all$rate_autoc |&gt; kbl(\"markdown\")\nrate_table_all$rate_qini |&gt; kbl(\"markdown\")\n\n# generate interpretation\nrate_interpretation_all &lt;- margot_interpret_rate(\n  rate_table_all, \n  flipped_outcomes = flipped_names\n)\n\n# view interpretations\ncat(rate_interpretation_all$autoc_results)\ncat(rate_interpretation_all$qini_results)\n\n# compare rate and qini -- see grf documentation\ncat(rate_interpretation_all$comparison)\n\n# check out model names for different ways of thinking about heterogeneity\nrate_interpretation_all$either_model_names\nrate_interpretation_all$qini_model_names\nrate_interpretation_all$both_model_names\nrate_interpretation_all$autoc_model_names\n\n# autoc plots ------------------------------------------------------------\n# generate batch rate plots for models with significant heterogeneity\nbatch_rate_autoc_plots &lt;- margot_plot_rate_batch(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  # just use rate autoc for rate plots\n  model_names = rate_interpretation_all$autoc_model_names\n)\n\n# extract individual plots from the batch result\nautoc_plots &lt;- batch_rate_autoc_plots\n\n# determine number of columns based on number of plots\nnum_cols &lt;- ifelse(length(autoc_plots) &gt; 3, 2, 1)\n\n# combine plots using patchwork\nlibrary(patchwork)\n\n# only proceed if there are plots to combine\nif (length(autoc_plots) &gt; 0) {\n  # initialize with first plot\n  combined_autoc_plot &lt;- autoc_plots[[1]]\n  \n  # add remaining plots if any\n  if (length(autoc_plots) &gt; 1) {\n    for (i in 2:length(autoc_plots)) {\n      combined_autoc_plot &lt;- combined_autoc_plot + autoc_plots[[i]]\n    }\n  }\n  \n  # apply the dynamic layout\n  combined_autoc_plot &lt;- combined_autoc_plot +\n    plot_layout(ncol = num_cols) &\n    plot_annotation(\n      title = \"AUTOC Model Plots\",\n      subtitle = paste0(length(autoc_plots), \" models with significant heterogeneity\"),\n      tag_levels = \"A\"\n    )\n  \n  # view the combined plot\n  print(combined_autoc_plot)\n  \n  # save the combined plot if needed\n  width &lt;- ifelse(num_cols == 1, 8, 12)\n  height &lt;- 6 * ceiling(length(autoc_plots) / num_cols)\n  \n  ggsave(\n    here::here(push_mods, \"combined_autoc_plots.pdf\"),\n    combined_autoc_plot,\n    width = width,\n    height = height\n  )\n} else {\n  # handle case with no plots\n  message(\"No AUTOC plots available\")\n}\n\nmodels_batch_qini_2L_test &lt;- margot_plot_policy_combo(\n  models_binary_flipped_all,\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_name =  \"model_t2_log_hours_exercise_z\",\n  max_depth  = 2L,\n  # ← new argument\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\nrate_interpretation_all$qini_model_names\n\n# qini --------------------------------------------------------------------\n# run the margot_policy function\nmodels_batch_qini_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$qini_results,\n  max_depth  = 2L,\n  # ← new argument\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\n\n# extract the plots from the results\nplots &lt;- lapply(seq_along(models_batch_qini_2L), function(i) {\n  models_batch_qini_2L[[i]][[4]]  # extract the 4th element (plot) from each model\n})\n\n# name the plots\nnames(plots) &lt;- rate_interpretation_all$qini_model_names\n\n# determine number of columns based on number of plots\nnum_cols &lt;- ifelse(length(plots) &gt; 3, 2, 1)\n\n# load the patchwork library for combining plots\nlibrary(patchwork)\n\n# check if there are any plots to combine\nif (length(plots) == 0) {\n  message(\"no plots available to combine\")\n  NULL  # removed return since this isn't in a function\n} else {\n  # create combined plot\n  combined_plot &lt;- plots[[1]]\n  \n  # only run the loop if there are at least 2 plots\n  if (length(plots) &gt; 1) {\n    for (i in 2:length(plots)) {\n      combined_plot &lt;- combined_plot + plots[[i]]\n    }\n  }\n  \n  # apply the dynamic layout\n  combined_plot &lt;- combined_plot + plot_layout(ncol = num_cols)\n  # add titles and annotations\n  combined_plot &lt;- combined_plot &\n    plot_annotation(\n      title = \"Qini Model Plots\",\n      subtitle = paste0(length(plots), \n                        ifelse(length(plots) == 1, \" model \", \" models \"), \n                        \"arranged in \", num_cols, \n                        ifelse(num_cols == 1, \" column\", \" columns\")),\n      tag_levels = \"A\"  # adds a, b, c, etc. to the plots\n    )\n  # view\n  combined_plot\n  # save (optional)\n  width &lt;- ifelse(num_cols == 1, 8, 12)\n  height &lt;- 6 * ceiling(length(plots)/num_cols)  # height per row * number of rows\n  # save\n  ggsave(here::here(push_mods, \"combined_qini_plots.pdf\"),\n         combined_plot,\n         width = width, height = height)\n  \n  combined_plot  # removed return since this isn't in a function\n}\n\n# interpretation ----------------------------------------------------------\n# interpret qini curves\ninterpretation_qini_curves_2L &lt;- margot_interpret_qini(\n  models_batch_qini_2L,\n  model_names = rate_interpretation_all$qini_model_names,\n  label_mapping = label_mapping_all\n)\ninterpretation_qini_curves_2L\n\n# view qini interpretation\ncat(interpretation_qini_curves_2L$qini_explanation)\n\n# view summary table\ninterpretation_qini_curves_2L$summary_table |&gt; kbl(\"markdown\")\n\n\n\n# policy tree analysis depth 1 L------------------------------------------------\n# make policy trees\n# 1 l decision trees are generally very bad\nplots_policy_trees_1L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names,\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 1L\n)\n\n# get number of models\nn_models &lt;- length(rate_interpretation_all$either_model_names)\n\n# # use purrr to map through and print each model\n# purrr::map(1:n_models, function(i) {\n#   # print model name as a header\n#   cat(\"# model\", i, \"\\n\")\n#   # print the corresponding model plot\n#   print(plots_policy_trees_1L[[i]][[3]])\n#   # add spacing between models\n#   cat(\"\\n\\n\")\n# })\n\nmodel_outputs_1L &lt;- purrr::map(1:n_models, ~plots_policy_trees_1L[[.x]][[3]])\n\n# name the list elements by model number\nnames(model_outputs_1L) &lt;- paste0(\"model_\", 1:n_models)\n\n\n# check number of models == n_models\nmodel_outputs_1L$model_1 # convincing?\nmodel_outputs_1L$model_2 # convincing?\nmodel_outputs_1L$model_3 # convincing?\n\n\n\n# policy tree analysis depth 2L -------------------------------------------------\n# make policy trees\n# *** 2l is much more persuasive ***\nplots_policy_trees_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names,\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 2L\n)\n\nn_models &lt;- length(rate_interpretation_all$either_model_names)\n\nmodel_outputs_2L &lt;- purrr::map(1:n_models, ~plots_policy_trees_2L[[.x]][[3]])\nnames(model_outputs_2L) &lt;- paste0(\"model_\", 1:n_models)\n\n# checks\nmodel_outputs_2L$model_1\nmodel_outputs_2L$model_2\nmodel_outputs_2L$model_3\n\n\n# convincing?\ninterpret_plots_policy_trees_2L &lt;- margot_interpret_policy_batch(\n  models_binary_flipped_all, model_names = rate_interpretation_all$either_model_names)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_2L)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n# you can investigate policy trees for all outcomes, mindful that the rate and qini are not reliable. \n# still, with appropriate caution, this may help to clarify psychologically interesting questions\n\nall_plots_policy_trees_1L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # model_names = rate_interpretation_all$either_model_names, # use all\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 1L\n)\nn_models &lt;- length(models_binary_flipped_all$results)\nn_models\n\n\nmodel_outputs_1L_all &lt;- purrr::map(1:n_models, ~all_plots_policy_trees_1L[[.x]][[3]])\n\n# view\nmodel_outputs_1L_all[[1]] # ← convincing? \nmodel_outputs_1L_all[[2]] # ← convincing? \nmodel_outputs_1L_all[[3]] # ← convincing? \nmodel_outputs_1L_all[[4]] # ← convincing? \nmodel_outputs_1L_all[[5]] # ← convincing? \nmodel_outputs_1L_all[[6]] # ← convincing? \nmodel_outputs_1L_all[[7]] # ← convincing? \nmodel_outputs_1L_all[[8]] # ← convincing? \nmodel_outputs_1L_all[[9]] # ← convincing?  \nmodel_outputs_1L_all[[10]] # ← convincing? \nmodel_outputs_1L_all[[11]] # ← convincing? \nmodel_outputs_1L_all[[12]] # ← convincing? \n\n\n\n# interpretation\ninterpret_plots_policy_trees_1L_all &lt;- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 1)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_1L_all)\n\n\n# ALL model 1L\nall_plots_policy_trees_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # model_names = rate_interpretation_all$either_model_names, # use all\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 2L\n)\nn_models &lt;- length(models_binary_flipped_all$results)\nn_models\n\n\nmodel_outputs_2L_all &lt;- purrr::map(1:n_models, ~all_plots_policy_trees_2L[[.x]][[3]])\n\n# view\nmodel_outputs_2L_all[[1]] # ← convincing? \nmodel_outputs_2L_all[[2]] # ← convincing? \nmodel_outputs_2L_all[[3]] # ← convincing? \nmodel_outputs_2L_all[[4]] # ← convincing? \nmodel_outputs_2L_all[[5]] # ← convincing? \nmodel_outputs_2L_all[[6]] # ← convincing? \nmodel_outputs_2L_all[[7]] # ← convincing? \nmodel_outputs_2L_all[[8]] # ← convincing? \nmodel_outputs_2L_all[[9]] # ← convincing? \nmodel_outputs_2L_all[[10]] # ← convincing? \nmodel_outputs_2L_all[[11]] # ← convincing? \nmodel_outputs_2L_all[[12]] # ←  convincing \n\n\n# interpretation\ninterpret_plots_policy_trees_2L_all &lt;- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 2)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_2L_all)\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n\n\n\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n#############################################################################\n# theoretical comparisons ---------------------------------------------------\n# individual theoretical comparisons (if relevant)\n# need to get values for wealth if wealth is compared\n\n# step 1 get information for wealth for conditonal comparisons\nhead(df_grf$t0_log_household_inc_z)\n\n# get mean on original data scale\nlog_mean_inc &lt;- mean(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# get sd on original data scale\nlog_sd_inc &lt;- sd(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# function to get back to data scale\nmargot_back_transform_log_z(\n  log_mean = log_mean_inc,\n  log_sd = log_sd_inc,\n  z_scores = c(-1, 0, 1),\n  label = \"data_scale\"\n)\n\n# define complex conditions for subsetting\ncomplex_condition_political &lt;- X[, \"t0_political_conservative_z\"] &gt; -1 &\n  X[, \"t0_political_conservative_z\"] &lt; 1\n\ncomplex_condition_wealth &lt;- X[, \"t0_log_household_inc_z\"] &gt; -1 &\n  X[, \"t0_log_household_inc_z\"] &lt; 1\n\ncomplex_condition_age &lt;- X[, \"t0_age_z\"] &gt; -1 &\n  X[, \"t0_age_z\"] &lt; 1\n\n# # if we have specific groups to compare\n# complex_condition_age_under_neg_1_sd  &lt;- X[, \"t0_age_z\"] &lt; -1\n# complex_condition_age_gr_eq_neg_1_sd  &lt;- X[, \"t0_age_z\"] &gt; -1\n\n# check ages to get number\nmean(original_df$t0_age) - sd(original_df$t0_age)\nmean(original_df$t0_age) + sd(original_df$t0_age)\n\n\n# wealth subsets\nsubsets_standard_wealth &lt;- list(\n  Poor = list(\n    var = \"t0_log_household_inc_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those HShold income &lt; -1 SD (NZD ~41k)\",\n    label = \"Poor\"  # label remains as is, but could be changed if desired\n  ),\n  MiddleIncome = list(subset_condition = complex_condition_wealth, description = \"Effects among those HS_hold income within +/-1SD (&gt; NZD 41k &lt; NZD 191k)\"),\n  Rich = list(\n    var = \"t0_log_household_inc_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those HS_hold income &gt; +1 SD (NZD 191k)\",\n    label = \"Rich\"\n  )\n)\n\n# political subsets\nsubsets_standard_political &lt;- list(\n  Liberal = list(\n    var = \"t0_political_conservative_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those &lt; -1 SD in political conservativism\",\n    label = \"Liberal\"\n  ),\n  Centrist = list(\n    var = \"t0_political_conservative_z\",\n    # operator = \"&lt;\",\n    subset_condition = complex_condition_political,\n    description = \"Effects among those &gt; -1 SD and &lt; +1 in political conservativism\",\n    label = \"Centrist\"\n  ),\n  Conservative = list(\n    var = \"t0_political_conservative_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those &gt; +1 SD in political conservativism\",\n    label = \"Conservative\"\n  )\n)\n\n\n# political subsets\nsubsets_standard_age &lt;- list(\n  Younger = list(\n    var = \"t0_age_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those &lt; under 35 years old\",\n    label = \"Age &lt; 35\"\n  ),\n  Middle = list(\n    var = \"t0_age_z\",\n    # operator = \"&lt;\",\n    subset_condition = complex_condition_age,\n    description = \"Effects among those 35-62\",\n    label = \"Age 35-62\"\n  ),\n  Older = list(\n    var = \"t0_age_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those &gt; 62\",\n    label = \"Age &gt; 62\"\n  )\n)\n\n\n# gender subsets\nsubsets_standard_gender &lt;- list(\n  Female = list(\n    var = \"t0_male_binary\",\n    value = 0,\n    description = \"Females\"\n  ),\n  Male = list(\n    var = \"t0_male_binary\",\n    value = 1,\n    description = \"Males\"\n  )\n)\n\n# ethnicity subsets\nsubsets_standard_ethnicity &lt;- list(\n  Asian = list(\n    var = \"t0_eth_cat_asian_binary\",\n    value = 1,\n    description = \"Asians\"\n  ),\n  Euro = list(\n    var = \"t0_eth_cat_euro_binary\",\n    value = 1,\n    description = \"Europeans (Pakeha)\"\n  ),\n  Pacific = list(\n    var = \"t0_eth_cat_pacific_binary\",\n    value = 1,\n    description = \"Pacific Peoples\"\n  ),\n  Maori = list(\n    var = \"t0_eth_cat_maori_binary\",\n    value = 1,\n    description = \"Māori\"\n  )\n)\n\n\n# batch planned subgroup analysis -----------------------------------------\n# set up lists of models, names, and subtitles\ndomain_models &lt;- list(\n  models_binary # HERE WE USE THE ORIGINAL MODELS\n)\n\n\n# set up domain names\ndomain_names &lt;- c(\"wellbeing\")\n\n# set up subtitles\nsubtitles &lt;- \"\"\n\n# set up subset types in a list\nsubset_types &lt;- list(\n  wealth = subsets_standard_wealth,\n  ethnicity = subsets_standard_ethnicity,\n  political = subsets_standard_political,\n  gender = subsets_standard_gender,\n  cohort = subsets_standard_age\n)\n\n\n# run model\nplanned_subset_results &lt;- margot_planned_subgroups_batch(\n  domain_models = domain_models,\n  X = X,\n  base_defaults = base_defaults_binary,\n  subset_types = subset_types,\n  original_df = original_df,\n  domain_names = domain_names,\n  subtitles = subtitles\n)\n\n\n# results\ncat(planned_subset_results$wellbeing$wealth$explanation)\ncat(planned_subset_results$wellbeing$ethnicity$explanation)\ncat(planned_subset_results$wellbeing$political$explanation)\ncat(planned_subset_results$wellbeing$gender$explanation)\ncat(planned_subset_results$wellbeing$cohort$explanation)\n\n\n\n# cohort subgroups --------------------------------------------------------\n\n# plots -------------------------------------------------------------------\n# results plots\n# health\nplots_subgroup_wealth&lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$wealth$results$Poor$plot,\n    planned_subset_results$wellbeing$wealth$results$MiddleIncome$plot,\n    planned_subset_results$wellbeing$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Wealth\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nplots_subgroup_wealth\n\n# plots\nplots_subgroup_ethnicity &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$ethnicity$results$Asian$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Euro$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Pacific$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Maori$plot\n    \n  ),\n  ncol = 2\n) +\n  patchwork::plot_annotation(title = \"Ethnicity\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_ethnicity)\n\n# plots\nplots_subgroup_political &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$political$results$Liberal$plot,\n    planned_subset_results$wellbeing$political$results$Centrist$plot,\n    planned_subset_results$wellbeing$political$results$Conservative$plot  \n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Political Orientation\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_political)\n\n# plots\nplots_subgroup_gender &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$gender$results$Female$plot,\n    planned_subset_results$wellbeing$gender$results$Male$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Gender\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_gender)\n\n# plots\nplots_subgroup_cohort &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$cohort$results$`Age &lt; 35`$plot,\n    planned_subset_results$wellbeing$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$wellbeing$cohort$results$`Age &gt; 62`$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Age Cohorts\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_cohort)\n\n\n\n# plot options: showcased ---------------------------------------------\n# default\nmargot_plot_decision_tree(models_binary, \"model_t2_support_z\", )\n# tighten branches for easier viewing in single graphs\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .30,\n  text_size = 3.8,\n  border_size = .1,\n  #  title = \"none\",\n  original_df = original_df\n)\n# colour decision node\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .3,\n  text_size = 4,\n  title = \"New Title\",\n  non_leaf_fill =  \"violet\",\n  original_df = original_df\n)\n# make new title\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .2,\n  text_size = 3,\n  title = \"New Title\",\n  non_leaf_fill =  \"white\",\n  original_df = original_df\n)\n\n# remove title\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  text_size = 5,\n  title = 'none',\n  # set title to none\n  original_df = original_df\n)\n\n\n# adjust only the alpha\nmargot::margot_plot_policy_tree(models_binary, \"model_t2_support_z\", point_alpha = .1)\nmargot::margot_plot_policy_tree(models_binary, \"model_t2_support_z\", point_alpha = .9)"
  },
  {
    "objectID": "content/08-content.html#script-0-synthetic-data-fetch",
    "href": "content/08-content.html#script-0-synthetic-data-fetch",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "Script 0: Synthetic Data Fetch",
    "text": "Script 0: Synthetic Data Fetch\n\n# for students: reproducibility is like following a recipe; each step ensures the same result\n# restart fresh session if needed\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# essential library ---------------------------------------------------------\n# install and load 'margot' from GitHub if missing\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.37\") {\n  stop(\"please install margot &gt;= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n# load packages ----------------------------------------------------------\n# install and load other packages from CRAN if missing\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\nif (!requireNamespace(\"qs\", quietly = TRUE)) {\n  install.packages(\"qs\")\n}\nlibrary(qs)\n\nif (!requireNamespace(\"here\", quietly = TRUE)) {\n  install.packages(\"here\")\n}\nlibrary(here)\n\n\n\n# create data directory if it doesn't exist -----------------------------\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\n# define file paths ------------------------------------------------------\n# use here() to build paths relative to your project root\ndata_dir &lt;- here::here(\"data\")\n\n# download synthetic data ------------------------------------------------\n# specify the url for the data file\nurl &lt;- \"https://www.dropbox.com/scl/fi/ru0ecayju04ja8ky1mhel/df_nz_long.qs?rlkey=prpk9a5v4vcg1ilhkgf357dhd&dl=1\"\n\n# download to a temporary file for safety\ntmp_file &lt;- tempfile(fileext = \".qs\")\ndownload.file(url, tmp_file, mode = \"wb\")\n\n# read the data into R using qread\ndf_nz_long &lt;- qread(tmp_file)\n\n# inspect the data -------------------------------------------------------\n# view the first few rows to check it loaded correctly\nprint(head(df_nz_long))\n\n# list column names so you know what variables are available\nprint(colnames(df_nz_long))\n\n# save a copy of the data ------------------------------------------------\n# save the dataset to your data directory for future use\nhere_save_qs(df_nz_long, \"df_nz_long\", data_dir)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+"
  },
  {
    "objectID": "content/08-content.html#script-1-initial-data-wrangling",
    "href": "content/08-content.html#script-1-initial-data-wrangling",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "Script 1: Initial Data Wrangling",
    "text": "Script 1: Initial Data Wrangling\n\n# script 1 workflow lecture 10\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# essential library ---------------------------------------------------------\n# install and load 'margot' from GitHub if missing\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.37\") {\n  stop(\"please install margot &gt;= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf,             # machine learning forests\n  kableExtra,      # tables\n  ggplot2,         # graphs\n  doParallel,       # parallel processing\n  grf,             # causal forests\n  janitor,          # variables names\n  stringr,          # variable names\n  patchwork,        # graphs\n  table1           # tables\n)\n\n\n# create directories --------------------------------------------------------\n# create data directory if it doesn't exist\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\nif (!dir.exists(\"save_directory\")) {\n  dir.create(\"save_directory\")  # first time only: make a folder named 'data'\n}\n\n# set up data directory structure\ndata_dir    &lt;- here::here(\"data\")\npush_mods &lt;- here::here(\"save_directory\") \n\n# load data -----------------------------------------------------------------\ndf_nz_long &lt;- margot::here_read_qs(\"df_nz_long\", data_dir)\n\n# initial data prep ---------------------------------------------------------\n# prepare intial data\n# define labels for rural classification\nrural_labels &lt;- c(\n  \"High Urban Accessibility\", \n  \"Medium Urban Accessibility\",\n  \"Low Urban Accessibility\", \n  \"Remote\", \n  \"Very Remote\"\n)\n\ndat_prep &lt;- df_nz_long |&gt;\n  arrange(id, wave) |&gt;\n  margot::remove_numeric_attributes() |&gt;\n  mutate(\n    # cap extreme values\n    alcohol_intensity = pmin(alcohol_intensity, 15),\n    # flag heavy drinkers: freq ≥3 → 1, ≤2 → 0, else NA\n    heavy_drinker = case_when(\n      alcohol_frequency &gt;= 3 ~ 1,\n      alcohol_frequency &lt;= 2 ~ 0,\n      TRUE                  ~ NA_real_\n    ),\n    # map freq categories to weekly counts\n    alcohol_frequency_weekly = recode(\n      alcohol_frequency,\n      `0` = 0, `1` = 0.25,\n      `2` = 1, `3` = 2.5,\n      `4` = 4.5,\n      .default = NA_real_\n    ),\n    # relabel rural factor\n    rural_gch_2018_l = factor(\n      rural_gch_2018_l,\n      levels = 1:5,\n      labels = rural_labels,\n      ordered = TRUE\n    )\n  ) |&gt;\n  droplevels()\n\n\n\n# view variable names -----------------------------------------------------\nprint(colnames(df_nz_long)) \n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# define study variables ----------------------------------------------------\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n\n# ** key decision 1: define your exposure variable **\nname_exposure &lt;- \"extraversion\"\n\n# exposure variable labels\nvar_labels_exposure &lt;- list(\n  \"extraversion\" = \"Extraversion\",\n  \"extraversion_binary\" = \"Extraversion (binary)\"\n)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n\n# +--------------------------+\n# | OPTIONALLY MODIFY SECTION|\n# +--------------------------+\n\n# **  define your study waves **\nbaseline_wave      &lt;- \"2018\"        # baseline measurement\nexposure_waves     &lt;- c(\"2019\")     # when exposure is measured\noutcome_wave       &lt;- \"2020\"        # when outcomes are measured\nall_waves          &lt;- c(baseline_wave, exposure_waves, outcome_wave)\n\n# **  define baseline covariates **\n# these are demographics, traits, etc. measured at baseline\n\nbaseline_vars &lt;- c(\n  # demographics\n  \"age\", \"born_nz_binary\", \"education_level_coarsen\",\n  \"employed_binary\", \"eth_cat\", \"male_binary\",\n  \"not_heterosexual_binary\", \"parent_binary\", \"partner_binary\",\n  \"rural_gch_2018_l\", \"sample_frame_opt_in_binary\",\n  \n  # personality traits (excluding exposure)\n  \"agreeableness\", \"conscientiousness\", \"neuroticism\", \"openness\",\n  \n  # health and lifestyle\n  \"alcohol_frequency\", \"alcohol_intensity\", \"hlth_disability_binary\",\n  \"log_hours_children\", \"log_hours_commute\", \"log_hours_exercise\",\n  \"log_hours_housework\", \"log_household_inc\",\n  \"short_form_health\", \"smoker_binary\",\n  \n  # social and psychological\n  \"belong\", \"nz_dep2018\", \"nzsei_13_l\",\n  \"political_conservative\", \"religion_identification_level\"\n)\n\n# +--------------------------+\n# |END OPTIONALLY MODIFY SEC.|\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n\n# ** key decision 3: define outcome variables **\n# here, we are focussing on a subset of wellbeing outcomes\n# chose outcomes relevant to * your * study. Might be all/some/none/exactly \n# these:\noutcome_vars &lt;- c(\n  # health outcomes\n  # \"alcohol_frequency_weekly\", \"alcohol_intensity\",\n  # \"hlth_bmi\", \n  \"log_hours_exercise\", \n  # \"hlth_sleep_hours\", \n  # \"short_form_health\",\n  \n  # psychological outcomes\n  # \"hlth_fatigue\", \n  \"kessler_latent_anxiety\", \n  \"kessler_latent_depression\", \n  \"rumination\",\n  \n  # well-being outcomes\n  # \"bodysat\", \n  #\"forgiveness\", \"gratitude\", \n  \"lifesat\", \"meaning_purpose\", \"meaning_sense\", \n  # \"perfectionism\", \n  \"pwi\", \n  #\"self_control\", \n  \"self_esteem\", \n  #\"sexual_satisfaction\",\n  \n  # social outcomes\n  \"belong\", \"neighbourhood_community\", \"support\"\n)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# after selecting your exposure/ baseline / outcome variables do not modify this\n# code\n\n# make binary variable (UNLESS YOUR EXPOSURE IS A BINARY VARIABLE)\nexposure_var_binary = paste0(name_exposure, \"_binary\")\n\n# make exposure variable list (we will keep both the continuous and binary variable)\nexposure_var  &lt;- c(name_exposure, paste0(name_exposure, \"_binary\"))\n\n# sort for easier reference\nbaseline_vars &lt;- sort(baseline_vars)\noutcome_vars &lt;- sort(outcome_vars)\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# save key variables --------------------------------------------------------\nmargot::here_save(name_exposure, \"name_exposure\")\nmargot::here_save(var_labels_exposure,\"var_labels_exposure\")\nmargot::here_save(baseline_vars,\"baseline_vars\")\nmargot::here_save(exposure_var, \"exposure_var\")\nmargot::here_save(exposure_var_binary, \"exposure_var_binary\")\nmargot::here_save(outcome_vars, \"outcome_vars\")\nmargot::here_save(baseline_wave, \"baseline_wave\")\nmargot::here_save(exposure_waves, \"exposure_waves\")\nmargot::here_save(outcome_wave, \"outcome_wave\")\nmargot::here_save(all_waves,\"all_waves\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# | OPTIONALLY MODIFY SECTION|\n# +--------------------------+\n\n# select eligible participants ----------------------------------------------\n# only include participants who have exposure data at baseline\n\n# You might require tighter conditions \n# for example, if you are interested in the effects of hours of childcare, \n# you might want to select only those who were parents at baseline. \n# talk to me if you think you might night tighter eligibility criteria.\n\nids_baseline &lt;- dat_prep |&gt; \n  # allow missing exposure at baseline\n  filter(wave == baseline_wave, !is.na(!!sym(name_exposure))) |&gt; \n  # option: do not allow missing exposure at baseline\n  filter(wave == baseline_wave, !is.na(!!sym(name_exposure))) |&gt; \n  pull(id)\n\n# filter data to include only eligible participants and relevant waves\ndat_long_1 &lt;- dat_prep |&gt; \n  filter(id %in% ids_baseline, wave %in% all_waves) |&gt; \n  droplevels()\n\n# +--------------------------+\n# |END OPTIONALLY MODIFY SEC.|\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n# plot distribution to help with cutpoint decision\n# get exposure wave to inspect exposure variable distribution\ndat_long_exposure &lt;- dat_long_1 |&gt; filter(wave %in% exposure_waves)\n\n# make graph \ngraph_cut &lt;- margot::margot_plot_categorical(\n  dat_long_exposure,\n  col_name         = name_exposure,\n  sd_multipliers = c(-1, 1), # select to suit\n  # either use n_divisions for equal-sized groups:\n  # n_divisions      = 2,\n  # or use custom_breaks for specific values:\n  custom_breaks    = c(1, 4),  # ** adjust as needed **\n  cutpoint_inclusive = \"upper\",\n  show_mean        = TRUE,\n  show_sd          = TRUE\n)\nprint(graph_cut)\n\n# save your graph\nmargot::here_save(graph_cut, \"graph_cut\", push_mods)\n\n# create binary exposure variable based on chosen cutpoint\ndat_long_2 &lt;- margot::create_ordered_variable(\n  dat_long_1,\n  var_name           = name_exposure,\n  custom_breaks      = c(1, 4),  # ** -- adjust based on your decision above -- **\n  cutpoint_inclusive = \"upper\"\n)\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# process binary variables and log-transform --------------------------------\n# convert binary factors to 0/1 format\ndat_long_3 &lt;- margot::margot_process_binary_vars(dat_long_2)\n\n# log-transform hours and income variables: tables for analysis (only logged versions of vars)\ndat_long_final &lt;- margot::margot_log_transform_vars(\n  dat_long_3,\n  vars            = c(starts_with(\"hours_\"), \"household_inc\"),\n  prefix          = \"log_\",\n  keep_original   = FALSE,\n  exceptions = exposure_var # omit original variables\n) |&gt; \n  # select only variables needed for analysis\n  select(all_of(c(baseline_vars, exposure_var, outcome_vars, \"id\", \"wave\", \"year_measured\", \"sample_weights\"))) |&gt; \n  droplevels()\n\n\n# check missing data --------------------------------------------------------\n# this is crucial to understand potential biases\nmissing_summary &lt;- naniar::miss_var_summary(dat_long_final)\nprint(missing_summary)\nmargot::here_save(missing_summary, \"missing_summary\", push_mods)\n\n# visualise missing data pattern\n# ** -- takes a while to render ** \nvis_miss &lt;- naniar::vis_miss(dat_long_final, warn_large_data = FALSE)\nprint(vis_miss)\nmargot::here_save(vis_miss, \"vis_miss\", push_mods)\n\n# calculate percentage of missing data at baseline\ndat_baseline_pct &lt;- dat_long_final |&gt; filter(wave == baseline_wave)\npercent_missing_baseline &lt;- naniar::pct_miss(dat_baseline_pct)\nmargot::here_save(percent_missing_baseline, \"percent_missing_baseline\", push_mods)\n\n# save prepared dataset for next stage --------------------------------------\nmargot::here_save(dat_long_final, \"dat_long_final\", push_mods)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n# create transition matrices to check positivity ----------------------------\n# this helps assess whether there are sufficient observations in all exposure states\ndt_positivity &lt;- dat_long_final |&gt;\n  filter(wave %in% c(baseline_wave, exposure_waves)) |&gt;\n  select(!!sym(name_exposure), id, wave) |&gt;\n  mutate(exposure = round(as.numeric(!!sym(name_exposure)), 0)) |&gt;\n  # create binary exposure based on cutpoint\n  mutate(exposure_binary = ifelse(exposure &gt;= 4, 1, 0)) |&gt; ## *-- modify this --* \n  mutate(wave = as.numeric(wave) -1 )\n\n# create transition tables\ntransition_tables &lt;- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"wave\",\n  table_name = \"transition_table\"\n)\nprint(transition_tables$tables[[1]])\nmargot::here_save(transition_tables, \"transition_tables\", push_mods)\n\n# create binary transition tables\ntransition_tables_binary &lt;- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure_binary\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"wave\",\n  table_name = \"transition_table_binary\"\n)\nprint(transition_tables_binary$tables[[1]])\nmargot::here_save(transition_tables_binary, \"transition_tables_binary\", push_mods)\n\n# create tables -----------------------------------------------------------\n# baseline variable labels\nvar_labels_baseline &lt;- list(\n  # demographics\n  \"age\" = \"Age\",\n  \"born_nz_binary\" = \"Born in NZ\",\n  \"education_level_coarsen\" = \"Education Level\",\n  \"employed_binary\" = \"Employed\",\n  \"eth_cat\" = \"Ethnicity\",\n  \"male_binary\" = \"Male\",\n  \"not_heterosexual_binary\" = \"Non-heterosexual\",\n  \"parent_binary\" = \"Parent\",\n  \"partner_binary\" = \"Has Partner\",\n  \"rural_gch_2018_l\" = \"Rural Classification\",\n  \"sample_frame_opt_in_binary\" = \"Sample Frame Opt-In\",\n  \n  # economic & social status\n  \"household_inc\" = \"Household Income\",\n  \"log_household_inc\" = \"Log Household Income\",\n  \"nz_dep2018\" = \"NZ Deprivation Index\",\n  \"nzsei_13_l\" = \"Occupational Prestige Index\",\n  \"household_inc\" = \"Household Income\",\n\n  \n  # personality traits\n  \"agreeableness\" = \"Agreeableness\",\n  \"conscientiousness\" = \"Conscientiousness\",\n  \"neuroticism\" = \"Neuroticism\",\n  \"openness\" = \"Openness\",\n  \n  # beliefs & attitudes\n  \"political_conservative\" = \"Political Conservatism\",\n  \"religion_identification_level\" = \"Religious Identification\",\n  \n  # health behaviors\n  \"alcohol_frequency\" = \"Alcohol Frequency\",\n  \"alcohol_intensity\" = \"Alcohol Intensity\",\n  \"hlth_disability_binary\" = \"Disability Status\",\n  \"smoker_binary\" = \"Smoker\",\n  \"hours_exercise\" = \"Hours of Exercise\",\n  \n  \n  # time use\n  \"hours_children\" = \"Hours with Children\",\n  \"hours_commute\" = \"Hours Commuting\",\n  \"hours_exercise\" = \"Hours Exercising\",\n  \"hours_housework\" = \"Hours on Housework\",\n  \"log_hours_children\" = \"Log Hours with Children\",\n  \"log_hours_commute\" = \"Log Hours Commuting\",\n  \"log_hours_exercise\" = \"Log Hours Exercising\",\n  \"log_hours_housework\" = \"Log Hours on Housework\"\n)\nhere_save(var_labels_baseline, \"var_labels_baseline\")\n\n# outcome variable labels, organized by domain\n# reivew your outcomes make sure they appear on the list below\n# comment out what you do not need\noutcome_vars\n\n# get names\nvar_labels_outcomes &lt;- list(\n  # \"alcohol_frequency_weekly\" = \"Alcohol Frequency (weekly)\",\n  # \"alcohol_intensity\" = \"Alcohol Intensity\",\n  # \"hlth_bmi\" = \"Body Mass Index\",\n  # \"hlth_sleep_hours\" = \"Sleep\",\n  \"log_hours_exercise\" = \"Hours of Exercise (log)\",\n # \"short_form_health\" = \"Short Form Health\",\n  \"hlth_fatigue\" = \"Fatigue\",\n  \"kessler_latent_anxiety\" = \"Anxiety\",\n  \"kessler_latent_depression\" = \"Depression\",\n # \"rumination\" = \"Rumination\",\n  \"bodysat\" = \"Body Satisfaction\",\n # \"forgiveness\" = \"Forgiveness\",\n # \"perfectionism\" = \"Perfectionism\",\n # \"self_control\" = \"Self Control\",\n  \"self_esteem\" = \"Self Esteem\",\n  \"sexual_satisfaction\" = \"Sexual Satisfaction\",\n # \"gratitude\" = \"Gratitude\",\n  \"lifesat\" = \"Life Satisfaction\",\n  \"meaning_purpose\" = \"Meaning: Purpose\",\n  \"meaning_sense\" = \"Meaning: Sense\",\n  \"pwi = Personal Well-being Index\",\n  \"belong\" = \"Social Belonging\",\n  \"neighbourhood_community\" = \"Neighbourhood Community\",\n  \"support\" = \"Social Support\"\n)\n\n# save for manuscript\nhere_save(var_labels_outcomes, \"var_labels_outcomes\")\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n# tables ------------------------------------------------------------------\n# create baseline characteristics table\ndat_baseline = dat_long_final |&gt;\n  filter(wave %in% c(baseline_wave)) |&gt;\n  mutate(\n    male_binary = factor(male_binary),\n    parent_binary = factor(parent_binary),\n    smoker_binary = factor(smoker_binary),\n    born_nz_binary = factor(born_nz_binary),\n    employed_binary = factor(employed_binary),\n    not_heterosexual_binary = factor(not_heterosexual_binary),\n    sample_frame_opt_in_binary = factor(sample_frame_opt_in_binary)\n  )\n\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n\n# save sample weights from baseline wave\n# save sample weights\nt0_sample_weights &lt;- dat_baseline$sample_weights\nhere_save(t0_sample_weights, \"t0_sample_weights\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# make baseline table -----------------------------------------------------\n\nbaseline_table &lt;- margot::margot_make_tables(\n  data = dat_baseline,\n  vars = baseline_vars,\n  by = \"wave\",\n  labels = var_labels_baseline,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(baseline_table)\nmargot::here_save(baseline_table, \"baseline_table\", push_mods)\n\n# create exposure table by wave\nexposure_table &lt;- margot::margot_make_tables(\n  data = dat_long_final |&gt; filter(wave %in% c(baseline_wave, exposure_waves)),\n  vars = exposure_var,\n  by = \"wave\",\n  labels = var_labels_exposure,\n  factor_vars = exposure_var_binary,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(exposure_table)\nmargot::here_save(exposure_table, \"exposure_table\", push_mods)\n\n# create outcomes table by wave\noutcomes_table &lt;- margot::margot_make_tables(\n  data = dat_long_final |&gt; filter(wave %in% c(baseline_wave, outcome_wave)),\n  vars = outcome_vars,\n  by = \"wave\",\n  labels = var_labels_outcomes,\n  format = \"markdown\"\n)\nprint(outcomes_table)\nmargot::here_save(outcomes_table, \"outcomes_table\", push_mods)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n\n\n# note: completed data preparation step -------------------------------------\n# you're now ready for the next steps:\n# 1. creating wide-format dataset for analysis \n# 2. applying causal inference methods\n# 3. conducting sensitivity analyses\n\n# key decisions summary:\n# exposure variable: extraversion\n# study waves: baseline (2018), exposure (2019), outcome (2020)\n# baseline covariates: demographics, traits, health measures (excluding exposure)\n# outcomes: health, psychological, wellbeing, and social variables\n# binary cutpoint for exposure: here, 4 on the extraversion scale\n# label names for tables\n\n\n\n\n# THIS IS FOR INTEREST ONLY ----------------------------------------------------\n# uncomment to view random chang in individuals\n# visualise individual changes in exposure over time ------------------------\n# useful for understanding exposure dynamics\n# individual_plot &lt;- margot_plot_individual_responses(\n#   dat_long_1,\n#   y_vars = name_exposure,\n#   id_col = \"id\",\n#   waves = c(2018:2019),\n#   random_draws = 56,  # number of randomly selected individuals to show\n#   theme = theme_classic(),\n#   scale_range = c(1, 7),  # range of the exposure variable\n#   full_response_scale = TRUE,\n#   seed = 123\n# )\n# print(individual_plot)"
  },
  {
    "objectID": "content/08-content.html#script-2-make-wide-data-format-with-censoring-weights",
    "href": "content/08-content.html#script-2-make-wide-data-format-with-censoring-weights",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "Script 2: Make Wide Data Format With Censoring Weights",
    "text": "Script 2: Make Wide Data Format With Censoring Weights\n\n# script 2: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# libraries ---------------------------------------------------------------\n# essential library ---------------------------------------------------------\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.37\") {\n  stop(\"please install margot &gt;= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\nlibrary(margot)\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf,             # machine learning forests\n  kableExtra,      # tables\n  ggplot2,         # graphs\n  doParallel,      # parallel processing\n  grf,             # causal forests\n  janitor,         # variables names\n  stringr,         # variable names\n  patchwork,       # graphs\n  table1           # tables\n)\n\n# save paths -------------------------------------------------------------------\npush_mods &lt;- here::here(\"save_directory\") \n\n# read data\ndat_long_final &lt;- margot::here_read(\"dat_long_final\")\n\n# read baseline sample weights\nt0_sample_weights &lt;- margot::here_read(\"t0_sample_weights\")\n\n# read exposure\nname_exposure &lt;- margot::here_read(\"name_exposure\")\nname_exposure_binary = paste0(name_exposure, \"_binary\")\nname_exposure_continuous = name_exposure\n\n# read variables\nbaseline_vars &lt;- margot::here_read(\"baseline_vars\")\nexposure_var &lt;- margot::here_read(\"exposure_var\")\noutcome_vars &lt;- margot::here_read(\"outcome_vars\")\nbaseline_wave &lt;- margot::here_read(\"baseline_wave\")\nexposure_waves &lt;- margot::here_read(\"exposure_waves\")\noutcome_wave &lt;- margot::here_read(\"outcome_wave\")\n\n# define continuous columns to keep\ncontinuous_columns_keep &lt;- c(\"t0_sample_weights\")\n\n# define ordinal columns that we will expand into binary variables\nordinal_columns &lt;- c(\"t0_education_level_coarsen\",\n                     \"t0_eth_cat\",\n                     \"t0_rural_gch_2018_l\")\n\n# check is this the exposure variable that you want? \nname_exposure_binary\nname_exposure_continuous\n\n# define wide variable names\nt0_name_exposure_binary &lt;- paste0(\"t0_\", name_exposure_binary)\nt0_name_exposure_binary\n\n# make exposure names (continuous not genreally used)\nt1_name_exposure_binary &lt;- paste0(\"t1_\", name_exposure_binary)\nt1_name_exposure_binary\n\n# treatments (continuous verion)\nt0_name_exposure &lt;- paste0(\"t0_\", name_exposure_continuous)\nt1_name_exposure &lt;- paste0(\"t1_\", name_exposure_continuous)\nt0_name_exposure_continuous &lt;- paste0(\"t0_\", name_exposure)\nt1_name_exposure_continuous &lt;- paste0(\"t1_\", name_exposure)\n\n# raw outcomes\n# read health outcomes\noutcome_vars &lt;- here_read(\"outcome_vars\")\nt2_outcome_z &lt;- paste0(\"t2_\", outcome_vars, \"_z\")\n\n# view\nt2_outcome_z\n\n# check\nstr(dat_long_final)\n\n# check\nnaniar::gg_miss_var(dat_long_final)\n\n# impute data --------------------------------------------------------------\n# ordinal use\nordinal_columns &lt;- c(\n  \"t0_education_level_coarsen\",\n  \"t0_eth_cat\",\n  \"t0_rural_gch_2018_l\",\n  \"t0_gen_cohort\"\n)\n\n# define cols we will not standardise\ncontinuous_columns_keep &lt;- c(\"t0_sample_weights\")\n\n# remove sample weights\ndat_long_final_2 &lt;- dat_long_final |&gt; select(-sample_weights)\n\n# prepare data for analysis ----------------------\ndat_long_final_2 &lt;- margot::remove_numeric_attributes(dat_long_final_2)\n# wide data\ndf_wide &lt;- margot_wide_machine(\n  dat_long_final,\n  id = \"id\",\n  wave = \"wave\",\n  baseline_vars,\n  exposure_var = exposure_var,\n  outcome_vars,\n  confounder_vars = NULL,\n  imputation_method = \"none\",\n  include_exposure_var_baseline = TRUE,\n  include_outcome_vars_baseline = TRUE,\n  extend_baseline = FALSE,\n  include_na_indicators = FALSE\n)\n\n# check\ncolnames(df_wide)\n\n# return sample weights\ndf_wide$t0_sample_weights &lt;-  t0_sample_weights\n\n# save\nmargot::here_save(df_wide, \"df_wide\")\n\n\n#df_wide &lt;- margot::here_read(\"df_wide\")\nnaniar::vis_miss(df_wide, warn_large_data = FALSE)\n\n\n# order data with missingness assigned to work with grf and lmtp\n# if any outcome is censored all are censored\n# create version for model reports\n\n# check\ncolnames(df_wide)\n\n\n# made data wide in correct format\n# ** ignore warning *** \ndf_wide_encoded  &lt;- margot::margot_process_longitudinal_data_wider(\n  df_wide,\n  ordinal_columns = ordinal_columns,\n  continuous_columns_keep = continuous_columns_keep,\n  not_lost_in_following_wave = \"not_lost_following_wave\",\n  lost_in_following_wave = \"lost_following_wave\",\n  remove_selected_columns = TRUE,\n  exposure_var = exposure_var,\n  scale_continuous = TRUE,\n  censored_if_any_lost = FALSE\n)\n\n# check\ncolnames(df_wide_encoded)\n\n# check\ntable(df_wide_encoded$t0_not_lost_following_wave)\n\n# make the binary variable numeric\ndf_wide_encoded[[t0_name_exposure_binary]] &lt;-\n  as.numeric(df_wide_encoded[[t0_name_exposure_binary]]) - 1\ndf_wide_encoded[[t1_name_exposure_binary]] &lt;-\n  as.numeric(df_wide_encoded[[t1_name_exposure_binary]]) - 1\n\n# view\ndf_wide_encoded[[t0_name_exposure_binary]]\ndf_wide_encoded[[t1_name_exposure_binary]]\n\n# 1. ensure both binaries only take values 0 or 1 (ignore NA)\nstopifnot(all(df_wide_encoded[[t0_name_exposure_binary]][!is.na(df_wide_encoded[[t0_name_exposure_binary]])] %in% 0:1),\n          all(df_wide_encoded[[t1_name_exposure_binary]][!is.na(df_wide_encoded[[t1_name_exposure_binary]])] %in% 0:1))\n\n# 2. ensure NA‐patterns match between t1_exposure and t0_lost flag\n# count n-as in t1 exposure\nn_na_t1 &lt;- sum(is.na(df_wide_encoded[[t1_name_exposure_binary]]))\n\n# count how many were lost at t0\nn_lost_t0 &lt;- sum(df_wide_encoded$t0_lost_following_wave == 1, na.rm = TRUE)\n\n# print them for inspection\nmessage(\"NAs in \", t1_name_exposure_binary, \": \", n_na_t1)\nmessage(\"t0_lost_following_wave == 1: \", n_lost_t0)\n\n# stop if they don’t match\nstopifnot(n_na_t1 == n_lost_t0)\n\n# 3. ensure if t1 is non‐NA then subject was not lost at t0\nstopifnot(all(is.na(df_wide_encoded[[t1_name_exposure_binary]]) |\n                df_wide_encoded[[\"t0_not_lost_following_wave\"]] == 1))\n\n# view\nhead(df_wide_encoded)\n\n#naniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\nnaniar::gg_miss_var(df_wide_encoded)\n\n\n# predict attrition and create censoring weights --------------------------\n# step 1: prepare baseline covariates\n# select all t0_ variables except the exposure binary and any _lost indicators, then sort their names\nt0_var_names &lt;- df_wide_encoded |&gt;\n  select(-all_of(t0_name_exposure_binary)) |&gt;\n  select(starts_with(\"t0_\"),-ends_with(\"_lost\"),-ends_with(\"lost_following_wave\"), -ends_with(\"_weights\")) |&gt;\n  colnames() |&gt;\n  sort()\n\n# get unique values (to be safe)\nE &lt;- unique(t0_var_names)\n\n# view\nprint(E)\n\n# save baseline covariates\nmargot::here_save(E, \"E\")\n\n# view\nprint(E)\n\n# step 2: calculate weights for t0\nD_0 &lt;- as.factor(df_wide_encoded$t0_lost_following_wave)\n\n# get co-variates\ncen_0 &lt;- df_wide_encoded[, E]\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# probability forest for censoring\n# this will take time\ncen_forest_0 &lt;- probability_forest(cen_0, D_0)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# get predictions\npredictions_grf_0 &lt;- predict(cen_forest_0, newdata = cen_0, type = \"response\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n\n# get propensity scores\npscore_0 &lt;- predictions_grf_0$pred[, 2]\n\n# use margot_adjust_weights for t0\nt0_weights &lt;- margot_adjust_weights(\n  pscore = pscore_0,\n  trim = TRUE,\n  normalize = TRUE,\n  # lower trimming\n  lower_percentile = 0.00,\n  # upper trimming\n  upper_percentile = 0.99,\n  censoring_indicator = df_wide_encoded$t0_lost_following_wave,\n  sample_weights = df_wide_encoded$t0_sample_weights\n)\n\n# view\nhist(t0_weights$adjusted_weights)\n\n# give weights\ndf_wide_encoded$t0_adjusted_weights &lt;- t0_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\n\n# remove lost next wave (censored)\ndf_wide_encoded_1 &lt;- df_wide_encoded %&gt;%\n  filter(t0_lost_following_wave == 0) %&gt;%\n  droplevels()\n\n# step 4: calculate weights for t1\nE_and_exposure &lt;- c(E, t1_name_exposure_continuous)\nD_1 &lt;- as.factor(df_wide_encoded_1$t1_lost_following_wave)\ncen_1 &lt;- df_wide_encoded_1[, E_and_exposure]\n\n# probability forest for censoring\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\ncen_forest_1 &lt;- probability_forest(cen_1, D_1, sample.weights = df_wide_encoded_1$t0_adjusted_weights)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# predict forest\n\npredictions_grf_1 &lt;- predict(cen_forest_1, newdata = cen_1, type = \"response\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# get propensity score\npscore_1 &lt;- predictions_grf_1$pred[, 2]\n\n# check\nhist(pscore_1)\n\n# use margot_adjust_weights for t1\n# we will use these weights for inference in our models\nt1_weights &lt;- margot_adjust_weights(\n  pscore = pscore_1,\n  trim = TRUE,\n  normalize = TRUE,\n  lower_percentile = 0.00,\n  # upper trimming\n  upper_percentile = 0.99,\n  censoring_indicator = df_wide_encoded_1$t1_lost_following_wave,\n  sample_weights = df_wide_encoded_1$t0_adjusted_weights # combine with weights\n)\n\n# add weights -- these will be the weights we use\ndf_wide_encoded_1$t1_adjusted_weights &lt;- t1_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded_1, warn_large_data = FALSE)\n\n# save\nhere_save(df_wide_encoded_1, \"df_wide_encoded_1\")\n\n# check names\ncolnames(df_wide_encoded_1)\n\n# check\ndf_wide_encoded_1[[t1_name_exposure_binary]]\n\n# step 5: prepare final dataset\nnrow(df_wide_encoded_1)\ntable(df_wide_encoded_1$t1_lost_following_wave)\n\n# arrange\ndf_grf &lt;- df_wide_encoded_1 |&gt;\n  filter(t1_lost_following_wave == 0) |&gt;\n  select(\n    where(is.factor),\n    ends_with(\"_binary\"),\n    ends_with(\"_lost_following_wave\"),\n    ends_with(\"_z\"),\n    ends_with(\"_weights\"),\n    starts_with(\"t0_\"),\n    starts_with(\"t1_\"),\n    starts_with(\"t2_\"),\n  ) |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\")) |&gt;\n  relocate(starts_with(\"t1_\"), .before = starts_with(\"t2_\")) |&gt;\n  relocate(\"t0_not_lost_following_wave\", .before = starts_with(\"t1_\")) |&gt;\n  relocate(all_of(t1_name_exposure_binary), .before = starts_with(\"t2_\")) |&gt;\n  droplevels()\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# make sure to do this\n# save final data\nmargot::here_save(df_grf, \"df_grf\")\ndf_grf &lt;- margot::here_read(\"df_grf\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# check final dataset\ncolnames(df_grf)\n\n# visualise missing\n# should have no missing in t1 and t2 variables\n# handled by IPCW\n# make final missing data graph\nmissing_final_data_plot &lt;- naniar::vis_miss(df_grf, warn_large_data = FALSE)\nmissing_final_data_plot\n\n# save plot\nmargot_save_png(missing_final_data_plot, prefix = \"missing_final_data\")\n\n# checks\ncolnames(df_grf)\nstr(df_grf)\n\n# check exposures\ntable(df_grf[[t1_name_exposure_binary]])\n\n# check\nhist(df_grf$t1_adjusted_weights)\n\n# calculate summary statistics\nt0_weight_summary &lt;- summary(df_wide_encoded)\n\n# check\nglimpse(df_grf$t1_adjusted_weights)\n\n# visualise weight distributions\nhist(df_grf$t1_adjusted_weights, main = \"t0_stabalised weights\", xlab = \"Weight\")\n\n# check n\nn_observed_grf &lt;- nrow(df_grf)\n\n# view\nn_observed_grf\n\n# save\nmargot::here_save(n_observed_grf, \"n_observed_grf\")\n\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n\n# this is just for your interest ------------------------------------------\n# not used in final manuscript\n# FOR INTEREESTS\n# inspect propensity scores -----------------------------------------------\n# get data\n# df_grf &lt;- here_read('df_grf')\n# \n# # assign weights var name\n# weights_var_name = \"t0_adjusted_weights\"\n# \n# # baseline covariates  # E already exists and is defined\n# E\n# \n# # must be a data frame, no NA in exposure\n# \n# # df_grf is a data frame - we must process this data frame in several steps\n# # user to specify which columns are outcomes, default to 'starts_with(\"t2_\")'\n# df_propensity_org &lt;- df_grf |&gt; select(!starts_with(\"t2_\"))\n# \n# # Remove NAs and print message that this has been done\n# df_propensity &lt;- df_propensity_org |&gt; drop_na() |&gt; droplevels()\n# \n# # E_propensity_names\n# # first run model for baseline propensity if this is selected.  The default should be to not select it.\n# propensity_model_and_plots &lt;- margot_propensity_model_and_plots(\n#   df_propensity = df_propensity,\n#   exposure_variable = t1_name_exposure_binary,\n#   baseline_vars = E,\n#   weights_var_name = weights_var_name,\n#   estimand = \"ATE\",\n#   method = \"ebal\",\n#   focal = NULL\n# )\n# \n# # visualise\n# summary(propensity_model_and_plots$match_propensity)\n# \n# # key plot\n# propensity_model_and_plots$love_plot\n# \n# # other plots\n# propensity_model_and_plots$summary_plot\n# propensity_model_and_plots$balance_table\n# propensity_model_and_plots$diagnostics\n# \n# \n# # check size\n# size_bytes &lt;- object.size(propensity_model_and_plots)\n# print(size_bytes, units = \"auto\") # Mb\n# \n# # use qs to save only if you have space\n# here_save_qs(propensity_model_and_plots,\n#              \"propensity_model_and_plots\",\n#              push_mods)"
  },
  {
    "objectID": "content/08-content.html#script-3-models-graphs",
    "href": "content/08-content.html#script-3-models-graphs",
    "title": "Estimation of ATE and CATE Using Machine Learning",
    "section": "Script 3: Models & Graphs",
    "text": "Script 3: Models & Graphs\n\n# script 3: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session\n\nrstudioapi::restartSession()\n\n\n\n# reproducibility ---------------------------------------------------------\n\n\nset.seed(123)\n\n\n# essential library ---------------------------------------------------------\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") &lt; \"1.0.37\") {\n  stop(\"please install margot &gt;= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n# check package version\npackageVersion(pkg = \"margot\")\n\n\n\n# load libraries ----------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf, ranger,     # machine learning forests\n  doParallel,      # parallel processing,\n  kableExtra,\n  ggplot2 ,        # graphs\n  rlang ,          # functions for base types/Core R/ 'Tidyverse'\n  purrr ,          # functional programming tools.\n  patchwork,      # nice graph placement\n  janitor,         # nice labels\n  glue            # format/ interpolate a string\n)\n\n\n\n# directory path configuration -----------------------------------------------\n# save path (customise for your own computer) ----------------------------\npush_mods &lt;- here::here(\"save_directory\") \n\n# read original data (for plots) ------------------------------------------\noriginal_df &lt;- margot::here_read(\"df_wide\", push_mods)\n\n# plot title --------------------------------------------------------------\ntitle_binary = \"Effects of {{name_exposure}} on {{name_outcomes}}\"\nfilename_prefix = \"grf_extraversion_wb\"\n\n# for manuscript later\nmargot::here_save(title_binary,\"title_binary\")\n\n# import names ------------------------------------------------------------\nname_exposure &lt;- margot::here_read(\"name_exposure\")\nname_exposure\n\n# make exposure names\nt1_name_exposure_binary &lt;- paste0(\"t1_\", name_exposure, \"_binary\")\n\n# check exposure name\nt1_name_exposure_binary\n\n# read outcome vars\noutcome_vars &lt;- margot::here_read(\"outcome_vars\")\n\n# read and sort outcome variables -----------------------------------------\n# we do this by domain: health, psych, present, life, social\nread_and_sort &lt;- function(key) {\n  raw  &lt;- margot::here_read(key, push_mods)\n  vars &lt;- paste0(\"t2_\", raw, \"_z\")\n  sort(vars)\n}\nt2_outcome_z  &lt;- read_and_sort(\"outcome_vars\")\n\n# view\nt2_outcome_z\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# define names for titles -------------------------------------------------\n\nnice_exposure_name = \"Extraversion\"\nnice_outcome_name = \"Wellbeing\"\ntitle = \"Effect of {{nice_exposure_name}} on {{nice_outcome_name}}\"\n\n# save for final rport\nhere_save(title, \"title\")\n\n# combine outcomes ---------------------------------------------------------\n# check outcome vars and make labels for graphs/tables\noutcome_vars\n\n\nlabel_mapping_all &lt;- list(\n  #\"t2_alcohol_frequency_weekly_z\" = \"Alcohol Frequency\",\n  #\"t2_alcohol_intensity_weekly_z\" = \"Alcohol Intensity\",\n  #\"t2_hlth_bmi_z\" = \"BMI\",\n  #\"t2_hlth_sleep_hours_z\" = \"Sleep\",\n  \"t2_log_hours_exercise_z\" = \"Hours of Exercise (log)\",\n  #\"t2_short_form_health_z\" = \"Short Form Health\"\n  \"t2_hlth_fatigue_z\" = \"Fatigue\",\n  \"t2_kessler_latent_anxiety_z\" = \"Anxiety\",\n  \"t2_kessler_latent_depression_z\" = \"Depression\",\n  \"t2_rumination_z\" = \"Rumination\",\n  # \"t2_bodysat_z\" = \"Body Satisfaction\",\n  \"t2_foregiveness_z\" = \"Forgiveness\",\n  \"t2_perfectionism_z\" = \"Perfectionism\", \n  \"t2_self_esteem_z\" = \"Self Esteem\",\n  # \"t2_self_control_z\" = \"Self Control\",\n  # \"t2_sexual_satisfaction_z\" = \"Sexual Satisfaction\".\n  \"t2_gratitude_z\" = \"Gratitude\",\n  \"t2_lifesat_z\" = \"Life Satisfaction\",\n  \"t2_meaning_purpose_z\" = \"Meaning: Purpose\",\n  \"t2_meaning_sense_z\" = \"Meaning: Sense\",\n  \"t2_pwi_z\" = \"Personal Well-being Index\",\n  \"t2_belong_z\" = \"Social Belonging\",\n  \"t2_neighbourhood_community_z\" = \"Neighbourhood Community\",\n  \"t2_support_z\" = \"Social Support\"\n)\n\n\n# save\nhere_save(label_mapping_all, \"label_mapping_all\")\n\n# check\nlabel_mapping_all\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# select options that make sense fo your study/results\n# might need to be tweaked after the analysis\n\n# make options -------------------------------------------------------------\n# titles\ntitle = \"ATE Effects of {{nice_name_exposure}} on {{nice_name_outcome}}\"\nsubtitle = \"\"\nfilename_prefix = \"final_report\"\n\n\n# settings\nx_offset = -.5\nx_lim_lo = -.5\nx_lim_hi = .5\n\n\n# defaults for ate plots\nbase_defaults_binary &lt;- list(\n  type = \"RD\",\n  title = title_binary,\n  e_val_bound_threshold = 1.2,\n  colors = c(\n    \"positive\" = \"#E69F00\",\n    \"not reliable\" = \"grey50\",\n    \"negative\" = \"#56B4E9\"\n  ),\n  x_offset = x_offset,\n  # will be set based on type\n  x_lim_lo = x_lim_lo,\n  # will be set based on type\n  x_lim_hi = x_lim_hi,\n  text_size = 4,\n  linewidth = 0.5,\n  estimate_scale = 1,\n  base_size = 18,\n  point_size = 2,\n  title_size = 19,\n  subtitle_size = 16,\n  legend_text_size = 10,\n  legend_title_size = 10,\n  include_coefficients = FALSE\n)\n\n# health graph options\noutcomes_options_all &lt;- margot_plot_create_options(\n  title = subtitle,\n  base_defaults = base_defaults_binary,\n  subtitle = subtitle,\n  filename_prefix = filename_prefix\n)\n\n\n# policy tree graph settings ----------------------------------------------\ndecision_tree_defaults &lt;- list(\n  span_ratio       = .3,\n  text_size        = 3.8,\n  y_padding        = 0.25,\n  edge_label_offset = .002,\n  border_size      = .05\n)\npolicy_tree_defaults &lt;- list(\n  point_alpha       = .5,\n  title_size        = 12,\n  subtitle_size     = 12,\n  axis_title_size   = 12,\n  legend_title_size = 12,\n  split_line_color  = \"red\",\n  split_line_alpha  = .8,\n  split_label_color = \"red\",\n  list(split_label_nudge_factor = 0.007)\n)\n\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +----------------------------------------------+\n# |       DO NOT ALTER  (except where noted)     |\n# +----------------------------------------------+\n\n# load GRF data and prepare inputs ----------------------------------------\ndf_grf &lt;- margot::here_read('df_grf', push_mods)\nE      &lt;- margot::here_read('E',      push_mods)\n# check exposure binary\nstopifnot(all(df_grf[[t1_name_exposure_binary]][!is.na(df_grf[[t1_name_exposure_binary]])] %in% 0:1))\n# set exposure and weights\nW       &lt;- as.vector(df_grf[[t1_name_exposure_binary]]) # note it is the processed weights for attrition \"t1\"\nweights &lt;- df_grf$t1_adjusted_weights\nhist(weights) # quick check for extreme weights\n# select covariates and drop numeric attributes\nX &lt;- margot::remove_numeric_attributes(df_grf[E])\n\n\n# set model defaults -----------------------------------------------------\ngrf_defaults &lt;- list(seed = 123, stabilize.splits = TRUE, num.trees = 2000)\n\n\n# example: fit causal forest on a toy subset ------------------------------\n# first, create a smaller test sample\nn   &lt;- nrow(X)\ntoy &lt;- sample(seq_len(n), floor(n / 4))\n# define toy data\ntoy_data     &lt;- df_grf[toy, ]\nX_toy        &lt;- X[toy, ]\nW_toy        &lt;- W[toy]\nweights_toy  &lt;- weights[toy]\n\n# fit the model\ncf_out &lt;- margot_causal_forest(\n  data         = toy_data,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  outcome_vars = \"t2_kessler_latent_depression_z\", # select variable in your outcome_variable set\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  covariates   = X_toy,\n  W            = W_toy,\n  weights      = weights_toy,\n  save_data    = TRUE,\n  save_models  = TRUE\n)\n\n# inspect propensities ------------------------------------------------------\nqini_tbl &lt;- margot::margot_inspect_qini(cf_out, propensity_bounds = c(0.01, 0.97))\n\n# show\nprint(qini_tbl)\n\n# plot policy-combo trees --------------------------------------------------\ncombo1 &lt;- margot_plot_policy_combo(\n  result_object    = cf_out,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_name       = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  max_depth        = 1L,          # depth-1 tree\n  decision_tree_args = list(text_size = 4),\n  policy_tree_args   = list(point_alpha = 0.7),\n  original_df        = original_df,\n  label_mapping      = label_mapping_all\n)\n\n# show\ncombo1$combined_plot\n\n# you can repeat for depth-2 ----------------------------------------------\ncombo2 &lt;- margot_plot_policy_combo(\n  result_object    = cf_out,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_name       = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  max_depth        = 2L,\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  original_df        = original_df,\n  label_mapping      = label_mapping_all\n)\n\n# show\ncombo2$combined_plot\n\n# batch plotting ----------------------------------------------------------\nmodels_batch_1L &lt;- margot_policy(\n  cf_out,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names        = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df        = original_df,\n  label_mapping      = label_mapping_all,\n  max_depth          = 1L\n)\n\n# view first model's plots\nmodels_batch_1L[[1]][[3]]  # combo plot\nmodels_batch_1L[[1]][[4]]  # qini plot\n\n# sub plots\nmodels_batch_1L[[1]][[1]]  # predictions of policy tree\nmodels_batch_1L[[1]][[2]]  # policy tree\n\n# qini interpretations at different spends\n# negative is bad\nmodels_batch_1L[[1]][[5]]  \n\n# 2L tree\nmodels_batch_2L &lt;- margot_policy(\n  cf_out,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names        = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df        = original_df,\n  label_mapping      = label_mapping_all,\n  max_depth          = 2L)\n# view first model's plots\nmodels_batch_2L[[1]][[3]]  # combo plot\nmodels_batch_2L[[1]][[4]]  # qini plot - not convincing\n\n# 2. flip the selected outcomes (and regen trees)\n# use -- when the outcome is undesirable and we want to minimise it \n# (assuming the exposure is something we'd prescribe)\n\n# select models whose outcomes are undesirable  when the intervention is meant to be 'good'\n# such variables will be specific to your study \n\n# +--------------------------+\n# |    MODIFY THIS           |\n# +--------------------------+\n\nflip_outcomes_test = c(\"t2_kessler_latent_depression_z\")\n\n# function to get the labels from the models (labels were defined above)\nflipped_names_test &lt;- margot_get_labels(flip_outcomes_test, label_mapping_all)\n\n# +--------------------------+\n# |   END MODIFY             |\n# +--------------------------+\n\n# run flip forests\ncf_out_f &lt;- margot_flip_forests(\n  model_results = cf_out,\n  flip_outcomes = flip_outcomes_test,\n  recalc_policy = TRUE\n)\n\n# where there are very low or high propensity scores (prob of exposure) \n# we might consider trimming\nmargot::margot_inspect_qini(cf_out_f, propensity_bounds = c(0.01, 0.97))\n\n\n# if we had extreme scores (not used here)\n# cf_out_flipped_trimmed &lt;- margot_rescue_qini(model_results      = cf_out_f,\n#                                              propensity_bounds  = c(0.05, 0.95)) \n\n# flipped batch model\nmodels_batch_flipped_2L &lt;- margot_policy(\n  cf_out_f,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names = c(\"model_t2_kessler_latent_depression_z\"),\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth     = 2L\n)\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# flipped\n# interpretation: exposure minimising depression\nmodels_batch_flipped_2L[[1]][[3]]\n\n\n# *** NOTE DIFFERENCES IN INTERPRETATION\n\n# not flipped: exposure as maximizing depression\nmodels_batch_2L[[1]][[3]]\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n# interpretation example \n\n# test interpretations ----------------------------------------------------\n\n\n# policy tree interpretation: search depth = 1\ninterpret_model_policy_test_1L &lt;- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 1)\ncat(interpret_model_policy_test_1L)\n\n\n# policy tree interpretation: search depth = 2\ninterpret_model_policy_test_2L &lt;- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 2)\ncat(interpret_model_policy_test_2L)\n\n\n\n# interpret rate ----------------------------------------------------------\n\n# create rate analysis table\nrate_table_all_test &lt;- margot_rate(\n  models = cf_out_f,\n  policy = \"treat_best\",  # or \"withold_best\" but don't attempt fitting curves or policytrees\n  label_mapping = label_mapping_all\n)\n\n# view rate tables\nrate_table_all_test$rate_autoc |&gt; kbl(\"markdown\")\nrate_table_all_test$rate_qini |&gt; kbl(\"markdown\")\n\n\n# generate interpretation\nrate_interpretation_all &lt;- margot_interpret_rate(\n  rate_table_all_test, \n  flipped_outcomes = flipped_names_test\n)\n\n\n# ** uncomment to run full model**\n\n# causal forest model -----------------------------------------------------------\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\nmodels_binary &lt;- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# save model\nmargot::here_save_qs(models_binary, \"models_binary\", push_mods)\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# read results ------------------------------------------------------------\n# if you save models you do not need to re-run them\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# reading models takes time\n# if you want to check the size of an object use\n# margot::margot_size(object)\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\nmodels_binary &lt;- margot::here_read_qs(\"models_binary\", push_mods)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# count models by category\n# just a check\ncat(\"Number of original models:\\n\", length(models_binary$results), \"\\n\")\n\n\n# make ate plots ----------------------------------------------------------\nbinary_results &lt;- margot_plot(\n  models_binary$combined_table,\n  options = outcomes_options_all,\n  label_mapping = label_mapping_all,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  order = \"evaluebound_asc\",\n  original_df = original_df,\n  e_val_bound_threshold = 1.2\n)\n\n# view\nbinary_results$transformed_table |&gt; rename(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\") |&gt;\n  kbl(format = 'markdown')\n\n# check\nbinary_results$plot\n\n# interpretation\ncat(binary_results$interpretation)\n\n# nice table\ntables_list &lt;- list(\n  Wellbeing = binary_results$transformed_table\n)\n\n# make markdown tables (to be imported into the manuscript)\nmargot_bind_tables_markdown &lt;- margot_bind_tables(\n  tables_list = tables_list,\n  #list(all_models$combined_table),\n  sort_E_val_bound = \"desc\",\n  e_val_bound_threshold = 1.2,\n  # ← choose threshold\n  highlight_color = NULL,\n  bold = TRUE,\n  rename_cols = TRUE,\n  col_renames = list(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\"),\n  rename_ate = TRUE,\n  threshold_col = \"E_Val_bound\",\n  output_format = \"markdown\",\n  kbl_args = list(\n    booktabs = TRUE,\n    caption = NULL,\n    align = NULL\n  )\n)\n\n# view markdown table\nmargot_bind_tables_markdown\n\n# save for publication\nhere_save(margot_bind_tables_markdown, \"margot_bind_tables_markdown\")\n\n\n# evaluate models ---------------------------------------------------------\n# trim models if extreme propensity scores dominate\n# diag_tbl_98 &lt;- margot_inspect_qini(models_binary,\n#                                        propensity_bounds = c(0.01, 0.99))\n\n\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n# flipping models: outcomes we want to minimise given the exposure --------\n# standard negative outcomes/  not used in this study\n\n# +--------------------------+\n# |    MODIFY THIS           |\n# +--------------------------+\nflip_outcomes_standard = c(\n  #\"t2_alcohol_frequency_weekly_z\",\n  #\"t2_alcohol_intensity_z\",\n  #\"t2_hlth_bmi_z\",\n  #\"t2_hlth_fatigue_z\",\n  \"t2_kessler_latent_anxiety_z\", #  ← select\n  \"t2_kessler_latent_depression_z\",#  ← select\n  \"t2_rumination_z\" #  ← select\n  #\"t2_perfectionism_z\" # the exposure variable was not investigated\n)\n\n\n# we will investigate losses to these outcomes\n# usual flipped names for positive interventions\n# commented out for this study\n\n# WHICH OUTCOMES -- if any ARE UNDESIREABLE? \n\n# NOT IF THE EXPOSURE IS NEGATIVE, FOCUS ON WHICH OUTCOMES, if any, ARE POSITIVE AND FLIP THESE?\nflip_outcomes &lt;- flip_outcomes_standard #c( setdiff(t2_outcomes_all, flip_outcomes_standard) )\n\n# check\nflip_outcomes\n\n\n# +--------------------------+\n# |   END MODIFY             |\n# +--------------------------+\n\n\n# checks for when exposure is *damaging** \n# neg_check &lt;- vapply(all_models$results[ paste0(\"model_\", flip_outcomes) ],\n#                     \\(x) mean(x$tau_hat, na.rm = TRUE) &lt; 0, logical(1))\n# stopifnot(all(neg_check))   # every chosen outcome has a negative mean cate\n\n# get labels\nflipped_names &lt;- margot_get_labels(flip_outcomes, label_mapping_all)\n\n# check\nflipped_names\n\n# save for publication\nhere_save(flipped_names, \"flipped_names\")\n\n\n\n# flip negatively oriented outcomes --------------------------------------\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n\n# flip models using margot's function\n\n#  *** this will take some time ***\n\n# ** give it time **\n# ** once run/ comment out **\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\nmodels_binary_flipped_all &lt;- margot_flip_forests(models_binary,\n                                                 flip_outcomes = flip_outcomes,\n                                                 recalc_policy = TRUE)\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# save\nhere_save_qs(models_binary_flipped_all, \"models_binary_flipped_all\", push_mods)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# read back if needed\nmodels_binary_flipped_all &lt;- here_read_qs(\"models_binary_flipped_all\", push_mods)\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# where there are very low or high propensity scores (prob of exposure) we might consider trimming\n# margot::margot_inspect_qini(models_binary_flipped_all, propensity_bounds = c(0.05, 0.95))\n# \n# \n# # if we had extreme scores (not used here)\n# models_binary_flipped_all_t &lt;- margot_rescue_qini(model_results  = models_binary_flipped_all,\n#                                              propensity_bounds  = c(0.05, 0.95))\n\n\n\n# omnibus heterogeneity tests --------------------------------------------\n# test for treatment effect heterogeneity across all outcomes\nresult_ominbus_hetero_all &lt;- margot::margot_omnibus_hetero_test(models_binary_flipped_all,\n                                                                label_mapping = label_mapping_all)\n\n# view results table\nresult_ominbus_hetero_all$summary_table |&gt; kbl(\"markdown\")\n\n# view test interpretation\ncat(result_ominbus_hetero_all$brief_interpretation)\n\n# rate test analysis -----------------------------------------------------\n\n# create rate analysis table\nrate_table_all &lt;- margot_rate(\n  models = models_binary_flipped_all,\n  policy = \"treat_best\",  # or \"withold_best\" but don't attempt fitting curves or policytrees\n  label_mapping = label_mapping_all\n)\n\n# view rate tables\nrate_table_all$rate_autoc |&gt; kbl(\"markdown\")\nrate_table_all$rate_qini |&gt; kbl(\"markdown\")\n\n# generate interpretation\nrate_interpretation_all &lt;- margot_interpret_rate(\n  rate_table_all, \n  flipped_outcomes = flipped_names\n)\n\n# view interpretations\ncat(rate_interpretation_all$autoc_results)\ncat(rate_interpretation_all$qini_results)\n\n# compare rate and qini -- see grf documentation\ncat(rate_interpretation_all$comparison)\n\n# check out model names for different ways of thinking about heterogeneity\nrate_interpretation_all$either_model_names\nrate_interpretation_all$qini_model_names\nrate_interpretation_all$both_model_names\nrate_interpretation_all$autoc_model_names\n\n# autoc plots ------------------------------------------------------------\n# generate batch rate plots for models with significant heterogeneity\nbatch_rate_autoc_plots &lt;- margot_plot_rate_batch(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  # just use rate autoc for rate plots\n  model_names = rate_interpretation_all$autoc_model_names\n)\n\n# extract individual plots from the batch result\nautoc_plots &lt;- batch_rate_autoc_plots\n\n# determine number of columns based on number of plots\nnum_cols &lt;- ifelse(length(autoc_plots) &gt; 3, 2, 1)\n\n# combine plots using patchwork\nlibrary(patchwork)\n\n# only proceed if there are plots to combine\nif (length(autoc_plots) &gt; 0) {\n  # initialize with first plot\n  combined_autoc_plot &lt;- autoc_plots[[1]]\n  \n  # add remaining plots if any\n  if (length(autoc_plots) &gt; 1) {\n    for (i in 2:length(autoc_plots)) {\n      combined_autoc_plot &lt;- combined_autoc_plot + autoc_plots[[i]]\n    }\n  }\n  \n  # apply the dynamic layout\n  combined_autoc_plot &lt;- combined_autoc_plot +\n    plot_layout(ncol = num_cols) &\n    plot_annotation(\n      title = \"AUTOC Model Plots\",\n      subtitle = paste0(length(autoc_plots), \" models with significant heterogeneity\"),\n      tag_levels = \"A\"\n    )\n  \n  # view the combined plot\n  print(combined_autoc_plot)\n  \n  # save the combined plot if needed\n  width &lt;- ifelse(num_cols == 1, 8, 12)\n  height &lt;- 6 * ceiling(length(autoc_plots) / num_cols)\n  \n  ggsave(\n    here::here(push_mods, \"combined_autoc_plots.pdf\"),\n    combined_autoc_plot,\n    width = width,\n    height = height\n  )\n} else {\n  # handle case with no plots\n  message(\"No AUTOC plots available\")\n}\n\nmodels_batch_qini_2L_test &lt;- margot_plot_policy_combo(\n  models_binary_flipped_all,\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_name =  \"model_t2_log_hours_exercise_z\",\n  max_depth  = 2L,\n  # ← new argument\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\nrate_interpretation_all$qini_model_names\n\n# qini --------------------------------------------------------------------\n# run the margot_policy function\nmodels_batch_qini_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$qini_results,\n  max_depth  = 2L,\n  # ← new argument\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\n\n# extract the plots from the results\nplots &lt;- lapply(seq_along(models_batch_qini_2L), function(i) {\n  models_batch_qini_2L[[i]][[4]]  # extract the 4th element (plot) from each model\n})\n\n# name the plots\nnames(plots) &lt;- rate_interpretation_all$qini_model_names\n\n# determine number of columns based on number of plots\nnum_cols &lt;- ifelse(length(plots) &gt; 3, 2, 1)\n\n# load the patchwork library for combining plots\nlibrary(patchwork)\n\n# check if there are any plots to combine\nif (length(plots) == 0) {\n  message(\"no plots available to combine\")\n  NULL  # removed return since this isn't in a function\n} else {\n  # create combined plot\n  combined_plot &lt;- plots[[1]]\n  \n  # only run the loop if there are at least 2 plots\n  if (length(plots) &gt; 1) {\n    for (i in 2:length(plots)) {\n      combined_plot &lt;- combined_plot + plots[[i]]\n    }\n  }\n  \n  # apply the dynamic layout\n  combined_plot &lt;- combined_plot + plot_layout(ncol = num_cols)\n  # add titles and annotations\n  combined_plot &lt;- combined_plot &\n    plot_annotation(\n      title = \"Qini Model Plots\",\n      subtitle = paste0(length(plots), \n                        ifelse(length(plots) == 1, \" model \", \" models \"), \n                        \"arranged in \", num_cols, \n                        ifelse(num_cols == 1, \" column\", \" columns\")),\n      tag_levels = \"A\"  # adds a, b, c, etc. to the plots\n    )\n  # view\n  combined_plot\n  # save (optional)\n  width &lt;- ifelse(num_cols == 1, 8, 12)\n  height &lt;- 6 * ceiling(length(plots)/num_cols)  # height per row * number of rows\n  # save\n  ggsave(here::here(push_mods, \"combined_qini_plots.pdf\"),\n         combined_plot,\n         width = width, height = height)\n  \n  combined_plot  # removed return since this isn't in a function\n}\n\n# interpretation ----------------------------------------------------------\n# interpret qini curves\ninterpretation_qini_curves_2L &lt;- margot_interpret_qini(\n  models_batch_qini_2L,\n  model_names = rate_interpretation_all$qini_model_names,\n  label_mapping = label_mapping_all\n)\ninterpretation_qini_curves_2L\n\n# view qini interpretation\ncat(interpretation_qini_curves_2L$qini_explanation)\n\n# view summary table\ninterpretation_qini_curves_2L$summary_table |&gt; kbl(\"markdown\")\n\n\n\n# policy tree analysis depth 1 L------------------------------------------------\n# make policy trees\n# 1 l decision trees are generally very bad\nplots_policy_trees_1L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names,\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 1L\n)\n\n# get number of models\nn_models &lt;- length(rate_interpretation_all$either_model_names)\n\n# # use purrr to map through and print each model\n# purrr::map(1:n_models, function(i) {\n#   # print model name as a header\n#   cat(\"# model\", i, \"\\n\")\n#   # print the corresponding model plot\n#   print(plots_policy_trees_1L[[i]][[3]])\n#   # add spacing between models\n#   cat(\"\\n\\n\")\n# })\n\nmodel_outputs_1L &lt;- purrr::map(1:n_models, ~plots_policy_trees_1L[[.x]][[3]])\n\n# name the list elements by model number\nnames(model_outputs_1L) &lt;- paste0(\"model_\", 1:n_models)\n\n\n# check number of models == n_models\nmodel_outputs_1L$model_1 # convincing?\nmodel_outputs_1L$model_2 # convincing?\nmodel_outputs_1L$model_3 # convincing?\n\n\n\n# policy tree analysis depth 2L -------------------------------------------------\n# make policy trees\n# *** 2l is much more persuasive ***\nplots_policy_trees_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names,\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 2L\n)\n\nn_models &lt;- length(rate_interpretation_all$either_model_names)\n\nmodel_outputs_2L &lt;- purrr::map(1:n_models, ~plots_policy_trees_2L[[.x]][[3]])\nnames(model_outputs_2L) &lt;- paste0(\"model_\", 1:n_models)\n\n# checks\nmodel_outputs_2L$model_1\nmodel_outputs_2L$model_2\nmodel_outputs_2L$model_3\n\n\n# convincing?\ninterpret_plots_policy_trees_2L &lt;- margot_interpret_policy_batch(\n  models_binary_flipped_all, model_names = rate_interpretation_all$either_model_names)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_2L)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n# you can investigate policy trees for all outcomes, mindful that the rate and qini are not reliable. \n# still, with appropriate caution, this may help to clarify psychologically interesting questions\n\nall_plots_policy_trees_1L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # model_names = rate_interpretation_all$either_model_names, # use all\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 1L\n)\nn_models &lt;- length(models_binary_flipped_all$results)\nn_models\n\n\nmodel_outputs_1L_all &lt;- purrr::map(1:n_models, ~all_plots_policy_trees_1L[[.x]][[3]])\n\n# view\nmodel_outputs_1L_all[[1]] # ← convincing? \nmodel_outputs_1L_all[[2]] # ← convincing? \nmodel_outputs_1L_all[[3]] # ← convincing? \nmodel_outputs_1L_all[[4]] # ← convincing? \nmodel_outputs_1L_all[[5]] # ← convincing? \nmodel_outputs_1L_all[[6]] # ← convincing? \nmodel_outputs_1L_all[[7]] # ← convincing? \nmodel_outputs_1L_all[[8]] # ← convincing? \nmodel_outputs_1L_all[[9]] # ← convincing?  \nmodel_outputs_1L_all[[10]] # ← convincing? \nmodel_outputs_1L_all[[11]] # ← convincing? \nmodel_outputs_1L_all[[12]] # ← convincing? \n\n\n\n# interpretation\ninterpret_plots_policy_trees_1L_all &lt;- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 1)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_1L_all)\n\n\n# ALL model 1L\nall_plots_policy_trees_2L &lt;- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # model_names = rate_interpretation_all$either_model_names, # use all\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 2L\n)\nn_models &lt;- length(models_binary_flipped_all$results)\nn_models\n\n\nmodel_outputs_2L_all &lt;- purrr::map(1:n_models, ~all_plots_policy_trees_2L[[.x]][[3]])\n\n# view\nmodel_outputs_2L_all[[1]] # ← convincing? \nmodel_outputs_2L_all[[2]] # ← convincing? \nmodel_outputs_2L_all[[3]] # ← convincing? \nmodel_outputs_2L_all[[4]] # ← convincing? \nmodel_outputs_2L_all[[5]] # ← convincing? \nmodel_outputs_2L_all[[6]] # ← convincing? \nmodel_outputs_2L_all[[7]] # ← convincing? \nmodel_outputs_2L_all[[8]] # ← convincing? \nmodel_outputs_2L_all[[9]] # ← convincing? \nmodel_outputs_2L_all[[10]] # ← convincing? \nmodel_outputs_2L_all[[11]] # ← convincing? \nmodel_outputs_2L_all[[12]] # ←  convincing \n\n\n# interpretation\ninterpret_plots_policy_trees_2L_all &lt;- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 2)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_2L_all)\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n\n\n\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n#############################################################################\n# theoretical comparisons ---------------------------------------------------\n# individual theoretical comparisons (if relevant)\n# need to get values for wealth if wealth is compared\n\n# step 1 get information for wealth for conditonal comparisons\nhead(df_grf$t0_log_household_inc_z)\n\n# get mean on original data scale\nlog_mean_inc &lt;- mean(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# get sd on original data scale\nlog_sd_inc &lt;- sd(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# function to get back to data scale\nmargot_back_transform_log_z(\n  log_mean = log_mean_inc,\n  log_sd = log_sd_inc,\n  z_scores = c(-1, 0, 1),\n  label = \"data_scale\"\n)\n\n# define complex conditions for subsetting\ncomplex_condition_political &lt;- X[, \"t0_political_conservative_z\"] &gt; -1 &\n  X[, \"t0_political_conservative_z\"] &lt; 1\n\ncomplex_condition_wealth &lt;- X[, \"t0_log_household_inc_z\"] &gt; -1 &\n  X[, \"t0_log_household_inc_z\"] &lt; 1\n\ncomplex_condition_age &lt;- X[, \"t0_age_z\"] &gt; -1 &\n  X[, \"t0_age_z\"] &lt; 1\n\n# # if we have specific groups to compare\n# complex_condition_age_under_neg_1_sd  &lt;- X[, \"t0_age_z\"] &lt; -1\n# complex_condition_age_gr_eq_neg_1_sd  &lt;- X[, \"t0_age_z\"] &gt; -1\n\n# check ages to get number\nmean(original_df$t0_age) - sd(original_df$t0_age)\nmean(original_df$t0_age) + sd(original_df$t0_age)\n\n\n# wealth subsets\nsubsets_standard_wealth &lt;- list(\n  Poor = list(\n    var = \"t0_log_household_inc_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those HShold income &lt; -1 SD (NZD ~41k)\",\n    label = \"Poor\"  # label remains as is, but could be changed if desired\n  ),\n  MiddleIncome = list(subset_condition = complex_condition_wealth, description = \"Effects among those HS_hold income within +/-1SD (&gt; NZD 41k &lt; NZD 191k)\"),\n  Rich = list(\n    var = \"t0_log_household_inc_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those HS_hold income &gt; +1 SD (NZD 191k)\",\n    label = \"Rich\"\n  )\n)\n\n# political subsets\nsubsets_standard_political &lt;- list(\n  Liberal = list(\n    var = \"t0_political_conservative_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those &lt; -1 SD in political conservativism\",\n    label = \"Liberal\"\n  ),\n  Centrist = list(\n    var = \"t0_political_conservative_z\",\n    # operator = \"&lt;\",\n    subset_condition = complex_condition_political,\n    description = \"Effects among those &gt; -1 SD and &lt; +1 in political conservativism\",\n    label = \"Centrist\"\n  ),\n  Conservative = list(\n    var = \"t0_political_conservative_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those &gt; +1 SD in political conservativism\",\n    label = \"Conservative\"\n  )\n)\n\n\n# political subsets\nsubsets_standard_age &lt;- list(\n  Younger = list(\n    var = \"t0_age_z\",\n    value = -1,\n    operator = \"&lt;\",\n    description = \"Effects among those &lt; under 35 years old\",\n    label = \"Age &lt; 35\"\n  ),\n  Middle = list(\n    var = \"t0_age_z\",\n    # operator = \"&lt;\",\n    subset_condition = complex_condition_age,\n    description = \"Effects among those 35-62\",\n    label = \"Age 35-62\"\n  ),\n  Older = list(\n    var = \"t0_age_z\",\n    value = 1,\n    operator = \"&gt;\",\n    description = \"Effects among those &gt; 62\",\n    label = \"Age &gt; 62\"\n  )\n)\n\n\n# gender subsets\nsubsets_standard_gender &lt;- list(\n  Female = list(\n    var = \"t0_male_binary\",\n    value = 0,\n    description = \"Females\"\n  ),\n  Male = list(\n    var = \"t0_male_binary\",\n    value = 1,\n    description = \"Males\"\n  )\n)\n\n# ethnicity subsets\nsubsets_standard_ethnicity &lt;- list(\n  Asian = list(\n    var = \"t0_eth_cat_asian_binary\",\n    value = 1,\n    description = \"Asians\"\n  ),\n  Euro = list(\n    var = \"t0_eth_cat_euro_binary\",\n    value = 1,\n    description = \"Europeans (Pakeha)\"\n  ),\n  Pacific = list(\n    var = \"t0_eth_cat_pacific_binary\",\n    value = 1,\n    description = \"Pacific Peoples\"\n  ),\n  Maori = list(\n    var = \"t0_eth_cat_maori_binary\",\n    value = 1,\n    description = \"Māori\"\n  )\n)\n\n\n# batch planned subgroup analysis -----------------------------------------\n# set up lists of models, names, and subtitles\ndomain_models &lt;- list(\n  models_binary # HERE WE USE THE ORIGINAL MODELS\n)\n\n\n# set up domain names\ndomain_names &lt;- c(\"wellbeing\")\n\n# set up subtitles\nsubtitles &lt;- \"\"\n\n# set up subset types in a list\nsubset_types &lt;- list(\n  wealth = subsets_standard_wealth,\n  ethnicity = subsets_standard_ethnicity,\n  political = subsets_standard_political,\n  gender = subsets_standard_gender,\n  cohort = subsets_standard_age\n)\n\n\n# run model\nplanned_subset_results &lt;- margot_planned_subgroups_batch(\n  domain_models = domain_models,\n  X = X,\n  base_defaults = base_defaults_binary,\n  subset_types = subset_types,\n  original_df = original_df,\n  domain_names = domain_names,\n  subtitles = subtitles\n)\n\n\n# results\ncat(planned_subset_results$wellbeing$wealth$explanation)\ncat(planned_subset_results$wellbeing$ethnicity$explanation)\ncat(planned_subset_results$wellbeing$political$explanation)\ncat(planned_subset_results$wellbeing$gender$explanation)\ncat(planned_subset_results$wellbeing$cohort$explanation)\n\n\n\n# cohort subgroups --------------------------------------------------------\n\n# plots -------------------------------------------------------------------\n# results plots\n# health\nplots_subgroup_wealth&lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$wealth$results$Poor$plot,\n    planned_subset_results$wellbeing$wealth$results$MiddleIncome$plot,\n    planned_subset_results$wellbeing$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Wealth\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nplots_subgroup_wealth\n\n# plots\nplots_subgroup_ethnicity &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$ethnicity$results$Asian$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Euro$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Pacific$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Maori$plot\n    \n  ),\n  ncol = 2\n) +\n  patchwork::plot_annotation(title = \"Ethnicity\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_ethnicity)\n\n# plots\nplots_subgroup_political &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$political$results$Liberal$plot,\n    planned_subset_results$wellbeing$political$results$Centrist$plot,\n    planned_subset_results$wellbeing$political$results$Conservative$plot  \n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Political Orientation\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_political)\n\n# plots\nplots_subgroup_gender &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$gender$results$Female$plot,\n    planned_subset_results$wellbeing$gender$results$Male$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Gender\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_gender)\n\n# plots\nplots_subgroup_cohort &lt;- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$cohort$results$`Age &lt; 35`$plot,\n    planned_subset_results$wellbeing$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$wellbeing$cohort$results$`Age &gt; 62`$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Age Cohorts\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_cohort)\n\n\n\n# plot options: showcased ---------------------------------------------\n# default\nmargot_plot_decision_tree(models_binary, \"model_t2_support_z\", )\n# tighten branches for easier viewing in single graphs\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .30,\n  text_size = 3.8,\n  border_size = .1,\n  #  title = \"none\",\n  original_df = original_df\n)\n# colour decision node\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .3,\n  text_size = 4,\n  title = \"New Title\",\n  non_leaf_fill =  \"violet\",\n  original_df = original_df\n)\n# make new title\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .2,\n  text_size = 3,\n  title = \"New Title\",\n  non_leaf_fill =  \"white\",\n  original_df = original_df\n)\n\n# remove title\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  text_size = 5,\n  title = 'none',\n  # set title to none\n  original_df = original_df\n)\n\n\n# adjust only the alpha\nmargot::margot_plot_policy_tree(models_binary, \"model_t2_support_z\", point_alpha = .1)\nmargot::margot_plot_policy_tree(models_binary, \"model_t2_support_z\", point_alpha = .9)"
  },
  {
    "objectID": "content/09-content.html#code-explanations",
    "href": "content/09-content.html#code-explanations",
    "title": "Causal inference: a step by step guide",
    "section": "Code Explanations",
    "text": "Code Explanations\n\nScript 01 Initial Data Wrangling\nKey checkpoints in the wrangle-1 script are as follows;\n\nFirst — pick your exposure*\nDecide which variable is the “cause”. Here it is extraversion. Verify it exists at baseline (t0_) and at the exposure wave (t1_)\n# define study variables ----------------------------------------------------\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# ** key decision 1: define your exposure variable **\n\nname_exposure &lt;- \"extraversion\"\n\n# exposure variable labels\nvar_labels_exposure &lt;- list(\n  \"extraversion\" = \"Extraversion\",\n  \"extraversion_binary\" = \"Extraversion (binary)\"\n)\n\n\nSecond – choose a binary cut-point\nCausal inference requires a contrast between two conditions. The splits give us the conditions of a hypothetical experiment.\nmargot_plot_categorical(df,\n                        col_name       = name_exposure,\n                        custom_breaks  = c(0, 1))\nAsk: which break meaningfully separate groups into two exposure groups? Is the split *theoretically motivated**? adjust custom_breaks until the answer is “yes”. then commit:\ncreate_ordered_variable(df,\n                        var_name       = name_exposure,\n                        custom_breaks  = c(0, 1))\n\n\nThird — select outcomes that fit your interests\nThis example considers multi-dimensional wellbeing outcomes. You can select from these outcomes if you like. Or you can pick different outcomes. Discuss with us if you are confused*.\noutcome_vars &lt;- c(\"lifesat\", \"pwi\", \"self_esteem\")\nhere_save(outcome_vars, \"outcome_vars\")\nEverything downstream—eligibility, transitions… depends on this trio.\n\n\nFouth — locking in the wave structure.\nDefine one baseline wave, one or more exposure waves, and one outcome wave:\nbaseline_wave  &lt;- \"2018\"\nexposure_waves &lt;- \"2019\"\noutcome_wave   &lt;- \"2020\"\nNote waves run from October –&gt; September of the year folloowing the wave.\n\n\nFifth — save as you go.\nUse margot::here_save() (objects) and here_save_qs() (large objects) right after you create something you’ll cite in your report. The convention keeps analysis, tables, and manuscript in sync.\nhere_save(dat_long_final,          \"dat_long_final\")\nhere_save(percent_missing_baseline,  \"percent_missing_baseline\")\n\n\nTake-away: Think about the hypothetical experiment you wish to perform with observational data.\nDecide early (exposure, cut-point, outcomes, waves), checkpoint everything with here_save(), and the rest of the pipeline flows.\n\n\n\nScript 2: Prepare Final Wide Data\nPurpose\nConvert the tidy long data into a grf-ready wide matrix and correct for dropout bias with inverse-probability-of-censoring weighting (IPCW).\n\nStep 1 – go wide automatically\nmargot_wide_machine() reshapes each wave into t0_, t1_, t2_ columns. You gave it all the names in script 1, so no new choices here. Inspect df_wide just to be sure the columns look sensible (baseline before exposure before outcome).\ncolnames(df_wide)\nnaniar::vis_miss(df_wide)\n\n\nStep 2 – Encode + scale for algorithms\nmargot_process_longitudinal_data_wider() turns factors into dummies, scales continuous variables, and appends loss-to-follow-up flags. The result is df_wide_encoded.\nQuick sanity check: the exposure binaries should be 0/1 and their NAs must match the *_lost* indicators.\ntable(df_wide_encoded$t0_not_lost_following_wave)\n\n\nStep 3 – Build IPCW weights*\nTwo probability forests estimate the chance of being observed at t1 and t2. margot_adjust_weights() then trims extreme values and normalises the weights. these appear as t0_adjusted_weights and t1_adjusted_weights. The weigths should not be too extreme (e.g. values &gt; 10)\nhist(df_wide_encoded$t1_adjusted_weights)\nWhy? observations that resemble those who dropped out are up-weighted, so estimates stay unbiased when data vanish.\n\n\nStep 4 – drop the censored\nRows with *_lost_following_wave == 1 are removed, leaving df_grf. This dataset feeds the causal forest and should now have no missing values in any t1_ or t2_ column. Missingness is permitted in t0_ columns\nnaniar::vis_miss(df_grf)  \n\n\n\nGraph of Missingness in Final Cleaned Dataset, time is ordered left (early) to right (late), no missing values in outcomes, missingness permitted for baseline variables\n\n\n\n\nStep 5 – checkpoint everything\nEach major object (df_wide, df_wide_encoded, df_grf, the weight vectors) is saved with here_save() or here_save_qs(). this keeps the analysis reproducible and the manuscript tables in sync.\n\n\nTakeaway: what you must do\n\nRun the script without edits.\n\nskim the weight histograms; if you see huge spikes, ask for help.\n\nconfirm df_grf has the expected number of rows and no missing cells.\n\nScript 2 is a conveyor belt: long \\to wide \\to weighted \\to clean. yYur only job is to eyeball the parcels before they roll into the modelling bay.\n\n\n\nScript 3 The Analysis\n\n1 Reproducibility scaffold\nset.seed(123)                 # lock randomness  \npacman::p_load(margot, qs …)  # load every tool once  \nNo choices here—just run and move on.\n\n\n2 Pull in prepared objects\ndf_grf           &lt;- here_read(\"df_grf\")     # wide, weighted data  \nname_exposure    &lt;- here_read(\"name_exposure\")  \nIf anything is NULL, return to scripts 1–2 and debug.\n\n\n3 Sanity-check with a toy forest\nFit one outcome on ¼ of the data to confirm the pipeline before burning CPU hours.\ncf_out &lt;- margot_causal_forest(\n  data         = df_grf[sample(nrow(df_grf), nrow(df_grf)/4), ],\n  outcome_vars = \"t2_kessler_latent_depression_z\",\n  covariates   = X,           # baseline covariates E\n  W            = W,           # exposure\n  weights      = weights\n)\nmargot_plot_policy_combo(cf_out, max_depth = 1L)\nSuccess = a plot appears without errors.\n\n\n4 Full model batches + ATE tables\nLoop over the five outcome domains, saving each model to models_example_2/.\nmodels_binary &lt;- margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_z,\n  covariates   = X, W = W, weights = weights,\n  save_models  = TRUE, save_data = TRUE\n)\nhere_save_qs(models_binary_health, \"models_binary\")\nThen create ATE plots and interpretations with margot_plot(). Save as these go straight into the manuscript.\n\n\n5 Heterogeneity hunt (CATE)\nEvidence phase\n\nRATE AUTOC & Qini curves via margot_rate() and margot_inspect_qini() flag outcomes with meaningful heterogeneity.\n\nAction phase\n\nFlip outcomes you want to minimise (e.g. depression) so positive CATE = good.\n\nflipped &lt;- margot_flip_forests(models_binary,\n                                   flip_outcomes = flip_outcomes,\n                                   recalc_policy = TRUE)\n\nFit policy trees (margot_policy(), max_depth = 2L) to discover actionable rules.\nDepth-1 trees are often trivial; depth-2 gives interpretable nuance.\n\nCheck that every split variable exists in the original data and that leaves map sensibly onto subgroups (no cells with &lt; 30 observations).\n\n\n6 Report subgroup analyses (optional)\nWhen theory predicts effect modification—age bands, income thirds, etc.—use margot_planned_subgroups_batch(). The helper auto-generates tables and mini-plots; just pass the subset definitions you crafted:\nplanned_subset_results &lt;- margot_planned_subgroups_batch(\n  domain_models = list(models_binary, …),\n  X             = X,\n  subset_types  = list(wealth = subsets_standard_wealth, …)\n)\nIf you do subgroup analyses, save the markdown tables/ plots using the protocols for the main analysis (For you to do, not done in the script.)\n\n\nTakeaway\nRun the script in order, eyes on three checkpoints: the toy forest plot, convincing Qini/RATE curves/interpretations, and sensible policy-tree leaves. Everything else is auto-saved for the manuscript."
  }
]