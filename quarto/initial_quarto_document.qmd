---
title: "Your Title"
abstract: |
  **Background**: (Brief few sentences)
  **Objectives**: 
    1. Estimate the causal effect of YOUR EXPOSURE on YOUR OUTCOMES measured one year later.
    2. Evaluate whether these effects vary across the population.
    3. Provide policy guidance on which individuals might benefit most.
  **Method**: We conducted a three-wave retrospective cohort study (waves XX-XXX, October XXXX--October XXXX) using data *SIMULATED* from the New Zealand Attitudes and Values Study, a nationally representative panel. Participants were eligible if they participated in the NZAVS in the baseline wave (XXXX,...). We defined the exposure as (XXXX  > NUMBER on a 1-7 Likert Scale (1 = yes, 0 = no)). To address attrition, we applied inverse probability of censoring weights; to improve external validity, we applied weights to the population distribution of Age, Ethnicity, and Gender. We computed expected mean outcomes for the population in each exposure condition (high XXXX/low XXXXX). Under standard causal assumptions of unconfoundedness, the contrast provides an unbiased average treatment effect. We then used causal forests to detect heterogeneity in these effects and employed policy tree algorithms to identify individuals ("strong responders") likely to experience the greatest benefits.
  **Results**:   Increasing XXXXX leads to XXXXX. Heterogeneous responses to (e.g. *Forgiveness*, *Personal Well-Being*, and *Life-Satisfaction*...) reveal structural variability in subpopulations...
  **Implications**: (Brief few sentences)
  **Keywords**: *Causal Inference*;  *Cross-validation*; *Distress*; *Employment*; *Longitudinal*; *Machine sLearning*; *Religion*; *Semi-parametric*; *Targeted Learning*.
author: 
  - name: YOUR NAME
    affiliation: Victoria University of Wellington, New Zealand
    email: XXXXX
    corresponding: yes
keywords: [Causal Inference, Cross-validation,...]
editor_options: 
  chunk_output_type: console
date: "last-modified"
fontfamily: libertinus
bibliography: references.bib
csl: apa7.csl
format:
  # docx: # comment this out if you want pdf
  #   default: false  # comment this out if you want pdf
  pdf:
    pdf-engine: lualatex
    sanitise: true
    keep-tex: true
    link-citations: true
    colorlinks: true
    documentclass: article
    classoption: ["single column"]
    lof: false
    lot: false
    geometry:
      - top=30mm
      - left=25mm
      - heightrounded
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
    header-includes:
      - \let\oldtabular\tabular
      - \renewcommand{\tabular}{\small\oldtabular}
      - \setlength{\tabcolsep}{4pt}  # adjust this value as needed
execute:
  echo: false
  warning: false
  include: true
  eval: true
---

```{r}
#| label: setup
#| echo: false
#| include: false
#| eval: true

# save this file in your project root (e.g. 'quarto/1_setup.R')

# load here to manage paths
dep <- requireNamespace("here", quietly = TRUE)
if (!dep) install.packages("here")
library(here)

# create required folders
dirs <- c(
  here("quarto"),
  here("bibliography"),
  here("save_directory"),
  here("csl")
)
for (d in dirs) {
  if (!dir.exists(d)) dir.create(d, recursive = TRUE)
}

# ensure tinytex for PDF rendering
if (!requireNamespace("tinytex", quietly = TRUE)) {
  install.packages("tinytex")
  tinytex::install_tinytex()
}

# ensure pacman for package management
if (!requireNamespace("pacman", quietly = TRUE)) {
  install.packages("pacman")
}


# min version of margot
if (packageVersion("margot") < "1.0.52") {
  stop(
    "please install margot >= 1.0.52 for this workflow\n
       run: 
       devtools::install_github('go-bayes/margot')
"
  )
}

# call library
library("margot")

# check package version
packageVersion(pkg = "margot")

# load (and install if needed) all required packages
pacman::p_load(
  boilerplate, shiny, tidyverse,
  kableExtra, glue, patchwork, stringr,
  ggplot2, ggeffects, parameters,
  table1, knitr, extrafont, here, cli
)

# load fonts (requires prior extrafont::font_import())
if (requireNamespace("extrafont", quietly = TRUE)) {
  extrafont::loadfonts(device = "all")
} else {
  message("'extrafont' not installed; skipping font loading")
}

# reproducibility
set.seed(123)

# copy CSL and BibTeX into quarto folder
src_files <- list(
  c(here("csl", "apa7.csl"),     here("quarto", "apa7.csl")),
  c(here("bibliography", "references.bib"), here("quarto", "references.bib"))
)
for (f in src_files) {
  if (!file.exists(f[2]) && file.exists(f[1])) {
    file.copy(f[1], f[2])
  }
}

# now you're ready to import data and proceed with analysis
# e.g.
# unified_db <- boilerplate_import(data_path = here("final_boilerplate_data"))
# df_grf     <- margot::here_read('df_grf',   here("save_directory"))

# ---- define paths and import data ----------------------------------------
push_mods             <- here::here("save_directory")
final_boilerplate_data <- here::here("final_boilerplate_data")
unified_db            <- boilerplate_import(data_path = final_boilerplate_data)

# check paths
# ---- inspect available boilerplate entries -------------------------------
cat(unified_db$methods$confounding_control$vanderweele)
cat(unified_db$methods$sample$nzavs)
cat(unified_db$methods$causal_intervention$grf_simple_text)
cat(unified_db$appendix$explain$grf_short)

# ---- read variable definitions -------------------------------------------
baseline_vars           <- margot::here_read("baseline_vars")
exposure_var            <- margot::here_read("exposure_var")
outcome_vars            <- margot::here_read("outcome_vars")

# ---- define study waves --------------------------------------------------
baseline_wave           <- margot::here_read("baseline_wave")
exposure_waves          <- margot::here_read("exposure_waves")
outcome_wave            <- margot::here_read("outcome_wave")

# 
baseline_wave_glued <- glue::glue(baseline_wave)

# ---- define study parameters ---------------------------------------------
study_years             <- "2018-2021"
name_exposure           <- here_read("name_exposure")
name_outcome_variables  <- "MY OUTCOME VARIABLES IN THIS STUDY"
name_exposure_lower     <- tolower(name_exposure)
name_exposure_lower

# ---- templates and thresholds --------------------------------------------
eligibility_template    <- "Participants were eligible if they participated in the {baseline wave}"
percent_missing_baseline <- margot::here_read("percent_missing_baseline")

# ---- read tables for manuscript ------------------------------------------
markdown_table_baseline  <- margot::here_read("baseline_table")
markdown_table_exposures <- margot::here_read("exposure_table")
markdown_table_outcomes  <- margot::here_read("outcomes_table")
margot_bind_tables_markdown <- margot::here_read("margot_bind_tables_markdown")
# ---- sample size information --------------------------------------------
n_total                  <- margot::here_read("n_total")
n_participants           <- here_read("n_participants")

# ---- variable labels and mappings ----------------------------------------
var_labels_measures      <- here_read("var_labels_measures")
label_mapping_all        <- here_read("label_mapping_all")

# ---- plot titles and analysis settings -----------------------------------
# ate_title                <- here_read("ate_title")
flipped_names            <- here_read("flipped_names")
flip_outcomes            <- here_read("flip_outcomes")
flipped_list             <- paste(flipped_names, collapse = ", ")

# ---- import data for visualisation --------------------------------------
original_df <- margot::here_read('df_wide', push_mods)
df_grf <- margot::here_read('df_grf', push_mods)

# co-variates
E <- margot::here_read('E', push_mods)
# select covariates and drop numeric attributes
X <- margot::remove_numeric_attributes(df_grf[E])


# ---- define nice names and regimes ---------------------------------------
nice_name_exposure  <- stringr::str_to_sentence(name_exposure)
name_outcomes_lower          <- "multi-dimensional wellbeing"
nice_name_outcome <- stringr::str_to_sentence(name_outcomes_lower)
# title
ate_title = glue("ATE Effects of {nice_name_exposure} on {nice_name_outcome}")


# ---- define exposure thresholds and regimes ------------------------------
lower_cut               <- here_read("lower_cut")
upper_cut               <- here_read("upper_cut")
threshold               <- here_read("threshold")
inverse_threshold       <- here_read("inverse_threshold")
scale_range             <- margot::here_read("scale_range")

# create and check variables
value_exposure = glue::glue( threshold,"", upper_cut,", ", scale_range)
value_control = glue::glue(inverse_threshold, upper_cut,", ", scale_range)

# regimes
name_control_regime_lower <- glue::glue("low {name_exposure_lower}")
value_exposure_regime     <- glue::glue("Set {name_exposure} {threshold} {upper_cut} {scale_range}")
value_control_regime      <- glue::glue("Set {name_exposure} {inverse_threshold} {upper_cut} {scale_range}")


contrast_template         <- "We used causal forests to estimate an average treatment effect as a contrast between *{name_control_regime_lower}* and *{name_exposure_lower}* on {name_outcomes_lower}."
contrast_text             <- glue(contrast_template)

#check
contrast_text

# ---- import histogram of binary exposure ---------------------------------

graph_cut <- margot::here_read("graph_cut")

# ---- verify assumptions (positivity) -------------------------------------
transition_tables        <- margot::here_read("transition_tables")
transition_tables_binary <- here_read("transition_tables_binary")

# ---- generate measures text for methods section -------------------------
baseline_measures_text   <- boilerplate_generate_measures(
  variable_heading = "Baseline Covariates",
  variables        = baseline_vars,
  db               = unified_db,
  heading_level    = 3,
  subheading_level = 4,
  print_waves      = FALSE,
  label_mappings   = var_labels_measures
)
cat(baseline_measures_text)
exposure_measures_text   <- boilerplate_generate_measures(
  variable_heading = "Exposure Variable",
  variables        = name_exposure,
  db               = unified_db,
  heading_level    = 3,
  subheading_level = 4,
  print_waves      = FALSE,
  label_mappings   = var_labels_measures
)
outcome_measures_text    <- boilerplate_generate_measures(
  variable_heading = "Outcome Variables",
  variables        = outcome_vars,
  db               = unified_db,
  heading_level    = 3,
  subheading_level = 4,
  print_waves      = FALSE,
  label_mappings   = var_labels_measures
)

# ---- exposure description from database --------------------------------
measures_exposure        <- glue::glue(unified_db$measures[[name_exposure]]$description)


# ---- set plot defaults for ate plots -------------------------------------
base_defaults_binary     <- list(
  type                   = "RD",
  title                  = ate_title,
  e_val_bound_threshold  = 1.2,
  colors                 = c(
    "positive"    = "#E69F00",
    "not reliable"= "grey50",
    "negative"    = "#56B4E9"
  ),
  x_offset               = -0.25,
  x_lim_lo               = -0.25,
  x_lim_hi               = 0.25,
  text_size              = 5,
  linewidth              = 0.75,
  estimate_scale         = 1,
  base_size              = 20, #<- change to make outcome labels bigger or smaller
  point_size             = 4,
  title_size             = 20,
  subtitle_size          = 16,
  legend_text_size       = 10,
  legend_title_size      = 10,
  include_coefficients   = FALSE
)

# ---- create plot options for outcomes -----------------------------------
outcomes_options_all     <- margot_plot_create_options(
  title         = ate_title,
  base_defaults = base_defaults_binary,
  subtitle      = "",
  filename_prefix = "grf"
)

# ---- load and check model results ---------------------------------------
# takes more time but allows you to flexibly modify plots in the quarto document
# if you use the option comment out this code above

# takes less time if you used the pre-processed results but a little harder to adjust
# ate_results           <- margot::here_read_qs("ate_results", push_mods)
# margot::margot_size(ate_results)

models_binary <- margot::here_read_qs("models_binary", push_mods)

# make ate plots ----------------------------------------------------------
#   ************* NEW - CORRECTION FOR FAMILY-WISE ERROR **********
# then pass to the results
ate_results <- margot_plot(
  models_binary$combined_table, # <- now pass the corrected results.
  options = outcomes_options_all,
  label_mapping = label_mapping_all,
  include_coefficients = FALSE,
  save_output = FALSE,
  order = "evaluebound_asc",
  original_df = original_df,
  e_val_bound_threshold = 1.2,
  rename_ate = TRUE,
  adjust = "bonferroni", #<- new 
  alpha = 0.05 # <- new 
)

# check
ate_results$plot

# view
cat(ate_results$interpretation)

# ---- heterogeneity analysis ---------------------------------------------
models_binary_flipped_all <- here_read_qs("models_binary_flipped_all", push_mods)

# this is a new function requires margot 1.0.48 or higher
# only useful if you flip labels outcomes -- if so replace "label_mapping_all" 
# with "label_mapping_all_flipped" 
label_mapping_all_flipped <- margot_reversed_labels(label_mapping_all, 
                                                    flip_outcomes)

# optional
# could be used in an appendix
# result_ominbus_hetero     <- margot_omnibus_hetero_test(
#   models_binary_flipped_all,
#   label_mapping  = label_mapping_all_flipped,
#   alpha          = 0.05,
#   detail_level   = "standard",
#   format         = "markdown"
# )
# result_ominbus_hetero$summary_table |> kbl("markdown")
# cat(result_ominbus_hetero$brief_interpretation)

# ──────────────────────────────────────────────────────────────────────────────
# SCRIPT:  HETEROGENEITY WORKFLOW
# PURPOSE: screen outcomes for heterogeneity, plot RATE & Qini curves,
#          fit shallow policy trees, and produce plain-language summaries.
# REQUIREMENTS:
#   • margot ≥ 1.0.52
#   • models_binary_flipped_all        – list returned by margot_causal_forest()
#   • original_df                      – raw data frame used in the forest
#   • label_mapping_all_flipped        – named vector of pretty labels
#   • flipped_names                    – vector of outcomes that were flipped
#   • decision_tree_defaults           – list of control parameters
#   • policy_tree_defaults             – list of control parameters
#   • push_mods                        – sub-folder for caches/outputs
#   • use 'models_binary', `label_mapping_all`, and set `flipped_names = ""` if no outcome flipped
# ──────────────────────────────────────────────────────────────────────────────

# check package version early
stopifnot(utils::packageVersion("margot") >= "1.0.52")

# helper: quick kable printer --------------------------------------------------
print_rate <- function(tbl) {
  tbl |>
    mutate(across(where(is.numeric), \(x) round(x, 2))) |>
    kbl(format = "markdown")
}

# 1  SCREEN FOR HETEROGENEITY (RATE AUTOC + RATE Qini)  ----------------------

rate_results <- margot_rate(
  models        = models_binary_flipped_all,
  policy        = "treat_best",
  alpha         = 0.20,        # keep raw p < .20
  adjust        = "fdr",       # false-discovery-rate correction
  label_mapping = label_mapping_all_flipped
)

print_rate(rate_results$rate_autoc)
print_rate(rate_results$rate_qini)
# convert RATE numbers into plain-language text
rate_interp <- margot_interpret_rate(
  rate_results,
  flipped_outcomes      = flipped_names,
  adjust_positives_only = TRUE
)

cat(rate_interp$comparison, "\n")
cli_h2("Analysis ready for Appendix ✔")

# organise model names by evidence strength
model_groups <- list(
  autoc       = rate_interp$autoc_model_names,
  qini        = rate_interp$qini_model_names,
  either      = rate_interp$either_model_names,
  exploratory = rate_interp$not_excluded_either
)

# 2  PLOT RATE AUTOC CURVES ---------------------------------------------------

autoc_plots <- margot_plot_rate_batch(
  models        = models_binary_flipped_all,
  save_plots    = FALSE,  # set TRUE to store .png files
  label_mapping = label_mapping_all_flipped,
  model_names   = model_groups$autoc
)

# inspect the first curve - note there may be more/none.
# if none, comment out
autoc_plots[[1]]
autoc_name_1 <- rate_results$rate_autoc$outcome[[1]]

# 3  QINI CURVES + GAIN INTERPRETATION ---------------------------------------
qini_results <- margot_policy(
  models_binary_flipped_all,
  save_plots         = FALSE,
  output_dir         = here::here(push_mods),
  decision_tree_args = decision_tree_args,
  policy_tree_args   = policy_tree_args,
  model_names        = names(models_binary_flipped_all$results),
  original_df        = original_df,
  label_mapping      = label_mapping_all_flipped,
  max_depth          = 2L,
  output_objects     = c("qini_plot", "diff_gain_summaries")
)

qini_gain <- margot_interpret_qini(
  qini_results,
  label_mapping = label_mapping_all_flipped
)

print_rate(qini_gain$summary_table)
cat(qini_gain$qini_explanation, "\n")

reliable_ids <- qini_gain$reliable_model_ids

# (re-)compute plots only for models that passed Qini reliability
qini_results_valid <- margot_policy(
  models_binary_flipped_all,
  save_plots         = FALSE,
  output_dir         = here::here(push_mods),
  decision_tree_args = decision_tree_args,
  policy_tree_args   = policy_tree_args,
  model_names        = reliable_ids,
  original_df        = original_df,
  label_mapping      = label_mapping_all_flipped,
  max_depth          = 2L,
  output_objects     = c("qini_plot", "diff_gain_summaries")
)

qini_plots <- map(qini_results_valid, ~ .x$qini_plot)

# grab pretty outcome names
qini_names <- margot_get_labels(reliable_ids, label_mapping_all_flipped)

cli_h1("Qini curves generated ✔")

# 4  POLICY TREES (max depth = 2) -------------------------------------------
# ---- policy tree graph settings 

policy_tree_defaults    <- list(
  point_alpha            = 0.5,
  title_size             = 12,
  subtitle_size          = 12,
  axis_title_size        = 12,
  legend_title_size      = 12,
  split_line_color       = "red",
  split_line_alpha       = 0.8,
  split_label_color      = "red",
  split_label_nudge_factor = 0.007
)

decision_tree_defaults  <- list(
  span_ratio         = 0.1,
  text_size          = 4,
  y_padding          = 0.5,
  edge_label_offset  = 0.02,
  border_size        = 0.01
)

policy_results_2L <- margot_policy(
  models_binary_flipped_all,
  save_plots         = FALSE,
  output_dir         = here::here(push_mods),
  decision_tree_args = decision_tree_defaults,
  policy_tree_args   = policy_tree_defaults,
  model_names        = reliable_ids,      # only those passing Qini
  max_depth          = 2L,
  original_df        = original_df,
  label_mapping      = label_mapping_all_flipped,
  output_objects     = c("combined_plot")
)
policy_plots <- map(policy_results_2L, ~ .x$combined_plot)
# policy_plots[[1]]

# ️5  PLAIN-LANGUAGE INTERPRETATION OF TREES ----------------------------------

policy_text <- margot_interpret_policy_batch(
  models            = models_binary_flipped_all,
  original_df       = original_df,
  model_names       = reliable_ids,
  label_mapping     = label_mapping_all_flipped,
  max_depth         = 2L
)

cat(policy_text, "\n")

cli::cli_h1("Finished: depth-2 policy trees analysed ✔")

# ───────────────────────────── EOF ────────────────────────────────────────────

# names of valid models
glued_policy_names_1 <- glue( qini_names[[1]])
glued_policy_names_2 <-glue( qini_names[[2]])
glued_policy_names_3 <- glue(qini_names[[3]])
glued_policy_names_4 <- glue(qini_names[[4]])
glued_policy_names_5 <-glue( qini_names[[5]])
glued_policy_names_6 <- glue(qini_names[[6]])
glued_policy_names_7 <-glue( qini_names[[7]])


# cli::cli_h1("Names of Reliable HTE Models Set")

# GROUP COMPARISON EXAMPLE ------------------------------------------------
# play around with these values
x_offset_comp <- 1.0
x_lim_lo_comp <- -1.0
x_lim_hi_comp <- 1.0 

base_defaults_comparisons <- list(
  type = "RD",
  title = ate_title,
  e_val_bound_threshold = 1.2,
  label_mapping = "label_mapping_all",
  adjust = "bonferroni", #<- new
  alpha = 0.05, # <- new
  colors = c(
    "positive" = "#E69F00",
    "not reliable" = "grey50",
    "negative" = "#56B4E9"
  ),
  x_offset = x_offset_comp,
  # will be set based on type
  x_lim_lo = x_lim_lo_comp,
  # will be set based on type
  x_lim_hi = x_lim_hi_comp,
  text_size = 8,
  linewidth = 0.75,
  estimate_scale = 1,
  base_size = 18,
  point_size = 4,
  title_size = 19,
  subtitle_size = 16,
  legend_text_size = 10,
  legend_title_size = 10,
  include_coefficients = FALSE
)

# see
complex_condition_age  <- between(X[,"t0_age_z"], -1, 1)

# sanity‑check age bounds on the raw scale
mean(original_df$t0_age) + c(-1, 1) * sd(original_df$t0_age)

# age subsets
subsets_standard_age <- list(
  Younger = list(
    var = "t0_age_z",
    value = -1,
    operator = "<",
    label = "Age < 35"
  ),
  Middle = list(
    var = "t0_age_z",
    # operator = "<",
    subset_condition = complex_condition_age,
    label = "Age 35-62"
  ),
  Older = list(
    var = "t0_age_z",
    value = 1,
    operator = ">",
    label = "Age > 62"
  )
)

# 3. batch subgroup analysis -----------------------------------------
planned_subset_results <- margot_planned_subgroups_batch(
  domain_models  = list(models_binary),
  X              = X,
  base_defaults  = base_defaults_comparisons,
  subset_types   = list(cohort = subsets_standard_age),
  original_df    = original_df,
  label_mapping  = label_mapping_all,          # ← supply it here
  domain_names   = "wellbeing",
  subtitles      = "",
  adjust         = "bonferroni",  # ← here
  alpha          = 0.05           # ← and here
)

# make comparison plot
plots_subgroup_age_young_old<- wrap_plots(
  list(
    planned_subset_results$wellbeing$cohort$results$`Age < 35`$plot,
    planned_subset_results$wellbeing$cohort$results$`Age > 62`$plot
  ), ncol = 1) +
  plot_annotation(
    title = "Younger vs Older",
    theme = theme(plot.title = element_text(size = 18, face = "bold"))
  )



# are groups different from each other? 
# example: young (<35) vs older (>62)
group_comparison_age_young_old <- margot_compare_groups(
  group1_name = "People Under 35 Years Old",
  group2_name = "People Over 62 Years Old", 
  planned_subset_results$wellbeing$cohort$results$`Age < 35`$transformed_table, # reference
  planned_subset_results$wellbeing$cohort$results$`Age > 62`$transformed_table, # comparison
  type            = "RD",          # risk‑difference scale
  decimal_places  = 3
)
print(group_comparison_age_young_old$results |> kbl("markdown", digits = 3))
cat(group_comparison_age_young_old$interpretation)


# ---- define global variables for text generation ------------------------
global_vars <- list(
  name_exposure_variable     = nice_name_exposure,
  n_total                    = n_total,
  ate_adjustment             = "bonferroni", 
  ate_alpha                  = "0.05",
  cate_adjustment            = "Benjamini–Hochberg false-discovery-rate adjustment",
  cate_alpha                 =  "0.1",             
  sample_ratio_policy        =  "70/30",
  n_participants             = n_participants,
  exposure_variable          = name_exposure,
  name_exposure_lower        = name_exposure_lower,
  name_control_regime_lower  = name_control_regime_lower,
  name_outcome_variables     = "Self Esteem", # <- adjust to your study, your decision
  name_outcomes_lower        = name_outcomes_lower,
  name_exposure_capfirst     = nice_name_exposure,
  measures_exposure          = measures_exposure,
  value_exposure_regime      = value_exposure_regime,
  value_control_regime       = value_control_regime,
  flipped_list               = flipped_list, # set flipped_list = "" if nothing flipped
  appendix_explain_grf       = "E",
  appendix_assumptions_grf   = "F",
  name_exposure_threshold    =  "1",
  name_control_threshold     =  "0",
  appendix_measures          =  "A",
  value_control              = value_control,     # ← named
  value_exposure             = value_exposure,    # ← named
  appendix_positivity        = "C",
  appendix_rate              = "D",
  appendix_qini_curve        = "D",
  train_proportion_decision_tree = ".7",
  traning_proportion         = ".7",
  sample_split              = "70/30",
  sample_ratio_policy        = "70/30",
  baseline_wave              = baseline_wave,
  exposure_waves             = exposure_waves,
  outcome_wave               = outcome_wave,
  protocol_url               = "https://osf.io/ce4t9/", # if used
  appendix_timeline          = "A" # if used
)
```

{{< pagebreak >}}

## Introduction

**Your place to shine here**

## Method

```{r, results='asis'}
#| eval: false # <- set to false/ copy and paste text so you can modify it/change the figure
cat(
  boilerplate::boilerplate_generate_text(
    category     = "methods",      # ← choose the right top-level list
    sections     = c(
      "student_sample.nzavs",
      "student_target_population",
      "eligibility.standard",
      "causal_intervention.grf_simple_text",
      "analytic_approach.general_approach_cate_long", # <- new
      "exposure_indicator",
      "causal_identification_criteria",
      "confounding_control.vanderweele",
      "statistical_models.grf_short_explanation",
      "missing_data.missing_grf_simple",
      "sensitivity_analysis.short_evalue"
    ),
    global_vars  = global_vars,
    db           = unified_db
  )
)
```

### Target Population

The data are simulated from the New Zealand Attitudes and Values Study. For the purposes of this assessment, the target population for this study comprises New Zealand residents as represented in the 2018 of the New Zealand Attitudes and Values Study (NZAVS) during the years 2018 weighted by New Zealand Census weights for age, gender, and ethnicity (refer to @sibley2021). The NZAVS is a national probability study designed to reflect the broader New Zealand population accurately. Despite its comprehensive scope, the NZAVS has some limitations in its demographic representation. Notably, it tends to under-sample males and individuals of Asian descent while over-sampling females and Māori (the indigenous peoples of New Zealand). To address these disparities and enhance the accuracy of our findings, we apply New Zealand Census survey weights to the sample data.

### Eligibility Criteria

To be included in the analysis of this study, participants needed to participate in the 2018 of the study and respond to the baseline measure of Extraversion.

Participants may have been lost to follow-up at the end of the study if they met eligibility criteria at 2018. We adjusted for attrition and non-response using censoring weights, described below.

A total of 39,635 individuals met these criteria and were included in the study.


### Average treatment effect

To learn how the outcome would shift if everyone received a different exposure, we emulate a **target trial** [@hernan2016c]. Making the hypothetical experiment explicit fixes the estimand, the data requirements, and the assumptions.

Our guiding question is:

> *How would the outcomes change if, for every individual, we set the exposure to **>4, scale range 1-7** rather than **<=4, scale range 1-7**, conditional on their baseline characteristics?*

We compare two interventions:

1. **1** — every participant is set to >4, scale range 1-7.
2. **0** — every participant is set to <=4, scale range 1-7.

The difference in population means defines the **average treatment effect (ATE)**. Figure @fig-exposure plots the exposure distribution and its dichotomisation; the centre dashed line marks the mean, flanked by one standard deviation.

```{r}
#| label: fig-exposure
#| fig-cap: "Histogram of exposure with binary groupin"
#| eval: true
#| echo: false
#| fig-height: 10   # tweak if needed
#| fig-width: 12    # tweak if needed

graph_cut

```

Because we test several outcomes, we adjust the ATE confidence intervals for multiplicity with bonferroni at $\alpha = 0.05$.

The longitudinal design and rich baseline covariates allow us -- under the standard identification assumptions of consistency, positivity, and no unmeasured confounding -- to attribute the differences between the two exposure means as a causal effect. Conditioning on demographics, personality traits, and other pretreatment factors renders exposure assignment ignorable [@rosenbaum1983central].

### Moderators and treatment policies

We pursued two complementary objectives: (i) to test whether personalised targeting, based on individual conditional average treatment effects $\hat\tau(x)$, yields welfare gains, and (ii) to convert any such gains into transparent, practitioner‐ready decision rules.

#### Pre-processing and honest model training

Our protocol is as follows: in settings where the exposure is positive, outcomes for which lower is better are sign‐flipped so that larger values always index improvement. Similarly, where the exposure is negative, outcomes for which 'higher is better' are sign flipped.Here, we flipped: Anxiety, Depression, Rumination. 

Each model estimate used an honest 70/30 split: the training fold built the causal forest with grf [@grf2024], while the held-out fold powered all diagnostic checks and provided data for fitting policy trees.  This separation curbs over-fitting yet keeps the workflow simple.

#### Budget-based screening with Qini curves.

Before constructing rules we asked a budget question: If resources allow treatment of only the top 20% or 50% of individuals ranked by $\hat\tau(x)$, what uplift is purchased relative to treating everyone?
Qini curves quantified the incremental gain; outcomes whose 95% confidence intervals excluded zero at either spend level were labelled actionable, signalling meaningful heterogeneity in benefit.

#### Deriving transparent decision rules.

For each actionable outcome we trained depth-2 policy trees with policytree [@policytree_package_2024; @athey_2021_policy_tree_econometrica] on the validation data.  The resulting if–then statements maximise expected welfare under the same budget cap and remain auditable by domain experts.

####  Global heterogeneity tests (supplementary).

Appendix D reports RATE-AUTOC and RATE-Qini statistics, asking whether any covariate information can beat a uniform policy.  These tests, controlled for false discovery at q=0.1 via Benjamini–Hochberg false-discovery-rate adjustment, are informative but not required for the budget-first pipeline.

Overall, combining Qini curves to screen and shallow policy trees to act isolates budget-relevant treatment heterogeneity and distils it into actionable rules, avoiding the chase for spurious complexity. Full technical details appear in Appendix E.

### Exposure Indicator


The New Zealand Attitudes and Values Study assesses Extraversion using the following question:


Mini-IPIP6 Extraversion dimension: (i) I am the life of the party. (ii) I don't talk a lot. (r) (iii) I keep in the background. (r) (iv) I talk to a lot of different people at parties.(Refer to [Appendix A](#appendix-measures)).

### Causal Identification Assumptions

This study relies on the following identification assumptions for estimating the causal effect of Extraversion:

1. **Consistency**: the observed outcome under the observed Extraversion is equal to the potential outcome under that exposure level. As part of consistency, we assume no interference: the potential outcomes for one individual are not affected by the Extraversion status of other individuals.

2. **No unmeasured confounding**: all variables that affect both Extraversion and the outcome have been measured and accounted for in the analysis.

3. **Positivity**: there is a non-zero probability of receiving each level of Extraversion for every combination of values of Extraversion and confounders in the population. Positivity is the only fundamental casual assumption that can be evaluated with data (refer to [Appendix C](#appendix-positivity)).

### Confounding Control

To manage confounding in our analysis, we implement @vanderweele2019's *modified disjunctive cause criterion* by following these steps:

1. **Identified all common causes** of both the treatment and outcomes.
2. **Excluded instrumental variables** that affect the exposure but not the outcome. Instrumental variables do not contribute to controlling confounding and can reduce the efficiency of the estimates.
3. **Included proxies for unmeasured confounders** affecting both exposure and outcome. According to the principles of d-separation @pearl2009a, using proxies allows us to control for their associated unmeasured confounders indirectly.
4. **Controlled for baseline exposure** and **baseline outcome**. Both are used as proxies for unmeasured common causes, enhancing the robustness of our causal estimates, refer to @vanderweele2020.

### Statistical Estimation

We estimate heterogeneous treatment effects with Generalized Random Forests (GRF) [@grf2024]. GRF extends random forests for causal inference by focusing on conditional average treatment effects (CATE). It handles complex interactions and non-linearities without explicit model specification, and it provides 'honest' estimates by splitting data between model-fitting and inference. GRF is doubly robust because it remains consistent if either the outcome model or the propensity model is correct. We evaluate policies with the `policytree` package [@policytree_package_2024; @athey_2021_policy_tree_econometrica] and visualise results with `margot` [@margot2024]. (Refer to [Appendix E](#appendix-explain-grf) for a detailed explanation of our approach.)

### Missing Data

The GRF package accepts missing values at baseline. To obtain valid inference for missing responses we computed inverse probability of censoring weights for censoring of the exposure, given that systematic censoring following the baseline wave may lead to selection bias that limit generalistion to the baseline target population [@bulbulia2024wierd]. See [Appendix E](#appendix-explain-grf).

### Sensitivity Analysis

We perform sensitivity analyses using the E-value metric [@vanderweele2017; @linden2020EVALUE]. The E-value represents the minimum association strength (on the risk-ratio scale) that an unmeasured confounder would need with both exposure and outcome—after adjusting for measured covariates—to explain away the observed association [@vanderweele2020; @linden2020EVALUE]. Confidence intervals for each E-value were derived from the multiplicity-adjusted confidence intervals of the corresponding coefficient estimates (bonferroni, $\alpha$ = 0.05), so the sensitivity analysis obeys the same error-control framework as the main results.

{{< pagebreak >}}

## Results

### Average Treatement Effects

```{r}
#| label: fig-ate
#| fig-cap: "Average Treatment Effects on Multi-dimensional Wellbeing"
#| eval: true # |eval: false # <- set to false as needed/desired for your own material
#| fig-height: 12
#| fig-width: 8
ate_results$plot
```

{{< pagebreak >}}

```{r}
#| label: tbl-outcomes
#| tbl-cap: "Average Treatment Effects on Multi-dimensional Wellbeing"
#| eval: true # |eval: false # <- set to false as needed/desired for your own material

# ate_results$transformed_table|> kbl("markdown")
margot_bind_tables_markdown
```



```{r, results = 'asis'}
#| eval: true # |eval: false # <- set to false as needed/desired for your own material

cat(ate_results$interpretation)
```


{{< pagebreak >}}


### Planned Subgroup Comparisons (Optional)

Based on theoretical findings we expected that the effects of {name_exposure} would vary by age...\@fig-planned-comparison and @tbl-planned-comparison

```{r}
#| label: fig-planned-comparison
#| fig-cap: "Planned Comparison Plot"
#| eval: true
#| echo: false
#| fig-height: 10
#| fig-width: 12
plots_subgroup_age_young_old
```

```{r}
#| label: tbl-planned-comparison
#| tbl-cap: "Planned Comparison Table"
#| eval: true  # <- set to false: copy and paste your own text using this material
#| echo: false
# table (only use if more than one qini gain interpretation)
group_comparison_age_young_old$results |> 
  mutate(across(where(is.numeric), ~ round(., 2))) %>%
  kbl(format = "markdown")

```

```{r, results = 'asis'}
#| eval: true # <- set to false if you want to copy and paste your own text
cat(group_comparison_age_young_old$interpretation)
```

{{< pagebreak >}}

### Heterogeneous Treatment Effects {#results-qini-curve}

```{r, results='asis'}
#| eval: true  # <- set to false: copy and paste your own text using this material

cat(
  boilerplate::boilerplate_generate_text(
    category     = "results",      # ← choose the right top-level list
    sections     = c(
    "grf.interpretation_qini"
    ),
    global_vars  = global_vars,
    db           = unified_db
  )
)
```


```{r, results = 'asis'}
# only reliable results
cat(qini_gain$qini_explanation) 
```

@tbl-qini presents results for our Qini curve analysis at different spend rates.

```{r}
#| label: tbl-qini
#| tbl-cap: "Qini Curve Results"
#| eval: true  # <- set to false: copy and paste your own text using this material

# table (only use if more than one qini gain interpretation)
qini_gain$summary_table |> 
  mutate(across(where(is.numeric), ~ round(., 2))) %>%
  kbl(format = "markdown") #<-- only if you have this, otherwise delete this code
```

@fig-qini-1 presents results for reliable Qini results

```{r}
#| label: fig-qini-1
#| fig-cap: "Qini Graphs"
#| eval: true # <- set to false as needed/desired
#| echo: false
#| fig-height: 18
#| fig-width: 12

library(patchwork)
# combine first column of plots (4,6,7,8) and second column (9,11,12)
# these showed reliable qini results

combined_qini <- (
  qini_plots[[1]] /
  qini_plots[[2]] /
  qini_plots[[3]] /
  qini_plots[[4]]
) | (
  # remove this block if you don't have plots 9,11,12
  qini_plots[[5]] /
  qini_plots[[6]] /
  qini_plots[[7]]
) +
  # collect all legends into one shared guide
  plot_layout(guides = "collect") +
  # add title (and optionally subtitle)
  plot_annotation(
    title    = "Combined Qini Plots",
    subtitle = "Panels arranged with shared legend"
  ) &
  # apply theme modifications to all subplots
  theme(
    legend.position   = "bottom",           # place legend below
    plot.title        = element_text(hjust = 0.5),  # centre title
    plot.subtitle     = element_text(hjust = 0.5)   # centre subtitle
  )

# draw it
print(combined_qini)

```
{{< pagebreak >}}

### Decision Rules (Who is Most Sensitive to Treatment?)

```{r, results='asis'}
#| eval: true  # <- set to false: copy and paste your own text using this material

cat(
  boilerplate::boilerplate_generate_text(
    category     = "results",      # ← choose the right top-level list
    sections     = c(
    "grf.interpretation_policy_tree"
    ),
    global_vars  = global_vars,
    db           = unified_db
  )
)
```

{{< pagebreak >}}

```{r}
#| label: fig-policy-1
#| fig-cap: "Decision Tree: {glued_policy_names_1}"
#| eval: true
#| echo: false
#| fig-height: 10
#| fig-width: 12
# plot 1
policy_plots[[1]]
```

**Findings for Life Satisfaction:**

Split 1: NZsei 13 l ≤ -0.121 (original: 52.05).  Within that subgroup, split 2a: NZ Dep2018 ≤ 0.466 (original: 6.043), → **Control**; NZ Dep2018 > 0.466 (original: 6.043) → **Treated**.

Split 2: NZsei 13 l > -0.121 (original: 52.05).  Within that subgroup, split 2b: Conscientiousness ≤ 1.528 (original: 6.719), → **Treated**; Conscientiousness > 1.528 (original: 6.719) → **Control**.

{{< pagebreak >}}

```{r}
#| label: fig-policy-2
#| fig-cap: "Decision Tree: {glued_policy_names_2}"
#| eval: true
#| fig-height: 10
#| fig-width: 12

policy_plots[[2]]
```

**Findings for Meaning: Purpose:**

Split 1: Meaning: Sense ≤ -0.586 (original: 4.996).  Within that subgroup, split 2a: Meaning: Sense ≤ -0.617 (original: 4.959), → **Treated**; Meaning: Sense > -0.617 (original: 4.959) → **Control**.

Split 2: Meaning: Sense > -0.586 (original: 4.996).  Within that subgroup, split 2b: NZ Dep2018 ≤ 1.552 (original: 9.007), → **Treated**; NZ Dep2018 > 1.552 (original: 9.007) → **Control**.

{{< pagebreak >}}

```{r}
#| label: fig-policy-3
#| fig-cap: "Decision Tree: {glued_policy_names_3}"
#| eval: true
#| echo: false
#| fig-height: 10
#| fig-width: 12

policy_plots[[3]]
```


**Findings for Meaning: Sense:**

Split 1: log Household Inc ≤ 0.15 (original: 100000.015).  Within that subgroup, split 2a: Age ≤ 0.612 (original: 57), → **Treated**; Age > 0.612 (original: 57) → **Control**.

Split 2: log Household Inc > 0.15 (original: 100000.015).  Within that subgroup, split 2b: Hours of Exercise (log) ≤ -0.543 (original: 1.965), → **Control**; Hours of Exercise (log) > -0.543 (original: 1.965) → **Treated**.


{{< pagebreak >}}

```{r}
#| label: fig-policy-4
#| fig-cap: "Decision Tree: {glued_policy_names_4}"
#| eval: true
#| fig-height: 10
#| fig-width: 12

policy_plots[[4]]
```


**Findings for Neighbourhood Community:**

Split 1: log Hours Housework ≤ -0.083 (original: 6.96).  Within that subgroup, split 2a: Life Satisfaction ≤ -1.515 (original: 3.481), → **Control**; Life Satisfaction > -1.515 (original: 3.481) → **Treated**.

Split 2: log Hours Housework > -0.083 (original: 6.96).  Within that subgroup, split 2b: Neuroticism ≤ -1.328 (original: 1.963), → **Control**; Neuroticism > -1.328 (original: 1.963) → **Treated**.



{{< pagebreak >}}

```{r}
#| label: fig-policy-5
#| fig-cap: "Decision Tree: {glued_policy_names_5}"
#| eval: true
#| fig-height: 10
#| fig-width: 10

policy_plots[[5]]
```


**Findings for Personal Well-being Index:**

Split 1: Social Support ≤ 0.035 (original: 5.987).  Within that subgroup, split 2a: Meaning: Sense ≤ -3.021 (original: 2.032), → **Control**; Meaning: Sense > -3.021 (original: 2.032) → **Treated**.

Split 2: Social Support > 0.035 (original: 5.987).  Within that subgroup, split 2b: Age ≤ -0.108 (original: 47), → **Treated**; Age > -0.108 (original: 47) → **Control**.


{{< pagebreak >}}

```{r}
#| label: fig-policy-6
#| fig-cap: "Decision Tree: {glued_policy_names_6}"
#| eval: true
#| fig-height: 10
#| fig-width: 12

policy_plots[[6]]

```

**Findings for Self Esteem:**

Split 1: Neighbourhood Community ≤ -0.114 (original: 4.002).  Within that subgroup, split 2a: Hours of Exercise (log) ≤ 1.011 (original: 10.038), → **Treated**; Hours of Exercise (log) > 1.011 (original: 10.038) → **Control**.

Split 2: Neighbourhood Community > -0.114 (original: 4.002).  Within that subgroup, split 2b: log Hours Housework ≤ -0.953 (original: 3.035), → **Control**; log Hours Housework > -0.953 (original: 3.035) → **Treated**.

{{< pagebreak >}}

```{r}
#| label: fig-policy-7
#| fig-cap: "Decision Tree: {glued_policy_names_7}"
#| eval: true
#| echo: false
#| fig-height: 10
#| fig-width: 12
policy_plots[[7]]
```


**Findings for Social Support:**

Split 1: Neighbourhood Community ≤ -0.134 (original: 3.968).  Within that subgroup, split 2a: Hours of Exercise (log) ≤ 1.652 (original: 17.983), → **Treated**; Hours of Exercise (log) > 1.652 (original: 17.983) → **Control**.

Split 2: Neighbourhood Community > -0.134 (original: 3.968).  Within that subgroup, split 2b: Age ≤ 0.468 (original: 55), → **Treated**; Age > 0.468 (original: 55) → **Control**.


A shallow policy tree recommends actions based on two splits for depth=2, or one split for depth=1.
We trained on 50% of the data and evaluated on the rest.


{{< pagebreak >}}

```{r, results = 'asis'}
#| eval: false # <- set to false if you want to copy and paste your own text

cat(policy_text, "\n")
```

{{< pagebreak >}}



## Discussion

```{r,  results='asis'}
#| eval: false  # <- set to false: copy and paste your own text using this material
#| echo: false
cat(boilerplate_generate_text(
  category = "discussion",
  sections = c(
    "student_ethics",
    "student_data",
    "student_authors_statement"  ),
  global_vars = list(
    exposure_variable = name_exposure
  ),
  db = unified_db
))
unified_db$discussion$student_authors_statement
```

{{< pagebreak >}}

## Appendix A: Measures {#appendix-measures}

### Measures

#### Baseline Covariate Measures

```{r, results='asis'}
cat(baseline_measures_text)
```

#### Exposure Measures

```{r, results='asis'}
cat(exposure_measures_text)
```

#### Outcome Measures

```{r, results='asis'}
cat(outcome_measures_text)
```

{{< pagebreak >}}

## Appendix B: Sample Characteristics {#appendix-sample}

#### Sample Statistics: Baseline Covariates

@tbl-appendix-baseline presents sample demographic statistics.

::: {#tbl-appendix-baseline}
```{r, results = 'asis'}
#| eval: true
#| include: true
#| echo: false
markdown_table_baseline
```

Demographic statistics for New Zealand Attitudes and Values Cohort: {baseline_wave_glued}.
:::

### Sample Statistics: Exposure Variable {#appendix-exposure}

<!-- @tbl-sample-exposures presents sample statistics for the exposure variable, religious service attendance, during the baseline and exposure waves. This variable was not measured in part of NZAVS time 12 (years 2020-2021) and part of NZAVS time 13 (years 2021-2022). To address missingness, if a value was observed after NZAVS time 14, we carried the previous observation forward and created and NA indicator. If there was no future observation, the participant was treated as censored, and inverse probability of censoring weights were applied, following our standard method for handling missing observations (see mansucript **Method**/**Handling of Missing Data**). Here, our carry-forward imputation approach may result in conservative causal effect estimation because it introduces measurement error. However, this approach would not generally bias causal effect estimation away from the null because the measurement error is unsystematic and random and unrelated to the outcomes. -->

::: {#tbl-appendix-exposures}
```{r, results = 'asis'}
#| eval: true
#| include: true
#| echo: false

markdown_table_exposures

```

Demographic statistics for New Zealand Attitudes and Values Cohort waves 2018.
:::

{{< pagebreak >}}

### Sample Statistics: Outcome Variables {#appendix-outcomes}

::: {#tbl-appendix-outcomes}
```{r, results = 'asis'}
#| eval: true
#| include: true
#| echo: false

markdown_table_outcomes

```

Outcome variables measured at {baseline_wave_glued} and {outcome_wave}
:::

{{< pagebreak >}}

## Appendix C: Transition Matrix to Check The Positivity Assumption {#appendix-transition}

```{r, results = 'asis'}
#| label: tbl-transition
#| tbl-cap: "Transition Matrix Showing Change"
#| eval: true
#| include: true
#| echo: false

transition_tables_binary$tables[[1]]
```

```{r, results = 'asis'}
cat(transition_tables_binary$explanation)
```

{{< pagebreak >}}

## Appendix D: RATE AUTOC and RATE Qini {#appendix-rate}

```{r, results='asis'}
#| eval: true # <- set to false as needed/desired
#| echo: false

cat(
  boilerplate::boilerplate_generate_text(
    category     = "results",
    # ← choose the right top-level list
    sections     = c("grf.interpretation_rate"),
    global_vars  = global_vars,
    db           = unified_db
  )
)
```

```{r, results='asis'}
#| eval: true # <- set to false as needed/desired
#| echo: false
cat(rate_interp$comparison)
```

Refer to [Appendix D](#appendix-cate-validation) for details.

##### RATE AUTOC RESULTS

```{r, results = 'asis'}
# only reliable results
cat(rate_interp$autoc_results)
```

```{r}
#| label: fig-rate-1
#| fig-cap: "RATE AUTOC Graphs"
#| eval: true  # <- set to false as needed/desired
#| echo: false
#| fig-height: 10
#| fig-width: 12
autoc_plots[[1]]
```

@fig-rate-1 presents the RATE AUTOC curve for `r autoc_name_1`

{{< pagebreak >}}

## Appendix E. Estimating and Interpreting Heterogeneous Treatment Effects with GRF {#appendix-explain-grf}

```{r, results='asis'}
#| eval: false
#| echo: false
library(boilerplate)
cat(
  boilerplate::boilerplate_generate_text(
    category     = "appendix",
    # ← choose the right top-level list
    sections     = c("explain.grf_short"),
    global_vars  = global_vars,
    db           = unified_db
  )
)
```

{{< pagebreak >}}

## Appendix F: Strengths and Limitations of Causal Forests  {#appendix-rate}

```{r, results='asis'}
#| eval: true # <- set to false as needed/desired

cat(
  boilerplate::boilerplate_generate_text(
    category     = "discussion",
    # ← choose the right top-level list
    sections     = c("strengths.strengths_grf_short"),
    global_vars  = global_vars,
    db           = unified_db
  )
)
```


Here we explain a heterogeneous‐treatment‐effect (HTE) analysis using causal forests [@grf2024]. In our workflow, we move from the average treatment effect (ATE) to individualised effects, quantify the practical value of targeting, and finish with interpretable decision rules.


### 1 Average Treatment Effect (ATE)

The ATE answers: *'What would happen, on average, if everyone received treatment versus no one?'*

$$
    \text{ATE}=E[Y(1)-Y(0)].
$$

Using the `grf` package, we estimate the ATE doubly-robustly.  Because we analyse several outcomes, we adjust ATE *p*-values with bonferroni ($\alpha$ = 0.05) to control the family-wise error rate.




### 2 Do Effects Vary?  Formal Test of Heterogeneity

Define the conditional average treatment effect (CATE)

$$
  \tau(x)=E[Y(1)-Y(0)\mid X=x].
$$

If $\tau(x)$ is constant, effects are homogeneous; otherwise they vary. Classical interaction models impose strong forms; **grf** uses *causal forests* to discover complex, nonlinear heterogeneity [@wager2018].  We assess heterogeneity with RATE *p*-values corrected via Benjamini–Hochberg false-discovery-rate adjustment (q = 0.1), controlling the false-discovery rate [@benjamini1995controlling].




### 3 Causal Forests for Individualised Estimates

A causal forest is an ensemble of 'honest' causal trees that split on covariates to maximise treated–control contrasts. For each unit $i$ we obtain

$$
  \widehat{\tau}(x_i)
$$

Strengths are flexibility, orthogonalisation, and per-person estimates.


### 4 Built-in Protection Against Over-fitting

  Honesty (split half/estimate half) plus out-of-bag (OOB) predictions yield unbiased $\widehat{\tau}(x)$ and standard errors without manual hyper-tuning.


### 5 Missing Data Handling

**grf** deploys 'Missing Incorporated in Attributes' (MIA): missingness is a valid split, so cases stay in the analysis -- no ad-hoc imputation required.


### 6 Testing for **Actionable** Heterogeneity: the TOC & RATE Metrics

Ranking units by $\widehat{\tau}$ defines a **Targeting Operator Characteristic** (TOC) curve: the cumulative gain from treating the top fraction $q$ of predicted responders. Two scalar summaries:

- **RATE AUTOC** – area under the entire TOC; emphasises the very highest responders.
- **RATE Qini** – weighted area with weight $q$; rewards sustained gains across larger coverage [@yadlowsky2021evaluating].

Under $H_0{:}\tau(x)$ constant, both equal 0.
`grf::rank_average_treatment_effect()` supplies point estimates, standard errors, and $t$-tests.

**Multiplicity control**: We adjust AUTOC and Qini *p*-values with Benjamini–Hochberg false-discovery-rate adjustment (q = 0.1) before declaring actionable heterogeneity.

Here is an **interpretation tip**:

* AUTOC answers *'How sharply can we prioritise?'*
* Qini answers *'How valuable is targeting when budgets are modest but not tiny?'*

### 7 Visualising Policy Value: the Qini Curve

Plotting the Qini curve (cumulative gain vs $q$) reveals where returns plateau. Investigators (and policy audiences) can see at a glance whether benefits concentrate in, say, the top 20 % or persist up to 50 %.


### 8 Valid Inference for RATE / Qini

Although OOB predictions are out-of-sample per tree, they inherit forest-level dependence. We use an explicit **sample split**:

1. **Train set**: fit the causal forest and compute $\widehat{\tau}(x)$.
2. **Test set**: compute RATE AUTOC/Qini and run $H_0$ tests.

This second split yields honest policy evaluation and guards against optimistic bias [@grf2024].

### 9 From Black Box to Simple Rules: Policy Trees

Stakeholders value transparent criteria. The **policytree** algorithm takes $\widehat{\tau}(x)$ or doubly-robust scores and learns a shallow decision tree that maximises expected welfare [@policytree_package_2024].

*Advantages*: interpretability, the possibility of fairness constraints, and easy communication (e.g., *'treat if age < 25 and baseline severity high'*).

Training mirrors the split above: learn the tree on one fold, evaluate welfare on another.

> **Caveat**  Splits identify predictors of *effect variation*, not causal levers. Changing a covariate in the tree does **not** guarantee an effect on $\tau(x)$.


### 10 Ethical and Practical Considerations

Statistical optimisation rarely aligns perfectly with equity or political feasibility. Decisions about who *should* receive treatment belong to democratic processes that weigh fairness, cost, and broader social values.


### Putting it together

The sequence—ATE, causal-forest CATEs, RATE/Qini diagnostics, Qini curve, and finally a shallow policy tree—delivers both rigorous evidence and a defensible targeting rule.  Researchers learn **how large** heterogeneity is, **where** targeting pays off under budget constraints, and **which** simple covariate splits capture most of the welfare gain, all while guarding against over-fitting and multiplicity.


{{< pagebreak >}}


## References {.appendix-refs}







