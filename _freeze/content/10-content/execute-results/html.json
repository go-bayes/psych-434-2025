{
  "hash": "c469077d974332708b01e3f49ff0511e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hands On Working With Quarto Manuscript\"\ndate: \"2025-MAY-13\"\nbibliography: /Users/joseph/GIT/templates/bib/references.bib\neditor_options: \n  chunk_output_type: console\nformat:\n  html:\n    warnings: FALSE\n    error: FALSE\n    messages: FALSE\n    code-overflow: scroll\n    highlight-style: Ayu\n    code-tools:\n      source: true\n      toggle: FALSE\nhtml-math-method: katex\nreference-location: margin\ncitation-location: margin\ncap-location: margin\ncode-block-border-left: true\n---\n\n## Part 1: Quarto manuscripts\n\n\n\n\n::: {.callout-note}\n**Required**\nDownload Quarto here: \n- Use the `prelease` version: https://quarto.org/docs/download/\n**Optional**\n- [@Bulbulia2024PracticalGuide] [link](https://osf.io/preprints/psyarxiv/uyg3d)\n- [@hoffman2023] [link](https://arxiv.org/pdf/2304.09460.pdf)\n:::\n\n::: {.callout-important}\n## Key concepts\n- Entering bibliographic details\n- Writing up your manuscript\n:::\n\n# Code review: YOUR analysis\n\n-  Create a new Rstudio project\n-  Modify *these* scripts\n\n\n::: {.callout-note-script-0}\n[Download full lab scripts 0](../laboratory/lab-10/00-setup-L10.R)\n:::\n\n\n::: {.callout-note-script-1}\n[Download full lab scripts 1](../laboratory/ ../laboratory/lab-10/01-init-L10.R)\n:::\n\n::: {.callout-note-script-2}\n[Download full lab scripts 2]( ../laboratory/lab-10/02-make-wide-L10.R)\n:::\n\n::: {.callout-note-script-3}\n[Download full lab scripts 3](../laboratory/lab-10/03-models-L10-v3.R)\n:::\n\n\n\n\n#  Quarto with `boilerplate`\n\n::: {.callout-quarto-manuscript-template}\n[Download Quarto Template](../quarto/initial_quarto_document.qmd)\n:::\n\n\n## Script 0: Set up your library/\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# script 00: read initial boilerplate data (for students)\n\n# load required packages --------------------------------------------------\nif (!requireNamespace(\"boilerplate\", quietly = TRUE)) {\n  if (!requireNamespace(\"devtools\", quietly = TRUE)) {\n    install.packages(\"devtools\")  # install devtools if missing\n  }\n  devtools::install_github(\"go-bayes/boilerplate\")  # install boilerplate\n}\n\nlibrary(boilerplate)  # manage boilerplate data\nlibrary(cli)          # friendly messages\nlibrary(here)         # project paths\nlibrary(fs)           # file system utilities\n\n# ensure correct boilerplate version --------------------------------------\nmin_version <- \"1.0.43\"\nif (utils::packageVersion(\"boilerplate\") < min_version) {\n  stop(\n    \"please install boilerplate >= \", min_version, \":\\n\",\n    \"  devtools::install_github('go-bayes/boilerplate')\"\n  )\n}\n\ncli::cli_h1(\"boilerplate loaded ✔\")\n\n# create local data folders ------------------------------------------------\npath_data   <- here::here(\"example_boilerplate_data\")\npath_quarto <- here::here(\"quarto\")\n\nfs::dir_create(path_data)    # create data folder if needed\ncli::cli_h2(\"data folder ready ✔\")\n\nfs::dir_create(path_quarto)  # create quarto folder if needed\ncli::cli_h2(\"quarto folder ready ✔\")\n\n# import student boilerplate data ------------------------------------------\nload_student_boilerplate <- function() {\n  base_url   <- \"https://raw.githubusercontent.com/go-bayes/templates/main/student_boilerplate_data/\"\n  categories <- c(\"measures\", \"methods\", \"results\", \"discussion\", \"appendix\")\n  cli::cli_text(\"loading student boilerplate data from GitHub...\")\n  \n  student_db <- list()\n  for (cat in categories) {\n    cli::cli_text(\"  - loading {cat} database...\")\n    rds_url <- paste0(base_url, cat, \"_db.rds\")\n    student_db[[cat]] <- tryCatch(\n      readRDS(url(rds_url)),\n      error = function(e) {\n        cli::cli_alert_warning(\"failed to load {cat}: {e$message}\")\n        list()  # fallback empty list\n      }\n    )\n  }\n  \n  cli::cli_text(\"successfully loaded {length(categories)} categories\")\n  student_db\n}\n\nstudent_unified_db <- load_student_boilerplate()\n\n# save imported data -------------------------------------------------------\nboilerplate_save(\n  student_unified_db,\n  data_path     = path_data,\n  create_backup = FALSE\n)\ncli::cli_h1(\"data saved ✔\")\n\n# set up bibliography and APA-7 template -----------------------------------\nfs::dir_create(\"template_partials\")  # for title.tex\n\ndownload.file(\n  url      = \"https://raw.githubusercontent.com/go-bayes/templates/refs/heads/main/quarto/title.tex\",\n  destfile = \"template_partials/title.tex\",\n  mode     = \"wb\"\n)\n\nfs::dir_create(\"bibliography\")\nfs::dir_create(\"csl\")\n\ndownload.file(\n  url      = \"https://raw.githubusercontent.com/go-bayes/templates/refs/heads/main/bib/references.bib\",\n  destfile = \"bibliography/references.bib\",\n  mode     = \"wb\"\n)\n\ndownload.file(\n  url      = \"https://raw.githubusercontent.com/go-bayes/templates/refs/heads/main/csl/apa-7.csl\",\n  destfile = \"csl/apa7.csl\",\n  mode     = \"wb\"\n)\n\ncli::cli_h1(\"bibliography and CSL setup complete ✔\")\n\n# end of script: do not rerun this file ------------------------------------\n```\n:::\n\n\n## Script 1: Adding/Revising Measures\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# script 01: add or revise measures (for students)\n# load required packages --------------------------------------------------\nif (!requireNamespace(\"boilerplate\", quietly = TRUE)) {\n  if (!requireNamespace(\"devtools\", quietly = TRUE)) {\n    install.packages(\"devtools\")  # install devtools if missing\n  }\n  devtools::install_github(\"go-bayes/boilerplate\")  # install boilerplate\n}\n\nlibrary(boilerplate)  # tools for measure management\nlibrary(cli)          # user-friendly messages\nlibrary(here)         # project-friendly file paths\n\n# ensure correct boilerplate version --------------------------------------\nif (utils::packageVersion(\"boilerplate\") < \"1.0.41\") {\n  stop(\"please install boilerplate >= 1.0.41: \\\n       devtools::install_github('go-bayes/boilerplate')\")\n}\n\ncli::cli_h1(\"boilerplate loaded ✔\")\n\n# define data paths --------------------------------------------------------\npath_src   <- here::here(\"example_boilerplate_data\")\npath_final <- here::here(\"final_boilerplate_data\")\n\n# create final data directory if needed -----------------------------------\nif (!dir.exists(path_final)) {\n  dir.create(path_final)\n}\ncli::cli_h1(\"data folder ready ✔\")\n\n# import unified database --------------------------------------------------\nunified_db <- boilerplate_import(data_path = path_src)\n\n# save a copy to avoid overwriting original --------------------------------\nboilerplate_save(\n  unified_db,\n  data_path   = path_final,\n  output_file = \"unified_db\"\n)\n\ncli::cli_h2(\"database imported and saved ✔\")\n\n# inspect structure and existing measures ----------------------------------\nstr(unified_db, max.level = 2)     # glance at top-level structure\nprint(names(unified_db$measures))  # list defined measures\n\n# example: check for a specific measure ------------------------------------\nmeasure_name <- \"emp_job_satisfaction\"\nif (is.null(unified_db$measures[[measure_name]])) {\n  cli::cli_alert_info(\"{measure_name} not defined yet\")\n} else {\n  print(unified_db$measures[[measure_name]])\n}\n\n# add a new measure --------------------------------------------------------\nunified_db$measures[[measure_name]] <- list(\n  name        = \"Job Satisfaction\",\n  description = \"job satisfaction was measured with a single item.\",\n  reference   = \"[@eisenbarth2022aspects]\",\n  waves       = \"1-present\",\n  keywords    = c(\"employment\", \"mental health\"),\n  items       = list(\"how satisfied are you with your current job?\")\n)\n\n# save with backup ---------------------------------------------------------\nboilerplate_save(\n  unified_db,\n  data_path     = path_final,\n  create_backup = TRUE\n)\ncli::cli_h2(\"new measure added and saved ✔\")\n\n# revise an existing measure ------------------------------------------------\nrevise_name <- \"family_time_binary\"\ncli::cli_h1(\"revising {revise_name}\")\n\n# use modifyList to update only changed fields\nunified_db$measures[[revise_name]] <- modifyList(\n  unified_db$measures[[revise_name]],\n  list(\n    name        = \"Family Time (binary)\",\n    description = \"code string (binary): 0 = none, 1 = any time\",\n    reference   = \"@sibley2020\",\n    waves       = \"10-13\",\n    keywords    = c(\"cooperation\")\n  )\n)\n\n# view revised measure ------------------------------------------------------\nprint(unified_db$measures[[revise_name]])\n\n# save all changes -----------------------------------------------------------\nboilerplate_save(\n  unified_db,\n  data_path = path_final,\n  confirm   = TRUE\n)\ncli::cli_h2(\"measure revised and saved ✔\")\n\n# to reload updated database, uncomment the following line --------------\n# unified_db <- boilerplate_import(data_path = path_final)\n```\n:::\n\n\n\n## Script 3: Setting Up Your Manuscript Document\n\n\n::: {.cell}\n\n````{.r .cell-code}\n---\ntitle: \"Your Title\"\nabstract: |\n  **Background**: (Brief few sentences)\n  \n  **Objectives**: \n    1. Estimate the causal effect of YOUR EXPOSURE on YOUR OUTCOMES measured one year later.\n    2. Evaluate whether these effects vary across the population.\n    3. Provide policy guidance on which individuals might benefit most.\n  \n  **Method**: We conducted a three-wave retrospective cohort study (waves XX-XXX, October XXXX--October XXXX) using data from the New Zealand Attitudes and Values Study, a nationally representative panel. Participants were eligible if they participated in the NZAVS in the baseline wave (XXXX, were under the age of 62, and were employed > 20 hours per week. We defined the exposure as (XXXX  > NUMBER on a 1-7 Likert Scale (1 = yes, 0 = no)). To address attrition, we applied inverse probability of censoring weights; to improve external validity, we applied weighted to the population distribution of Age, Ethnicity, and Gender. We computed expected mean outcomes for the population in each exposure condition (high XXXX/low XXXXX). Under standard causal assumptions of unconfoundedness, the contrast provides an unbiased average treatment effect. We then used causal forests to detect heterogeneity in these effects and employed policy tree algorithms to identify individuals (\"strong responders\") likely to experience the greatest benefits.\n  \n  **Results**:   Increasing XXXXX leads to XXXXX. Heterogeneous responses to (e.g. *Forgiveness*, *Personal Well-Being*, and *Life-Satisfaction*...) reveal structural variability in subpopulations...\n  \n  **Implications**: (Brief few sentences)\n  **Keywords**: *Causal Inference*;  *Cross-validation*; *Distress*; *Employment*; *Longitudinal*; *Machine sLearning*; *Religion*; *Semi-parametric*; *Targeted Learning*.\nauthor: \n  - name: YOUR NAME\n    affiliation: Victoria University of Wellington, New Zealand\n    email: XXXXX\n    corresponding: yes\nkeywords: [Causal Inference, Cross-validation,...]\neditor_options: \n  chunk_output_type: console\ndate: \"last-modified\"\nfontfamily: libertinus\nbibliography: references.bib\ncsl: apa7.csl\nformat:\n  docx: \n    default: false\n  pdf:\n    pdf-engine: lualatex\n    sanitise: true\n    keep-tex: true\n    link-citations: true\n    colorlinks: true\n    documentclass: article\n    classoption: [\"single column\"]\n    lof: false\n    lot: false\n    geometry:\n      - top=30mm\n      - left=25mm\n      - heightrounded\n      - headsep=22pt\n      - headheight=11pt\n      - footskip=33pt\n      - ignorehead\n      - ignorefoot\n    header-includes:\n      - \\let\\oldtabular\\tabular\n      - \\renewcommand{\\tabular}{\\small\\oldtabular}\n      - \\setlength{\\tabcolsep}{4pt}  # adjust this value as needed\nexecute:\n  echo: false\n  warning: false\n  include: true\n  eval: true\n---\n\n```{r}\n#| label: load-libraries\n#| echo: false\n#| include: false\n#| eval: true\n\n# ---- package setup -------------------------------------------------------\nlibrary(here)\npacman::p_load(\n  boilerplate, margot, tinytex, extrafont,\n  tidyverse, kableExtra, glue, patchwork,\n  stringr, ggplot2, ggeffects, parameters,\n  table1, here, knitr\n)\n\n# ---- copy resources to quarto -------------------------------------------\nif (!file.exists(here::here(\"quarto\", \"apa7.csl\"))) {\n  file.copy(\"csl/apa7.csl\", here::here(\"quarto\", \"apa7.csl\"))\n}\nif (!file.exists(here::here(\"quarto\", \"references.bib\"))) {\n  file.copy(\"bibliography/references.bib\", here::here(\"quarto\", \"references.bib\"))\n}\n\n# ---- install GitHub packages if missing ----------------------------------\nif (!requireNamespace(\"boilerplate\", quietly = TRUE)) {\n  install.packages(\"devtools\")\n  devtools::install_github(\"go-bayes/boilerplate\")\n}\nif (!requireNamespace(\"margot\", quietly = TRUE)) {\n  install.packages(\"devtools\")\n  devtools::install_github(\"go-bayes/margot\")\n}\n\n# ---- check package versions ---------------------------------------------\nutils::packageVersion(\"margot\")      # should be > 1.0.38\nutils::packageVersion(\"boilerplate\") # should be > 1.0.42\n\n# ---- ensure reproducibility and load fonts --------------------------------\nset.seed(123)\nloadfonts(device = \"all\")\n\n# ---- define paths and import data ----------------------------------------\npush_mods             <- here::here(\"save_directory\")\nfinal_boilerplate_data <- here::here(\"final_boilerplate_data\")\nunified_db            <- boilerplate_import(data_path = final_boilerplate_data)\n\n# ---- inspect available boilerplate entries -------------------------------\ncat(unified_db$methods$confounding_control$vanderweele)\ncat(unified_db$methods$sample$nzavs)\n\n# ---- read variable definitions -------------------------------------------\nbaseline_vars           <- margot::here_read(\"baseline_vars\")\nexposure_var            <- margot::here_read(\"exposure_var\")\noutcome_vars            <- margot::here_read(\"outcome_vars\")\n\n# ---- define study waves --------------------------------------------------\nbaseline_wave           <- margot::here_read(\"baseline_wave\")\nexposure_waves          <- margot::here_read(\"exposure_waves\")\noutcome_wave            <- margot::here_read(\"outcome_wave\")\n\n# ---- define study parameters ---------------------------------------------\nstudy_years             <- \"2018-2021\"\nname_exposure           <- here_read(\"name_exposure\")\nname_outcome_variables  <- \"MY OUTCOME VARIABLES IN THIS STUDY\"\nname_exposure_lower     <- tolower(name_exposure)\nname_exposure_lower\n\n# ---- templates and thresholds --------------------------------------------\neligibility_template    <- \"Participants were eligible if they participated in the {baseline wave}\"\npercent_missing_baseline <- margot::here_read(\"percent_missing_baseline\")\n\n# ---- read tables for manuscript ------------------------------------------\nmarkdown_table_baseline  <- margot::here_read(\"baseline_table\")\nmarkdown_table_exposures <- margot::here_read(\"exposure_table\")\nmarkdown_table_outcomes  <- margot::here_read(\"outcomes_table\")\n\n# ---- sample size information --------------------------------------------\nn_total                  <- margot::here_read(\"n_total\")\nn_participants           <- here_read(\"n_participants\")\n\n# ---- variable labels and mappings ----------------------------------------\nvar_labels_measures      <- here_read(\"var_labels_measures\")\nlabel_mapping_all        <- here_read(\"label_mapping_all\")\n\n# ---- plot titles and analysis settings -----------------------------------\nate_title                <- here_read(\"ate_title\")\nflipped_names            <- here_read(\"flipped_names\")\nflipped_list             <- paste(flipped_names, collapse = \", \")\n\n# ---- import data for visualisation --------------------------------------\noriginal_df              <- margot::here_read('df_wide', push_mods)\n\n# ---- define nice names and regimes ---------------------------------------\nnice_name_exposure_variable <- stringr::str_to_sentence(name_exposure)\nname_outcomes_lower          <- \"multi-dimensional wellbeing\"\n\n# ---- define exposure thresholds and regimes ------------------------------\nlower_cut               <- here_read(\"lower_cut\")\nupper_cut               <- here_read(\"upper_cut\")\nthreshold               <- here_read(\"threshold\")\ninverse_threshold       <- here_read(\"inverse_threshold\")\nscale_range             <- margot::here_read(\"scale_range\")\n\nname_control_regime_lower <- glue::glue(\"low {name_exposure_lower}\")\nvalue_exposure_regime     <- glue::glue(\"Set {name_exposure} {threshold} {upper_cut} {scale_range}\")\nvalue_control_regime      <- glue::glue(\"Set {name_exposure} {inverse_threshold} {upper_cut} {scale_range}\")\n\ncontrast_template         <- \"We used causal forests to estimate an average treatment effect as a contrast between *{name_control_regime_lower}* and *{name_exposure_lower}* on {name_outcomes_lower}.\"\ncontrast_text             <- glue(contrast_template)\n\n# ---- verify assumptions (positivity) -------------------------------------\ntransition_tables        <- margot::here_read(\"transition_tables\")\ntransition_tables_binary <- here_read(\"transition_tables_binary\")\n\n# ---- generate measures text for methods section -------------------------\nbaseline_measures_text   <- boilerplate_generate_measures(\n  variable_heading = \"Baseline Covariates\",\n  variables        = baseline_vars,\n  db               = unified_db,\n  heading_level    = 3,\n  subheading_level = 4,\n  print_waves      = FALSE,\n  label_mappings   = var_labels_measures\n)\nexposure_measures_text   <- boilerplate_generate_measures(\n  variable_heading = \"Exposure Variable\",\n  variables        = name_exposure,\n  db               = unified_db,\n  heading_level    = 3,\n  subheading_level = 4,\n  print_waves      = FALSE,\n  label_mappings   = var_labels_measures\n)\noutcome_measures_text    <- boilerplate_generate_measures(\n  variable_heading = \"Outcome Variables\",\n  variables        = outcome_vars,\n  db               = unified_db,\n  heading_level    = 3,\n  subheading_level = 4,\n  print_waves      = FALSE,\n  label_mappings   = var_labels_measures\n)\n\n# ---- exposure description from database --------------------------------\nmeasures_exposure        <- glue::glue(unified_db$measures[[name_exposure]]$description)\n\n# ---- set plot defaults for ate plots -------------------------------------\nbase_defaults_binary     <- list(\n  type                   = \"RD\",\n  title                  = ate_title,\n  e_val_bound_threshold  = 1.2,\n  colors                 = c(\n    \"positive\"    = \"#E69F00\",\n    \"not reliable\"= \"grey50\",\n    \"negative\"    = \"#56B4E9\"\n  ),\n  x_offset               = -0.5,\n  x_lim_lo               = -0.5,\n  x_lim_hi               = 0.5,\n  text_size              = 4,\n  linewidth              = 0.5,\n  estimate_scale         = 1,\n  base_size              = 18,\n  point_size             = 2,\n  title_size             = 19,\n  subtitle_size          = 16,\n  legend_text_size       = 10,\n  legend_title_size      = 10,\n  include_coefficients   = FALSE\n)\n\n# ---- create plot options for outcomes -----------------------------------\noutcomes_options_all     <- margot_plot_create_options(\n  title         = \"\",\n  base_defaults = base_defaults_binary,\n  subtitle      = \"\",\n  filename_prefix = \"grf\"\n)\n\n# ---- policy tree graph settings -----------------------------------------\ndecision_tree_defaults  <- list(\n  span_ratio         = 0.3,\n  text_size          = 3.8,\n  y_padding          = 0.25,\n  edge_label_offset  = 0.002,\n  border_size        = 0.05\n)\n\npolicy_tree_defaults    <- list(\n  point_alpha            = 0.5,\n  title_size             = 12,\n  subtitle_size          = 12,\n  axis_title_size        = 12,\n  legend_title_size      = 12,\n  split_line_color       = \"red\",\n  split_line_alpha       = 0.8,\n  split_label_color      = \"red\",\n  split_label_nudge_factor = 0.007\n)\n\n# ---- load and check model results ---------------------------------------\nmodels_binary           <- margot::here_read_qs(\"models_binary\", push_mods)\nmargot::margot_size(models_binary)\n\n# ---- create ate plots and interpretation --------------------------------\nbinary_results          <- margot_plot(\n  models_binary$combined_table,\n  options            = outcomes_options_all,\n  label_mapping      = label_mapping_all,\n  include_coefficients = FALSE,\n  save_output       = FALSE,\n  order             = \"evaluebound_asc\",\n  original_df       = original_df,\n  e_val_bound_threshold = 1.2\n)\nbinary_results$plot\ncat(binary_results$interpretation)\n\n# ---- heterogeneity analysis ---------------------------------------------\nmodels_binary_flipped_all <- here_read_qs(\"models_binary_flipped_all\", push_mods)\nresult_ominbus_hetero     <- margot_omnibus_hetero_test(\n  models_binary_flipped_all,\n  label_mapping  = label_mapping_all,\n  alpha          = 0.05,\n  detail_level   = \"standard\",\n  format         = \"markdown\"\n)\nresult_ominbus_hetero$summary_table |> kbl(\"markdown\")\ncat(result_ominbus_hetero$brief_interpretation)\n\n# ---- rate test and autocorrelation plots -------------------------------\nrate_table_all            <- here_read(\"rate_table_all\")\nrate_interpretation_all   <- here_read(\"rate_interpretation_all\")\nbatch_rate_autoc_plots    <- margot_plot_rate_batch(\n  models_binary_flipped_all,\n  save_plots     = FALSE,\n  model_names    = rate_interpretation_all$autoc_model_names\n)\n\n# ---- policy analysis with qini and trees -------------------------------\nmodels_batch_2L            <- margot_policy(\n  models_binary_flipped_all,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  model_names        = rate_interpretation_all$qini_model_names,\n  max_depth          = 2L,\n  original_df        = original_df,\n  label_mapping      = label_mapping_all\n)\n\nqini_plots               <- lapply(seq_along(models_batch_2L), function(i) {\n  models_batch_2L[[i]][[4]]\n})\nnames(qini_plots)       <- rate_interpretation_all$qini_model_names\ninterpretation_qini_curves_2L <- margot_interpret_qini(\n  models_batch_2L,\n  model_names        = rate_interpretation_all$qini_model_names,\n  label_mapping      = label_mapping_all\n)\n\nplots_policy_trees_1L     <- margot_policy(\n  models_binary_flipped_all,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  model_names        = rate_interpretation_all$either_model_names,\n  original_df        = original_df,\n  label_mapping      = label_mapping_all,\n  max_depth          = 1L\n)\nplots_policy_trees_2L     <- margot_policy(\n  models_binary_flipped_all,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  model_names        = rate_interpretation_all$either_model_names,\n  original_df        = original_df,\n  label_mapping      = label_mapping_all,\n  max_depth          = 2L\n)\ninterpret_plots_policy_trees_2L <- margot_interpret_policy_batch(\n  models_binary_flipped_all,\n  model_names       = rate_interpretation_all$either_model_names\n)\n\n# ---- define global variables for text generation ------------------------\nglobal_vars <- list(\n  name_exposure_variable = nice_name_exposure_variable,\n  n_total                = n_total,\n  n_participants         = n_participants,\n  exposure_variable      = name_exposure,\n  name_exposure_lower    = name_exposure_lower,\n  name_control_regime_lower = name_control_regime_lower,\n  name_outcome_variables = \"Multi-dimensional Wellbeing\",\n  name_outcomes_lower    = name_outcomes_lower,\n  name_exposure_capfirst = nice_name_exposure_variable,\n  measures_exposure      = measures_exposure,\n  value_exposure_regime  = value_exposure_regime,\n  value_control_regime   = value_control_regime,\n  flipped_list           = flipped_list,\n  baseline_wave          = \"NZAVS time 10, years 2018-2019\",\n  exposure_waves         = \"NZAVS time 11, year 2019-2020\",\n  outcome_wave           = \"NZAVS time 12, years 2020-2021\",\n  protocol_url           = \"https://osf.io/ce4t9/\"\n)\n\n```\n\n{{< pagebreak >}}\n\n## Method\n\n```{r, results='asis'}\n#| eval: true\n#| echo: false\nlibrary(boilerplate)\ncat(boilerplate_generate_text(\n  category = \"methods\",\n  sections = c(\n    \"sample.nzavs\",\n    \"target_population\",\n    \"eligibility.standard\",\n    \"causal_intervention.grf_simple_text\",\n    \"analytic_approach.general_approach_cate_short\",\n    \"exposure_indicator\",\n    \"causal_identification_criteria\",\n    \"confounding_control.vanderweele\",\n    \"statistical_models.grf_short_explanation\",\n    \"missing_data.missing_grf_simple\",\n    \"sensitivity_analysis.short_evalue\"\n  ),\n  global_vars = global_vars,\n  # global_vars = list( # variables you might need\n  #   exposure_variable = extraversion,\n  #   n_total = n_total,\n  #   n_participants = n_participants,\n  #   appendix_timeline = \"Appendix A\",\n  #   appendix_measures = \"Appendix B\",\n  #   measures_exposure = measures_exposure,\n  #   appendix_analytic_approach = \"Appendix C\",\n  #   appendix_assumptions_grf = \"Appendix D\",\n  #   appendix_cate_validation_grf = \"Appendix D\",\n  #   appendices_sample = \"Appendix A\",\n  #   appendix_outcomes = \"Appendix B\",\n  #   appendix_positivity = \"Appendix E\",\n  #   value_exposure_regime = \"Regular Religious Service Attendance\",\n  #   value_treatment_regime = \"No Religious Service Attendance\",\n  #   value_exposure_regime = \" At each wave, if attendance is below four times per month, we shift it to four; otherwise, we leave it unchanged.\",\n  #   value_control_regime = \"At each wave, if attendance is above zero, we shift it to zero; otherwise, we leave it unchanged.\",\n  #   name_exposure_regime = \"Regular Religious Service Attendance.\",\n  #   name_control_regime = \"No Religious Service Attendance.\",\n  #   name_exposure_threshold = \"Attends Religious Service\",\n  #   name_control_threshold = \"No Religious Service Attendance.\",\n  #   value_exposure = \"1\",\n  #   value_control = \"0\",\n  #   name_target_population = \"all adults in New Zealand from the years 2018-2021\",\n  #   number_exposure_waves = \"one\",\n  #   time_varying_confounders = \"(physical disability, employment status, partner status, and parenting status)\",\n  #   flipped_list = c(\"Neuroticism\"),\n  #   grf_appendix = \"Appendix D\",\n  #   baseline_wave = \"NZAVS time 10, years 2018-2019\",\n  #   exposure_waves = \"NZAVS time 11, year 2019\",\n  #   outcome_wave = \"NZAVS time 12, years 2020-2021\",\n  #   flipped_example = \"Neuroticism\",\n  #   appendix_analytic_approach = \"Appendix E\",\n  #   protocol_url = \"https://osf.io/ce4t9/\"\n  # ),\n  db = unified_db\n))\n```\n\n\n{{< pagebreak >}}\n\n## Results\n\n### Average Treatement Effects\n\n```{r}\n#| label: fig-ate\n#| fig-cap: \"Average Treatment Effects on Multi-dimensional Wellbeing\"\n#| eval: true\n#| echo: false\n#| fig-height: 14\n#| fig-width: 18\nbinary_results$plot\n```\n\n{{< pagebreak >}}\n\n```{r}\n#| label: tbl-outcomes\n#| tbl-cap: \"Average Treatment Effects on Multi-dimensional Wellbeing\"\n#| eval: true\n#| echo: false\n\nbinary_results$transformed_table |> kbl(\"markdown\")\n\n```\n\n```{r, results = 'asis'}\ncat(binary_results$interpretation)\n```\n\n\n\n\n{{< pagebreak >}}\n\n### Heterogeneous Treatment Effects \n\n```{r, results='asis'}\n#| eval: true\n#| echo: false\ncat(rate_interpretation_all$comparison)\n```\n\nRefer to [Appendix F](#appendix-cate-validation) for details.\n\n\n\n### RATE AUTOC Results\n\n```{r, results = 'asis'}\n# only reliable results\ncat(rate_interpretation_all$autoc_results)\n```\n\n```{r}\n#| label: fig-rate\n#| fig-cap: \"RATE AUTOC Graphs\"\n#| eval: true\n#| echo: false\n#| fig-height: 16\n#| fig-width: 9\nbatch_rate_autoc_plots$model_t2_log_hours_exercise_z\n\n```\n\n\n### QINI Curve Results\n\n```{r, results = 'asis'}\n# only reliable results\ncat(rate_interpretation_all$qini_results)\n```\n\n```{r}\n#| label: tbl-qini\n#| tbl-cap: \"Qini Curve Results\"\n#| eval: true\n#| echo: false\n\ninterpretation_qini_curves_2L$summary_table |> \n  mutate(across(where(is.numeric), ~ round(., 2))) %>%\n  kbl(format = \"markdown\")\n```\n\n```{r, results = 'asis'}\n# only reliable results\ncat(interpretation_qini_curves_2L$qini_explanation)\n```\n\n```{r}\n#| label: fig-qini\n#| fig-cap: \"RATE AUTOC Graphs\"\n#| eval: true\n#| echo: false\n#| fig-height: 16\n#| fig-width: 9\nqini_plots$model_t2_meaning_sense_z\n```\n\n\n{{< pagebreak >}}\n\n### Policy Trees\n\n```{r, results = 'asis'}\ncat(interpret_plots_policy_trees_2L)\n```\n\n        \n```{r}\n#| label: fig-policy-1\n#| fig-cap: \"Decision Tree: Exercise\"\n#| eval: true\n#| echo: false\n#| fig-height: 16\n#| fig-width: 9\nplots_policy_trees_2L$model_t2_log_hours_exercise_z$combined_plot\n```\n\n```{r}\n#| label: fig-policy-2\n#| fig-cap: \"Decision Tree: Meaning Sense\"\n#| eval: true\n#| echo: false\n#| fig-height: 16\n#| fig-width: 9\nplots_policy_trees_2L$model_t2_meaning_sense_z$combined_plot\n```\n\n\n\n## Discussion\n\n```{r,  results='asis'}\n#| eval: true\n#| echo: false\ncat(boilerplate_generate_text(\n  category = \"discussion\",\n  sections = c(\n    \"ethics.nzavs_2021_2027\",\n    \"authors_statment_empty\",\n    \"acknowlegements.nzavs_acknowledgements_2025\",\n    \"nzavs_data_availabily\"\n  ),\n  global_vars = list(\n    exposure_variable = name_exposure\n  ),\n  db = unified_db\n))\n```\n\n\n\n{{< pagebreak >}}\n\n## Appendix A: Measures {#appendix-measures}\n\n\n### Measures \n\n####  Covariate Measures \n```{r, results='asis'}\ncat(baseline_measures_text)\n```\n\n#### Exposure Measures\n\n```{r, results='asis'}\ncat(exposure_measures_text)\n```\n\n#### Outcome Measures\n\n```{r, results='asis'}\ncat(outcome_measures_text)\n```\n\n## Appendix B: Sample {#appendix-sample}\n\n@tbl-appendix-baseline presents sample demographic statistics.\n\n::: {#tbl-appendix-baseline}\n```{r, results = 'asis'}\n#| eval: true\n#| include: true\n#| echo: false\nmarkdown_table_baseline\n```\nDemographic statistics for New Zealand Attitudes and Values Cohort wave 2018.\n:::\n\n\n### Exposure Variable {#appendix-exposure} \n\n<!-- @tbl-sample-exposures presents sample statistics for the exposure variable, religious service attendance, during the baseline and exposure waves. This variable was not measured in part of NZAVS time 12 (years 2020-2021) and part of NZAVS time 13 (years 2021-2022). To address missingness, if a value was observed after NZAVS time 14, we carried the previous observation forward and created and NA indicator. If there was no future observation, the participant was treated as censored, and inverse probability of censoring weights were applied, following our standard method for handling missing observations (see mansucript **Method**/**Handling of Missing Data**). Here, our carry-forward imputation approach may result in conservative causal effect estimation because it introduces measurement error. However, this approach would not generally bias causal effect estimation away from the null because the measurement error is unsystematic and random and unrelated to the outcomes. -->\n\n\n::: {#tbl-appendix-exposures}\n```{r, results = 'asis'}\n#| eval: true\n#| include: true\n#| echo: false\n\nmarkdown_table_exposures\n\n```\nDemographic statistics for New Zealand Attitudes and Values Cohort waves 2018.\n:::\n\n{{< pagebreak >}}\n\n### Outcome Variables {#appendix-outcomes} \n\n::: {#tbl-appendix-outcomes}\n```{r, results = 'asis'}\n#| eval: true\n#| include: true\n#| echo: false\n\nmarkdown_table_outcomes\n\n```\nOutcome variables measured at baseline (NZAVS time 10, years 2018-2019, and time 15, years 2023-2024).\n:::\n\n\n{{< pagebreak >}}\n\n\n### Appendix C: Transition Matrix to Check The Positivity Assumption {#appendix-transition}\n\n```{r, results = 'asis'}\n#| label: tbl-transition\n#| tbl-cap: \"Transition Matrix Showing Change\"\n#| eval: true\n#| include: true\n#| echo: false\n\ntransition_tables_binary$tables[[1]]\n```\n\n```{r, results = 'asis'}\ncat(transition_tables_binary$explanation)\n```\n\n\n{{< pagebreak >}}\n\n\n## Appendix D: Evidence of Heterogeneity {#appendix-cate-validation}\n\n        \n```{r, results='asis'}\n#| label: tbl-omnibus\n#| eval: true\n#| echo: false\nresult_ominbus_hetero$summary_table |> kbl(\"markdown\")\n```\n\n```{r, results='asis'}\n#| eval: true\n#| echo: false\ncat(result_ominbus_hetero$interpretations)\n```\n\n```{r, results='asis'}\n#| eval: true\n#| echo: false\ncat(boilerplate::boilerplate_generate_text(\n  category = \"results\",\n  sections = c(\n    \"grf.interpretation_rate\"\n  ),\n  global_vars = list(\n    appendix_omnibus = \"Appendix F\",\n    flipped_list = flipped_list\n  ),\n  db = unified_db\n))\n```\n\n```{r, results='asis'}\n#| label: tbl-rate-autoc\n#| eval: true\n#| echo: false\n# view rate tables\nrate_table_all$rate_autoc |> kbl(\"markdown\")\n#rate_table_all$rate_qini |> kbl(\"markdown\")\n```\n\n```{r, results='asis'}\n#| eval: true\n#| echo: false\ncat(rate_interpretation_all$autoc_results)\n```\n\n```{r, results='asis'}\n#| label: tbl-rate-qini\n#| eval: true\n#| echo: false\nrate_table_all$rate_qini |> kbl(\"markdown\")\n```\n\n```{r, results='asis'}\n#| eval: true\n#| echo: false\ncat(rate_interpretation_all$qini_results)\n```\n\n\n{{< pagebreak >}}\n\n<!-- ```{r,  results='asis'} -->\n<!-- #| eval: true -->\n<!-- #| echo: false -->\n<!-- #| include: true -->\n<!-- text_explain_grf <- boilerplate_generate_text( -->\n<!--   category = \"appendix\", -->\n<!--   sections = c( -->\n<!--     \"explain.grf\" -->\n<!--   ), -->\n<!--   global_vars = list( -->\n<!--     traning_proportion = \"70%\", -->\n<!--     train_proportion_decision_tree = \"70%\", -->\n<!--     appendix_letter_explain_grf = \"G\" -->\n<!--   ), -->\n<!--   db = unified_db -->\n<!-- ) -->\n<!-- cat(knitr::knit_child(text = text_explain_grf, quiet = TRUE)) -->\n<!-- ``` -->\n\n\n\n## References {.appendix-refs}\n````\n:::\n\n\n\n\n## What You Will Learn\n\n- **How to create a publication quality manuscript** \n\n- **How to create a workflow for references** \n\n- **How to import results into your manuscript** \n\n- **How to make graphs of your results (using) `margot`** \n\n- **How to report your results**\n\n- **How to interpret your results**\n\n\n### Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::cite_packages()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 <https://doi.org/10.5281/zenodo.10907724>, R package version 1.0.44 Functions to obtain MARGinal Observational Treatment-effects from observational data., <https://go-bayes.github.io/margot/>.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont <https://doi.org/10.32614/CRAN.package.extrafont>, R package version 0.19, <https://CRAN.R-project.org/package=extrafont>.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. <https://www.jstatsoft.org/v40/i03/>.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. doi:10.32614/CRAN.package.tibble <https://doi.org/10.32614/CRAN.package.tibble>, R package version 3.2.1, <https://CRAN.R-project.org/package=tibble>.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. <https://www.R-project.org/>.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, <https://ggplot2.tidyverse.org>.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. doi:10.32614/CRAN.package.forcats <https://doi.org/10.32614/CRAN.package.forcats>, R package version 1.0.0, <https://CRAN.R-project.org/package=forcats>.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. doi:10.32614/CRAN.package.stringr <https://doi.org/10.32614/CRAN.package.stringr>, R package version 1.5.1, <https://CRAN.R-project.org/package=stringr>.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. doi:10.32614/CRAN.package.dplyr <https://doi.org/10.32614/CRAN.package.dplyr>, R package version 1.1.4, <https://CRAN.R-project.org/package=dplyr>.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. doi:10.32614/CRAN.package.purrr <https://doi.org/10.32614/CRAN.package.purrr>, R package version 1.0.4, <https://CRAN.R-project.org/package=purrr>.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. doi:10.32614/CRAN.package.readr <https://doi.org/10.32614/CRAN.package.readr>, R package version 2.1.5, <https://CRAN.R-project.org/package=readr>.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. doi:10.32614/CRAN.package.tidyr <https://doi.org/10.32614/CRAN.package.tidyr>, R package version 1.3.1, <https://CRAN.R-project.org/package=tidyr>.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, <https://github.com/rstudio/tinytex>. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. <https://tug.org/TUGboat/Contents/contents40-1.html>.\n```\n\n\n:::\n:::\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}