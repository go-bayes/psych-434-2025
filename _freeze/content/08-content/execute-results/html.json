{
  "hash": "cf03962dc716596806fb693dd4bbff4b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Estimation of ATE and CATE Using Machine Learning\"\ndate: \"2025-APR-29\"\nformat:\n  html:\n    warnings: false\n    error: false\n    messages: false\n    code-overflow: scroll\n    highlight-style: Ayu\n    code-line-numbers: true\n    code-fold: false\n    code-tools:\n      source: true\n      toggle: false\nhtml-math-method: katex\nreference-location: margin\ncitation-location: margin\ncap-location: margin\ncode-block-border-left: true\nbibliography: /Users/joseph/GIT/templates/bib/references.bib\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n::: {.callout-note}\n**Required**\n- [https://grf-labs.github.io/grf/](https://grf-labs.github.io/grf/)\n\n\n\n**Optional**\n- [@vanderweele2020] [link](https://www.dropbox.com/scl/fi/srpynr0dvjcndveplcydn/OutcomeWide_StatisticalScience.pdf?rlkey=h4fv32oyjegdfl3jq9u1fifc3&dl=0)\n- [@suzuki2020] [link](https://www.dropbox.com/scl/fi/4midxwr9ltg9oce02e0ss/suzuki-causal-diagrams.pdf?rlkey=uktzf3nurtgpbj8m4h0xz82dn&dl=0)\n- [@Bulbulia2024PracticalGuide] [link](https://osf.io/preprints/psyarxiv/uyg3d)\n- [@hoffman2023] [link](https://arxiv.org/pdf/2304.09460.pdf)\n:::\n\n\n::: {.callout-important}\n### Key concepts  \n\nThe workflow below introduces **heterogeneous-treatment-effect (HTE) analysis** with *causal forests*. By the end of the lecture you should understand six technical ideas:\n(1) ATE (lectures 1-5)\n(2) CATE (lecture 6)\n(3) The estimator $\\widehat{\\tau}(x)$, (lecture 6)\n(4) the RATE statistics drawn from a **Targeting-Operator Characteristic** (TOC) curve (new)\n(5) Qini Curves (new)\n(6) **policy trees**—and know how each fits into an applied research pipelin (new)\n:::\n\n\n# PART 1 Heterogeneous-Treatment-Effect Analysis with **causal forests**\n\n### Why worry about heterogeneity?  \n\n\nRelying on the average treatment effect (ATE) is a bit like handing out size-nine shoes to an entire student body: on *average* they might fit, but watch the tall students hobble and the small ones trip.  \n\nToday we will focus on the causal question: \"What would be the effects on multi-dimensional well-being if everyone spent at least one hour a week socialising with their community?\"\n\nNote that a one-hour boost in weekly community socialising could send some students' sense of belonging soaring while leaving others desolated.  Spotting that spread, measuring how big it really is, and deciding whether it is worth tailoring an exposure to individual 'shoe sizes' are the three practical goals of Heterogeneous Treatment Effects analysis.\n\n\n---\n\n### 1 Start with estimating the average treatment effect (ATE)  \n\nAssume the [Three Fundamental Assumptions of Causal Inference here](https://go-bayes.github.io/psych-434-2025/content/05-content.html#how-can-we-make-contrasts-between-counterfactual-potential-outcomes) are met. Suppose we wish to estimate the average treatment effect for socialising with one's community. \n\nWe begin with the most straightforward (and secretly impossible) counterfactual: *run two parallel universes—one where **everyone** gets the treatment, another where **no-one** does—and compare the final scores.  The resulting difference is the **average treatment effect**:  \n\n\n$$\n\\text{ATE}=E\\!\\bigl[Y(1)-Y(0)\\bigr].\n$$\n\nThis gives us the average response -- the shoe size... You've seen this before.\n\n\n---\n\n### 2 Do effects differ across people?  \n\nVariation is captured by the **conditional average treatment effect (CATE)**,  \n\n$$\n\\tau(x)=E\\!\\bigl[Y(1)-Y(0)\\mid X=x\\bigr],\n$$\n\nwhere $X$ gathers pre-treatment covariates -- age, baseline wellbeing, personality, whatever we have measured and included in our model. Normally these will be our *baseline confounders.* \n\nIf $\\tau(x)$ turns out to be flat, we say there is no evidence for heterogeneity worth targeting. \n\nPeople differ in countless, overlapping ways. Think of age, baseline wellbeing, personality traits, study habits, and more. \n\nA linear interaction model tests whether the treatment works differently along one straight dimension, such as gender, by fitting a straight line. \n\nBut real‐world data often twist and turn. If the true relationship bends like a garden hose, a straight line will miss the curve. \n\nRegression forests fix this by letting the data place splits wherever the shape changes, so they can follow any bends that appear [@wager2018].  \n\nStraight-line models are fine for simple patterns, but regression forests can trace the curves that simple lines overlook.\n\nCausal forests are based on regression forests, where the splitting attempts to maximise differences in causal effect estimates. What this means will soon be clear. \n\n\n### 3. From straight lines to trees  \n\nTraditional 'parametric' models (like simple regression) guess a single functional shape -- often a straight line -- before seeing the data.  A **non-parametric** model, by contrast, lets the data decide the shape.  A *regression tree* is the simplest non-parametric learner we will use.  \n\n1. **Regression tree**  \n\n*Idea*: split the covariate space by asking yes/no questions— 'Age ≤ 20?', 'Baseline wellbeing > 0.3?' — until each terminal **leaf** is fairly homogeneous.  Inside a leaf the predicted outcome is just the sample mean, so the tree builds a *piece-wise constant* surface instead of a global line.    \n*Analogy*: think of tiling a garden with stepping-stones: each stone is flat, but taken together they follow the ground’s contours.\n\n2. **Regression forest**  \n   A single tree is quick and interpretable but unstable: small changes in the data can move the splits and shift predictions.  A **random forest** grows many trees on bootstrap samples and averages their outputs.  Averaging cancels much of the noise [@breiman2001random].  \n\n3. **Causal Forests**  \n   To estimate treatment effects rather than outcomes, each tree plays a two-step 'honest' game [@wager2018]:  \n   - use one half of its sample to choose splits that separate treated from control units;  \n   - use the other half to compute treatment-control differences within every leaf.  \n\n   For a new individual with covariates $x_i$ each tree supplies a noisy leaf-level effect; the forest reports the **average**, written  \n\n$$\n  \\widehat{\\tau}(x)=E[Y(1)-Y(0)\\mid X=x].\n$$\n\nBecause the noisy estimates point in many directions, their average is markedly less variable -- *the wisdom of trees is a wisdom of crowds*.\n\nStraight‑line models suit simple patterns; regression forests flex to any bends; causal forests add a third dimension -- variation in treatment responses. We'll see this in action/\n\n\nFor more about causal forests see (~18mins in...)\n\n{{< video https://www.youtube.com/watch?v=YBbnCDRCcAI >}}\n\n\n<!-- So, a regression tree chops the data into locally flat chunks; a regression forest averages many such trees to smooth away chance idiosyncrasies; a causal forest adds honesty so that its averaged differences, though never directly observable for any one person, give our best data-driven forecast of individual treatment effects. -->\n\n---\n\n### 4 Building Honest Trees: Avoiding Over-Fitting  \n\nSample splitting meanings partitioning your data into training and testing sets. This avoids overfittign the model to observations (remember we seek to estimate parameters for an entire population under two different exposures, at most, only one of which is observed on any individual.) Sample splitting is a feature of estimation in cauasal forests -- we separate model selection from estimation. Moreover, the forest adds a second safeguard: **out-of-bag (OOB) prediction**. Each $\\widehat{\\tau}(x_i)$ is averaged only over trees that never used $i$ in their split phase.  Together, honesty and OOB prediction deliver reliable uncertainty estimates even in high-dimensional settings (i.e. settings with *many* covariates.)\n\n---\n\n### 5 Handling missing data  \n\nThe `grf` package adopts **Missing Incorporated in Attributes (MIA)** splitting.  'Missing' can itself become a branch, so cases are neither discarded nor randomly imputed.  This pragmatic approach keeps all observations in play while preserving the forest’s interpretability. \n\n---\n\n### 6 Is the heterogeneity *actionable*? — RATE statistics  \n\nOnce we have a personalised score $\\widehat{\\tau}(x)$ for every unit, the practical question is whether *targeting* high scorers delivers a benefit large enough to justify the extra effort.  The tool of choice is the **Targeting-Operator Characteristic (TOC)** curve:\n\n$$\nG(q)=\\frac{1}{n}\\sum_{i=1}^{\\lfloor qn\\rfloor}\\widehat{\\tau}_{(i)}, \\qquad 0\\le q\\le1,\n$$\n\nwhere $\\widehat{\\tau}_{(1)}\\ge\\widehat{\\tau}_{(2)}\\ge\\cdots$ are the estimated effects sorted from largest to smallest.  The horizontal axis $q$ is the fraction of the population we would treat; the vertical axis $G(q)$ is the cumulative gain we expect from treating that top slice.\n\nTwo integrals of the TOC curve summarise how lucrative targeting could be:\n\n* **RATE AUTOC** (Area *Under* the TOC) puts equal weight on every $q$.  This answers: *If benefits are concentrated among the very best prospects, how much can we harvest by cherry-picking them?*  \n\n* **RATE Qini** applies heavier weight to the mid-range of $q$.  This is the go-to metric when investigators face a fixed, moderate-sized budget—say, \"we can afford to treat 40 % of individuals; will targeting help?\"  [@yadlowsky2021evaluating]. We will evaluate the curve at treatment of 20% and 50% of the population.\n\n\nTo quantify the economic or policy value of heterogeneity, rank units by $\\widehat{\\tau}(x)$ and draw a **Targeting-Operator Characteristic (TOC)** curve that plots cumulative gain against the fraction $q$ of the population treated.  \n\n---\n\n### 7 RATE AUTOC EXAMPLE\n\nAlthough OOB predictions are 'out-of-sample' for individual trees, the full forest still reuses information.  A simple remedy when estimating the RATE AUTOC and Qini is to split the data, **training** the forest on one fold and **testing** RATE/Qini on the other.  Again, this explicit splitting blocks optimistic bias and yields honest test statistics (such as confidence intervals) [@grf2024].\n\n\n::: {.cell .column-screen}\n![RATE AUTOC: Hours Socialising → Sense of Meaning](08-content_files/figure-html/fig-rate-example-1.png){#fig-rate-example width=672}\n:::\n\n\n@fig-rate-example depicts a typical RATE AUTOC curve with sample splitting.  A steep initial rise indicates that a small, correctly targeted programme could deliver large gains. Note that the curve begins dipping below zero past about 30% of the sample. At that point we might be doing worse than the ATE by targeting the CATE -- at least for some.\n\n**Remember -- figuring out who will benefit from a treatment is a difficult statistical problem [@grf2024].**\n\n---\n\n### 8 Visualising policy value: the Qini curve  \n\nA **Qini curve** displays cumulative benefit on the vertical axis and treatment coverage (% of the population treated) on the horizontal.  As with the AUTOC curve we are using a held-out test fold to validate the response curve.\n\n\n::: {.cell .column-screen}\n![Qini Curve: Hours Socialising → Social Belonging](08-content_files/figure-html/fig-qini-example-1.png){#fig-qini-example width=672}\n:::\n\n\n\n\n@fig-qini-example: we find that focussing on the top 20 % of individuals nets a gain of 0.08 units (95 % CI 0.04–0.12).  Widening the net to 50 % bumps the haul to 0.13 units (95 % CI 0.07–0.19).  After that the curve flattens -- once we’ve treated everyone who offers a decent return, there are no more 'big fish' left to catch.\n\n---\n\n\n### 9 From 'a black box' to simple rules: policy trees  \n\nThe causal forest hands us a personalised CATE for every individual, mapping a **high-dimensional** covariate vector $X$ to a number $\\widehat{\\tau}(X)$.  Helpful as that forecast may be, it stops short of telling us *what to do*: the function itself is too tangled --- thousands of overlapping splits -- to translate directly into a policy.  \n\nThe **policytree** algorithm bridges that gap by collapsing the forest's many $\\widehat{\\tau}(X)$ values into a single, shallow decision tree whose depth you choose; each split is chosen to maximise expected benefit [@policytree_package_2024].  In this course we cap the depth at **two** for a practical balance, specifially:\n\n- At most three yes/no questions per rule, so the logic fits on a slide you can present to policy-makers\n- Each leaf still contains enough observations to yield a stable effect estimate;  \n- Deeper trees increase computational complexity faster than they improve payoffs.\n\n\n::: {.cell .column-screen}\n![Decision tree for Social Belonging](08-content_files/figure-html/fig-decision-tree-1.png){#fig-decision-tree width=672}\n:::\n\n\n \n**Policy Tree Findings for Effect of Hour Socialising on Social Belonging:**\n\nParticipants are first split by Self Esteem at -0.925 (original scale: 3.958). For those with Self Esteem $<=$ this threshold, the next split is by Neuroticism at 0.642 (original scale: 4.228). Within that subgroup, individuals with Neuroticism $<=$ the threshold are recommended **control**, while those with Neuroticism $>$ the threshold are recommended **treated**.\n\nFor participants with Self Esteem $>$ -0.925 (original scale: 3.958), the second split is by Social Belonging at 0.776 (original scale: 5.972). In this subgroup, individuals with Social Belonging $<=$ the threshold are recommended **treated**, while those with Social Belonging $>$ the threshold are recommended **control**.\n\n\n![Predicted treatment assignment (predictions out of training sample)](08-content_files/figure-html/fig-policy-map-1.png){#fig-policy-map width=1536}\n\n\n\n---\n\n### 10 Ethical and practical considerations  \n\nThere is no guarantee that statistical optimality will line up with social optimality. A rule that maximises expected health gains might still be **unaffordable** for a public agency, **unfair** to a protected group, or **opaque** to those asked to trust it.  We all have our notions of fairness, and we can't be expected to ignore them. Moreover, the estimation of CATE is always senstive to which variables we include in our model (see the caveats in [Lecture 6](https://go-bayes.github.io/psych-434-2025/content/06-content.html#appendix-b-evidence-for-effect-modification-is-relative-to-inclusion-of-other-variables-in-the-model)).  \n\nSo, we should not consider CATE an absolute guide to practice. We should be cautious. \n\nYet the very same CATE machinery that powers targeting also helps science move past a *one-size-fits-all* mindset.  By mapping treatment effects across a high-dimensional covariate space, we can test whether our favourite categories -- gender, age group, clinical severity -- actually capture the differences that matter.  Sometimes they do; often they don't, revealing that nature is not carved at the joints of our folk classifications.  Discovering *where* the forest finds meaningful splits can generate fresh psychological hypotheses about who responds, why, and under what circumstances, even when no policy decision is on the table. Over the next several weeks, we shall return to this point with examples.\n\n\n<!-- Before deployment we therefore need three extra layers of scrutiny: -->\n\n<!-- 1. **Cost realism**   Will the programme's administrative and opportunity costs erase the forecast benefit?  A cost–effectiveness analysis can reveal when a simpler, less 'optimal' rule actually delivers better value.   -->\n<!-- 2. **Fairness auditing**  check whether error rates or treatment probabilities differ by gender, ethnicity, or socioeconomic status.  If gaps appear, consider adjusting the loss function or adding fairness constraints [@mitchell2021algorithmic].   -->\n<!-- 3. **Transparency and consent**   Publish the rule, document data sources, and secure stakeholder buy-in.  Transparent governance limits the risk of political backlash and encourages external replication. -->\n\n---\n\n### Summary/next steps  \n\nOur workflow answers three questions in sequence:\n\n1. **Is there substantial heterogeneity?**  Reject $H_0{:}\\tau(x)$ constant if RATE AUTOC or RATE Qini is positive and statistically reliable  \n2. **Does targeting pay at realistic budgets?**  Inspect the slope of the Qini curve around plausible coverage levels.\n3. **Can we express the targeting rule in a few defensible steps?**  fit and validate a shallow policy tree.\n\nIn the lab section you will reproduce each stage on a simulated dataset.\n\n---\n\n\n# PART 1 Laboratory: Data Preparation and Analysis Scripts\n\n\n### Link to data dictionary\n\n::: {.callout-note}\nFor information about the variables in the synthetic data, download the New Zealand Attitudes and Values Data Dictionary here under \"Primary Resources\"\n\n[https://osf.io/75snb/](https://osf.io/75snb/)\n\n:::\n\n## Script 0: Synthetic Data Fetch\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for students: reproducibility is like following a recipe; each step ensures the same result\n# restart fresh session if needed\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# essential library ---------------------------------------------------------\n# install and load 'margot' from GitHub if missing\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") < \"1.0.37\") {\n  stop(\"please install margot >= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n# load packages ----------------------------------------------------------\n# install and load other packages from CRAN if missing\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\nif (!requireNamespace(\"qs\", quietly = TRUE)) {\n  install.packages(\"qs\")\n}\nlibrary(qs)\n\nif (!requireNamespace(\"here\", quietly = TRUE)) {\n  install.packages(\"here\")\n}\nlibrary(here)\n\n\n\n# create data directory if it doesn't exist -----------------------------\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\n# define file paths ------------------------------------------------------\n# use here() to build paths relative to your project root\ndata_dir <- here::here(\"data\")\n\n# download synthetic data ------------------------------------------------\n# specify the url for the data file\nurl <- \"https://www.dropbox.com/scl/fi/ru0ecayju04ja8ky1mhel/df_nz_long.qs?rlkey=prpk9a5v4vcg1ilhkgf357dhd&dl=1\"\n\n# download to a temporary file for safety\ntmp_file <- tempfile(fileext = \".qs\")\ndownload.file(url, tmp_file, mode = \"wb\")\n\n# read the data into R using qread\ndf_nz_long <- qread(tmp_file)\n\n# inspect the data -------------------------------------------------------\n# view the first few rows to check it loaded correctly\nprint(head(df_nz_long))\n\n# list column names so you know what variables are available\nprint(colnames(df_nz_long))\n\n# save a copy of the data ------------------------------------------------\n# save the dataset to your data directory for future use\nhere_save_qs(df_nz_long, \"df_nz_long\", data_dir)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n```\n:::\n\n\n\n\n\n\n## Script 1: Initial Data Wrangling\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# script 1 workflow lecture 10\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# essential library ---------------------------------------------------------\n# install and load 'margot' from GitHub if missing\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") < \"1.0.37\") {\n  stop(\"please install margot >= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf,             # machine learning forests\n  kableExtra,      # tables\n  ggplot2,         # graphs\n  doParallel,       # parallel processing\n  grf,             # causal forests\n  janitor,          # variables names\n  stringr,          # variable names\n  patchwork,        # graphs\n  table1           # tables\n)\n\n\n# create directories --------------------------------------------------------\n# create data directory if it doesn't exist\nif (!dir.exists(\"data\")) {\n  dir.create(\"data\")  # first time only: make a folder named 'data'\n}\n\nif (!dir.exists(\"save_directory\")) {\n  dir.create(\"save_directory\")  # first time only: make a folder named 'data'\n}\n\n# set up data directory structure\ndata_dir    <- here::here(\"data\")\npush_mods <- here::here(\"save_directory\") \n\n# load data -----------------------------------------------------------------\ndf_nz_long <- margot::here_read_qs(\"df_nz_long\", data_dir)\n\n# initial data prep ---------------------------------------------------------\n# prepare intial data\n# define labels for rural classification\nrural_labels <- c(\n  \"High Urban Accessibility\", \n  \"Medium Urban Accessibility\",\n  \"Low Urban Accessibility\", \n  \"Remote\", \n  \"Very Remote\"\n)\n\ndat_prep <- df_nz_long |>\n  arrange(id, wave) |>\n  margot::remove_numeric_attributes() |>\n  mutate(\n    # cap extreme values\n    alcohol_intensity = pmin(alcohol_intensity, 15),\n    # flag heavy drinkers: freq ≥3 → 1, ≤2 → 0, else NA\n    heavy_drinker = case_when(\n      alcohol_frequency >= 3 ~ 1,\n      alcohol_frequency <= 2 ~ 0,\n      TRUE                  ~ NA_real_\n    ),\n    # map freq categories to weekly counts\n    alcohol_frequency_weekly = recode(\n      alcohol_frequency,\n      `0` = 0, `1` = 0.25,\n      `2` = 1, `3` = 2.5,\n      `4` = 4.5,\n      .default = NA_real_\n    ),\n    # relabel rural factor\n    rural_gch_2018_l = factor(\n      rural_gch_2018_l,\n      levels = 1:5,\n      labels = rural_labels,\n      ordered = TRUE\n    )\n  ) |>\n  droplevels()\n\n\n\n# view variable names -----------------------------------------------------\nprint(colnames(df_nz_long)) \n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# define study variables ----------------------------------------------------\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n\n# ** key decision 1: define your exposure variable **\nname_exposure <- \"extraversion\"\n\n# exposure variable labels\nvar_labels_exposure <- list(\n  \"extraversion\" = \"Extraversion\",\n  \"extraversion_binary\" = \"Extraversion (binary)\"\n)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n\n# +--------------------------+\n# | OPTIONALLY MODIFY SECTION|\n# +--------------------------+\n\n# **  define your study waves **\nbaseline_wave      <- \"2018\"        # baseline measurement\nexposure_waves     <- c(\"2019\")     # when exposure is measured\noutcome_wave       <- \"2020\"        # when outcomes are measured\nall_waves          <- c(baseline_wave, exposure_waves, outcome_wave)\n\n# **  define baseline covariates **\n# these are demographics, traits, etc. measured at baseline\n\nbaseline_vars <- c(\n  # demographics\n  \"age\", \"born_nz_binary\", \"education_level_coarsen\",\n  \"employed_binary\", \"eth_cat\", \"male_binary\",\n  \"not_heterosexual_binary\", \"parent_binary\", \"partner_binary\",\n  \"rural_gch_2018_l\", \"sample_frame_opt_in_binary\",\n  \n  # personality traits (excluding exposure)\n  \"agreeableness\", \"conscientiousness\", \"neuroticism\", \"openness\",\n  \n  # health and lifestyle\n  \"alcohol_frequency\", \"alcohol_intensity\", \"hlth_disability_binary\",\n  \"log_hours_children\", \"log_hours_commute\", \"log_hours_exercise\",\n  \"log_hours_housework\", \"log_household_inc\",\n  \"short_form_health\", \"smoker_binary\",\n  \n  # social and psychological\n  \"belong\", \"nz_dep2018\", \"nzsei_13_l\",\n  \"political_conservative\", \"religion_identification_level\"\n)\n\n# +--------------------------+\n# |END OPTIONALLY MODIFY SEC.|\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n\n# ** key decision 3: define outcome variables **\n# here, we are focussing on a subset of wellbeing outcomes\n# chose outcomes relevant to * your * study. Might be all/some/none/exactly \n# these:\noutcome_vars <- c(\n  # health outcomes\n  # \"alcohol_frequency_weekly\", \"alcohol_intensity\",\n  # \"hlth_bmi\", \n  \"log_hours_exercise\", \n  # \"hlth_sleep_hours\", \n  # \"short_form_health\",\n  \n  # psychological outcomes\n  # \"hlth_fatigue\", \n  \"kessler_latent_anxiety\", \n  \"kessler_latent_depression\", \n  \"rumination\",\n  \n  # well-being outcomes\n  # \"bodysat\", \n  #\"forgiveness\", \"gratitude\", \n  \"lifesat\", \"meaning_purpose\", \"meaning_sense\", \n  # \"perfectionism\", \n  \"pwi\", \n  #\"self_control\", \n  \"self_esteem\", \n  #\"sexual_satisfaction\",\n  \n  # social outcomes\n  \"belong\", \"neighbourhood_community\", \"support\"\n)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# after selecting your exposure/ baseline / outcome variables do not modify this\n# code\n\n# make binary variable (UNLESS YOUR EXPOSURE IS A BINARY VARIABLE)\nexposure_var_binary = paste0(name_exposure, \"_binary\")\n\n# make exposure variable list (we will keep both the continuous and binary variable)\nexposure_var  <- c(name_exposure, paste0(name_exposure, \"_binary\"))\n\n# sort for easier reference\nbaseline_vars <- sort(baseline_vars)\noutcome_vars <- sort(outcome_vars)\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# save key variables --------------------------------------------------------\nmargot::here_save(name_exposure, \"name_exposure\")\nmargot::here_save(var_labels_exposure,\"var_labels_exposure\")\nmargot::here_save(baseline_vars,\"baseline_vars\")\nmargot::here_save(exposure_var, \"exposure_var\")\nmargot::here_save(exposure_var_binary, \"exposure_var_binary\")\nmargot::here_save(outcome_vars, \"outcome_vars\")\nmargot::here_save(baseline_wave, \"baseline_wave\")\nmargot::here_save(exposure_waves, \"exposure_waves\")\nmargot::here_save(outcome_wave, \"outcome_wave\")\nmargot::here_save(all_waves,\"all_waves\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# | OPTIONALLY MODIFY SECTION|\n# +--------------------------+\n\n# select eligible participants ----------------------------------------------\n# only include participants who have exposure data at baseline\n\n# You might require tighter conditions \n# for example, if you are interested in the effects of hours of childcare, \n# you might want to select only those who were parents at baseline. \n# talk to me if you think you might night tighter eligibility criteria.\n\nids_baseline <- dat_prep |> \n  # allow missing exposure at baseline\n  filter(wave == baseline_wave, !is.na(!!sym(name_exposure))) |> \n  # option: do not allow missing exposure at baseline\n  filter(wave == baseline_wave, !is.na(!!sym(name_exposure))) |> \n  pull(id)\n\n# filter data to include only eligible participants and relevant waves\ndat_long_1 <- dat_prep |> \n  filter(id %in% ids_baseline, wave %in% all_waves) |> \n  droplevels()\n\n# +--------------------------+\n# |END OPTIONALLY MODIFY SEC.|\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n# plot distribution to help with cutpoint decision\n# get exposure wave to inspect exposure variable distribution\ndat_long_exposure <- dat_long_1 |> filter(wave %in% exposure_waves)\n\n# make graph \ngraph_cut <- margot::margot_plot_categorical(\n  dat_long_exposure,\n  col_name         = name_exposure,\n  sd_multipliers = c(-1, 1), # select to suit\n  # either use n_divisions for equal-sized groups:\n  # n_divisions      = 2,\n  # or use custom_breaks for specific values:\n  custom_breaks    = c(1, 4),  # ** adjust as needed **\n  cutpoint_inclusive = \"upper\",\n  show_mean        = TRUE,\n  show_sd          = TRUE\n)\nprint(graph_cut)\n\n# save your graph\nmargot::here_save(graph_cut, \"graph_cut\", push_mods)\n\n# create binary exposure variable based on chosen cutpoint\ndat_long_2 <- margot::create_ordered_variable(\n  dat_long_1,\n  var_name           = name_exposure,\n  custom_breaks      = c(1, 4),  # ** -- adjust based on your decision above -- **\n  cutpoint_inclusive = \"upper\"\n)\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# process binary variables and log-transform --------------------------------\n# convert binary factors to 0/1 format\ndat_long_3 <- margot::margot_process_binary_vars(dat_long_2)\n\n# log-transform hours and income variables: tables for analysis (only logged versions of vars)\ndat_long_final <- margot::margot_log_transform_vars(\n  dat_long_3,\n  vars            = c(starts_with(\"hours_\"), \"household_inc\"),\n  prefix          = \"log_\",\n  keep_original   = FALSE,\n  exceptions = exposure_var # omit original variables\n) |> \n  # select only variables needed for analysis\n  select(all_of(c(baseline_vars, exposure_var, outcome_vars, \"id\", \"wave\", \"year_measured\", \"sample_weights\"))) |> \n  droplevels()\n\n\n# check missing data --------------------------------------------------------\n# this is crucial to understand potential biases\nmissing_summary <- naniar::miss_var_summary(dat_long_final)\nprint(missing_summary)\nmargot::here_save(missing_summary, \"missing_summary\", push_mods)\n\n# visualise missing data pattern\n# ** -- takes a while to render ** \nvis_miss <- naniar::vis_miss(dat_long_final, warn_large_data = FALSE)\nprint(vis_miss)\nmargot::here_save(vis_miss, \"vis_miss\", push_mods)\n\n# calculate percentage of missing data at baseline\ndat_baseline_pct <- dat_long_final |> filter(wave == baseline_wave)\npercent_missing_baseline <- naniar::pct_miss(dat_baseline_pct)\nmargot::here_save(percent_missing_baseline, \"percent_missing_baseline\", push_mods)\n\n# save prepared dataset for next stage --------------------------------------\nmargot::here_save(dat_long_final, \"dat_long_final\", push_mods)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n# create transition matrices to check positivity ----------------------------\n# this helps assess whether there are sufficient observations in all exposure states\ndt_positivity <- dat_long_final |>\n  filter(wave %in% c(baseline_wave, exposure_waves)) |>\n  select(!!sym(name_exposure), id, wave) |>\n  mutate(exposure = round(as.numeric(!!sym(name_exposure)), 0)) |>\n  # create binary exposure based on cutpoint\n  mutate(exposure_binary = ifelse(exposure >= 4, 1, 0)) |> ## *-- modify this --* \n  mutate(wave = as.numeric(wave) -1 )\n\n# create transition tables\ntransition_tables <- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"wave\",\n  table_name = \"transition_table\"\n)\nprint(transition_tables$tables[[1]])\nmargot::here_save(transition_tables, \"transition_tables\", push_mods)\n\n# create binary transition tables\ntransition_tables_binary <- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure_binary\",\n  id_var = \"id\",\n  waves = c(0, 1),\n  wave_var = \"wave\",\n  table_name = \"transition_table_binary\"\n)\nprint(transition_tables_binary$tables[[1]])\nmargot::here_save(transition_tables_binary, \"transition_tables_binary\", push_mods)\n\n# create tables -----------------------------------------------------------\n# baseline variable labels\nvar_labels_baseline <- list(\n  # demographics\n  \"age\" = \"Age\",\n  \"born_nz_binary\" = \"Born in NZ\",\n  \"education_level_coarsen\" = \"Education Level\",\n  \"employed_binary\" = \"Employed\",\n  \"eth_cat\" = \"Ethnicity\",\n  \"male_binary\" = \"Male\",\n  \"not_heterosexual_binary\" = \"Non-heterosexual\",\n  \"parent_binary\" = \"Parent\",\n  \"partner_binary\" = \"Has Partner\",\n  \"rural_gch_2018_l\" = \"Rural Classification\",\n  \"sample_frame_opt_in_binary\" = \"Sample Frame Opt-In\",\n  \n  # economic & social status\n  \"household_inc\" = \"Household Income\",\n  \"log_household_inc\" = \"Log Household Income\",\n  \"nz_dep2018\" = \"NZ Deprivation Index\",\n  \"nzsei_13_l\" = \"Occupational Prestige Index\",\n  \"household_inc\" = \"Household Income\",\n\n  \n  # personality traits\n  \"agreeableness\" = \"Agreeableness\",\n  \"conscientiousness\" = \"Conscientiousness\",\n  \"neuroticism\" = \"Neuroticism\",\n  \"openness\" = \"Openness\",\n  \n  # beliefs & attitudes\n  \"political_conservative\" = \"Political Conservatism\",\n  \"religion_identification_level\" = \"Religious Identification\",\n  \n  # health behaviors\n  \"alcohol_frequency\" = \"Alcohol Frequency\",\n  \"alcohol_intensity\" = \"Alcohol Intensity\",\n  \"hlth_disability_binary\" = \"Disability Status\",\n  \"smoker_binary\" = \"Smoker\",\n  \"hours_exercise\" = \"Hours of Exercise\",\n  \n  \n  # time use\n  \"hours_children\" = \"Hours with Children\",\n  \"hours_commute\" = \"Hours Commuting\",\n  \"hours_exercise\" = \"Hours Exercising\",\n  \"hours_housework\" = \"Hours on Housework\",\n  \"log_hours_children\" = \"Log Hours with Children\",\n  \"log_hours_commute\" = \"Log Hours Commuting\",\n  \"log_hours_exercise\" = \"Log Hours Exercising\",\n  \"log_hours_housework\" = \"Log Hours on Housework\"\n)\nhere_save(var_labels_baseline, \"var_labels_baseline\")\n\n# outcome variable labels, organized by domain\n# reivew your outcomes make sure they appear on the list below\n# comment out what you do not need\noutcome_vars\n\n# get names\nvar_labels_outcomes <- list(\n  # \"alcohol_frequency_weekly\" = \"Alcohol Frequency (weekly)\",\n  # \"alcohol_intensity\" = \"Alcohol Intensity\",\n  # \"hlth_bmi\" = \"Body Mass Index\",\n  # \"hlth_sleep_hours\" = \"Sleep\",\n  \"log_hours_exercise\" = \"Hours of Exercise (log)\",\n # \"short_form_health\" = \"Short Form Health\",\n  \"hlth_fatigue\" = \"Fatigue\",\n  \"kessler_latent_anxiety\" = \"Anxiety\",\n  \"kessler_latent_depression\" = \"Depression\",\n # \"rumination\" = \"Rumination\",\n  \"bodysat\" = \"Body Satisfaction\",\n # \"forgiveness\" = \"Forgiveness\",\n # \"perfectionism\" = \"Perfectionism\",\n # \"self_control\" = \"Self Control\",\n  \"self_esteem\" = \"Self Esteem\",\n  \"sexual_satisfaction\" = \"Sexual Satisfaction\",\n # \"gratitude\" = \"Gratitude\",\n  \"lifesat\" = \"Life Satisfaction\",\n  \"meaning_purpose\" = \"Meaning: Purpose\",\n  \"meaning_sense\" = \"Meaning: Sense\",\n  \"pwi = Personal Well-being Index\",\n  \"belong\" = \"Social Belonging\",\n  \"neighbourhood_community\" = \"Neighbourhood Community\",\n  \"support\" = \"Social Support\"\n)\n\n# save for manuscript\nhere_save(var_labels_outcomes, \"var_labels_outcomes\")\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n# tables ------------------------------------------------------------------\n# create baseline characteristics table\ndat_baseline = dat_long_final |>\n  filter(wave %in% c(baseline_wave)) |>\n  mutate(\n    male_binary = factor(male_binary),\n    parent_binary = factor(parent_binary),\n    smoker_binary = factor(smoker_binary),\n    born_nz_binary = factor(born_nz_binary),\n    employed_binary = factor(employed_binary),\n    not_heterosexual_binary = factor(not_heterosexual_binary),\n    sample_frame_opt_in_binary = factor(sample_frame_opt_in_binary)\n  )\n\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n\n# save sample weights from baseline wave\n# save sample weights\nt0_sample_weights <- dat_baseline$sample_weights\nhere_save(t0_sample_weights, \"t0_sample_weights\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# make baseline table -----------------------------------------------------\n\nbaseline_table <- margot::margot_make_tables(\n  data = dat_baseline,\n  vars = baseline_vars,\n  by = \"wave\",\n  labels = var_labels_baseline,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(baseline_table)\nmargot::here_save(baseline_table, \"baseline_table\", push_mods)\n\n# create exposure table by wave\nexposure_table <- margot::margot_make_tables(\n  data = dat_long_final |> filter(wave %in% c(baseline_wave, exposure_waves)),\n  vars = exposure_var,\n  by = \"wave\",\n  labels = var_labels_exposure,\n  factor_vars = exposure_var_binary,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\nprint(exposure_table)\nmargot::here_save(exposure_table, \"exposure_table\", push_mods)\n\n# create outcomes table by wave\noutcomes_table <- margot::margot_make_tables(\n  data = dat_long_final |> filter(wave %in% c(baseline_wave, outcome_wave)),\n  vars = outcome_vars,\n  by = \"wave\",\n  labels = var_labels_outcomes,\n  format = \"markdown\"\n)\nprint(outcomes_table)\nmargot::here_save(outcomes_table, \"outcomes_table\", push_mods)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n\n\n# note: completed data preparation step -------------------------------------\n# you're now ready for the next steps:\n# 1. creating wide-format dataset for analysis \n# 2. applying causal inference methods\n# 3. conducting sensitivity analyses\n\n# key decisions summary:\n# exposure variable: extraversion\n# study waves: baseline (2018), exposure (2019), outcome (2020)\n# baseline covariates: demographics, traits, health measures (excluding exposure)\n# outcomes: health, psychological, wellbeing, and social variables\n# binary cutpoint for exposure: here, 4 on the extraversion scale\n# label names for tables\n\n\n\n\n# THIS IS FOR INTEREST ONLY ----------------------------------------------------\n# uncomment to view random chang in individuals\n# visualise individual changes in exposure over time ------------------------\n# useful for understanding exposure dynamics\n# individual_plot <- margot_plot_individual_responses(\n#   dat_long_1,\n#   y_vars = name_exposure,\n#   id_col = \"id\",\n#   waves = c(2018:2019),\n#   random_draws = 56,  # number of randomly selected individuals to show\n#   theme = theme_classic(),\n#   scale_range = c(1, 7),  # range of the exposure variable\n#   full_response_scale = TRUE,\n#   seed = 123\n# )\n# print(individual_plot)\n```\n:::\n\n\n\n\n\n## Script 2: Make Wide Data Format With Censoring Weights\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# script 2: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session for a clean workspace\nrstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# libraries ---------------------------------------------------------------\n# essential library ---------------------------------------------------------\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n}\n\n\nif (packageVersion(\"margot\") < \"1.0.37\") {\n  stop(\"please install margot >= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\nlibrary(margot)\n\n# load packages -------------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf,             # machine learning forests\n  kableExtra,      # tables\n  ggplot2,         # graphs\n  doParallel,      # parallel processing\n  grf,             # causal forests\n  janitor,         # variables names\n  stringr,         # variable names\n  patchwork,       # graphs\n  table1           # tables\n)\n\n# save paths -------------------------------------------------------------------\npush_mods <- here::here(\"save_directory\") \n\n# read data\ndat_long_final <- margot::here_read(\"dat_long_final\")\n\n# read baseline sample weights\nt0_sample_weights <- margot::here_read(\"t0_sample_weights\")\n\n# read exposure\nname_exposure <- margot::here_read(\"name_exposure\")\nname_exposure_binary = paste0(name_exposure, \"_binary\")\nname_exposure_continuous = name_exposure\n\n# read variables\nbaseline_vars <- margot::here_read(\"baseline_vars\")\nexposure_var <- margot::here_read(\"exposure_var\")\noutcome_vars <- margot::here_read(\"outcome_vars\")\nbaseline_wave <- margot::here_read(\"baseline_wave\")\nexposure_waves <- margot::here_read(\"exposure_waves\")\noutcome_wave <- margot::here_read(\"outcome_wave\")\n\n# define continuous columns to keep\ncontinuous_columns_keep <- c(\"t0_sample_weights\")\n\n# define ordinal columns that we will expand into binary variables\nordinal_columns <- c(\"t0_education_level_coarsen\",\n                     \"t0_eth_cat\",\n                     \"t0_rural_gch_2018_l\")\n\n# check is this the exposure variable that you want? \nname_exposure_binary\nname_exposure_continuous\n\n# define wide variable names\nt0_name_exposure_binary <- paste0(\"t0_\", name_exposure_binary)\nt0_name_exposure_binary\n\n# make exposure names (continuous not genreally used)\nt1_name_exposure_binary <- paste0(\"t1_\", name_exposure_binary)\nt1_name_exposure_binary\n\n# treatments (continuous verion)\nt0_name_exposure <- paste0(\"t0_\", name_exposure_continuous)\nt1_name_exposure <- paste0(\"t1_\", name_exposure_continuous)\nt0_name_exposure_continuous <- paste0(\"t0_\", name_exposure)\nt1_name_exposure_continuous <- paste0(\"t1_\", name_exposure)\n\n# raw outcomes\n# read health outcomes\noutcome_vars <- here_read(\"outcome_vars\")\nt2_outcome_z <- paste0(\"t2_\", outcome_vars, \"_z\")\n\n# view\nt2_outcome_z\n\n# check\nstr(dat_long_final)\n\n# check\nnaniar::gg_miss_var(dat_long_final)\n\n# impute data --------------------------------------------------------------\n# ordinal use\nordinal_columns <- c(\n  \"t0_education_level_coarsen\",\n  \"t0_eth_cat\",\n  \"t0_rural_gch_2018_l\",\n  \"t0_gen_cohort\"\n)\n\n# define cols we will not standardise\ncontinuous_columns_keep <- c(\"t0_sample_weights\")\n\n# remove sample weights\ndat_long_final_2 <- dat_long_final |> select(-sample_weights)\n\n# prepare data for analysis ----------------------\ndat_long_final_2 <- margot::remove_numeric_attributes(dat_long_final_2)\n# wide data\ndf_wide <- margot_wide_machine(\n  dat_long_final,\n  id = \"id\",\n  wave = \"wave\",\n  baseline_vars,\n  exposure_var = exposure_var,\n  outcome_vars,\n  confounder_vars = NULL,\n  imputation_method = \"none\",\n  include_exposure_var_baseline = TRUE,\n  include_outcome_vars_baseline = TRUE,\n  extend_baseline = FALSE,\n  include_na_indicators = FALSE\n)\n\n# check\ncolnames(df_wide)\n\n# return sample weights\ndf_wide$t0_sample_weights <-  t0_sample_weights\n\n# save\nmargot::here_save(df_wide, \"df_wide\")\n\n\n#df_wide <- margot::here_read(\"df_wide\")\nnaniar::vis_miss(df_wide, warn_large_data = FALSE)\n\n\n# order data with missingness assigned to work with grf and lmtp\n# if any outcome is censored all are censored\n# create version for model reports\n\n# check\ncolnames(df_wide)\n\n\n# made data wide in correct format\n# ** ignore warning *** \ndf_wide_encoded  <- margot::margot_process_longitudinal_data_wider(\n  df_wide,\n  ordinal_columns = ordinal_columns,\n  continuous_columns_keep = continuous_columns_keep,\n  not_lost_in_following_wave = \"not_lost_following_wave\",\n  lost_in_following_wave = \"lost_following_wave\",\n  remove_selected_columns = TRUE,\n  exposure_var = exposure_var,\n  scale_continuous = TRUE,\n  censored_if_any_lost = FALSE\n)\n\n# check\ncolnames(df_wide_encoded)\n\n# check\ntable(df_wide_encoded$t0_not_lost_following_wave)\n\n# make the binary variable numeric\ndf_wide_encoded[[t0_name_exposure_binary]] <-\n  as.numeric(df_wide_encoded[[t0_name_exposure_binary]]) - 1\ndf_wide_encoded[[t1_name_exposure_binary]] <-\n  as.numeric(df_wide_encoded[[t1_name_exposure_binary]]) - 1\n\n# view\ndf_wide_encoded[[t0_name_exposure_binary]]\ndf_wide_encoded[[t1_name_exposure_binary]]\n\n# 1. ensure both binaries only take values 0 or 1 (ignore NA)\nstopifnot(all(df_wide_encoded[[t0_name_exposure_binary]][!is.na(df_wide_encoded[[t0_name_exposure_binary]])] %in% 0:1),\n          all(df_wide_encoded[[t1_name_exposure_binary]][!is.na(df_wide_encoded[[t1_name_exposure_binary]])] %in% 0:1))\n\n# 2. ensure NA‐patterns match between t1_exposure and t0_lost flag\n# count n-as in t1 exposure\nn_na_t1 <- sum(is.na(df_wide_encoded[[t1_name_exposure_binary]]))\n\n# count how many were lost at t0\nn_lost_t0 <- sum(df_wide_encoded$t0_lost_following_wave == 1, na.rm = TRUE)\n\n# print them for inspection\nmessage(\"NAs in \", t1_name_exposure_binary, \": \", n_na_t1)\nmessage(\"t0_lost_following_wave == 1: \", n_lost_t0)\n\n# stop if they don’t match\nstopifnot(n_na_t1 == n_lost_t0)\n\n# 3. ensure if t1 is non‐NA then subject was not lost at t0\nstopifnot(all(is.na(df_wide_encoded[[t1_name_exposure_binary]]) |\n                df_wide_encoded[[\"t0_not_lost_following_wave\"]] == 1))\n\n# view\nhead(df_wide_encoded)\n\n#naniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\nnaniar::gg_miss_var(df_wide_encoded)\n\n\n# predict attrition and create censoring weights --------------------------\n# step 1: prepare baseline covariates\n# select all t0_ variables except the exposure binary and any _lost indicators, then sort their names\nt0_var_names <- df_wide_encoded |>\n  select(-all_of(t0_name_exposure_binary)) |>\n  select(starts_with(\"t0_\"),-ends_with(\"_lost\"),-ends_with(\"lost_following_wave\"), -ends_with(\"_weights\")) |>\n  colnames() |>\n  sort()\n\n# get unique values (to be safe)\nE <- unique(t0_var_names)\n\n# view\nprint(E)\n\n# save baseline covariates\nmargot::here_save(E, \"E\")\n\n# view\nprint(E)\n\n# step 2: calculate weights for t0\nD_0 <- as.factor(df_wide_encoded$t0_lost_following_wave)\n\n# get co-variates\ncen_0 <- df_wide_encoded[, E]\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# probability forest for censoring\n# this will take time\ncen_forest_0 <- probability_forest(cen_0, D_0)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# get predictions\npredictions_grf_0 <- predict(cen_forest_0, newdata = cen_0, type = \"response\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n\n# get propensity scores\npscore_0 <- predictions_grf_0$pred[, 2]\n\n# use margot_adjust_weights for t0\nt0_weights <- margot_adjust_weights(\n  pscore = pscore_0,\n  trim = TRUE,\n  normalize = TRUE,\n  # lower trimming\n  lower_percentile = 0.00,\n  # upper trimming\n  upper_percentile = 0.99,\n  censoring_indicator = df_wide_encoded$t0_lost_following_wave,\n  sample_weights = df_wide_encoded$t0_sample_weights\n)\n\n# view\nhist(t0_weights$adjusted_weights)\n\n# give weights\ndf_wide_encoded$t0_adjusted_weights <- t0_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\n\n# remove lost next wave (censored)\ndf_wide_encoded_1 <- df_wide_encoded %>%\n  filter(t0_lost_following_wave == 0) %>%\n  droplevels()\n\n# step 4: calculate weights for t1\nE_and_exposure <- c(E, t1_name_exposure_continuous)\nD_1 <- as.factor(df_wide_encoded_1$t1_lost_following_wave)\ncen_1 <- df_wide_encoded_1[, E_and_exposure]\n\n# probability forest for censoring\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\ncen_forest_1 <- probability_forest(cen_1, D_1, sample.weights = df_wide_encoded_1$t0_adjusted_weights)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# predict forest\n\npredictions_grf_1 <- predict(cen_forest_1, newdata = cen_1, type = \"response\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# get propensity score\npscore_1 <- predictions_grf_1$pred[, 2]\n\n# check\nhist(pscore_1)\n\n# use margot_adjust_weights for t1\n# we will use these weights for inference in our models\nt1_weights <- margot_adjust_weights(\n  pscore = pscore_1,\n  trim = TRUE,\n  normalize = TRUE,\n  lower_percentile = 0.00,\n  # upper trimming\n  upper_percentile = 0.99,\n  censoring_indicator = df_wide_encoded_1$t1_lost_following_wave,\n  sample_weights = df_wide_encoded_1$t0_adjusted_weights # combine with weights\n)\n\n# add weights -- these will be the weights we use\ndf_wide_encoded_1$t1_adjusted_weights <- t1_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded_1, warn_large_data = FALSE)\n\n# save\nhere_save(df_wide_encoded_1, \"df_wide_encoded_1\")\n\n# check names\ncolnames(df_wide_encoded_1)\n\n# check\ndf_wide_encoded_1[[t1_name_exposure_binary]]\n\n# step 5: prepare final dataset\nnrow(df_wide_encoded_1)\ntable(df_wide_encoded_1$t1_lost_following_wave)\n\n# arrange\ndf_grf <- df_wide_encoded_1 |>\n  filter(t1_lost_following_wave == 0) |>\n  select(\n    where(is.factor),\n    ends_with(\"_binary\"),\n    ends_with(\"_lost_following_wave\"),\n    ends_with(\"_z\"),\n    ends_with(\"_weights\"),\n    starts_with(\"t0_\"),\n    starts_with(\"t1_\"),\n    starts_with(\"t2_\"),\n  ) |>\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\")) |>\n  relocate(starts_with(\"t1_\"), .before = starts_with(\"t2_\")) |>\n  relocate(\"t0_not_lost_following_wave\", .before = starts_with(\"t1_\")) |>\n  relocate(all_of(t1_name_exposure_binary), .before = starts_with(\"t2_\")) |>\n  droplevels()\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# make sure to do this\n# save final data\nmargot::here_save(df_grf, \"df_grf\")\ndf_grf <- margot::here_read(\"df_grf\")\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# check final dataset\ncolnames(df_grf)\n\n# visualise missing\n# should have no missing in t1 and t2 variables\n# handled by IPCW\n# make final missing data graph\nmissing_final_data_plot <- naniar::vis_miss(df_grf, warn_large_data = FALSE)\nmissing_final_data_plot\n\n# save plot\nmargot_save_png(missing_final_data_plot, prefix = \"missing_final_data\")\n\n# checks\ncolnames(df_grf)\nstr(df_grf)\n\n# check exposures\ntable(df_grf[[t1_name_exposure_binary]])\n\n# check\nhist(df_grf$t1_adjusted_weights)\n\n# calculate summary statistics\nt0_weight_summary <- summary(df_wide_encoded)\n\n# check\nglimpse(df_grf$t1_adjusted_weights)\n\n# visualise weight distributions\nhist(df_grf$t1_adjusted_weights, main = \"t0_stabalised weights\", xlab = \"Weight\")\n\n# check n\nn_observed_grf <- nrow(df_grf)\n\n# view\nn_observed_grf\n\n# save\nmargot::here_save(n_observed_grf, \"n_observed_grf\")\n\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |     END                  |\n# +--------------------------+\n\n# this is just for your interest ------------------------------------------\n# not used in final manuscript\n# FOR INTEREESTS\n# inspect propensity scores -----------------------------------------------\n# get data\n# df_grf <- here_read('df_grf')\n# \n# # assign weights var name\n# weights_var_name = \"t0_adjusted_weights\"\n# \n# # baseline covariates  # E already exists and is defined\n# E\n# \n# # must be a data frame, no NA in exposure\n# \n# # df_grf is a data frame - we must process this data frame in several steps\n# # user to specify which columns are outcomes, default to 'starts_with(\"t2_\")'\n# df_propensity_org <- df_grf |> select(!starts_with(\"t2_\"))\n# \n# # Remove NAs and print message that this has been done\n# df_propensity <- df_propensity_org |> drop_na() |> droplevels()\n# \n# # E_propensity_names\n# # first run model for baseline propensity if this is selected.  The default should be to not select it.\n# propensity_model_and_plots <- margot_propensity_model_and_plots(\n#   df_propensity = df_propensity,\n#   exposure_variable = t1_name_exposure_binary,\n#   baseline_vars = E,\n#   weights_var_name = weights_var_name,\n#   estimand = \"ATE\",\n#   method = \"ebal\",\n#   focal = NULL\n# )\n# \n# # visualise\n# summary(propensity_model_and_plots$match_propensity)\n# \n# # key plot\n# propensity_model_and_plots$love_plot\n# \n# # other plots\n# propensity_model_and_plots$summary_plot\n# propensity_model_and_plots$balance_table\n# propensity_model_and_plots$diagnostics\n# \n# \n# # check size\n# size_bytes <- object.size(propensity_model_and_plots)\n# print(size_bytes, units = \"auto\") # Mb\n# \n# # use qs to save only if you have space\n# here_save_qs(propensity_model_and_plots,\n#              \"propensity_model_and_plots\",\n#              push_mods)\n```\n:::\n\n\n\n## Script 3: Models & Graphs\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# script 3: causal workflow for estimating average treatment effects using margot\n# may 2025\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n# restart fresh session\n\nrstudioapi::restartSession()\n\n\n\n# reproducibility ---------------------------------------------------------\n\n\nset.seed(123)\n\n\n# essential library ---------------------------------------------------------\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n\nif (packageVersion(\"margot\") < \"1.0.37\") {\n  stop(\"please install margot >= 1.0.37 for this workflow\\n\n       run: devtools::install_github(\\\"go-bayes/margot\\\")\n\")\n}\n\n# call library\nlibrary(\"margot\")\n\n# check package version\npackageVersion(pkg = \"margot\")\n\n\n\n# load libraries ----------------------------------------------------------\n# pacman will install missing packages automatically\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,       # data wrangling + plotting\n  qs,              # fast data i/o\n  here,            # project-relative file paths\n  data.table,      # fast data manipulation\n  fastDummies,     # dummy variable creation\n  naniar,          # missing data handling\n  skimr,           # summary statistics\n  grf, ranger,     # machine learning forests\n  doParallel,      # parallel processing,\n  kableExtra,\n  ggplot2 ,        # graphs\n  rlang ,          # functions for base types/Core R/ 'Tidyverse'\n  purrr ,          # functional programming tools.\n  patchwork,      # nice graph placement\n  janitor,         # nice labels\n  glue            # format/ interpolate a string\n)\n\n\n\n# directory path configuration -----------------------------------------------\n# save path (customise for your own computer) ----------------------------\npush_mods <- here::here(\"save_directory\") \n\n# read original data (for plots) ------------------------------------------\noriginal_df <- margot::here_read(\"df_wide\", push_mods)\n\n# plot title --------------------------------------------------------------\ntitle_binary = \"Effects of {{name_exposure}} on {{name_outcomes}}\"\nfilename_prefix = \"grf_extraversion_wb\"\n\n# for manuscript later\nmargot::here_save(title_binary,\"title_binary\")\n\n# import names ------------------------------------------------------------\nname_exposure <- margot::here_read(\"name_exposure\")\nname_exposure\n\n# make exposure names\nt1_name_exposure_binary <- paste0(\"t1_\", name_exposure, \"_binary\")\n\n# check exposure name\nt1_name_exposure_binary\n\n# read outcome vars\noutcome_vars <- margot::here_read(\"outcome_vars\")\n\n# read and sort outcome variables -----------------------------------------\n# we do this by domain: health, psych, present, life, social\nread_and_sort <- function(key) {\n  raw  <- margot::here_read(key, push_mods)\n  vars <- paste0(\"t2_\", raw, \"_z\")\n  sort(vars)\n}\nt2_outcome_z  <- read_and_sort(\"outcome_vars\")\n\n# view\nt2_outcome_z\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n# define names for titles -------------------------------------------------\n\nnice_exposure_name = \"Extraversion\"\nnice_outcome_name = \"Wellbeing\"\ntitle = \"Effect of {{nice_exposure_name}} on {{nice_outcome_name}}\"\n\n# save for final rport\nhere_save(title, \"title\")\n\n# combine outcomes ---------------------------------------------------------\n# check outcome vars and make labels for graphs/tables\noutcome_vars\n\n\nlabel_mapping_all <- list(\n  #\"t2_alcohol_frequency_weekly_z\" = \"Alcohol Frequency\",\n  #\"t2_alcohol_intensity_weekly_z\" = \"Alcohol Intensity\",\n  #\"t2_hlth_bmi_z\" = \"BMI\",\n  #\"t2_hlth_sleep_hours_z\" = \"Sleep\",\n  \"t2_log_hours_exercise_z\" = \"Hours of Exercise (log)\",\n  #\"t2_short_form_health_z\" = \"Short Form Health\"\n  \"t2_hlth_fatigue_z\" = \"Fatigue\",\n  \"t2_kessler_latent_anxiety_z\" = \"Anxiety\",\n  \"t2_kessler_latent_depression_z\" = \"Depression\",\n  \"t2_rumination_z\" = \"Rumination\",\n  # \"t2_bodysat_z\" = \"Body Satisfaction\",\n  \"t2_foregiveness_z\" = \"Forgiveness\",\n  \"t2_perfectionism_z\" = \"Perfectionism\", \n  \"t2_self_esteem_z\" = \"Self Esteem\",\n  # \"t2_self_control_z\" = \"Self Control\",\n  # \"t2_sexual_satisfaction_z\" = \"Sexual Satisfaction\".\n  \"t2_gratitude_z\" = \"Gratitude\",\n  \"t2_lifesat_z\" = \"Life Satisfaction\",\n  \"t2_meaning_purpose_z\" = \"Meaning: Purpose\",\n  \"t2_meaning_sense_z\" = \"Meaning: Sense\",\n  \"t2_pwi_z\" = \"Personal Well-being Index\",\n  \"t2_belong_z\" = \"Social Belonging\",\n  \"t2_neighbourhood_community_z\" = \"Neighbourhood Community\",\n  \"t2_support_z\" = \"Social Support\"\n)\n\n\n# save\nhere_save(label_mapping_all, \"label_mapping_all\")\n\n# check\nlabel_mapping_all\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# select options that make sense fo your study/results\n# might need to be tweaked after the analysis\n\n# make options -------------------------------------------------------------\n# titles\ntitle = \"ATE Effects of {{nice_name_exposure}} on {{nice_name_outcome}}\"\nsubtitle = \"\"\nfilename_prefix = \"final_report\"\n\n\n# settings\nx_offset = -.5\nx_lim_lo = -.5\nx_lim_hi = .5\n\n\n# defaults for ate plots\nbase_defaults_binary <- list(\n  type = \"RD\",\n  title = title_binary,\n  e_val_bound_threshold = 1.2,\n  colors = c(\n    \"positive\" = \"#E69F00\",\n    \"not reliable\" = \"grey50\",\n    \"negative\" = \"#56B4E9\"\n  ),\n  x_offset = x_offset,\n  # will be set based on type\n  x_lim_lo = x_lim_lo,\n  # will be set based on type\n  x_lim_hi = x_lim_hi,\n  text_size = 4,\n  linewidth = 0.5,\n  estimate_scale = 1,\n  base_size = 18,\n  point_size = 2,\n  title_size = 19,\n  subtitle_size = 16,\n  legend_text_size = 10,\n  legend_title_size = 10,\n  include_coefficients = FALSE\n)\n\n# health graph options\noutcomes_options_all <- margot_plot_create_options(\n  title = subtitle,\n  base_defaults = base_defaults_binary,\n  subtitle = subtitle,\n  filename_prefix = filename_prefix\n)\n\n\n# policy tree graph settings ----------------------------------------------\ndecision_tree_defaults <- list(\n  span_ratio       = .3,\n  text_size        = 3.8,\n  y_padding        = 0.25,\n  edge_label_offset = .002,\n  border_size      = .05\n)\npolicy_tree_defaults <- list(\n  point_alpha       = .5,\n  title_size        = 12,\n  subtitle_size     = 12,\n  axis_title_size   = 12,\n  legend_title_size = 12,\n  split_line_color  = \"red\",\n  split_line_alpha  = .8,\n  split_label_color = \"red\",\n  list(split_label_nudge_factor = 0.007)\n)\n\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n# +----------------------------------------------+\n# |       DO NOT ALTER  (except where noted)     |\n# +----------------------------------------------+\n\n# load GRF data and prepare inputs ----------------------------------------\ndf_grf <- margot::here_read('df_grf', push_mods)\nE      <- margot::here_read('E',      push_mods)\n# check exposure binary\nstopifnot(all(df_grf[[t1_name_exposure_binary]][!is.na(df_grf[[t1_name_exposure_binary]])] %in% 0:1))\n# set exposure and weights\nW       <- as.vector(df_grf[[t1_name_exposure_binary]]) # note it is the processed weights for attrition \"t1\"\nweights <- df_grf$t1_adjusted_weights\nhist(weights) # quick check for extreme weights\n# select covariates and drop numeric attributes\nX <- margot::remove_numeric_attributes(df_grf[E])\n\n\n# set model defaults -----------------------------------------------------\ngrf_defaults <- list(seed = 123, stabilize.splits = TRUE, num.trees = 2000)\n\n\n# example: fit causal forest on a toy subset ------------------------------\n# first, create a smaller test sample\nn   <- nrow(X)\ntoy <- sample(seq_len(n), floor(n / 4))\n# define toy data\ntoy_data     <- df_grf[toy, ]\nX_toy        <- X[toy, ]\nW_toy        <- W[toy]\nweights_toy  <- weights[toy]\n\n# fit the model\ncf_out <- margot_causal_forest(\n  data         = toy_data,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  outcome_vars = \"t2_kessler_latent_depression_z\", # select variable in your outcome_variable set\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  covariates   = X_toy,\n  W            = W_toy,\n  weights      = weights_toy,\n  save_data    = TRUE,\n  save_models  = TRUE\n)\n\n# inspect propensities ------------------------------------------------------\nqini_tbl <- margot::margot_inspect_qini(cf_out, propensity_bounds = c(0.01, 0.97))\n\n# show\nprint(qini_tbl)\n\n# plot policy-combo trees --------------------------------------------------\ncombo1 <- margot_plot_policy_combo(\n  result_object    = cf_out,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_name       = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  max_depth        = 1L,          # depth-1 tree\n  decision_tree_args = list(text_size = 4),\n  policy_tree_args   = list(point_alpha = 0.7),\n  original_df        = original_df,\n  label_mapping      = label_mapping_all\n)\n\n# show\ncombo1$combined_plot\n\n# you can repeat for depth-2 ----------------------------------------------\ncombo2 <- margot_plot_policy_combo(\n  result_object    = cf_out,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_name       = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  max_depth        = 2L,\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  original_df        = original_df,\n  label_mapping      = label_mapping_all\n)\n\n# show\ncombo2$combined_plot\n\n# batch plotting ----------------------------------------------------------\nmodels_batch_1L <- margot_policy(\n  cf_out,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names        = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df        = original_df,\n  label_mapping      = label_mapping_all,\n  max_depth          = 1L\n)\n\n# view first model's plots\nmodels_batch_1L[[1]][[3]]  # combo plot\nmodels_batch_1L[[1]][[4]]  # qini plot\n\n# sub plots\nmodels_batch_1L[[1]][[1]]  # predictions of policy tree\nmodels_batch_1L[[1]][[2]]  # policy tree\n\n# qini interpretations at different spends\n# negative is bad\nmodels_batch_1L[[1]][[5]]  \n\n# 2L tree\nmodels_batch_2L <- margot_policy(\n  cf_out,\n  save_plots         = FALSE,\n  output_dir         = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args   = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names        = \"model_t2_kessler_latent_depression_z\",\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df        = original_df,\n  label_mapping      = label_mapping_all,\n  max_depth          = 2L)\n# view first model's plots\nmodels_batch_2L[[1]][[3]]  # combo plot\nmodels_batch_2L[[1]][[4]]  # qini plot - not convincing\n\n# 2. flip the selected outcomes (and regen trees)\n# use -- when the outcome is undesirable and we want to minimise it \n# (assuming the exposure is something we'd prescribe)\n\n# select models whose outcomes are undesirable  when the intervention is meant to be 'good'\n# such variables will be specific to your study \n\n# +--------------------------+\n# |    MODIFY THIS           |\n# +--------------------------+\n\nflip_outcomes_test = c(\"t2_kessler_latent_depression_z\")\n\n# function to get the labels from the models (labels were defined above)\nflipped_names_test <- margot_get_labels(flip_outcomes_test, label_mapping_all)\n\n# +--------------------------+\n# |   END MODIFY             |\n# +--------------------------+\n\n# run flip forests\ncf_out_f <- margot_flip_forests(\n  model_results = cf_out,\n  flip_outcomes = flip_outcomes_test,\n  recalc_policy = TRUE\n)\n\n# where there are very low or high propensity scores (prob of exposure) \n# we might consider trimming\nmargot::margot_inspect_qini(cf_out_f, propensity_bounds = c(0.01, 0.97))\n\n\n# if we had extreme scores (not used here)\n# cf_out_flipped_trimmed <- margot_rescue_qini(model_results      = cf_out_f,\n#                                              propensity_bounds  = c(0.05, 0.95)) \n\n# flipped batch model\nmodels_batch_flipped_2L <- margot_policy(\n  cf_out_f,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # +--------------------------+\n  # |    MODIFY THIS           |\n  # +--------------------------+\n  model_names = c(\"model_t2_kessler_latent_depression_z\"),\n  # +--------------------------+\n  # |   END MODIFY             |\n  # +--------------------------+\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth     = 2L\n)\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# flipped\n# interpretation: exposure minimising depression\nmodels_batch_flipped_2L[[1]][[3]]\n\n\n# *** NOTE DIFFERENCES IN INTERPRETATION\n\n# not flipped: exposure as maximizing depression\nmodels_batch_2L[[1]][[3]]\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n# interpretation example \n\n# test interpretations ----------------------------------------------------\n\n\n# policy tree interpretation: search depth = 1\ninterpret_model_policy_test_1L <- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 1)\ncat(interpret_model_policy_test_1L)\n\n\n# policy tree interpretation: search depth = 2\ninterpret_model_policy_test_2L <- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 2)\ncat(interpret_model_policy_test_2L)\n\n\n\n# interpret rate ----------------------------------------------------------\n\n# create rate analysis table\nrate_table_all_test <- margot_rate(\n  models = cf_out_f,\n  policy = \"treat_best\",  # or \"withold_best\" but don't attempt fitting curves or policytrees\n  label_mapping = label_mapping_all\n)\n\n# view rate tables\nrate_table_all_test$rate_autoc |> kbl(\"markdown\")\nrate_table_all_test$rate_qini |> kbl(\"markdown\")\n\n\n# generate interpretation\nrate_interpretation_all <- margot_interpret_rate(\n  rate_table_all_test, \n  flipped_outcomes = flipped_names_test\n)\n\n\n# ** uncomment to run full model**\n\n# causal forest model -----------------------------------------------------------\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\nmodels_binary <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# save model\nmargot::here_save_qs(models_binary, \"models_binary\", push_mods)\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# read results ------------------------------------------------------------\n# if you save models you do not need to re-run them\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# reading models takes time\n# if you want to check the size of an object use\n# margot::margot_size(object)\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\nmodels_binary <- margot::here_read_qs(\"models_binary\", push_mods)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# count models by category\n# just a check\ncat(\"Number of original models:\\n\", length(models_binary$results), \"\\n\")\n\n\n# make ate plots ----------------------------------------------------------\nbinary_results <- margot_plot(\n  models_binary$combined_table,\n  options = outcomes_options_all,\n  label_mapping = label_mapping_all,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  order = \"evaluebound_asc\",\n  original_df = original_df,\n  e_val_bound_threshold = 1.2\n)\n\n# view\nbinary_results$transformed_table |> rename(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\") |>\n  kbl(format = 'markdown')\n\n# check\nbinary_results$plot\n\n# interpretation\ncat(binary_results$interpretation)\n\n# nice table\ntables_list <- list(\n  Wellbeing = binary_results$transformed_table\n)\n\n# make markdown tables (to be imported into the manuscript)\nmargot_bind_tables_markdown <- margot_bind_tables(\n  tables_list = tables_list,\n  #list(all_models$combined_table),\n  sort_E_val_bound = \"desc\",\n  e_val_bound_threshold = 1.2,\n  # ← choose threshold\n  highlight_color = NULL,\n  bold = TRUE,\n  rename_cols = TRUE,\n  col_renames = list(\"E-Value\" = \"E_Value\", \"E-Value bound\" = \"E_Val_bound\"),\n  rename_ate = TRUE,\n  threshold_col = \"E_Val_bound\",\n  output_format = \"markdown\",\n  kbl_args = list(\n    booktabs = TRUE,\n    caption = NULL,\n    align = NULL\n  )\n)\n\n# view markdown table\nmargot_bind_tables_markdown\n\n# save for publication\nhere_save(margot_bind_tables_markdown, \"margot_bind_tables_markdown\")\n\n\n# evaluate models ---------------------------------------------------------\n# trim models if extreme propensity scores dominate\n# diag_tbl_98 <- margot_inspect_qini(models_binary,\n#                                        propensity_bounds = c(0.01, 0.99))\n\n\n\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n# flipping models: outcomes we want to minimise given the exposure --------\n# standard negative outcomes/  not used in this study\n\n# +--------------------------+\n# |    MODIFY THIS           |\n# +--------------------------+\nflip_outcomes_standard = c(\n  #\"t2_alcohol_frequency_weekly_z\",\n  #\"t2_alcohol_intensity_z\",\n  #\"t2_hlth_bmi_z\",\n  #\"t2_hlth_fatigue_z\",\n  \"t2_kessler_latent_anxiety_z\", #  ← select\n  \"t2_kessler_latent_depression_z\",#  ← select\n  \"t2_rumination_z\" #  ← select\n  #\"t2_perfectionism_z\" # the exposure variable was not investigated\n)\n\n\n# we will investigate losses to these outcomes\n# usual flipped names for positive interventions\n# commented out for this study\n\n# WHICH OUTCOMES -- if any ARE UNDESIREABLE? \n\n# NOT IF THE EXPOSURE IS NEGATIVE, FOCUS ON WHICH OUTCOMES, if any, ARE POSITIVE AND FLIP THESE?\nflip_outcomes <- flip_outcomes_standard #c( setdiff(t2_outcomes_all, flip_outcomes_standard) )\n\n# check\nflip_outcomes\n\n\n# +--------------------------+\n# |   END MODIFY             |\n# +--------------------------+\n\n\n# checks for when exposure is *damaging** \n# neg_check <- vapply(all_models$results[ paste0(\"model_\", flip_outcomes) ],\n#                     \\(x) mean(x$tau_hat, na.rm = TRUE) < 0, logical(1))\n# stopifnot(all(neg_check))   # every chosen outcome has a negative mean cate\n\n# get labels\nflipped_names <- margot_get_labels(flip_outcomes, label_mapping_all)\n\n# check\nflipped_names\n\n# save for publication\nhere_save(flipped_names, \"flipped_names\")\n\n\n\n# flip negatively oriented outcomes --------------------------------------\n\n# +--------------------------+\n# |       DO NOT ALTER       |\n# +--------------------------+\n\n\n# flip models using margot's function\n\n#  *** this will take some time ***\n\n# ** give it time **\n# ** once run/ comment out **\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\nmodels_binary_flipped_all <- margot_flip_forests(models_binary,\n                                                 flip_outcomes = flip_outcomes,\n                                                 recalc_policy = TRUE)\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# save\nhere_save_qs(models_binary_flipped_all, \"models_binary_flipped_all\", push_mods)\n\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n\n# +--------------------------+\n# |          ALERT           |\n# +--------------------------+\n# !!!! THIS WILL TAKE TIME  !!!!!\n# read back if needed\nmodels_binary_flipped_all <- here_read_qs(\"models_binary_flipped_all\", push_mods)\n# +--------------------------+\n# |        END ALERT         |\n# +--------------------------+\n\n# where there are very low or high propensity scores (prob of exposure) we might consider trimming\n# margot::margot_inspect_qini(models_binary_flipped_all, propensity_bounds = c(0.05, 0.95))\n# \n# \n# # if we had extreme scores (not used here)\n# models_binary_flipped_all_t <- margot_rescue_qini(model_results  = models_binary_flipped_all,\n#                                              propensity_bounds  = c(0.05, 0.95))\n\n\n\n# omnibus heterogeneity tests --------------------------------------------\n# test for treatment effect heterogeneity across all outcomes\nresult_ominbus_hetero_all <- margot::margot_omnibus_hetero_test(models_binary_flipped_all,\n                                                                label_mapping = label_mapping_all)\n\n# view results table\nresult_ominbus_hetero_all$summary_table |> kbl(\"markdown\")\n\n# view test interpretation\ncat(result_ominbus_hetero_all$brief_interpretation)\n\n# rate test analysis -----------------------------------------------------\n\n# create rate analysis table\nrate_table_all <- margot_rate(\n  models = models_binary_flipped_all,\n  policy = \"treat_best\",  # or \"withold_best\" but don't attempt fitting curves or policytrees\n  label_mapping = label_mapping_all\n)\n\n# view rate tables\nrate_table_all$rate_autoc |> kbl(\"markdown\")\nrate_table_all$rate_qini |> kbl(\"markdown\")\n\n# generate interpretation\nrate_interpretation_all <- margot_interpret_rate(\n  rate_table_all, \n  flipped_outcomes = flipped_names\n)\n\n# view interpretations\ncat(rate_interpretation_all$autoc_results)\ncat(rate_interpretation_all$qini_results)\n\n# compare rate and qini -- see grf documentation\ncat(rate_interpretation_all$comparison)\n\n# check out model names for different ways of thinking about heterogeneity\nrate_interpretation_all$either_model_names\nrate_interpretation_all$qini_model_names\nrate_interpretation_all$both_model_names\nrate_interpretation_all$autoc_model_names\n\n# autoc plots ------------------------------------------------------------\n# generate batch rate plots for models with significant heterogeneity\nbatch_rate_autoc_plots <- margot_plot_rate_batch(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  # just use rate autoc for rate plots\n  model_names = rate_interpretation_all$autoc_model_names\n)\n\n# extract individual plots from the batch result\nautoc_plots <- batch_rate_autoc_plots\n\n# determine number of columns based on number of plots\nnum_cols <- ifelse(length(autoc_plots) > 3, 2, 1)\n\n# combine plots using patchwork\nlibrary(patchwork)\n\n# only proceed if there are plots to combine\nif (length(autoc_plots) > 0) {\n  # initialize with first plot\n  combined_autoc_plot <- autoc_plots[[1]]\n  \n  # add remaining plots if any\n  if (length(autoc_plots) > 1) {\n    for (i in 2:length(autoc_plots)) {\n      combined_autoc_plot <- combined_autoc_plot + autoc_plots[[i]]\n    }\n  }\n  \n  # apply the dynamic layout\n  combined_autoc_plot <- combined_autoc_plot +\n    plot_layout(ncol = num_cols) &\n    plot_annotation(\n      title = \"AUTOC Model Plots\",\n      subtitle = paste0(length(autoc_plots), \" models with significant heterogeneity\"),\n      tag_levels = \"A\"\n    )\n  \n  # view the combined plot\n  print(combined_autoc_plot)\n  \n  # save the combined plot if needed\n  width <- ifelse(num_cols == 1, 8, 12)\n  height <- 6 * ceiling(length(autoc_plots) / num_cols)\n  \n  ggsave(\n    here::here(push_mods, \"combined_autoc_plots.pdf\"),\n    combined_autoc_plot,\n    width = width,\n    height = height\n  )\n} else {\n  # handle case with no plots\n  message(\"No AUTOC plots available\")\n}\n\nmodels_batch_qini_2L_test <- margot_plot_policy_combo(\n  models_binary_flipped_all,\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_name =  \"model_t2_log_hours_exercise_z\",\n  max_depth  = 2L,\n  # ← new argument\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\nrate_interpretation_all$qini_model_names\n\n# qini --------------------------------------------------------------------\n# run the margot_policy function\nmodels_batch_qini_2L <- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$qini_results,\n  max_depth  = 2L,\n  # ← new argument\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\n\n# extract the plots from the results\nplots <- lapply(seq_along(models_batch_qini_2L), function(i) {\n  models_batch_qini_2L[[i]][[4]]  # extract the 4th element (plot) from each model\n})\n\n# name the plots\nnames(plots) <- rate_interpretation_all$qini_model_names\n\n# determine number of columns based on number of plots\nnum_cols <- ifelse(length(plots) > 3, 2, 1)\n\n# load the patchwork library for combining plots\nlibrary(patchwork)\n\n# check if there are any plots to combine\nif (length(plots) == 0) {\n  message(\"no plots available to combine\")\n  NULL  # removed return since this isn't in a function\n} else {\n  # create combined plot\n  combined_plot <- plots[[1]]\n  \n  # only run the loop if there are at least 2 plots\n  if (length(plots) > 1) {\n    for (i in 2:length(plots)) {\n      combined_plot <- combined_plot + plots[[i]]\n    }\n  }\n  \n  # apply the dynamic layout\n  combined_plot <- combined_plot + plot_layout(ncol = num_cols)\n  # add titles and annotations\n  combined_plot <- combined_plot &\n    plot_annotation(\n      title = \"Qini Model Plots\",\n      subtitle = paste0(length(plots), \n                        ifelse(length(plots) == 1, \" model \", \" models \"), \n                        \"arranged in \", num_cols, \n                        ifelse(num_cols == 1, \" column\", \" columns\")),\n      tag_levels = \"A\"  # adds a, b, c, etc. to the plots\n    )\n  # view\n  combined_plot\n  # save (optional)\n  width <- ifelse(num_cols == 1, 8, 12)\n  height <- 6 * ceiling(length(plots)/num_cols)  # height per row * number of rows\n  # save\n  ggsave(here::here(push_mods, \"combined_qini_plots.pdf\"),\n         combined_plot,\n         width = width, height = height)\n  \n  combined_plot  # removed return since this isn't in a function\n}\n\n# interpretation ----------------------------------------------------------\n# interpret qini curves\ninterpretation_qini_curves_2L <- margot_interpret_qini(\n  models_batch_qini_2L,\n  model_names = rate_interpretation_all$qini_model_names,\n  label_mapping = label_mapping_all\n)\ninterpretation_qini_curves_2L\n\n# view qini interpretation\ncat(interpretation_qini_curves_2L$qini_explanation)\n\n# view summary table\ninterpretation_qini_curves_2L$summary_table |> kbl(\"markdown\")\n\n\n\n# policy tree analysis depth 1 L------------------------------------------------\n# make policy trees\n# 1 l decision trees are generally very bad\nplots_policy_trees_1L <- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names,\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 1L\n)\n\n# get number of models\nn_models <- length(rate_interpretation_all$either_model_names)\n\n# # use purrr to map through and print each model\n# purrr::map(1:n_models, function(i) {\n#   # print model name as a header\n#   cat(\"# model\", i, \"\\n\")\n#   # print the corresponding model plot\n#   print(plots_policy_trees_1L[[i]][[3]])\n#   # add spacing between models\n#   cat(\"\\n\\n\")\n# })\n\nmodel_outputs_1L <- purrr::map(1:n_models, ~plots_policy_trees_1L[[.x]][[3]])\n\n# name the list elements by model number\nnames(model_outputs_1L) <- paste0(\"model_\", 1:n_models)\n\n\n# check number of models == n_models\nmodel_outputs_1L$model_1 # convincing?\nmodel_outputs_1L$model_2 # convincing?\nmodel_outputs_1L$model_3 # convincing?\n\n\n\n# policy tree analysis depth 2L -------------------------------------------------\n# make policy trees\n# *** 2l is much more persuasive ***\nplots_policy_trees_2L <- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names,\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 2L\n)\n\nn_models <- length(rate_interpretation_all$either_model_names)\n\nmodel_outputs_2L <- purrr::map(1:n_models, ~plots_policy_trees_2L[[.x]][[3]])\nnames(model_outputs_2L) <- paste0(\"model_\", 1:n_models)\n\n# checks\nmodel_outputs_2L$model_1\nmodel_outputs_2L$model_2\nmodel_outputs_2L$model_3\n\n\n# convincing?\ninterpret_plots_policy_trees_2L <- margot_interpret_policy_batch(\n  models_binary_flipped_all, model_names = rate_interpretation_all$either_model_names)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_2L)\n\n# +--------------------------+\n# |     END DO NOT ALTER     |\n# +--------------------------+\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n# you can investigate policy trees for all outcomes, mindful that the rate and qini are not reliable. \n# still, with appropriate caution, this may help to clarify psychologically interesting questions\n\nall_plots_policy_trees_1L <- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # model_names = rate_interpretation_all$either_model_names, # use all\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 1L\n)\nn_models <- length(models_binary_flipped_all$results)\nn_models\n\n\nmodel_outputs_1L_all <- purrr::map(1:n_models, ~all_plots_policy_trees_1L[[.x]][[3]])\n\n# view\nmodel_outputs_1L_all[[1]] # ← convincing? \nmodel_outputs_1L_all[[2]] # ← convincing? \nmodel_outputs_1L_all[[3]] # ← convincing? \nmodel_outputs_1L_all[[4]] # ← convincing? \nmodel_outputs_1L_all[[5]] # ← convincing? \nmodel_outputs_1L_all[[6]] # ← convincing? \nmodel_outputs_1L_all[[7]] # ← convincing? \nmodel_outputs_1L_all[[8]] # ← convincing? \nmodel_outputs_1L_all[[9]] # ← convincing?  \nmodel_outputs_1L_all[[10]] # ← convincing? \nmodel_outputs_1L_all[[11]] # ← convincing? \nmodel_outputs_1L_all[[12]] # ← convincing? \n\n\n\n# interpretation\ninterpret_plots_policy_trees_1L_all <- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 1)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_1L_all)\n\n\n# ALL model 1L\nall_plots_policy_trees_2L <- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  # model_names = rate_interpretation_all$either_model_names, # use all\n  # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all,\n  max_depth = 2L\n)\nn_models <- length(models_binary_flipped_all$results)\nn_models\n\n\nmodel_outputs_2L_all <- purrr::map(1:n_models, ~all_plots_policy_trees_2L[[.x]][[3]])\n\n# view\nmodel_outputs_2L_all[[1]] # ← convincing? \nmodel_outputs_2L_all[[2]] # ← convincing? \nmodel_outputs_2L_all[[3]] # ← convincing? \nmodel_outputs_2L_all[[4]] # ← convincing? \nmodel_outputs_2L_all[[5]] # ← convincing? \nmodel_outputs_2L_all[[6]] # ← convincing? \nmodel_outputs_2L_all[[7]] # ← convincing? \nmodel_outputs_2L_all[[8]] # ← convincing? \nmodel_outputs_2L_all[[9]] # ← convincing? \nmodel_outputs_2L_all[[10]] # ← convincing? \nmodel_outputs_2L_all[[11]] # ← convincing? \nmodel_outputs_2L_all[[12]] # ←  convincing \n\n\n# interpretation\ninterpret_plots_policy_trees_2L_all <- margot_interpret_policy_batch(models_binary_flipped_all, max_depth = 2)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_2L_all)\n\n\n# +--------------------------+\n# |   END MODIFY SECTION     |\n# +--------------------------+\n\n\n\n\n\n\n\n# +--------------------------+\n# |    MODIFY THIS SECTION   |\n# +--------------------------+\n\n\n#############################################################################\n# theoretical comparisons ---------------------------------------------------\n# individual theoretical comparisons (if relevant)\n# need to get values for wealth if wealth is compared\n\n# step 1 get information for wealth for conditonal comparisons\nhead(df_grf$t0_log_household_inc_z)\n\n# get mean on original data scale\nlog_mean_inc <- mean(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# get sd on original data scale\nlog_sd_inc <- sd(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# function to get back to data scale\nmargot_back_transform_log_z(\n  log_mean = log_mean_inc,\n  log_sd = log_sd_inc,\n  z_scores = c(-1, 0, 1),\n  label = \"data_scale\"\n)\n\n# define complex conditions for subsetting\ncomplex_condition_political <- X[, \"t0_political_conservative_z\"] > -1 &\n  X[, \"t0_political_conservative_z\"] < 1\n\ncomplex_condition_wealth <- X[, \"t0_log_household_inc_z\"] > -1 &\n  X[, \"t0_log_household_inc_z\"] < 1\n\ncomplex_condition_age <- X[, \"t0_age_z\"] > -1 &\n  X[, \"t0_age_z\"] < 1\n\n# # if we have specific groups to compare\n# complex_condition_age_under_neg_1_sd  <- X[, \"t0_age_z\"] < -1\n# complex_condition_age_gr_eq_neg_1_sd  <- X[, \"t0_age_z\"] > -1\n\n# check ages to get number\nmean(original_df$t0_age) - sd(original_df$t0_age)\nmean(original_df$t0_age) + sd(original_df$t0_age)\n\n\n# wealth subsets\nsubsets_standard_wealth <- list(\n  Poor = list(\n    var = \"t0_log_household_inc_z\",\n    value = -1,\n    operator = \"<\",\n    description = \"Effects among those HShold income < -1 SD (NZD ~41k)\",\n    label = \"Poor\"  # label remains as is, but could be changed if desired\n  ),\n  MiddleIncome = list(subset_condition = complex_condition_wealth, description = \"Effects among those HS_hold income within +/-1SD (> NZD 41k < NZD 191k)\"),\n  Rich = list(\n    var = \"t0_log_household_inc_z\",\n    value = 1,\n    operator = \">\",\n    description = \"Effects among those HS_hold income > +1 SD (NZD 191k)\",\n    label = \"Rich\"\n  )\n)\n\n# political subsets\nsubsets_standard_political <- list(\n  Liberal = list(\n    var = \"t0_political_conservative_z\",\n    value = -1,\n    operator = \"<\",\n    description = \"Effects among those < -1 SD in political conservativism\",\n    label = \"Liberal\"\n  ),\n  Centrist = list(\n    var = \"t0_political_conservative_z\",\n    # operator = \"<\",\n    subset_condition = complex_condition_political,\n    description = \"Effects among those > -1 SD and < +1 in political conservativism\",\n    label = \"Centrist\"\n  ),\n  Conservative = list(\n    var = \"t0_political_conservative_z\",\n    value = 1,\n    operator = \">\",\n    description = \"Effects among those > +1 SD in political conservativism\",\n    label = \"Conservative\"\n  )\n)\n\n\n# political subsets\nsubsets_standard_age <- list(\n  Younger = list(\n    var = \"t0_age_z\",\n    value = -1,\n    operator = \"<\",\n    description = \"Effects among those < under 35 years old\",\n    label = \"Age < 35\"\n  ),\n  Middle = list(\n    var = \"t0_age_z\",\n    # operator = \"<\",\n    subset_condition = complex_condition_age,\n    description = \"Effects among those 35-62\",\n    label = \"Age 35-62\"\n  ),\n  Older = list(\n    var = \"t0_age_z\",\n    value = 1,\n    operator = \">\",\n    description = \"Effects among those > 62\",\n    label = \"Age > 62\"\n  )\n)\n\n\n# gender subsets\nsubsets_standard_gender <- list(\n  Female = list(\n    var = \"t0_male_binary\",\n    value = 0,\n    description = \"Females\"\n  ),\n  Male = list(\n    var = \"t0_male_binary\",\n    value = 1,\n    description = \"Males\"\n  )\n)\n\n# ethnicity subsets\nsubsets_standard_ethnicity <- list(\n  Asian = list(\n    var = \"t0_eth_cat_asian_binary\",\n    value = 1,\n    description = \"Asians\"\n  ),\n  Euro = list(\n    var = \"t0_eth_cat_euro_binary\",\n    value = 1,\n    description = \"Europeans (Pakeha)\"\n  ),\n  Pacific = list(\n    var = \"t0_eth_cat_pacific_binary\",\n    value = 1,\n    description = \"Pacific Peoples\"\n  ),\n  Maori = list(\n    var = \"t0_eth_cat_maori_binary\",\n    value = 1,\n    description = \"Māori\"\n  )\n)\n\n\n# batch planned subgroup analysis -----------------------------------------\n# set up lists of models, names, and subtitles\ndomain_models <- list(\n  models_binary # HERE WE USE THE ORIGINAL MODELS\n)\n\n\n# set up domain names\ndomain_names <- c(\"wellbeing\")\n\n# set up subtitles\nsubtitles <- \"\"\n\n# set up subset types in a list\nsubset_types <- list(\n  wealth = subsets_standard_wealth,\n  ethnicity = subsets_standard_ethnicity,\n  political = subsets_standard_political,\n  gender = subsets_standard_gender,\n  cohort = subsets_standard_age\n)\n\n\n# run model\nplanned_subset_results <- margot_planned_subgroups_batch(\n  domain_models = domain_models,\n  X = X,\n  base_defaults = base_defaults_binary,\n  subset_types = subset_types,\n  original_df = original_df,\n  domain_names = domain_names,\n  subtitles = subtitles\n)\n\n\n# results\ncat(planned_subset_results$wellbeing$wealth$explanation)\ncat(planned_subset_results$wellbeing$ethnicity$explanation)\ncat(planned_subset_results$wellbeing$political$explanation)\ncat(planned_subset_results$wellbeing$gender$explanation)\ncat(planned_subset_results$wellbeing$cohort$explanation)\n\n\n\n# cohort subgroups --------------------------------------------------------\n\n# plots -------------------------------------------------------------------\n# results plots\n# health\nplots_subgroup_wealth<- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$wealth$results$Poor$plot,\n    planned_subset_results$wellbeing$wealth$results$MiddleIncome$plot,\n    planned_subset_results$wellbeing$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Wealth\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nplots_subgroup_wealth\n\n# plots\nplots_subgroup_ethnicity <- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$ethnicity$results$Asian$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Euro$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Pacific$plot,\n    planned_subset_results$wellbeing$ethnicity$results$Maori$plot\n    \n  ),\n  ncol = 2\n) +\n  patchwork::plot_annotation(title = \"Ethnicity\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_ethnicity)\n\n# plots\nplots_subgroup_political <- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$political$results$Liberal$plot,\n    planned_subset_results$wellbeing$political$results$Centrist$plot,\n    planned_subset_results$wellbeing$political$results$Conservative$plot  \n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Political Orientation\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_political)\n\n# plots\nplots_subgroup_gender <- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$gender$results$Female$plot,\n    planned_subset_results$wellbeing$gender$results$Male$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Gender\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_gender)\n\n# plots\nplots_subgroup_cohort <- wrap_plots(\n  list(\n    planned_subset_results$wellbeing$cohort$results$`Age < 35`$plot,\n    planned_subset_results$wellbeing$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$wellbeing$cohort$results$`Age > 62`$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(title = \"Age Cohorts\",\n                             theme = theme(plot.title = element_text(size = 18, face = \"bold\")))\n\n# view\nprint(plots_subgroup_cohort)\n\n\n\n# plot options: showcased ---------------------------------------------\n# default\nmargot_plot_decision_tree(models_binary, \"model_t2_support_z\", )\n# tighten branches for easier viewing in single graphs\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .30,\n  text_size = 3.8,\n  border_size = .1,\n  #  title = \"none\",\n  original_df = original_df\n)\n# colour decision node\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .3,\n  text_size = 4,\n  title = \"New Title\",\n  non_leaf_fill =  \"violet\",\n  original_df = original_df\n)\n# make new title\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  span_ratio = .2,\n  text_size = 3,\n  title = \"New Title\",\n  non_leaf_fill =  \"white\",\n  original_df = original_df\n)\n\n# remove title\nmargot::margot_plot_decision_tree(\n  models_binary,\n  \"model_t2_support_z\",\n  text_size = 5,\n  title = 'none',\n  # set title to none\n  original_df = original_df\n)\n\n\n# adjust only the alpha\nmargot::margot_plot_policy_tree(models_binary, \"model_t2_support_z\", point_alpha = .1)\nmargot::margot_plot_policy_tree(models_binary, \"model_t2_support_z\", point_alpha = .9)\n```\n:::\n\n\n\n\n## HOMEWORK: Prepare a fresh set of analysis scripts using a different exposure\n\n- E.g. Ask: what are the effects of a shift in religious service `religion_church` on multi-dimensional well-being. \n- Consider what variables you need for confounding control at baseline. \n- Think about how to make the exposure variable binary.\n- You may consider different outcome(s) as well as a different exposure. \n\n\n\n\n### Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::cite_packages()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Bulbulia J (2024). _boilerplate_. doi:10.5281/zenodo.13370825 <https://doi.org/10.5281/zenodo.13370825>, R package version 1.0.4, <https://go-bayes.github.io/biolerplate/>.\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 <https://doi.org/10.5281/zenodo.10907724>, R package version 1.0.37 Functions to obtain MARGinal Observational Treatment-effects from observational data., <https://go-bayes.github.io/margot/>.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont <https://doi.org/10.32614/CRAN.package.extrafont>, R package version 0.19, <https://CRAN.R-project.org/package=extrafont>.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. <https://www.jstatsoft.org/v40/i03/>.\n  - Müller K (2020). _here: A Simpler Way to Find Your Files_. doi:10.32614/CRAN.package.here <https://doi.org/10.32614/CRAN.package.here>, R package version 1.0.1, <https://CRAN.R-project.org/package=here>.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. doi:10.32614/CRAN.package.tibble <https://doi.org/10.32614/CRAN.package.tibble>, R package version 3.2.1, <https://CRAN.R-project.org/package=tibble>.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. doi:10.32614/CRAN.package.patchwork <https://doi.org/10.32614/CRAN.package.patchwork>, R package version 1.3.0, <https://CRAN.R-project.org/package=patchwork>.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. <https://www.R-project.org/>.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, <https://ggplot2.tidyverse.org>.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. doi:10.32614/CRAN.package.forcats <https://doi.org/10.32614/CRAN.package.forcats>, R package version 1.0.0, <https://CRAN.R-project.org/package=forcats>.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. doi:10.32614/CRAN.package.stringr <https://doi.org/10.32614/CRAN.package.stringr>, R package version 1.5.1, <https://CRAN.R-project.org/package=stringr>.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. doi:10.32614/CRAN.package.dplyr <https://doi.org/10.32614/CRAN.package.dplyr>, R package version 1.1.4, <https://CRAN.R-project.org/package=dplyr>.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. doi:10.32614/CRAN.package.purrr <https://doi.org/10.32614/CRAN.package.purrr>, R package version 1.0.4, <https://CRAN.R-project.org/package=purrr>.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. doi:10.32614/CRAN.package.readr <https://doi.org/10.32614/CRAN.package.readr>, R package version 2.1.5, <https://CRAN.R-project.org/package=readr>.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. doi:10.32614/CRAN.package.tidyr <https://doi.org/10.32614/CRAN.package.tidyr>, R package version 1.3.1, <https://CRAN.R-project.org/package=tidyr>.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, <https://github.com/rstudio/tinytex>. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. <https://tug.org/TUGboat/Contents/contents40-1.html>.\n  - Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. doi:10.32614/CRAN.package.kableExtra <https://doi.org/10.32614/CRAN.package.kableExtra>, R package version 1.4.0, <https://CRAN.R-project.org/package=kableExtra>.\n```\n\n\n:::\n:::\n\n\n\n\n## Appendix \n\n## Review: The Fundamental Problem of Causal Inference as a Missing Data Problem\n\nRecall the fundamental problem of causal inference, returning to the question of whether bilingualism improves cognitive abilities:\n\n-   $Y_i^{a = 1}$: The cognitive ability of child $i$ if they were bilingual. This is the counterfactual outcome when A = 1.\n-   $Y_i^{a = 0}$:: The cognitive ability of child $i$ if they were monolingual. This is the counterfactual outcome when A = 0.\n\nThe causal effect of bilingualism on cognitive ability for individual $i$ is then defined as the difference between these potential outcomes:\n\n$$\n\\text{Causal Effect}_i = Y_i^{a=1} - Y_i^{a=0} \n$$\n\nWe say there is a causal effect if:\n\n$$\nY_i^{a=1} - Y_i^{a=0}  \\neq 0\n$$\n\nHowever, we only observe one of the potential outcomes for each child. The other outcome is not observed because physics prevents a child from both receiving and not receiving bilingual exposure.\n\nThe fact that causal contrasts are not observed in individuals is called \"The fundamental problem of causal inference.\"\n\nAlthough we typically cannot observe individual causal effects, we can obtain average causal effects when certain assumptions are satisfied.\n\n```{=tex}\n\\begin{align}\nE(\\delta) = E(Y^{a=1} - Y^{a=0})\\\\\n          ~  = E(Y^{a=1}) - E(Y^{a=0}) \\\\\n          ~  = ATE\n\\end{align}\n```\nWe may identify average causal effects from the data when the following assumptions are met:\n\n-   **Causal Consistency:** The exposure values under comparisons correspond to well-defined interventions that, in turn, correspond to the treatment versions in the data.[]\n-   **Positivity:** The probability of receiving every value of the exposure within all strata of co-variates is greater than zero []\n-   **Exchangeability:** The conditional probability of receiving every value of an exposure level, though not decided by the investigators, depends only on the measured covariates []\n\nFurther assumptions:\n\n-   **No Interference,** also known as the **Stable Unit Treatment Value Assumption** (SUTVA), requires that the treatment given to one unit (e.g., person, group, organization) does not interfere with the potential outcomes of another unit. Put differently, there are no \"spillover\" effects. Note: this assumption may be thought to be part of causal consistency, namely individual has only one potential outcome under each treatment condition.\n-   **Correctly specified model**: the requirement that the underlying statistical model used to estimate causal effects accurately represents the true relationships between the variables of interest. We say the model should be able to capture \"the functional form\" of the relationship between the treatment, the outcome, and any covariates. The model's functional form should be flexible enough to capture the true underlying relationship. The estimated causal effects may be biased if the model's functional form is incorrect. Additionally, the model must handle omitted variable bias by including all relevant confounders and should correctly handle missing data from non-response or loss-to follow up. We will return to the bias arising from missing data in the weeks ahead. For now, it is important to note that causal inference assumes that our model is correctly specified.\n\n\n## Subgroup analysis\n\nRedcall, **Effect Modification** (also known as \"heterogeneity of treatment effects\", and \"Effect-measure modification\") occurs when the causal effect of intervention $A$ varies across different levels of another variable $R$:\n\n$$E(Y^{a=1}|G=g_1, L=l) - E(Y^{a=0}|G=g_1, L=l) \\neq E(Y^{a=1}|G=g_2, L=l) - E(Y^{a=0}|G=g_2, L=l)$$\n\nEffect modification indicates that the magnitude of the causal effect of intervention $A$ is related to the modifier variable $G$ level. As discussed last week, effect modification can be observed even when there is no direct causal interaction between the treatment and the modifier variable. We noted that **interaction in causal inference refers to a situation where the combined effect of two interventions is not equal to the sum of their individual effects**. **Effect modification, on the other hand, occurs when the causal effect of one intervention varies across different levels of another variable.**\n\n\nWe also noted that \n\n> **For comparative research, we are typically interested in effect-modification, which requires subgroup analysis.**\n\n\n### Causal Estimand, Statistical Estimand, Statistical Estimator\n\nLet's set subgroup analysis to the side for a moment and begin focussing on statistical estimation.  \n\nSuppose a researcher wants to understand the causal effect of marriage on individual happiness. Participants in the study are surveyed for their marital status (\"married\" or \"not married\") and their self-reported happiness on a scale from 1 to 10.\n\n#### Causal Estimand\n\n- **Definition**: The causal estimand is the specific quantity or parameter that we aim to estimate to understand the causal effect of an intervention or treatment on an outcome.\n\n- **Example**: Here, the **Causal Estimand** would be the Average Treatment Effect (ATE) of being married on happiness. Specifically, we define the ATE as the difference in the potential outcomes of happiness if all individuals were married versus if no individuals were married:\n\n  $$\n  \\text{ATE} = E[Y^{a=1} - Y^{a=0}]\n  $$\n\n\n  Here, $Y^{a=1}$ represents the potential happiness score if an individual is married, and $Y^{a=0}$ if they are not married.\n\n\n#### Next step: Are Causal Assumptions Met? \n\n- Identification (Exchangeability): balance in the confounders across the treatments to be compared\n\n- Consistency: well-defined interventions\n\n- Positivity: treatments occur within levels of covariates $L$\n\n\n#### Statistical Estimand (next step)\n\n- **The problem**: how do we bridge the gap between potential outcomes and data? \n\n- **Definition**: the statistical estimand is the parameter or function that summarises the relationship between variables as described by a statistical model applied to data. \n\n- **Example**: for our study, the **Statistical Estimand** might be the mean difference in happiness scores between individuals who are married and those who are not, as derived from a linear regression model:\n\n  $$\n  \\text{Happiness} = \\beta_0 + \\beta_1 \\times \\text{Married} + \\epsilon\n  $$\n\n  In this equation, $\\beta_1$ represents the estimated difference in happiness scores between the married and non-married groups.\n\n#### Statistical Estimator\n\n- **Definition**: a statistical estimator is a rule or method by which a numerical estimate of a statistical estimand is calculated from the data.\n\n- **Example**: in our marriage study, the **Statistical Estimator** for $\\beta_1$ is the ordinary least squares (OLS) estimator. This estimator is used to calculate $\\beta_1$ from the sample data provided by the survey. It provides an estimate of the impact of being married on happiness, calculated using:\n  $$\n  \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n  $$\n  where $X_i$ is a binary indicator for being married (1 for married, 0 for not married), $Y_i$ is the observed happiness score, and $\\bar{X}$, $\\bar{Y}$ are the sample means of $X$ and $Y$, respectively.\n\nThe upshot, we anchor our causal inquiries within a multi-step framework of data analysis. This involves: \n\n1. clearly defining our causal estimand within a specified *target population,*\n2. clarifying assumptions, & especially identification assumptions, \n3. describing a statistical strategy for extracting this estimand from the data, and then \n4. applying an algorithm that embodies this statistical method.\n\n\n<!-- ## Methods for Statistical Estimation in Causal Inference: Inverse Probability of Treatment Weights Using Propensity Scores -->\n\n<!-- Last week, we discussed confounding control using regression adjustment. Recall the formula for the average treatment effect (ATE) when conditioning on a set of covariates $L$: -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\text{ATE} = E[Y^{a=1} \\mid L = l] - E[Y^{a=0} \\mid L = l] \\quad \\text{for any value of } l -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- > \"We say that a set $L$ of measured non-descendants of $L$ is a sufficient set for confounding adjustment when conditioning on $L$ blocks all backdoor paths—that is, the treated and the untreated are exchangeable within levels of $L$\" (Hernán & Robins, *Causal Inference*, p. 86). -->\n\n<!-- This formula calculates the expected outcome difference between treated ($a=1$) and untreated ($a=0$) groups, given a specific value of the covariates $l$. -->\n\n<!-- Inverse Probability of Treatment Weighting (IPTW) takes a different approach. We create a pseudo-population where the treatment assignment is independent of the observed covariates by assigning weights to each individual based on their propensity scores. -->\n\n<!-- **We do this by modelling the treatment** -->\n\n<!-- Denote the treatment indicator by $A$, where $A = 1$ if an individual receives treatment and $A = 0$ otherwise. $L$ represents the vector of observed covariates, and $Y^a$ the potential outcomes. The propensity score, $e(L)$, is defined as the probability of receiving the treatment given the observed covariates: -->\n\n<!-- $$ -->\n<!-- \\hat{e}(L) = P(A = 1 \\mid L) -->\n<!-- $$ -->\n\n<!-- To obtain IPTW weights, compute the inverse probability of treatment: -->\n\n<!-- $$ -->\n<!-- v_i = \\frac{A_i}{\\hat{e}(L_i)} + \\frac{1 - A_i}{1 - \\hat{e}(L_i)} -->\n<!-- $$ -->\n\n<!-- Which simplifies to  -->\n\n<!-- $$ -->\n<!-- v_i =  -->\n<!-- \\begin{cases}  -->\n<!-- \\frac{1}{\\hat{e}} & \\text{if } A_i = 1 \\\\ -->\n<!-- \\frac{1}{1-\\hat{e}} & \\text{if } A_i = 0  -->\n<!-- \\end{cases} -->\n<!-- $$ -->\n\n<!-- where $v_i$ is the IPTW weight for individual $i$, $A_i$ is the treatment indicator for individual $i$, and $\\hat{e}(L_i)$ is the estimated propensity score for individual $i$.    -->\n\n<!-- How might we use these weights to obtain causal effect estimates? -->\n\n\n\n<!-- ## Marginal Structural Models (MSMs) -->\n\n<!-- Marginal Structural Models (MSMs) estimate causal effects without requiring an \"outcome model\" that stratifies on covariates. Rather, MSMs employ weights derived from the inverse probability of treatment weighting (IPTW) to create a pseudo-population in which the distribution of covariates is independent of treatment assignment over time. -->\n\n<!-- The general form of an MSM can be expressed as follows: -->\n\n<!-- $$ -->\n<!-- E[Y^a] = \\beta_0 + \\beta_1a -->\n<!-- $$ -->\n\n<!-- where $E[Y^a]$ is the expected outcome under treatment $a$  and $\\beta_0$ and $\\beta_1$ are parameters estimated by fitting the weighted model. Again, the weights used in the MSM, typically derived from the IPTW (or another treatment model), adjust for the confounding, allowing the model to estimate the unbiased effect of the treatment on the outcome without requiring covariates in the model. -->\n\n<!-- Where do weights fit in?   Note, we have $E[Y^a]$ in please of $E[Y|A=a]$.  When applying propensity score weights in the linear regression model $E[Y^a] = \\beta_0 + \\beta_1a$, each observation is weighted by $v_i$, such that $v_i(\\beta_0 + \\beta_1a)$. This changes the estimation process to focus on a weighted sum of outcomes, where each individual's contribution is adjusted to reflect their probability of receiving the treatment, given their covariates. -->\n\n\n<!-- ## Interpretation of $\\beta_0$ and $\\beta_1$  in a Marginal Structural Model -->\n\n<!-- ### Binary Treatment -->\n\n<!-- In models where the treatment $a$ is binary (e.g., $a = 0$ or $a = 1$), such as in many causal inference studies: -->\n\n<!-- - **$\\beta_0$**: the expected value of the outcome $Y$ when the treatment is not applied ($a = 0$). This is the baseline level of the outcome in the absence of treatment. -->\n<!-- - **$\\beta_1$**: the change in the expected outcome when the treatment status changes from 0 to 1. In logistic regression, $\\beta_1$ represents the log-odds ratio of the outcome for the treatment group relative to the control group. In linear regression, $\\beta_1$ quantifies the difference in the average outcome between the treated and untreated groups. -->\n\n<!-- ### Continuous Treatment -->\n\n<!-- When the treatment $a$ is continuous, the interpretation of $\\beta_0$ and $\\beta_1$ adjusts slightly: -->\n\n<!-- - **$\\beta_0$**: represents the expected value of the outcome $Y$ when the treatment $a$ is at its reference value (often zero).  -->\n<!-- - **$\\beta_1$**: represents the expected change in the outcome for each unit increase in the treatment. In this case, $\\beta_1$ measures the gradient or slope of the relationship between the treatment and the outcome. For every one-unit increase in treatment, the outcome changes by $\\beta_1$ units, assuming all other factors remain constant. -->\n\n\n<!-- ###  How can we apply marginal structural models in subgroups?  -->\n\n\n<!-- ### Assumptions -->\n\n<!-- - **Model assumptions**: the treatment model is correctly specified. -->\n<!-- - **Causal assumptions**: all confounders are appropriately controlled, positivity and consistency assumptions hold. -->\n\n\n<!-- ### Calculating Treatment Weights (Propensity Scores) and Confounding Control in Subgroups -->\n\n<!-- We may often achieve greater balance when conducting weighted analyses in subgroups by estimating propensity scores *within* these subgroups. The propensity score $ e(L, G) $ is the conditional probability of receiving the exposure $ A = 1 $, given the covariates $ L $ and subgroup indicator $ G $. This is often modelled using logistic regression or other methods that ensure covariate balance -->\n<!-- We define the estimated propensity score as follows: -->\n\n<!-- $$ -->\n<!-- \\hat{e} = P(A = 1 \\mid L, G) = f_A(L, G; \\theta_A) -->\n<!-- $$ -->\n\n<!-- Here, $ f_A(L, G; \\theta_A) $ is the statistical model estimating the probability of exposure $A = 1$ given covariates $L$ and subgroup $G$. We then calculate the weights for each individual, denoted $v$, using the estimated propensity score: -->\n\n<!-- $\\theta_A$ encapsulates all the coefficients (parameters) in this model, including intercepts, slopes, and potentially other parameters depending on the model complexity (e.g., interaction terms, non-linear effects...etc). -->\n\n<!-- These weights $v$ depend on $A$ and are calculated as the inverse of the propensity score for exposed individuals and as the inverse of $ 1-\\hat{e} $ for unexposed individuals. -->\n\n<!-- Propensity scores are estimated *separately* within strata of the subgroup to control for potential confounding tailored to each subgroup. These weights $v$ are specific to each individual in subgroup $G$. In the lab, we will clarify how to fit models to estimate contrasts for the causal effects within groups $\\hat{\\delta}_{g}, \\hat{\\delta}_{g'}$, etc., and how to obtain estimates for group-wise differences: -->\n\n<!-- $$ -->\n<!-- \\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y^a \\mid G=g] - \\hat{E}[Y^{a'} \\mid G=g] \\big)}^{\\hat{\\delta}_g} - \\overbrace{\\big( \\hat{E}[Y^{a'} \\mid G=g'] - \\hat{E}[Y^a \\mid G=g'] \\big)}^{\\hat{\\delta}_{g'}} -->\n<!-- $$ -->\n\n\n<!-- - **$\\hat{E}[Y^a \\mid G=g]$**: Estimated expected outcome when treatment $a$ is applied to subgroup $G=g$. -->\n<!-- - **$\\hat{E}[Y^{a'} \\mid G=g]$**: Estimated expected outcome when a different treatment or control $a'$ is applied to the same subgroup $G=g$. -->\n<!-- - **$\\hat{\\delta}_g$**: Represents the estimated treatment effect within subgroup $G=g$, computed as the difference in expected outcomes between treatment $a$ and $a'$ within this subgroup. -->\n\n<!-- - **$\\hat{E}[Y^{a'} \\mid G=g']$**: Estimated expected outcome when treatment $a'$ is applied to a different subgroup $G=g'$. -->\n<!-- - **$\\hat{E}[Y^a \\mid G=g']$**: Estimated expected outcome when treatment $a$ is applied to subgroup $G=g'$. -->\n<!-- - **$\\hat{\\delta}_{g'}$**: Represents the estimated treatment effect within subgroup $G=g'$, computed as the difference in expected outcomes between treatment $a'$ and $a$ within this subgroup. -->\n\n<!-- - **$\\hat{\\gamma}$**: The overall measure calculated from your formula represents the difference in treatment effects between two subgroups, $G=g$ and $G=g'$. It quantifies how the effect of switching between treatments $a$ and $a'$ differs across the two subgroups. -->\n\n\n<!-- ### Considerations -->\n\n<!-- - **Estimation**: to estimate the expected outcomes $\\hat{E}[Y^a \\mid G]$ and $\\hat{E}[Y^{a'} \\mid G]$, we require statistical models. If we use regression, we include interaction terms between treatment and subgroup indicators to directly estimate subgroup-specific treatment effects. Our use depends on correct model specification. -->\n<!-- - **Confidence intervals**: we may compute confidence intervals for $\\hat{\\gamma}$ using bootstrap, the delta method, or -- in our excercises -- simulation based methods. -->\n<!-- - **Causal assumptions**: again, a causal interpretation of $\\hat{\\gamma}$ relies on satisfying both causal assumptions and modelling assumptions.  Here, we have described estimation using propensity scores. -->\n\n\n<!-- ## Doubly Robust Estimation -->\n\n<!-- We can combine regression-based estimation with propensity score estimation to obtain *doubly robust* estimation. I will walk you through the steps in the lab. The TL;DR is this: doubly robust estimation reduces reliance on correct model specification. If either the PS model or the regression model is correctly specified, the model will be unbiased -- if the other causal inference assumptions are met. -->\n\n<!-- We cannot know whether these assumptions are met, we will need to do a sensitivity analysis, the topic of next week. -->\n\n<!-- I'll show you in lab how to employ simulation-based inference methods to compute standard errors and confidence intervals, following the approaches suggested by Greifer (2023)[]. -->\n\n<!-- ## Extra Readings n on Propensity Scores: -->\n\n<!-- Noah Griefer's Software and Blogs: [https://ngreifer.github.io/blog/subgroup-analysis-psm/](https://ngreifer.github.io/blog/) -->",
    "supporting": [
      "08-content_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}