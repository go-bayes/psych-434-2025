{
  "hash": "8cf2e9bf1cfc1a03da074cb6cc8f7c21",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Estimation of ATE and CATE Using Machine Learning\"\ndate: \"2025-APR-29\"\nformat:\n  html:\n    warnings: false\n    error: false\n    messages: false\n    code-overflow: scroll\n    highlight-style: kate\n    code-line-numbers: true\n    code-fold: false\n    code-tools:\n      source: true\n      toggle: false\nhtml-math-method: katex\nreference-location: margin\ncitation-location: margin\ncap-location: margin\ncode-block-border-left: true\nbibliography: /Users/joseph/GIT/templates/bib/references.bib\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n\n\n::: {.callout-note}\n**Required**\n- [https://grf-labs.github.io/grf/](https://grf-labs.github.io/grf/)\n\n\n\n**Optional**\n- [@vanderweele2020] [link](https://www.dropbox.com/scl/fi/srpynr0dvjcndveplcydn/OutcomeWide_StatisticalScience.pdf?rlkey=h4fv32oyjegdfl3jq9u1fifc3&dl=0)\n- [@suzuki2020] [link](https://www.dropbox.com/scl/fi/4midxwr9ltg9oce02e0ss/suzuki-causal-diagrams.pdf?rlkey=uktzf3nurtgpbj8m4h0xz82dn&dl=0)\n- [@Bulbulia2024PracticalGuide] [link](https://osf.io/preprints/psyarxiv/uyg3d)\n- [@hoffman2023] [link](https://arxiv.org/pdf/2304.09460.pdf)\n:::\n\n\n::: {.callout-important}\n### Key concepts  \nThe workflow below introduces **heterogeneous-treatment-effect (HTE) analysis** with *causal forests*. By the end of the lecture you should recognise five technical ideas - ATE, CATE, the estimator $\\widehat{\\tau}(x)$, the RATE statistics drawn from a **Targeting-Operator Characteristic** (TOC) curve, and **policy trees**—and know how each fits into an applied research pipeline.\n:::\n\n::: {.callout-important}\n### For the lab  \n\nThere are **Three** R scripts we will be using over the next few weeks. \n\nFirst:\n\n[Script 1](https://raw.githubusercontent.com/go-bayes/psych-434-2025/refs/heads/main/laboratory/01-example-script-data-wrangling.R)\n\n[Script 2](https://raw.githubusercontent.com/go-bayes/psych-434-2025/refs/heads/main/laboratory/02-example-script-data-wrangling-2.R)\n\n[Script 3](https://raw.githubusercontent.com/go-bayes/psych-434-2025/refs/heads/main/laboratory/03-example-script-estimation-results.R)\n\n:::\n\n\n\n## Heterogeneous-Treatment-Effect Analysis with **causal forests**\n\n### Why worry about heterogeneity?  \n\n\nRelying on the average treatment effect (ATE) is a bit like handing out size-nine shoes to an entire student body: on *average* they might fit, but watch the tall students hobble and the small ones trip.  In the same way, a one-hour boost in weekly community socialising could send some students' sense of belonging soaring while leaving others cold—or even wishing they'd stayed home with the cat.  Spotting that spread, measuring how big it really is, and deciding whether it is worth tailoring the 'shoe size' are the three practical goals of HTE analysis.\n\n\n---\n\n### 1 Start with the average treatment effect (ATE)  \n\nWe begin with the most straightforward (and secretly impossible) counterfactual: *run two parallel universes—one where **everyone** gets the treatment, another where **no-one** does—and compare the final scores.  The resulting difference is the **average treatment effect**:  \n\n\n$$\n\\text{ATE}=E\\!\\bigl[Y(1)-Y(0)\\bigr].\n$$\n\nThis gives us the average response -- the shoe size...\n\n\n---\n\n### 2 Do effects differ across people?  \n\nVariation is captured by the **conditional average treatment effect (CATE)**,  \n\n$$\n\\tau(x)=E\\!\\bigl[Y(1)-Y(0)\\mid X=x\\bigr],\n$$\n\nwhere $X$ gathers pre-treatment covariates -- age, baseline wellbeing, personality, etc... Normally these will be our baseline confounders. \n\nIf $\\tau(x)$ turns out to be flat, there is no heterogeneity worth targeting. \n\nPeople differ in countless, overlapping ways—think of age, baseline wellbeing, personality traits, study habits, and more. A linear interaction model tests whether the treatment works differently along one straight dimension, such as gender, by fitting a straight line. But real‐world data often twist and turn. If the true relationship bends like a garden hose, a straight line will miss the curve. Causal forests fix this by letting the data place splits wherever the shape changes, so they can follow any bends that appear [@wager2018].  Straight-line models are fine for simple patterns, but causal forests can trace the curves that simple lines overlook.\n\n\n### 3. From straight lines to trees  \n\nTraditional 'parametric' models (like simple regression) guess a single functional shape -- often a straight line -- before seeing the data.  A **non-parametric** model, by contrast, lets the data decide the shape.  A *regression tree* is the simplest non-parametric learner we will use.  \n\n1. **Regression tree**  \n\n*Idea*: split the covariate space by asking yes/no questions— 'Age ≤ 20?', 'Baseline wellbeing > 0.3?' — until each terminal **leaf** is fairly homogeneous.  Inside a leaf the predicted outcome is just the sample mean, so the tree builds a *piece-wise constant* surface instead of a global line.  \n\n*Analogy*: think of tiling a garden with stepping-stones: each stone is flat, but taken together they follow the ground’s contours.\n\n2. **Regression forest**  \n   A single tree is quick and interpretable but unstable: small changes in the data can move the splits and shift predictions.  A **random forest** grows many trees on bootstrap samples and averages their outputs.  Averaging cancels much of the noise [@breiman2001random].  \n\n3. **Causal Forests**  \n   To estimate treatment effects rather than outcomes, each tree plays a two-step 'honest' game [@wager2018]:  \n   - use one half of its sample to choose splits that separate treated from control units;  \n   - use the other half to compute treatment-control differences within every leaf.  \n\n   For a new individual with covariates $x_i$ each tree supplies a noisy leaf-level effect; the forest reports the **average**, written  \n\n$$\n  \\widehat{\\tau}(x)=E[Y(1)-Y(0)\\mid X=x].\n$$\n\nBecause the noisy estimates point in many directions, their average is markedly less variable -- *the wisdom of trees is a wisdom of crowds*.\n\nIn sum, a regression tree chops the data into locally flat chunks; a regression forest averages many such trees to smooth away chance idiosyncrasies; a causal forest adds honesty so that its averaged differences, though never directly observable for any one person, give our best data-driven forecast of individual treatment effects.\n\n---\n\n### 4 Built-in protection against over-fitting  \n\nHonesty already separates model selection from estimation, but the forest adds a second safeguard: **out-of-bag (OOB) prediction**. Each $\\widehat{\\tau}(x_i)$ is averaged only over trees that never used $i$ in their split phase.  Together, honesty and OOB prediction deliver reliable uncertainty estimates even in high-dimensional settings.\n\n---\n\n\n### 5 Handling missing data  \n\nThe `grf` package adopts **Missing Incorporated in Attributes (MIA)** splitting.  'Missing' can itself become a branch, so cases are neither discarded nor randomly imputed.  This pragmatic approach keeps all observations in play while preserving the forest’s interpretability.\n\n---\n\n### 6 Is the heterogeneity *actionable*? — RATE statistics  \n\n\nOnce we have a personalised score $\\widehat{\\tau}(x)$ for every unit, the practical question is whether *targeting* high scorers delivers a benefit large enough to justify the extra effort.  The tool of choice is the **Targeting-Operator Characteristic (TOC)** curve:\n\n$$\nG(q)=\\frac{1}{n}\\sum_{i=1}^{\\lfloor qn\\rfloor}\\widehat{\\tau}_{(i)}, \\qquad 0\\le q\\le1,\n$$\n\nwhere $\\widehat{\\tau}_{(1)}\\ge\\widehat{\\tau}_{(2)}\\ge\\cdots$ are the estimated effects sorted from largest to smallest.  The horizontal axis $q$ is the fraction of the population we would treat; the vertical axis $G(q)$ is the cumulative gain we expect from treating that top slice.\n\nTwo integrals of the TOC curve summarise how lucrative targeting could be:\n\n* **RATE AUTOC** (Area *Under* the TOC) puts equal weight on every $q$.  This answers: *If benefits are concentrated among the very best prospects, how much can we harvest by cherry-picking them?*  \n\n* **RATE Qini** applies heavier weight to the mid-range of $q$.  This is the go-to metric when investigators face a fixed, moderate-sized budget—say, \"we can afford to treat 40 % of individuals; will targeting help?\"  [@yadlowsky2021evaluating]. We will evaluate the curve at treatment of 20% and 50% of the population.\n\n\nTo quantify the economic or policy value of heterogeneity, rank units by $\\widehat{\\tau}(x)$ and draw a **Targeting-Operator Characteristic (TOC)** curve that plots cumulative gain against the fraction $q$ of the population treated.  \n\n---\n\n### 7 RATE AUTOC EXAMPLE\n\nAlthough OOB predictions are 'out-of-sample' for individual trees, the full forest still reuses information.  A simple remedy is to cut the data in half: **train** the forest on one fold and **test** RATE/Qini on the other.  This explicit split blocks optimistic bias and yields honest test statistics ($p$-values) [@grf2024].\n\n\n\n\n::: {.cell .column-screen}\n![RATE AUTOC: Hours Socialising → Sense of Meaning](08-content_files/figure-html/fig-rate-example-1.png){#fig-rate-example width=672}\n:::\n\n\n\n\n@fig-rate-example depicts a typical RATE AUTOC curve with sample splitting.  A steep initial rise indicates that a small, correctly targeted programme could deliver large gains. Note that the curve begins dipping below zero past about 30%. At that point we might be doing worse than the ATE by targeting the CATE.\n\n---\n\n### 8 Visualising policy value: the Qini curve  \n\nA **Qini curve** displays cumulative benefit on the vertical axis and treatment coverage on the horizontal.  As with the AUTOC curve we are using a held-out test fold to validate the reponse curve.\n\n\n\n\n::: {.cell .column-screen}\n![Qini Curve: Hours Socialising → Social Belonging](08-content_files/figure-html/fig-qini-example-1.png){#fig-qini-example width=672}\n:::\n\n\n\n\n\n\n\n@fig-qini-example: we find that focussing on the top 20 % of individuals nets a gain of 0.08 units (95 % CI 0.04–0.12).  Widening the net to 50 % bumps the haul to 0.13 units (95 % CI 0.07–0.19).  After that the curve flattens -- once you’ve treated everyone who offers a decent return, there are no more 'big fish' left to catch.\n\n---\n\n\n### 9 From 'a black box' to simple rules: policy trees  \n\nThe causal forest hands us a personalised CATE for every individual, mapping a **high-dimensional** covariate vector $X$ to a number $\\widehat{\\tau}(X)$.  Helpful as that forecast is, it stops short of telling us *what to do*: the function itself is too tangled --- thousands of overlapping splits -- to translate directly into a policy.  \n\nThe **policytree** algorithm bridges that gap by collapsing the forest's many $\\widehat{\\tau}(X)$ values into a single, shallow decision tree whose depth you choose; each split is chosen to maximise expected benefit [@policytree_package_2024].  In this course we cap the depth at **2** for a practical balance:\n\n- at most two yes/no questions per rule, so the logic fits on a slide you can present to policy-makers;\n- each leaf still contains enough observations to yield a stable effect estimate;  \n- deeper trees increase complexity faster than they improve payoff.\n\n\n\n\n::: {.cell .column-screen}\n![Decision tree for Social Belonging](08-content_files/figure-html/fig-decision-tree-1.png){#fig-decision-tree width=672}\n:::\n\n\n\n\n \n**Policy Tree Findings for Effect of Hour Socialising on Social Belonging:**\n\nParticipants are first split by Self Esteem at -0.925 (original scale: 3.958). For those with Self Esteem <= this threshold, the next split is by Neuroticism at 0.642 (original scale: 4.228). Within that subgroup, individuals with Neuroticism <= the threshold are recommended **control**, while those with Neuroticism > the threshold are recommended **treated**.\n\nFor participants with Self Esteem > -0.925 (original scale: 3.958), the second split is by Social Belonging at 0.776 (original scale: 5.972). In this subgroup, individuals with Social Belonging <= the threshold are recommended **treated**, while those with Social Belonging > the threshold are recommended **control**.\n\n**Policy Rule**\n\n> If self-esteem ≤ −0.93 and neuroticism ≤ 0.64, do **not** recommend extra socialising; otherwise, recommend it unless current belonging > 0.78.*\n\n\n\n\n\n![Predicted treatment assignment (predictions out of training sample)](08-content_files/figure-html/fig-policy-map-1.png){#fig-policy-map width=1536}\n\n\n\n\n\n\n---\n\n### 10 Ethical and practical considerations  \n\nStatistical optimality rarely lines up with social acceptability.  A rule that maximises expected health gains might still be **unaffordable** for a public agency, **unfair** to a protected group, or **opaque** to those asked to trust it.  Typically these trade-offs lie beyond the statistician’s remit (see the caveats in [Lecture 6](https://go-bayes.github.io/psych-434-2025/content/06-content.html#appendix-b-evidence-for-effect-modification-is-relative-to-inclusion-of-other-variables-in-the-model)).  \n\nYet the very same CATE machinery that powers targeting also helps science move past a *one-size-fits-all* mindset.  By mapping treatment effects across a high-dimensional covariate space, we can test whether our favourite categories -- gender, age group, clinical severity -- actually capture the differences that matter.  Sometimes they do; often they don't, revealing that nature is not carved at the joints of our folk classifications.  Discovering *where* the forest sees meaningful splits can generate fresh psychological hypotheses about who responds, why, and under what circumstances, even when no policy decision is on the table.\n\n\n<!-- Before deployment we therefore need three extra layers of scrutiny: -->\n\n<!-- 1. **Cost realism**   Will the programme's administrative and opportunity costs erase the forecast benefit?  A cost–effectiveness analysis can reveal when a simpler, less 'optimal' rule actually delivers better value.   -->\n<!-- 2. **Fairness auditing**  check whether error rates or treatment probabilities differ by gender, ethnicity, or socioeconomic status.  If gaps appear, consider adjusting the loss function or adding fairness constraints [@mitchell2021algorithmic].   -->\n<!-- 3. **Transparency and consent**   Publish the rule, document data sources, and secure stakeholder buy-in.  Transparent governance limits the risk of political backlash and encourages external replication. -->\n\n---\n\n### Summary/next steps  \n\nOur workflow answers three questions in sequence:\n\n1. **Is there substantial heterogeneity?**  Reject $H_0{:}\\tau(x)$ constant if RATE AUTOC or RATE Qini is positive and statistically reliable  \n2. **Does targeting pay at realistic budgets?**  Inspect the slope of the Qini curve around plausible coverage levels.\n3. **Can we express the targeting rule in a few defensible steps?**  fit and validate a shallow policy tree.\n\nIn the lab section you will reproduce each stage on a simulated dataset.\n\n---\n\n\n## Lab: Data Preparation and Analysis Scripts\n\n\n### Link to data dictionary\n\n::: {.callout-note}\nFor information about the variables in the synthetic data, download the New Zealand Attitudes and Values Data Dictionary here under \"Primary Resources\"\n\n[https://osf.io/75snb/](https://osf.io/75snb/)\n\n:::\n\n\n### Script 1:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# example script 1: data wrangling\n# spring 2025\n# example estimation of average treatment effect - script 1\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# restart fresh session if needed\n# rstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# save paths -------------------------------------------------------------------\n# specify the path where data will be saved\n# this is the path used by joseph\n# push_mods <- here::here('/Users/joseph/v-project\\ Dropbox/data/courses/25-psych-434')\n# replace with your own path after creating a data file\npush_mods <- here::here(\"data\")\n\n# load libraries ---------------------------------------------------------\n# install and load 'margot' package if not already installed\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\") # ensure version is at least 1.0.21\n}\n\n# install and load 'boilerplate' package if not already installed\nif (!require(boilerplate, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/boilerplate\")\n}\n\n# load required libraries\nlibrary(margot)\nlibrary(boilerplate)\nlibrary(tidyverse)\nlibrary(qs)\nlibrary(here)\n\n# import synthetic data ---------------------------------------------------\n# link to synthetic data\n# url <- \"https://www.dropbox.com/scl/fi/ru0ecayju04ja8ky1mhel/df_nz_long.qs?rlkey=prpk9a5v4vcg1ilhkgf357dhd&dl=1\"\n# \n# # download data to a temporary file\n# tmp <- tempfile(fileext = \".qs\")\n# download.file(url, tmp, mode = \"wb\")\n# \n# # read data into R\n# library(qs)\n# df_nz_long <- qread(tmp)\n# \n# # view first few rows of synthetic data\n# head(df_nz_long)\n# \n# # save data to your directory for later use and comment the above\n# margot::here_save_qs(df_nz_long,\"df_nz_long\" ,push_mods)\n\n# comment the above and henceforth read your data: \n# remove old data \nrm(df_nz_long)\n\n# read data from saved file\ndf_nz_long <- margot::here_read_qs(\"df_nz_long\", push_mods)\n\n# define study variables --------------------------------------------------------\n\n# view variable names in the dataset\nglimpse(df_nz_long)\n\n# count the number of unique participants\nlength(unique(df_nz_long$id))\n\n# define exposure variable\nname_exposure <- c(\"hours_community\")\nvar_labels_exposure = c(\"hours_community\" = \"Weekly Hours Community Socialising\",\n                        \"hours_community_binary\" = \"Weekly Hours Community Socialising (binary)\")\n\n# save variable labels for manuscript\nhere_save(var_labels_exposure, \"var_labels_exposure\")\n\n# define variable names for binary exposure\nname_exposure_binary <- paste0(name_exposure, \"_binary\")\nt0_name_exposure_continuous <- paste0(\"t0_\", name_exposure)\nt0_name_exposure_binary <- paste0(\"t0_\", name_exposure, \"_binary\")\n\n# define wide variable names\nt0_name_exposure <- paste0(\"t0_\", name_exposure)\nt0_name_exposure_continuous <- paste0(\"t0_\", name_exposure)\nt0_name_exposure_binary <- paste0(\"t0_\", name_exposure, \"_binary\")\n\n# define variable names for exposures used in models\nt1_name_exposure <- paste0(\"t1_\", name_exposure)\nt1_name_exposure_binary <- paste0(\"t1_\", name_exposure, \"_binary\")\n# define study waves -----------------------------------------------------------\nbaseline_wave <- \"2018\"\nexposure_waves <- c(\"2019\")\noutcome_wave <- \"2020\"\n\n# define wave combinations for analysis\nall_waves <- c(baseline_wave, exposure_waves, outcome_wave)\nbaseline_and_exposure_waves <- c(baseline_wave, exposure_waves)\nbaseline_and_outcome_waves <- c(baseline_wave, outcome_wave)\n\n# define scale ranges ----------------------------------------------------------\nscale_range_exposure <- c(0, 20)  # used for descriptive graphs\nscale_ranges_outcomes <- c(1, 7)  # used for descriptive graphs\n\n# check package versions\npackageVersion(pkg = 'margot')    # make sure it is 1.0.19 or greater\npackageVersion(pkg = 'boilerplate')\n\n# load required packages -------------------------------------------------------\npacman::p_load(\n  # causal inference\n  clarify,      # sensitivity analysis\n  cobalt,       # covariate balance tables and plots\n  lmtp,         # longitudinal targeted maximum likelihood estimation\n  margot,       # functions for causal inference\n  MatchIt,      # matching methods\n  MatchThem,    # matching for multiply imputed datasets\n  policytree,   # causal inference with policy trees\n  WeightIt,     # weighting methods for covariate balancing\n  \n  # data processing\n  data.table,   # fast data wrangling\n  fastDummies,  # fast creation of dummy variables\n  fs,           # cross-platform file system operations\n  here,         # simple and robust file referencing\n  janitor,      # data cleaning and validation\n  naniar,       # handling and visualization of missing data\n  skimr,        # summary statistics for data frames\n  tidyverse,    # collection of \"R\" packages for data science\n  \n  # machine learning\n  glmnet,       # lasso and elastic-net regularized models\n  grf,          # generalized random forests\n  ranger,       # fast implementation of random forests\n  SuperLearner, # ensemble learning\n  xgboost,      # extreme gradient boosting\n  \n  # visualization\n  DiagrammeR,   # graph and network visualization\n  ggbeeswarm,   # data visualization   \n  ggplot2,      # data visualization\n  gt,           # \"HTML\" tables for data frames\n  gtsummary,    # summary tables for regression models\n  kableExtra,   # advanced table formatting\n  \n  # parallel processing\n  doParallel,   # parallel processing with foreach\n  progressr,    # progress reporting for \"R\"\n  \n  # analysis\n  parameters,   # parameters and performance metrics\n  EValue        # compute e-values\n)\n# data preparation -------------------------------------------------------------\n# import and prepare data\n\n# get total sample size\nn_total <- skimr::n_unique(df_nz_long$id)\nn_total <- margot::pretty_number(n_total)\nmargot::here_save(n_total, \"n_total\")\n\n# view\nn_total\n\n# Define Baseline Variables ----------------------------------------------------\nbaseline_vars <- c(\n  # note all outcomes will be added to baseline vars later. \n  # demographics\n  \"age\",\n  \"born_nz_binary\",\n  \"education_level_coarsen\",\n  \"employed_binary\",\n  \"eth_cat\",\n  \"male_binary\",\n  \"not_heterosexual_binary\",\n  \"parent_binary\",\n  \"partner_binary\",\n  \"rural_gch_2018_l\",\n  \"sample_frame_opt_in_binary\",\n  \n  # personality traits\n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"neuroticism\",\n  \"openness\",\n  \n  # health and lifestyle\n  \"alcohol_frequency\",\n  \"alcohol_intensity\",\n  \"hlth_disability_binary\",\n  \"log_hours_children\",\n  \"log_hours_commute\",\n  \"log_hours_exercise\",\n  \"log_hours_housework\",\n  \"log_household_inc\",\n  # \"log_hours_community\", # commented out because using as exposure\n  \"short_form_health\",\n  \"smoker_binary\",\n  \n  # social and psychological\n  \"belong\",\n  \"nz_dep2018\",\n  \"nzsei_13_l\",\n  \"political_conservative\",\n  \"religion_identification_level\"\n)\n\n# Sort baseline variables\nbaseline_vars <- sort(baseline_vars)\n\n\n# baseline vars no log ----------------------------------------------------\n\nbaseline_vars_no_log <- c(\n  # Demographics\n  \"age\",\n  \"born_nz_binary\",\n  \"education_level_coarsen\",\n  \"eth_cat\",\n  \"employed_binary\",\n  \"male_binary\",\n  \"not_heterosexual_binary\",\n  \"parent_binary\",\n  \"partner_binary\",\n  \"rural_gch_2018_l\",\n  \"sample_frame_opt_in_binary\",\n  \n  # Personality traits\n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"neuroticism\",\n  \"openness\",\n  \n  # Health and lifestyle\n  \"alcohol_frequency\",\n  \"alcohol_intensity\",\n  \"hlth_disability_binary\",\n  \"log_hours_children\",\n  \"hours_children\",\n  \"log_hours_commute\",\n  \"hours_commute\",\n  \"log_hours_exercise\",\n  \"hours_exercise\",\n  \"log_hours_housework\",\n  \"hours_housework\",\n  \"log_household_inc\",\n  \"household_inc\",\n  # \"log_hours_community\",\n  \"hours_community\",\n  \"short_form_health\",\n  \"smoker_binary\",\n  \n  # Social and psychological\n  \"belong\",\n  \"nz_dep2018\",\n  \"nzsei_13_l\",\n  \"political_conservative\",\n  \"religion_identification_level\"\n)\n\n# define outcome variables ----------------------------------------------------\n# define outcome variables without log transformation\n# outcomes\noutcome_vars <- c(\n  \"alcohol_frequency\",\n  \"alcohol_intensity\",\n  \"belong\",\n  \"bodysat\",\n  \"forgiveness\",\n  \"gratitude\",\n  \"hlth_bmi\",\n  \"hlth_fatigue\",\n  \"hlth_sleep_hours\",\n  \"kessler_latent_anxiety\",\n  \"kessler_latent_depression\",\n  \"lifesat\",\n  \"log_hours_exercise\",\n  \"meaning_purpose\", \n  \"meaning_sense\",\n  \"neighbourhood_community\",\n  \"perfectionism\",\n  \"pwi\", \n  \"rumination\",\n  \"self_control\",\n  \"self_esteem\",\n  \"sexual_satisfaction\",\n  \"short_form_health\",\n  \"support\"\n)\n\n# sort and save outcome variables\noutcome_vars <- sort(outcome_vars)\nhere_save(outcome_vars, \"outcome_vars\")\n\n# Create standardized outcome variables\nt2_outcome_vars_z <- paste0(\"t2_\", outcome_vars, \"_z\")\n\n# view the result\nt2_outcome_vars_z\n\n\n# define outcome variables without log transformation\n# for tables\noutcome_vars_no_log <- c(\n  \"alcohol_frequency\",\n  \"alcohol_intensity\",\n  \"belong\",\n  \"bodysat\",\n  \"forgiveness\",\n  \"gratitude\",\n  \"hlth_bmi\",\n  \"hlth_fatigue\",\n  \"hlth_sleep_hours\",\n  \"hours_exercise\", # logged \n  \"kessler_latent_anxiety\",\n  \"kessler_latent_depression\",\n  \"lifesat\",\n  \"meaning_purpose\",\n  \"meaning_sense\",\n  \"neighbourhood_community\",\n  \"perfectionism\",\n  \"pwi\", \n  \"rumination\",\n  \"self_control\",\n  \"self_esteem\",\n  \"sexual_satisfaction\",\n  \"short_form_health\",\n  \"support\"\n)\n\n# exposure_var\nexposure_var = c(name_exposure, name_exposure_binary)\n\n# check\n# exposure_var_continuous = exposure_var[[1]]\n# exposure_var_continuous\n# \n# exposure_var_binary =  exposure_var[[2]]\n# exposure_var_binary\n\n# raw outcomes \nraw_outcomes_health <- c(\n  \"alcohol_frequency\",\n  \"alcohol_intensity\",\n  \"hlth_bmi\", \n  \"log_hours_exercise\", \n  \"hlth_sleep_hours\", \n  \"short_form_health\"\n)\n# sort\nraw_outcomes_health <- sort(raw_outcomes_health)\n\n# save \nhere_save(raw_outcomes_health, \"raw_outcomes_health\")\n\n# define psych outcomes\nraw_outcomes_psych <- c( \n  \"hlth_fatigue\", \n  \"kessler_latent_anxiety\", \n  \"kessler_latent_depression\",  \n  \"rumination\"\n)\n\n# sort\nraw_outcomes_psych <- sort(raw_outcomes_psych)\n\n# save\nhere_save(raw_outcomes_psych, \"raw_outcomes_psych\")\n\n# define present outcomes\nraw_outcomes_present <- c(\n  \"bodysat\",\n  \"forgiveness\",\n  \"perfectionism\", \n  \"self_control\" , \n  \"self_esteem\", \n  \"sexual_satisfaction\" \n)\n\n# sort\nraw_outcomes_present <- sort(raw_outcomes_present)\n\n# save\nhere_save(raw_outcomes_present, \"raw_outcomes_present\")\n\n# define life outcomes\nraw_outcomes_life <- c( \n  \"gratitude\", \n  \"lifesat\", \n  \"meaning_purpose\", \n  \"meaning_sense\",\n  \"pwi\"  # move personal well-being here if not using individual facents\n)\n\n# sort\nraw_outcomes_life <- sort(raw_outcomes_life)\n\n# save\nhere_save(raw_outcomes_life, \"raw_outcomes_life\")\n\n# define social outcomes\nraw_outcomes_social <- c(\n  \"belong\",\n  \"neighbourhood_community\", \n  \"support\" \n)\n# sort\nraw_outcomes_social <- sort(raw_outcomes_social)\n\n# save\nhere_save(raw_outcomes_social, \"raw_outcomes_social\")\n\n# save for publication\nraw_outcomes_all <- c(raw_outcomes_health, \n                      raw_outcomes_psych,\n                      raw_outcomes_present, \n                      raw_outcomes_life, \n                      raw_outcomes_social)\n\n# save\nhere_save(raw_outcomes_all, \"raw_outcomes_all\")\n\n#table1::table1(~ religion_spiritual_identification |wave, data = dat)\noutcome_vars <-sort(outcome_vars)\noutcome_vars_no_log<-sort(outcome_vars_no_log)\nextra_vars <- c(\"id\", \"wave\", \"year_measured\", \"not_lost\", \"sample_weights\") \nall_vars <- c(baseline_vars, exposure_var, outcome_vars, extra_vars)\n\nextra_vars_table <- extra_vars\nnot_all_vars<- c(baseline_vars_no_log, exposure_var, outcome_vars_no_log, extra_vars_table)\n\n# sort\nall_vars <- sort(all_vars)\nnot_all_vars <- sort(not_all_vars)\n\n# define columns that will later be handled in a special way\n# define continuous columns that we will not standardise\ncontinuous_columns_keep <- c(\"t0_sample_weights\")\n\n\n# define ordinal columns that we will expand into binary variables\nordinal_columns <- c(\"t0_education_level_coarsen\", \"t0_eth_cat\", \"t0_rural_gch_2018_l\")\n\n# checks\noverlap <- intersect(baseline_vars, outcome_vars)\nprint(overlap)\n\n# read and preprocess data ------------------------------------------------\ndf_nz_long$hours_community\n\n# prepare initial dataframe -----------------------------------------------\ndat_prep <- df_nz_long |>\n  arrange(id, wave) |>\n  as.data.frame() |>\n  margot::remove_numeric_attributes() |>\n  mutate(sample_weights = sample_weights) |> \n  mutate(religion_church = round( ifelse(religion_church > 8, 8, religion_church)),1) |>  # to simplify\n  arrange(id, wave) |>\n  droplevels()\n\n# define variable names using paste0 # *-- needed in this study --*\n# name_exposure_cat <- paste0(name_exposure, \"_cat\")\n# name_exposure_binary <- paste0(name_exposure, \"_binary\")\n\n# define wide variable names\nt0_name_exposure <- paste0(\"t0_\", name_exposure)\nt1_name_exposure <- paste0(\"t1_\", name_exposure)\nhere_save(t0_name_exposure , 't0_name_exposure')\n\n# eligibility  ------------------------------------------------------------\n# select baseline and exposure ids based on eligibility criteria \nids_baseline <- dat_prep |>\n  filter(year_measured == 1, wave == baseline_wave) |> \n  filter(!is.na(!!sym(name_exposure))) |> # exposure observed at baseline\n  pull(id)\n\n# get number of participants at baseline\nn_participants <- length(ids_baseline)\nn_participants <- margot::pretty_number(n_participants)\n\n# check number of participants\nn_participants\n\n# save number of participants\nhere_save(n_participants, \"n_participants\")\n\n# select ids for baseline cohort ------------------------------------------\n# eligibility: participated in baseline wave, no missingness in the exposure\n# may have been lost to follow up\ndat_long_1 <- dat_prep |>\n  filter(id %in% ids_baseline & wave %in% c(baseline_wave, exposure_waves, outcome_wave)) |> \n  droplevels()\n\n# check wave structure\nstr(dat_long_1$wave)\n\n# aside -------------------------------------------------------------------\n# example of censoring with more conditions\n\n# censor exposure if lost\n# ids_baseline_2 <- dat_long_censored_0 |>\n#   filter(year_measured == 1, wave == baseline_wave, employed == 1, hours_work >= 20) |>\n#   pull(id)\n# \n# dat_long_censored <- dat_long_censored_0 |>\n#   filter(id %in% ids_baseline_2 & wave %in% c(baseline_wave, exposure_waves, outcome_wave)) |> \n#   droplevels()\n\n# check for specific conditions in a subset\n# subset_vals <- dat_long_censored$year_measured[\n#   dat_long_censored$wave == 2021 & dat_long_censored$employed == 0\n# ]\n\n# check if all year_measured are zero in this subset\n# all_zero <- all(subset_vals == 0, na.rm = TRUE)\n\n# view result\n# all_zero\n\n# data with eligibility criteria\n# dat_long_1 <- dat_long_censored\n\n\n# aside over --------------------------------------------------------------\n\n# evaluate exposure variable(s)\ndat_long_exposure <- dat_long_1 |>\n  filter(wave == exposure_waves) |>\n  droplevels()\n\n# check wave counts\ntable(dat_long_exposure$wave)\n\n# check missingness in religion_church\ntable(is.na(dat_long_exposure[[exposure_var]]))\n\n# check exposure variable name\nname_exposure\n\n# check missingness in exposure\n# naniar::gg_miss_var(dat_long_exposure)\n\n# check quantile breaks for exposure\nquantile(dat_long_exposure[[name_exposure]], na.rm = TRUE, probs = seq(0, 1, .25))\nmean(dat_long_exposure[[name_exposure]], na.rm = TRUE)\nmedian(dat_long_exposure[[name_exposure]], na.rm = TRUE)\nmax(dat_long_exposure[[name_exposure]], na.rm = TRUE)\nsd(dat_long_exposure[[name_exposure]], na.rm = TRUE)\n\n# count above and below break at one\nsum(dat_long_exposure[[name_exposure]] >= 1, na.rm = TRUE)\nsum(dat_long_exposure[[name_exposure]] < 1, na.rm = TRUE)\n\n# binary graph ------------------------------------------------------------\n# graph binary break at lower quartile (5)\ngraph_exposure_binary <- margot::margot_plot_categorical(\n  dat_long_exposure,\n  col_name = name_exposure,\n  cutpoint_inclusive = \"lower\",\n  # n_divisions = 2,\n  custom_breaks = c(0,1),\n  binwidth = .5)\n\n# view binary graph\nprint(graph_exposure_binary)\n\n# check graph size\nmargot_size(graph_exposure_binary)\n\n# save binary graph for manuscript\nmargot::here_save_qs(graph_exposure_binary, \"graph_exposure_binary\", push_mods)\n\n# resume wrangling --------------------------------------------------------\n# create categorical variable (if desired)\n# check exposure variable name\nname_exposure\n\ndat_long_2 <- margot::create_ordered_variable(\n  dat_long_1,\n  var_name = name_exposure,\n  cutpoint_inclusive = \"lower\",\n  custom_breaks = c(0, 1)\n)\n\n# check binary exposure variable name\nname_exposure_binary\n\n# check missingness in binary exposure variable\ntable(is.na(dat_long_2[[name_exposure_binary]]))\n\n# convert binary factors to 0, 1\n# use custom function\ndat_long_3 <- margot_process_binary_vars(dat_long_2)\n\n# make 'not lost' variable ------------------------------------------------\n# used for attrition handling\ndat_long_4 <- dat_long_3 |>\n  arrange(id, wave) |>\n  mutate(\n    not_lost = ifelse(lead(year_measured) == 1, 1, 0),\n    not_lost = ifelse(is.na(not_lost) & year_measured == 1, 1, not_lost),\n    not_lost = ifelse(is.na(not_lost), 0, not_lost)\n  ) |>\n  droplevels()\n\n# log-transform 'hours_' variables ----------------------------------------\ndat_long_table <- margot_log_transform_vars(\n  dat_long_4,\n  vars = c(starts_with(\"hours_\"), \"household_inc\"),\n  exceptions = name_exposure,\n  prefix = \"log_\",\n  keep_original = TRUE # set to TRUE for tables\n) |>\n  select(all_of(not_all_vars)) |>\n  droplevels()\n\n# make wave a factor\ndat_long_table$wave <- as.factor(dat_long_table$wave)\n\n\n\n# summary tables ----------------------------------------------------------\n# useful function that creates summary tables in one place\nsummary_tables  <- margot_summary_tables(\n  data = dat_long_table,\n  baseline_wave = baseline_wave,\n  exposure_waves = exposure_waves,\n  outcome_wave = outcome_wave,\n  name_exposure = name_exposure,\n  baseline_vars = baseline_vars_no_log,\n  outcome_vars = outcome_vars,\n  extra_vars = extra_vars\n)\n\n# show details\nsummary_tables$baseline_table\nsummary_tables$exposure_table\nsummary_tables$outcome_table\nsummary_tables$n_participants\n\n\n# newer tables (for manuscript) -------------------------------------------\n# sort\nvar_labels_health <- list(\n  \"alcohol_frequency\" = \"Alcohol Frequency\",\n  \"alcohol_intensity\" = \"Alcohol Intensity\",\n  \"hlth_bmi\" = \"Body Mass Index\", \n  \"hlth_sleep_hours\" = \"Sleep\", \n  \"hours_exercise\" = \"Hours of Exercise\",   # logged in models\n  \"short_form_health\" = \"Short Form Health\" \n)\n\n# define psych outcomes \nvar_labels_psych <- list(\n  \"hlth_fatigue\" = \"Fatigue\", \n  \"kessler_latent_anxiety\" = \"Anxiety\", \n  \"kessler_latent_depression\" = \"Depression\",  \n  \"rumination\" = \"Rumination\"\n)\n\n# define present outcomes\nvar_labels_present <- list(\n  \"bodysat\" = \"Body Satisfaction\",\n  \"foregiveness\" = \"Forgiveness\",\n  \"perfectionism\" = \"Perfectionism\",\n  \"self_control\" = \"Self Control\",\n  \"self_esteem\" = \"Self Esteem\",\n  \"sexual_satisfaction\" = \"Sexual Satisfaction\"\n)\n\n# define life outcomes\nvar_labels_life <- list(\n  \"gratitude\" = \"Gratitude\", \n  \"lifesat\" = \"Life Satisfaction\", \n  \"meaning_purpose\" = \"Meaning: Purpose\", # exposure variable\n  \"meaning_sense\" = \"Meaning: Sense\",\n  \"pwi\" = \"Personal Well-being Index\"\n)\n\n# define social outcome names\nvar_labels_social <- list(\n  \"belong\" = \"Belonging\",\n  \"neighbourhood_community\" = \"Neighbourhood Belonging\", \n  \"support\" = \"Support\" \n)\n\n# make labels\nvar_labels_baseline <- list(\n  \"sdo\" = \"Social Dominance Orientation\",\n  \"belong\" = \"Social Belonging\",\n  \"born_nz_binary\" = \"Born in New Zealand (binary)\",\n  \"rural_gch_2018_l\" = \"Rural Gch 2018 Levels\",\n  \"education_level_coarsen\" = \"Education Level\",\n  \"employed_binary\" = \"Employed (binary)\",\n  \"eth_cat\" = \"Ethnicity\",\n  \"household_inc\" = \"Household Income\",\n  \"log_household_inc\" = \"Log Household Income\",\n  \"male_binary\" = \"Male (binary)\",\n  \"nz_dep2018\" = \"NZ Deprevation Index 2018\",\n  \"nzsei_13_l\" = \"NZSEI (Occupational Prestige Index)\",\n  \"parent_binary\" = \"Parent (binary)\",\n  \"rwa\" = \"Right Wing Authoritarianism\",\n  \"sample_frame_opt_in_binary\" = \"Sample Frame Opt-In (binary)\",\n  \"sdo\" = \"Social Dominance Orientation\",\n  \"smoker_binary\" = \"Smoker (binary)\",\n  \"support\" = \"Social Support (perceived)\",\n  \"hours_community\" = \"Hours Community Socialising\"\n)\n\n# labels for factors\nrural_labels <- c(\n  \"High Urban Accessibility\",\n  \"Medium Urban Accessibility\",\n  \"Low Urban Accessibility\",\n  \"Remote\",\n  \"Very Remote\"\n)\n\n# save labels -------------------------------------------------------------\nhere_save(var_labels_baseline, \"var_labels_baseline\")\nhere_save(var_labels_health, \"var_labels_health\")\nhere_save(var_labels_psych, \"var_labels_psych\")\nhere_save(var_labels_present, \"var_labels_present\")\nhere_save(var_labels_life, \"var_labels_life\")\nhere_save(var_labels_social, \"var_labels_social\")\n\n\n# combine all variable labels\nvar_labels_measures <- c(\n  var_labels_baseline,\n  var_labels_health,\n  var_labels_psych,\n  var_labels_present,\n  var_labels_life,\n  var_labels_social\n)\n\n# save\nhere_save(var_labels_measures, \"var_labels_measures\")\n\n# new df for table\ndat_long_table_x <- dat_long_table\ndat_long_table_x$rural_gch_2018_l <- factor(\n  dat_long_table_x$rural_gch_2018_l,\n  levels = 1:5,\n  labels = rural_labels,\n  ordered = TRUE  # optional: if the levels have an inherent order\n)\n\n# nicer for church attendance -- if you use it\n# dat_long_table_x$religion_church <- factor(\n#   dat_long_table_x$religion_church,\n#   levels = 0:8,\n#   ordered = TRUE  \n# )\n\n# create dataframes for each wave\ndat_long_table_baseline <- dat_long_table_x |> \n  filter(wave %in% c(baseline_wave, exposure_waves, outcome_wave))\n\n# create dataframes for each wave\ndat_long_table_exposure_waves <- dat_long_table_x |> \n  filter(wave %in% c(baseline_wave, exposure_waves))\n\n# create dataframes for each wave\ndat_long_table_outcome_waves <- dat_long_table_x |> \n  filter(wave %in% c(baseline_wave, outcome_wave))\n\n# make tables\nmarkdown_table_baseline <- margot_make_tables(\n  data = dat_long_table_baseline,\n  vars = baseline_vars_no_log,\n  by = \"wave\",\n  labels = var_labels_baseline,\n  factor_vars = c(\"rural_gch_2018_l\", \"eth_cat\"),\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\n\n# view\nmarkdown_table_baseline\n\n# save\nmargot::here_save(markdown_table_baseline, \"markdown_table_baseline\")\n\n# make exposure table\nmarkdown_table_exposures <- margot_make_tables(\n  data = dat_long_table_exposure_waves,\n  vars = name_exposure,\n  by = \"wave\",\n  labels = var_labels_exposure,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"\n)\n\n# view\nmarkdown_table_exposures\n\n# save\nhere_save(markdown_table_exposures, \"markdown_table_exposures\")\n\n# names of outcome vars for health no logs\nraw_outcomes_health_no_log <- c(\n  \"alcohol_intensity\",\n  \"alcohol_frequency\",\n  \"hlth_bmi\", \n  \"hlth_sleep_hours\", \n  \"hours_exercise\",\n  \"short_form_health\"\n)\n\n# make tables\n# make tables\n# latex_table_outcomes_all <- margot_make_tables(\n#   data = dat_long_table_outcome_waves,\n#   vars = raw_outcomes_all,\n#   by = \"wave\",\n#   labels = var_labels_measures,\n#   table1_opts = list(overall = FALSE, transpose = FALSE),\n#   format = \"latex\",\n#   kable_opts = list(\n#     booktabs = TRUE,\n#     longtable = TRUE,\n#     font_size = 10,\n#     latex_options = c(\"hold_position\", \"repeat_header\", \"striped\", \"longtable\")\n#   ),\n#   quarto_label = \"tbl-appendix-outcomes\"  # This is the key addition!\n# )\n# \n# cat(latex_table_outcomes_all)\n# # view\n# here_save(latex_table_outcomes_all, \"latex_table_outcomes_all\")\n\nmarkdown_table_outcomes_all <- margot_make_tables(\n  data = dat_long_table_outcome_waves,\n  vars = raw_outcomes_all,\n  by = \"wave\",\n  labels = var_labels_measures,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\",\n  quarto_label = \"tbl-sample-outcomes\"  # This is the key addition!\n)\n\n# view\nmarkdown_table_outcomes_all\n\n# save\nhere_save(markdown_table_outcomes_all, \"markdown_table_outcomes_all\")\n\n\n# data for study, remove originals ----------------------------------------\n## create gender weights if needed\ndat_long_selected<- dat_long_4\n\n\n# aside -------------------------------------------------------------------\n# only if gender weights\n# dat_long_selected$gender_weights <- margot_compute_gender_weights_by_wave(\n#   dat_long_selected,\n#   male_col = \"male_binary\",\n#   wave_col = \"wave\",\n#   target_wave = baseline_wave,\n#   target_male_prop = 0.52 # source https://www.stats.govt.nz/news/women-in-paid-work\n# )\n\n# end aside  --------------------------------------------------------------\n# checks\ntable(dat_long_selected$sample_weights[dat_long_selected$wave == baseline_wave])\n# hist(dat_long_selected$g_sample_weights)\n# hist(dat_long_selected$gender_weights)\n\n# prepare data with log transformations\ndat_long_prepare <- margot::margot_log_transform_vars(\n  dat_long_selected,\n  vars = c(starts_with(\"hours_\"), \"household_inc\"),\n  exceptions = name_exposure,\n  prefix = \"log_\",\n  keep_original = TRUE # set to false if you do not want to keep originals\n) |>\n  # uncomment the next two lines if using gender weights\n  # select(-sample_weights) |>\n  # dplyr::rename(sample_weights = gender_weights) |>\n  select(all_of(all_vars)) |>\n  droplevels()\n\n# check distribution and missingness\nhist(dat_long_selected$sample_weights)\ntable(is.na(dat_long_selected$sample_weights))\ntable(is.na(dat_long_selected$male_binary))\n\n# extract baseline data\ndat_baseline <- dat_long_prepare |>\n  filter(wave == baseline_wave)\n\n# check for missing values in baseline data\ntable(is.na(dat_baseline$sample_weights))\ntable(is.na(dat_baseline$male_binary))\n\n# extract sample weights from baseline\nt0_sample_weights <- dat_baseline$sample_weights # use age/gender/ethnicity\nhist(dat_baseline$sample_weights)\nhist(t0_sample_weights)\n\n# alternative weights option (commented out)\n# t0_sample_weights <- dat_baseline$gender_weights\n\n# save sample weights\nmargot::here_save(t0_sample_weights, \"t0_sample_weights\")\n\n# create tables -----------------------------------------------------------\nexposure_waves\n\n# create transition matrix for continuous exposure -----------------------\n# prepare data for transition matrix\ndt_positivity <- dat_long_selected |>\n  filter(wave == baseline_wave | wave %in% exposure_waves) |>\n  select(!!sym(name_exposure), id, wave) |>\n  mutate(exposure = as.numeric(!!sym(name_exposure))) |>\n  mutate(exposure = round(ifelse(exposure > 5, 5, exposure), 0)) |> \n  droplevels()\n\n# create transition table\ntransition_tables <- margot_transition_table(dt_positivity, \"exposure\", \"id\", wave_var = \"wave\")\n\n# check explanation of transition tables\ncat(transition_tables$explanation)\n\n# view first transition table\ntransition_tables$tables[[1]]\n\n# generate code for quarto document\ntransition_tables$quarto_code()\n\n# save transition tables for later use\nhere_save(transition_tables, \"transition_tables\")\n\n# create transition matrix for binary exposure ---------------------------\n# prepare data for binary transition matrix\ndt_positivity_binary <- dat_long_selected |>\n  filter(wave == baseline_wave | wave %in% exposure_waves) |>\n  select(!!sym(name_exposure_binary), id, wave) |>\n  mutate(exposure_binary = as.numeric(!!sym(name_exposure_binary))) |>\n  droplevels()\n\n# create binary transition table\ntransition_tables_binary <- margot_transition_table(\n  dt_positivity_binary, \n  \"exposure_binary\", \n  \"id\", \n  wave_var = \"wave\"\n)\n\n# view binary transition table\ntransition_tables_binary$tables[[1]]\n\n# check explanation for binary transition tables\ncat(transition_tables_binary$explanation)\n\n# generate code for quarto document\ntransition_tables_binary$quarto_code()\n\n# save binary transition tables\nmargot::here_save(transition_tables_binary, \"transition_tables_binary\")\n\n# check missing values ---------------------------------------------------\n# examine overall missingness\nnaniar::miss_var_summary(dat_long_prepare) |> print(n = 100)\n\n# filter to baseline wave\ndat_baseline <- dat_long_prepare |> filter(wave == baseline_wave)\n\n# calculate percent missing at baseline\npercent_missing_baseline <- naniar::pct_miss(dat_baseline)\n\n# view missing percentage\npercent_missing_baseline\n\n# save for manuscript\nhere_save(percent_missing_baseline, \"percent_missing_baseline\")\n\n# visualise missingness patterns\nnaniar::vis_miss(dat_baseline, warn_large_data = FALSE)\n\n# visualise missingness by variable\nnaniar::gg_miss_var(dat_baseline)\n\n# save the data ----------------------------------------------------------\n# save prepared dataset\nmargot::here_save(dat_long_prepare, \"dat_long_prepare\")\nmargot::here_save(name_exposure, \"name_exposure\")\n\n# save variable names for later use\nmargot::here_save(baseline_vars, \"baseline_vars\")\nmargot::here_save(exposure_var, \"exposure_var\")\nmargot::here_save(outcome_vars, \"outcome_vars\")\nmargot::here_save(baseline_wave, \"baseline_wave\")\nmargot::here_save(exposure_waves, \"exposure_waves\")\nmargot::here_save(outcome_wave, \"outcome_wave\")\nmargot::here_save(continuous_columns_keep, \"continuous_columns_keep\")\nmargot::here_save(ordinal_columns, \"ordinal_columns\")\n\n# visualise individual patterns ------------------------------------------\n# create plot of individual response trajectories\nindividual_plot_exposure <- margot_plot_individual_responses(\n  dat_long_1,\n  y_vars = name_exposure,\n  id_col = \"id\",\n  waves = c(2018:2019), # only measured for two waves\n  theme = theme_classic(),\n  random_draws = 80,\n  title = NULL,\n  y_label = NULL,\n  x_label = NULL,\n  color_palette = NULL,\n  include_timestamp = FALSE,\n  # save_path = here::here(push_mods), # uncomment to save\n  width = 16,\n  height = 8,\n  seed = 123,\n  full_response_scale = TRUE,\n  scale_range = scale_range_exposure\n)\n\n# display individual trajectories plot\nindividual_plot_exposure\n\n# check plot dimensions\nmargot_size(individual_plot_exposure)\n```\n:::\n\n\n\n\n\n### Script 2\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 02-data-wrangling-grf-model\n# get data into wide format and ready for modelling using grf\n# joseph.bulbulia@gmail.com\n# april 2025\n\n# restart fresh session\n# rstudioapi::restartSession()\n# set reproducibility\nset.seed(123)\n\n# save paths -------------------------------------------------------------------\n# create a save path to your on computer\n# this is mine\n# push_mods <- here::here('/Users/joseph/v-project\\ Dropbox/data/courses/25-psych-434')\n# yours might be (after creating a data file)\npush_mods <- here::here(\"data\")\n\n# load libraries ---------------------------------------------------------\n# install and load 'margot' package\n# make sure you have at least margot 1.0.21\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n}\n\nif (!require(boilerplate, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/boilerplate\")\n}\n# load margot library\nlibrary(margot)\n\n# load necessary libraries\npacman::p_load(\n  clarify,     # sensitivity analysis for causal inference\n  cobalt,      # covariate balance tables and plots\n  DiagrammeR,  # graph and network visualization\n  doParallel,  # parallel processing with foreach\n  fastDummies, # fast creation of dummy variables\n  fs,          # cross-platform file system operations\n  ggbeeswarm,  # data visualisation\n  ggplot2,     # data visualisation\n  glmnet,      # lasso and elastic-net regularized models\n  grf,         # generalized random forests\n  gt,          # html tables for data frames\n  gtsummary,   # summary tables for regression models\n  here,        # simple and robust file referencing\n  janitor,     # data cleaning and validation\n  kableExtra,  # advanced table formatting\n  lmtp,        # longitudinal targeted maximum likelihood estimation\n  MatchIt,     # matching methods for causal inference\n  MatchThem,   # matching methods for multiply imputed datasets\n  naniar,      # handling and visualization of missing data\n  parameters,  # parameters and performance metrics\n  policytree,  # causal inference with policy trees\n  progressr,   # progress reporting for R\n  ranger,      # fast implementation of random forests\n  skimr,       # summary statistics for data frames\n  SuperLearner,# ensemble learning\n  tidyverse,   # collection of R packages for data science\n  WeightIt,    # weighting methods for covariate balancing\n  xgboost,     # extreme gradient boosting\n  EValue,      # compute Evalues\n  data.table,  # fast data wrangling\n  maq,         # qini curves\n  purrr,       # data wrangling\n  patchwork,   # multiple plots\n  labelled,\n  purrr,\n  boilerplate\n)\n\n# check\npush_mods\n\n# read data in\ndat_long_prepare <- margot::here_read(\"dat_long_prepare\")\nname_exposure <- margot::here_read(\"name_exposure\")\n\n# check\nname_exposure_binary = paste0(name_exposure, \"_binary\")\nname_exposure_continuous = name_exposure\n\n# check\nname_exposure_binary\nname_exposure_continuous\n\n# get vars\nbaseline_vars <- margot::here_read(\"baseline_vars\")\noutcome_vars <- margot::here_read(\"outcome_vars\")\n\nbaseline_wave <- margot::here_read(\"baseline_wave\")\nexposure_waves <- margot::here_read(\"exposure_waves\")\noutcome_wave <- margot::here_read(\"outcome_wave\")\nt0_sample_weights <- margot::here_read(\"t0_sample_weights\")\n\n# define wide variable names\nt0_name_exposure_binary <- paste0(\"t0_\", name_exposure_binary)\n\n# make exposure names (continuous not generally used)\nt1_name_exposure_binary <- paste0(\"t1_\", name_exposure_binary)\nt1_name_exposure_binary\n\nt0_name_exposure_continuous <- paste0(\"t0_\", name_exposure)\nt0_name_exposure_continuous\n\n# make exposure names (continuous not generally used)\nt1_name_exposure_continuous <- paste0(\"t1_\", name_exposure)\nt1_name_exposure_continuous\n\n# ordinal use\nordinal_columns <- c(\n  \"t0_education_level_coarsen\",\n  \"t0_eth_cat\",\n  \"t0_rural_gch_2018_l\",\n  \"t0_gen_cohort\"\n)\n\n# keep sample weights without standardisation\ncontinuous_columns_keep <- c(\"t0_sample_weights\")\n\n# prepare data for analysis ----------------------\ndat_long_prepare <- margot::remove_numeric_attributes(dat_long_prepare)\n\n# update wave to numeric\ndat_long_prepare_updated <- dat_long_prepare %>%\n  mutate(wave = as.numeric(factor(wave, levels = sort(unique(wave)))))\n\n# make both exposure variables\nname_exposure_both <- c(name_exposure_binary,name_exposure_continuous)\n# check\nname_exposure_both\n\n# wide data\ndf_wide <- margot::margot_wide(\n  dat_long_prepare_updated,\n  baseline_vars = baseline_vars,\n  exposure_var = name_exposure_both,\n  outcome_vars = outcome_vars\n)\n\n# check\nhead(df_wide)\n\n# check\n# remove baseline binary\nt0_sample_weights\n\n# add weights back to data\ndf_wide$t0_sample_weights <- t0_sample_weights\n\n# make sure that rural is a factor\ndf_wide$t0_rural_gch_2018_l <- as.factor(df_wide$t0_rural_gch_2018_l)\n\n# save the wide data\nmargot::here_save(df_wide, \"df_wide\")\n\n# naniar::vis_miss(df_wide, warn_large_data = FALSE)\n# read back if needed\n# df_wide <- margot::here_read(\"df_wide\")\n\n# check\ncolnames(df_wide)\n\n# this will order the data correctly\n# see margot documentation\n# standardise outcomes, create not-lost indicator\ndf_wide_encoded <- margot::margot_process_longitudinal_data_wider(\n  df_wide,\n  ordinal_columns = ordinal_columns,\n  continuous_columns_keep = continuous_columns_keep,\n  not_lost_in_following_wave = \"not_lost_following_wave\",\n  lost_in_following_wave = \"lost_following_wave\",\n  remove_selected_columns = TRUE,\n  exposure_var = name_exposure_both,\n  scale_continuous = TRUE,\n  censored_if_any_lost = FALSE # not relevant here\n)\n\n# check\ncolnames(df_wide_encoded)\n\n# check\ntable(df_wide_encoded$t0_not_lost_following_wave)\n\n# make the binary variable numeric! \ndf_wide_encoded[[t0_name_exposure_binary]] <- as.numeric(df_wide_encoded[[t0_name_exposure_binary]]) - 1\ndf_wide_encoded[[t1_name_exposure_binary]] <- as.numeric(df_wide_encoded[[t1_name_exposure_binary]]) - 1\n\n# check\ntable(df_wide_encoded[[t0_name_exposure_binary]])\ntable(df_wide_encoded[[t1_name_exposure_binary]])\n\n# save data for models\nhere_save_qs(df_wide_encoded, \"df_wide_encoded\", push_mods)\n\n# save data for models\n# make exposure numeric\n\n# check\ncolnames(df_wide_encoded)\ntable(is.na(df_wide_encoded[[t1_name_exposure_binary]]))\ntable(df_wide_encoded$t0_lost_following_wave)\n\n# check\nstopifnot(\n  all(\n    is.na(df_wide_encoded[[t1_name_exposure_binary]]) |\n      df_wide_encoded[[\"t0_not_lost_following_wave\"]] == 1\n  )\n)\n\n# naniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\nnaniar::gg_miss_var(df_wide_encoded)\n\ndf_wide_encoded$t0_sample_weights\n\n# predict attrition and create censoring weights --------------------------\n# step 1: prepare baseline covariates\nE <- df_wide_encoded %>%\n  select(\n    -all_of(t0_name_exposure_binary), # inserted by function but irrelevant\n    -\"t0_sample_weights\") %>%\n  select(starts_with(\"t0_\"),\n         -ends_with(\"_lost\"),\n         -ends_with(\"lost_following_wave\")) %>%\n  colnames() %>%\n  sort()\n\n# note for baseline confounding we use the continuous var\nE # we use the continuous variable\n\n# save baseline covariates\nmargot::here_save(E, \"E\")\n\n# view\nprint(E)\n\n# step 2: calculate weights for t0\nD_0 <- as.factor(df_wide_encoded$t0_lost_following_wave)\n\n# get covariates\ncen_0 <- df_wide_encoded[, E]\n\n# probability forest for censoring\ncen_forest_0 <- probability_forest(cen_0, D_0)\n\n# save if needed, very large!\n# here_save_qs(cen_forest_0, \"cen_forest_0\", push_mods)\n\n# predict\npredictions_grf_0 <- predict(cen_forest_0, newdata = cen_0, type = \"response\")\n\n# get probability of censoring\npscore_0 <- predictions_grf_0$pred[, 2]\n\n# view\nhist(pscore_0)\n\n# check correct sample weights  \nhist(t0_sample_weights)\n\n# use margot_adjust_weights for t0\nt0_weights <- margot_adjust_weights(\n  pscore = pscore_0,\n  trim = TRUE,\n  normalize = TRUE,\n  # lower trimming\n  lower_percentile = 0.01,\n  upper_percentile = 0.99,\n  # upper trimming\n  censoring_indicator = df_wide_encoded$t0_lost_following_wave,\n  sample_weights = df_wide_encoded$t0_sample_weights \n)\n\n# view\nlength(t0_weights$adjusted_weights)\nlength(df_wide_encoded$t0_sample_weights)\n\n# check\nnrow(df_wide_encoded)\nhist(t0_weights$adjusted_weights)\n\n# assign weights\ndf_wide_encoded$t0_adjusted_weights <- t0_weights$adjusted_weights\n\n# if you only wanted censoring weights\n# df_wide_encoded$t0_propensity_score_model_weights <- t0_weights$censoring_weights\n\n# check\nnaniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\n\n# view\nhead(df_wide_encoded)\ncolnames(df_wide_encoded)\n\n# remove lost next wave (censored)\ndf_wide_encoded_1 <- df_wide_encoded %>%\n  filter(t0_lost_following_wave == 0) %>%\n  droplevels()\n\n# step 4: calculate weights for t1\nE_and_exposure <- c(E, t1_name_exposure_continuous)\nD_1 <- as.factor(df_wide_encoded_1$t1_lost_following_wave)\ncen_1 <- df_wide_encoded_1[, E_and_exposure]\n\n# probability forest for censoring\ncen_forest_1 <- probability_forest(cen_1, D_1, sample.weights = df_wide_encoded_1$t0_adjusted_weights)\n\n# save if needed, very large\n# here_save(cen_forest_1, \"cen_forest_1\")\n\npredictions_grf_1 <- predict(cen_forest_1, newdata = cen_1, type = \"response\")\npscore_1 <- predictions_grf_1$pred[, 2]\n\n# check\nhist(pscore_1)\n\n# use margot_adjust_weights for t1\nt1_weights <- margot_adjust_weights(\n  pscore = pscore_1,\n  trim = TRUE,\n  normalize = TRUE,\n  lower_percentile = 0.01,\n  upper_percentile = 0.99,\n  # upper trimming\n  censoring_indicator = df_wide_encoded_1$t1_lost_following_wave,\n  sample_weights = df_wide_encoded_1$t0_adjusted_weights # combine with weights\n)\n\n# check\nhist(t1_weights$adjusted_weights)\n\n# add weights -- these will be the weights we use\ndf_wide_encoded_1$t1_adjusted_weights <- t1_weights$adjusted_weights\n\n# calculate summary statistics\nt0_adjusted_weight_summary <- summary(df_wide_encoded$t0_adjusted_weights)\nt1_adjusted_weight_summary <- summary(df_wide_encoded_1$t1_adjusted_weights)\n\n# view summaries\nt0_adjusted_weight_summary\nt1_adjusted_weight_summary\n\n# check\nnaniar::vis_miss(df_wide_encoded_1, warn_large_data = FALSE)\n\n# save\nhere_save(df_wide_encoded_1, \"df_wide_encoded_1\")\n\n# read if needed\n# df_wide_encoded_1 <- here_read(\"df_wide_encoded_1\")\n\n# check names\ncolnames(df_wide_encoded_1)\n\n# check\ndf_wide_encoded_1[[t1_name_exposure_binary]]\n\n# step 5: prepare final dataset\nnrow(df_wide_encoded_1)\ntable(df_wide_encoded_1$t1_lost_following_wave)\n\n# arrange\ndf_grf <- df_wide_encoded_1 |>\n  filter(t1_lost_following_wave == 0) |>\n  select(\n    where(is.factor),\n    ends_with(\"_binary\"),\n    ends_with(\"_lost_following_wave\"),\n    ends_with(\"_z\"),\n    ends_with(\"_weights\"),\n    starts_with(\"t0_\"),\n    starts_with(\"t1_\"),\n    starts_with(\"t2_\"),\n  ) |>\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\")) |>\n  relocate(starts_with(\"t1_\"), .before = starts_with(\"t2_\")) |>\n  relocate(\"t0_not_lost_following_wave\", .before = starts_with(\"t1_\")) |>\n  relocate(all_of(t1_name_exposure_binary), .before = starts_with(\"t2_\")) |>\n  droplevels()\n\n# save final data\nmargot::here_save(df_grf, \"df_grf\")\n\n# check final dataset\n#check\ncolnames(df_grf)\n\n# visualise missing\nnaniar::vis_miss(df_grf, warn_large_data = FALSE)\n\n# check weights\ndf_grf$t1_adjusted_weights\n\n#checks\ncolnames(df_grf)\nstr(df_grf)\n\n# check exposures\ntable(df_grf[[t1_name_exposure_binary]])\n\n# check\nhist(df_grf$t1_adjusted_weights)\n\n# calculate summary statistics\nt0_weight_summary <- summary(df_wide_encoded)\n\n# check\nglimpse(df_grf$t0_adjusted_weights)\n\n# visualise weight distributions\nhist(df_grf$t0_adjusted_weights, main = \"t0_stabalised weights\", xlab = \"Weight\")\n\n# visualise and check missing values\nnaniar::vis_miss(df_grf, warn_large_data = FALSE)\nnaniar::gg_miss_var(df_grf)\nnaniar::miss_var_summary(dat_long_prepare) |> print(n = 100)\n\n# check n\nn_observed_grf <- nrow(df_grf)\n\n# view\nn_observed_grf\n\n# save\nmargot::here_save(n_observed_grf, \"n_observed_grf\")\n\n\n# inspect propensity scores -----------------------------------------------\n# get data # read in if needed\n# df_grf <- here_read('df_grf')\n\n# assign weights var name\nweights_var_name = \"t0_adjusted_weights\"\n\n# baseline covariates  # E already exists and is defined\nE\n\n# must be a data frame, no NA in exposure\n\n# df_grf is a data frame - we must process this data frame in several steps\n# user to specify which columns are outcomes, default to 'starts_with(\"t2_\")'\ndf_propensity_org <- df_grf |> select(!starts_with(\"t2_\"))\n\n# Remove NAs and print message that this has been done\ndf_propensity <- df_propensity_org |> drop_na() |> droplevels()\n\n# first run model for baseline propensity if this is selected.  The default should be to not select it.\npropensity_model_and_plots <- margot_propensity_model_and_plots(\n  df_propensity = df_propensity,\n  exposure_variable = t1_name_exposure_binary,\n  baseline_vars = E,\n  weights_var_name = weights_var_name,\n  estimand = \"ATE\",\n  method = \"ebal\",\n  focal = NULL\n)\n\n# visualise\nsummary(propensity_model_and_plots$match_propensity)\n\n# key plot\npropensity_model_and_plots$love_plot\n\n# other diagnostic plots\npropensity_model_and_plots$summary_plot\npropensity_model_and_plots$balance_table\npropensity_model_and_plots$diagnostics\n\n\n# check size\nsize_bytes <- object.size(propensity_model_and_plots)\nprint(size_bytes, units = \"auto\") # Mb\n\n# use qs to save only if you have space\nhere_save_qs(propensity_model_and_plots,\n             \"propensity_model_and_plots\",\n             push_mods)\n```\n:::\n\n\n\n\n\n### Script 3\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 03-estimating-models-grf\n# joseph.bulbulia@vuw.ac.nz\n# april 2025\n# devtools::load_all(\"/Users/joseph/GIT/margot/\") # use version >= 1.0.21 \n# for information on causal forests see: \n# https://grf-labs.github.io/grf/\n\n# ** NOTE THESE DATA ARE SIMULATED SO RESULTS ARE NOT INTERPRETABLE **\n\n# restart fresh session\n#rstudioapi::restartSession()\n\n# set reproducibility\nset.seed(123)\n\n\n# save paths -------------------------------------------------------------------\n# create a save path to your on computer\n# this is mine\n# push_mods <- here::here('/Users/joseph/v-project\\ Dropbox/data/courses/25-psych-434')\n# yours might be (after creating a data file)\n\n# data path\npush_mods <- here::here(\"data\")\n\n\n# load libraries ---------------------------------------------------------\n# install and load 'margot' package\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n}\n\n# load margot library\nlibrary(margot)\n\n# load necessary libraries\npacman::p_load(\n  clarify,      # sensitivity analysis for causal inference\n  cobalt,       # covariate balance tables and plots\n  DiagrammeR,   # graph and network visualisation\n  doParallel,   # parallel processing with foreach\n  fastDummies,  # fast creation of dummy variables\n  fs,           # cross-platform file system operations\n  ggplot2,      # data visualisation\n  glmnet,       # lasso and elastic-net regularised models\n  grf,          # generalised random forests\n  gt,           # html tables for data frames\n  gtsummary,    # summary tables for regression models\n  here,         # simple and robust file referencing\n  janitor,      # data cleaning and validation\n  kableExtra,   # advanced table formatting\n  lmtp,         # longitudinal targeted maximum likelihood estimation\n  margot,       # functions for casual inference\n  MatchIt,      # matching methods for causal inference\n  MatchThem,    # matching methods for multiply imputed datasets\n  naniar,       # handling and visualisation of missing data\n  parameters,   # parameters and performance metrics\n  policytree,   # causal inference with policy trees\n  progressr,    # progress reporting for R\n  ranger,       # fast implementation of random forests\n  skimr,        # summary statistics for data frames\n  SuperLearner, # ensemble learning\n  tidyverse,    # collection of R packages for data science\n  WeightIt,     # weighting methods for covariate balancing\n  xgboost,      # extreme gradient boosting\n  EValue,       # compute Evalues\n  data.table,   # fast data wrangling\n  maq,          # qini curves\n  purrr,        # data wrangling\n  patchwork,    # multiple plots\n  labelled,     # allows working with labelled data (variable and value labels)\n  cli,          # tools for building command line interfaces\n  crayon,       # colored terminal output\n  rlang,        # tools for programming with R\n  kableExtra    # enhance kable output with additional formatting\n)\n\n\n# variable names and labels -----------------------------------------------\n# import exposure variable (binary) \nname_exposure <- margot::here_read(\"name_exposure\")\nname_exposure\n\n# make exposure names\nt1_name_exposure_binary <- paste0(\"t1_\", name_exposure, \"_binary\")\n\n# check exposure name\nt1_name_exposure_binary\n\n# read health outcomes \nraw_outcomes_health <- here_read(\"raw_outcomes_health\")\nt2_outcome_health_z <- paste0(\"t2_\", raw_outcomes_health, \"_z\")\nt2_outcome_health_z <- sort(t2_outcome_health_z)\n\n# read raw outcomes \nraw_outcomes_psych <- here_read(\"raw_outcomes_psych\")\nt2_outcome_psych_z <- paste0(\"t2_\", raw_outcomes_psych, \"_z\")\nt2_outcome_psych_z <- sort(t2_outcome_psych_z)\n\n# read raw outcomes\nraw_outcomes_present <- here_read(\"raw_outcomes_present\")\nt2_outcome_present_z <- paste0(\"t2_\", raw_outcomes_present, \"_z\")\nt2_outcome_present_z <- sort(t2_outcome_present_z)\n\n# read raw outcomes\nraw_outcomes_life <- here_read(\"raw_outcomes_life\")\nt2_outcome_life_z <- paste0(\"t2_\", raw_outcomes_life, \"_z\")\nt2_outcome_life_z <- sort(t2_outcome_life_z)\n\n# read raw outcomes\nraw_outcomes_social <- here_read(\"raw_outcomes_social\")\nt2_outcome_social_z <- paste0(\"t2_\", raw_outcomes_social, \"_z\")\nt2_outcome_social_z <- sort(t2_outcome_social_z)\n\n# combine all raww outcome names\nraw_outcomes_all <- c(raw_outcomes_health, \n                      raw_outcomes_psych,\n                      raw_outcomes_present, \n                      raw_outcomes_life, \n                      raw_outcomes_social)\n\n# save all outcomes\nhere_save(raw_outcomes_all, \"raw_outcomes_all\")\n\n\n# label mappings for health outcomes\nlabel_mapping_health <- list(\n  \"t2_alcohol_frequency\" = \"Alcohol Frequency\",\n  \"t2_alcohol_intensity\" = \"Alcohol Intensity\",\n  \"t2_hlth_bmi_z\" = \"BMI\", \n  \"t2_hlth_sleep_hours_z\" = \"Sleep\", \n  \"t2_log_hours_exercise_z\" = \"Hours of Exercise (log)\",\n  \"t2_short_form_health_z\" = \"Short Form Health\" \n)\n\n# label mappings for psychological well-being outcomes\nlabel_mapping_psych <- list(\n  \"t2_hlth_fatigue_z\" = \"Fatigue\", \n  \"t2_kessler_latent_anxiety_z\" = \"Anxiety\", \n  \"t2_kessler_latent_depression_z\" = \"Depression\",  \n  \"t2_rumination_z\" = \"Rumination\"\n)\n\n# label mappings for present reflective outcomes\nlabel_mapping_present <- list(\n  \"t2_bodysat_z\" = \"Body Satisfaction\",\n  \"t2_foregiveness_z\" = \"Forgiveness\",  \n  \"t2_perfectionism_z\" = \"Perfectionism\",  \n  \"t2_self_control_z\" = \"Self Control\",  \n  \"t2_sexual_satisfaction_z\" = \"Sexual Satisfaction\"\n)\n\n# label mappings for life reflective outcomes\nlabel_mapping_life <- list(\n  \"t2_gratitude_z\" = \"Gratitude\", \n  \"t2_lifesat_z\" = \"Life Satisfaction\", \n  \"t2_meaning_purpose_z\" = \"Meaning: Purpose\",  \n  \"t2_meaning_sense_z\" = \"Meaning: Sense\", \n  \"t2_pwi_z\" = \"Personal Well-being Index\"\n)\n\n# label mappings for social outcomes\nlabel_mapping_social <- list(\n  \"t2_belong_z\" = \"Social Belonging\",\n  \"t2_neighbourhood_community_z\" = \"Neighbourhood Community\", \n  \"t2_support_z\" = \"Social Support\" \n)\n\n# n participants (can be useful for graphs)\nn_participants <- here_read(\"n_participants\")\n\n# label mapping all -------------------------------------------------------\nlabel_mapping_all <- c(\n  label_mapping_health,\n  label_mapping_psych,\n  label_mapping_present,\n  label_mapping_life,\n  label_mapping_social\n)\n\n\n# save label mappings -----------------------------------------------------\nhere_save(label_mapping_health, \"label_mapping_health\")\nhere_save(label_mapping_psych, \"label_mapping_psych\")\nhere_save(label_mapping_present, \"label_mapping_present\")\nhere_save(label_mapping_life, \"label_mapping_life\")\nhere_save(label_mapping_social, \"label_mapping_social\")\nhere_save(label_mapping_all, \"label_mapping_all\")\n\n# start analysis here ----------------------------------------------------\n# import data\ndf_grf <- margot::here_read('df_grf')\n\n# check\ncolnames(df_grf)\n\n# read original dataframe / used to get measures on data scale\noriginal_df <- margot::here_read(\"df_wide\", push_mods)\n\n# check missing values (baseline missingness is handled by grf)\n# takes a long time to render so commented out\n# naniar::vis_miss(df_grf, warn_large_data = FALSE)\n\n# check another way\nnaniar::gg_miss_var(df_grf)\n\n# import names of baseline covariates\nE <- margot::here_read(\"E\")\n# check\nE\n\n# exposure variable (we use the GRF convention where W is the exposure)\n# *****SET ***************\n# make binary\n\n# *********************************\n# check that variables are 0 or 1\ndf_grf[[t1_name_exposure_binary]]\n# *********************************\n\n# needs to be a matrix\nW <- as.vector(df_grf[[t1_name_exposure_binary]])\n\n# check that these values are 0 or 1\nW\n\n# set weights\nweights <- df_grf$t1_adjusted_weights \n\n# view/ check none too extreme\nhist(weights)\n\n# remove attributes of baseline co-variaties\nX <-  margot::remove_numeric_attributes(df_grf[E]) \n\n# set parameters ----------------------------------------------------------\n# set model defaults -----------------------------------------------------\ngrf_defaults <- list(\n  seed = 123, # reproduce results\n  stabilize.splits = TRUE, # robustness\n  # min.node.size = 5,  # default is five/ requires at least 5 observed in control and treated\n  # set higher for greater smoothing\n  num.trees = 2000 # grf default = 2000 # set lower or higher depending on storage\n)\n\n# set defaults for graphs (see bottom of script for options)\ndecision_tree_defaults <- list(\n  span_ratio = .3,\n  text_size = 3.8,\n  y_padding = 0.25  # use full parameter name\n  # edge_label_offset = .002, # options\n  # border_size = .05\n)\n\n# set defaults for graphs (see bottom of script for options)\n# set policy tree defaults\npolicy_tree_defaults <- list(\n  point_alpha = .5,\n  title_size = 12,\n  subtitle_size = 14,\n  axis_title_size = 14,\n  legend_title_size = 14,\n  split_line_color = \"red\",\n  split_line_alpha = 0.8,\n  split_label_color = \"red\"\n)\n\n# test --------------------------------------------------------------------\nn <- nrow(X) # n in sample\n\n# define training sample\ntoy <- sample(1:n, n / 4) # get half sample\n\n# test data\ntoy_data = df_grf[toy, ]\n\n# check\nnrow(toy_data)\n\n# test covariates\nX_toy = X[toy, ]\n\n# check\nstr(X_toy)\n\n# test exposure\nW_toy = W[toy]\n\n# test weights\nweights_toy = weights[toy]\n\n# # test model\n# ignore warnings\ncf.test <- margot::margot_causal_forest(\n  data          = toy_data,\n  outcome_vars  = \"t2_log_hours_exercise_z\",\n  covariates    = X_toy,\n  W             = W_toy,\n  weights       = weights_toy,\n  top_n_vars = 15,\n  save_models = TRUE # save the models\n)\n\n# test plots\nmodels_binary_batch_test <- margot::margot_policy(\n  cf.test,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = \"model_t2_log_hours_exercise_z\",\n  original_df = original_df,\n  label_mapping = label_mapping_psych\n)\n\n# test plots\nmodels_binary_batch_test$model_t2_log_hours_exercise_z$qini_plot\nmodels_binary_batch_test$model_t2_log_hours_exercise_z$decision_tree\nmodels_binary_batch_test$model_t2_log_hours_exercise_z$policy_tree\n\n\n# run binary model --------------------------------------------------------\n# health models -----------------------------------------------------------\nmodels_binary_health <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_health_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  train_proportion = 0.7\n)\n\n# check size if needed\nmargot::margot_size(models_binary_health)\n\n# save model\nmargot::here_save_qs(models_binary_health, \"models_binary_health\", push_mods)\n\n# psych models ------------------------------------------------------------\nmodels_binary_psych <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_psych_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  train_proportion = 0.7\n)\n\n# save model\nmargot::here_save_qs(models_binary_psych, \"models_binary_psych\", push_mods)\n\n\n# present models ----------------------------------------------------------\nmodels_binary_present <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_present_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  train_proportion = 0.7\n)\n# save model\nmargot::here_save_qs(models_binary_present, \"models_binary_present\", push_mods)\n\n\n# life models -------------------------------------------------------------\nmodels_binary_life <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_life_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  train_proportion = 0.7\n)\n\n# save model\nmargot::here_save_qs(models_binary_life, \"models_binary_life\", push_mods)\n\n\n# social models -----------------------------------------------------------\nmodels_binary_social <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_social_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  train_proportion = 0.7\n)\n\n# save model\nmargot::here_save_qs(models_binary_social, \"models_binary_social\", push_mods)\n\n# graphs ------------------------------------------------------------------\n# plot defaults -----------------------------------------------------------\ntitle_binary = \"Community Socialising (binary)\"\nfilename_prefix = \"grf_extraversion_wb\"\n\n# titles\nsubtitle_health = \"Health\"\nsubtitle_psych = \"Psychological Well-being\"\nsubtitle_present = \"Present-Focussed Well-being\"\nsubtitle_life = \"Life-Focussed Well-being\"\nsubtitle_social = \"Social Well-being\"\n\n# defaults\nbase_defaults_binary <- list(\n  type = \"RD\",\n  title = title_binary,\n  #interpret_all_E_gt1 = TRUE,\n  e_val_bound_threshold = 1.1,\n  colors = c(\n    \"positive\" = \"#E69F00\",\n    \"not reliable\" = \"grey50\",\n    \"negative\" = \"#56B4E9\"\n  ),\n  x_offset = -.275,\n  # will be set based on type\n  x_lim_lo = -.275,\n  # will be set based on type\n  x_lim_hi = .275,\n  text_size = 4,\n  linewidth = 0.5,\n  estimate_scale = 1,\n  base_size = 18,\n  point_size = 2,\n  title_size = 19,\n  subtitle_size = 16,\n  legend_text_size = 10,\n  legend_title_size = 10,\n  include_coefficients = FALSE\n)\n\n# set options for graphs\n# health graph options\noutcomes_options_health <- margot_plot_create_options(\n  title = subtitle_health,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n# psych graph options\noutcomes_options_psych <- margot_plot_create_options(\n  title = subtitle_psych,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n\n# present graph options ---------------------------------------------------\noutcomes_options_present <- margot_plot_create_options(\n  title = subtitle_present,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n\n# life graph options ------------------------------------------------------\noutcomes_options_life <- margot_plot_create_options(\n  title = subtitle_life,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n\n# social graph options ----------------------------------------------------\noutcomes_options_social <- margot_plot_create_options(\n  title = subtitle_social,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n# all graph options ------------------------------------------------------\noptions_all_models <- margot_plot_create_options(\n  title = \"Outcomewide Wellbeing\",\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n\n# make graphs -------------------------------------------------------------\n# read results\n# warning: reading models will take time\n# import\nmodels_binary_health <- margot::here_read_qs(\"models_binary_health\", push_mods)\nmodels_binary_psych <- margot::here_read_qs(\"models_binary_psych\", push_mods)\nmodels_binary_present <- margot::here_read_qs(\"models_binary_present\", push_mods)\nmodels_binary_life <- margot::here_read_qs(\"models_binary_life\", push_mods)\nmodels_binary_social <- margot::here_read_qs(\"models_binary_social\", push_mods)\n\n# check size (example)\nmargot::margot_size(models_binary_health)\n\n# make ATE plots ----------------------------------------------------------\n# health plots ------------------------------------------------------------\nbinary_results_health <- margot_plot(\n  models_binary_health$combined_table,\n  options = outcomes_options_health,\n  label_mapping = label_mapping_health,\n  include_coefficients = FALSE,\n  save_output = FALSE, \n  order = \"evaluebound_asc\",\n  original_df = original_df,\n  e_val_bound_threshold = 1.1\n)\n\n# view\nbinary_results_health$transformed_table |> rename(\n  \"E-Value\" = \"E_Value\",\n  \"E-Value bound\" = \"E_Val_bound\"\n) |>\n  kbl(format = 'markdown')\n\n# check\ncat(binary_results_health$interpretation)\n\n# interpretation\ncat(binary_results_health$interpretation)\n\n# plot psych\nbinary_results_psych_asc <- margot_plot(\n  models_binary_psych$combined_table,\n  options = outcomes_options_psych,\n  label_mapping = label_mapping_psych,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  original_df = original_df,\n  e_val_bound_threshold = 1.1,\n  order = \"evaluebound_asc\")\n\n\n# order\nbinary_results_psych_asc$plot\n\n# reorder for descriptions\nbinary_results_psych <- margot_plot(\n  models_binary_psych$combined_table,\n  options = outcomes_options_psych,\n  label_mapping = label_mapping_psych,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  e_val_bound_threshold = 1.1,\n  original_df = original_df,\n  order = \"evaluebound_asc\"\n)\n\n# table\nbinary_results_psych$transformed_table |> rename(\n  \"E-Value\" = \"E_Value\",\n  \"E-Value bound\" = \"E_Val_bound\"\n) |>\n  kbl(format = 'markdown')\n\n# interpretation\ncat(binary_results_psych$interpretation)\n\n# plot present\n# order\nbinary_results_present <- margot_plot(\n  models_binary_present$combined_table,\n  options = outcomes_options_present,\n  label_mapping = label_mapping_present,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  original_df = original_df,\n  order = \"evaluebound_asc\"\n)\n\n# plot\nbinary_results_present$plot\n\n# interpretation\ncat(binary_results_present$interpretation)\n(binary_results_present$transformed_table)\n\n# plot life\nbinary_results_life <- margot_plot(\n  models_binary_life$combined_table,\n  options = outcomes_options_life,\n  label_mapping = label_mapping_life,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  order = \"evaluebound_asc\",\n  original_df = original_df\n)\n\n# table\nbinary_results_life$transformed_table |> rename(\n  \"E-Value\" = \"E_Value\",\n  \"E-Value bound\" = \"E_Val_bound\"\n) |>\n  kbl(format = 'markdown')\n\n# plot\nbinary_results_life$transformed_table\n\n# interpretation\ncat(binary_results_life$interpretation)\n\n# plot social\nbinary_results_social <- margot_plot(\n  models_binary_social$combined_table,\n  options = outcomes_options_social,\n  label_mapping = label_mapping_social,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  original_df = original_df,\n  order = \"evaluebound_asc\"\n)\n\n# table\nbinary_results_social$transformed_table |> rename(\n  \"E-Value\" = \"E_Value\",\n  \"E-Value bound\" = \"E_Val_bound\"\n) |>\n  kbl(format = 'markdown')\n\n# interpretation\ncat(binary_results_social$interpretation)\n\n\n# combine ate plots ------------------------------------------------------\n# plot_ate_health <- binary_results_health_asc$plot\n# plot_ate_psych <- binary_results_psych_asc$plot\n# plot_ate_present <- binary_results_present_asc$plot\n# plot_ate_life <- binary_results_life_asc$plot\n# plot_ate_social <- binary_results_social_asc$plot\n# \n# \n# \n# # create combined plot with annotations\n# ate_plots_combined <- plot_ate_health + \n#   plot_ate_psych + \n#   plot_ate_present + \n#   plot_ate_life + \n#   plot_ate_social +\n#   plot_annotation(\n#     title = title_binary,\n#     tag_levels = \"A\",\n#     theme = theme(\n#       plot.title = element_text(size = 20),\n#       legend.position = \"top\"\n#     )\n#   ) +\n#   plot_layout(guides = \"collect\")\n# \n# # view combined plot\n# ate_plots_combined\n\n# combine all models -----------------------------------------------------\n# merge all domain models into single object\nall_models <- margot_bind_models(\n  models_binary_health,\n  models_binary_psych,\n  models_binary_present,\n  models_binary_life,\n  models_binary_social\n)\n\n\n# graph\nplot_all_models <- margot_plot(\n  all_models$combined_table,\n  options = options_all_models,\n  save_output = FALSE,\n  e_val_bound_threshold = 1.1,\n  label_mapping = label_mapping_all,\n  save_path = here::here(push_mods),\n  original_df = original_df,\n  include_coefficients = FALSE,\n  order = \"evaluebound_asc\"\n)\n\n# view plot\nplot_all_models$plot\n\n# interpretation\ncat(plot_all_models$interpretation)\n\n# table\nplot_all_models$transformed_table\n\n# nice table\ntables_list <- list(\n  Health = binary_results_health$transformed_table,\n  Psych = binary_results_psych$transformed_table,\n  Present = binary_results_present$transformed_table,\n  Life = binary_results_life$transformed_table,\n  Social = binary_results_social$transformed_table\n)\n\nmargot_bind_tables_markdown <- margot_bind_tables(\n  tables_list = tables_list, #list(all_models$combined_table),\n  sort_E_val_bound = \"desc\",\n  e_val_bound_threshold = 1.1,\n  highlight_color = NULL,\n  bold = TRUE,\n  rename_cols = TRUE,\n  col_renames = list(\n    \"E-Value\" = \"E_Value\",\n    \"E-Value bound\" = \"E_Val_bound\"\n  ),\n  rename_ate = TRUE,\n  threshold_col = \"E_Val_bound\",\n  output_format = \"markdown\",\n  kbl_args = list(\n    booktabs = TRUE,\n    caption = NULL,\n    align = NULL\n  )\n)\n\n# view markdown table\nmargot_bind_tables_markdown\n\n# save for publication\nhere_save(margot_bind_tables_markdown, \"margot_bind_tables_markdown\")\n\n\n# count models by category\ncat(\"Number of original models:\\n\")\ncat(\"Social models:\", length(models_binary_social$results), \"\\n\")\ncat(\"Psych models:\", length(models_binary_psych$results), \"\\n\")\ncat(\"Health models:\", length(models_binary_health$results), \"\\n\")\ncat(\"Present models:\", length(models_binary_present$results), \"\\n\")\ncat(\"Life models:\", length(models_binary_life$results), \"\\n\")\ncat(\"\\nTotal models in combined object:\", length(all_models$results), \"\\n\")\n\n\n# flip negatively oriented outcomes --------------------------------------\n# flip outcomes where higher values are worse for interpretation consistency\nmodels_binary_flipped_all <- margot_flip_forests(\n  all_models,\n  flip_outcomes = c(\n    \"t2_alcohol_frequency\",\n    \"t2_alcohol_intensity\",\n    \"t2_hlth_bmi_z\", \n    \"t2_hlth_fatigue_z\",\n    \"t2_kessler_latent_anxiety_z\",\n    \"t2_kessler_latent_depression_z\",\n    \"t2_rumination_z\",\n    \"t2_perfectionism_z\"\n  )\n)\n\n# size\nmargot_size(models_binary_flipped_all)\n\n# save (only jb - for lecture)\nhere_save_qs(models_binary_flipped_all, \"models_binary_flipped_all\", push_mods)\n\n# omnibus heterogeneity tests --------------------------------------------\n# test for treatment effect heterogeneity across all outcomes\nresult_ominbus_hetero_all <- margot::margot_omnibus_hetero_test(models_binary_flipped_all, label_mapping = label_mapping_all)\n\n# view results table\nresult_ominbus_hetero_all$summary_table |> kbl(\"markdown\")\n\n# view test interpretation\ncat(result_ominbus_hetero_all$brief_interpretation)\n\n\n# rate test analysis -----------------------------------------------------\n# define flipped outcome names for interpretation\nflipped_names <- c(\"Alcohol Frequency\", \"Alcohol Intensity\", \"BMI\", \"Fatigue\", \"Anxiety\", \"Depression\", \"Perfectionism\", \"Rumination\")\n\n# save for publication\nhere_save(flipped_names, \"flipped_names\")\n\n# create rate analysis table\nrate_table_all <- margot_rate(\n  models_binary_flipped_all, \n  label_mapping = label_mapping_all, \n  highlight_significant = TRUE\n)\n\n# view rate tables\nrate_table_all$rate_autoc |> kbl(\"markdown\")\nrate_table_all$rate_qini |> kbl(\"markdown\")\n\n\n# generate interpretation\nrate_interpretation_all <- margot_interpret_rate(\n  rate_table_all, \n  flipped_outcomes = flipped_names\n)\n\n# view interpretations\ncat(rate_interpretation_all$autoc_results)\ncat(rate_interpretation_all$qini_results)\ncat(rate_interpretation_all$comparison)\n\n\n# check out model names\nrate_interpretation_all$either_model_names\nrate_interpretation_all$qini_model_names\nrate_interpretation_all$both_model_names\nrate_interpretation_all$autoc_model_names\n\n\n# autoc plots ------------------------------------------------------------\n# generate batch rate plots for models with significant heterogeneity\nbatch_rate_autoc_plots <- margot_plot_rate_batch(\n  models_binary_flipped_all, \n  save_plots = FALSE,\n  # just use rate autoc\n  model_names = rate_interpretation_all$autoc_model_names  \n)\n\n# view selected autoc plots\nbatch_rate_autoc_plots$model_t2_log_hours_exercise_z\nbatch_rate_autoc_plots$model_t2_hlth_fatigue_z\nbatch_rate_autoc_plots$model_t2_self_control_z\nbatch_rate_autoc_plots$model_t2_meaning_sense_z\n\n# for instruction\nbatch_rate_qini_plots <- margot_plot_rate_batch(\n  models_binary_flipped_all, \n  save_plots = FALSE,\n  model_names = rate_interpretation_all$qini_model_names\n)\n\n# for instruction -- note that initial heterogeneity dips below zero\nbatch_rate_qini_plots$model_t2_bodysat_z\nbatch_rate_qini_plots$model_t2_self_esteem_z\nbatch_rate_qini_plots$model_t2_belong_z\n\n\n# QINI --------------------------------------------------------------------\n# batch process heterogeneity results for qini\nmodels_binary_batch_qini <- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$qini_model_names,\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\n\n# check size\nmargot_size(models_binary_batch_qini)\n\n# save\nhere_save_qs(models_binary_batch_qini, \"models_binary_batch_qini\", push_mods)\n\n# view models\n# first make graphs\nplot_qini_exercise <- models_binary_batch_qini$model_t2_log_hours_exercise_z$qini_plot\nplot_qini_fatigue <- models_binary_batch_qini$model_t2_hlth_fatigue_z$qini_plot\nplot_qini_bodysat <- models_binary_batch_qini$model_t2_bodysat_z$qini_plot\nplot_qini_self_control <- models_binary_batch_qini$model_t2_self_control_z$qini_plot\nplot_qini_self_esteem <- models_binary_batch_qini$model_t2_self_esteem_z$qini_plot\nplot_qini_belong <- models_binary_batch_qini$model_t2_belong_z$qini_plot\n\n# recall the ate\nplot_all_models$plot\n\n# interpret qini curves\ninterpretation_qini_curves <- margot_interpret_qini(\n  models_binary_batch_qini,\n  model_names = rate_interpretation_all$qini_model_names,\n  label_mapping = label_mapping_all\n)\n\n# view qini interpretation\ncat(interpretation_qini_curves$qini_explanation)\n\n# view summary table\ninterpretation_qini_curves$summary_table |> kbl(\"markdown\")\n\n# combine qini plots for visualisation\n# patchwork allows us to group graphs together\nlibrary(patchwork)\nqini_plots_combined <- \n  (plot_qini_exercise + plot_qini_fatigue + plot_qini_bodysat) / \n  (plot_qini_self_control + plot_qini_self_esteem + plot_qini_belong) + \n  plot_annotation(\n    title = \"Qini Plots: Reliable Priority 'Spending' at Fractional Budgets\",\n    tag_levels = \"A\",\n    theme = theme(legend.position = \"top\")\n  ) +\n  plot_layout(guides = \"collect\")\n\n# view combined qini plots\nqini_plots_combined\n\n# again compare with ate\nplot_all_models$plot \n\n\n# policy tree analysis ---------------------------------------------------\n# model_names_subset <- c(\n#   \"model_t2_log_hours_exercise_z\",\n#   \"model_t2_self_control_z\", \n#   \"model_t2_sexual_satisfaction_z\",\n#   \"model_t2_belong_z\",\n#   \"model_t2_support_z\"\n# )\n\n# make policy trees\nplots_policy_trees <- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names, # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\n\n# generate policy tree interpretations\ninterpretation_policy_trees <- margot_interpret_policy_batch(\n  models_binary_flipped_all,\n  # use eithre model\n  model_names = rate_interpretation_all$either_model_names, # defined above\n  train_proportion = 0.8,\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\n\n# this will give you results\ncat(interpretation_policy_trees)\n\n# view plots --------------------------------------------------------------\n# log hours exercise\nplots_policy_trees$model_t2_log_hours_exercise_z$decision_tree\nplots_policy_trees$model_t2_log_hours_exercise_z$policy_tree\nplots_policy_trees$model_t2_log_hours_exercise_z$combined_plot\n\n# fatigue\nplots_policy_trees$model_t2_hlth_fatigue_z$decision_tree\nplots_policy_trees$model_t2_hlth_fatigue_z$policy_tree\nplots_policy_trees$model_t2_hlth_fatigue_z$combined_plot\n\n# self control\nplots_policy_trees$model_t2_self_control_z$decision_tree\nplots_policy_trees$model_t2_self_control_z$policy_tree\nplots_policy_trees$model_t2_self_control_z$combined_plot\n\n# meaning sense\nplots_policy_trees$model_t2_meaning_sense_z$decision_tree\nplots_policy_trees$model_t2_meaning_sense_z$policy_tree\nplots_policy_trees$model_t2_meaning_sense_z$combined_plot\n\n# body satisfaction\nplots_policy_trees$model_t2_bodysat_z$decision_tree\nplots_policy_trees$model_t2_bodysat_z$policy_tree\nplots_policy_trees$model_t2_bodysat_z$combined_plot\n\n# self esteem\nplots_policy_trees$model_t2_self_esteem_z$decision_tree\nplots_policy_trees$model_t2_self_esteem_z$policy_tree\nplots_policy_trees$model_t2_self_esteem_z$combined_plot\n\n# belonging\nplots_policy_trees$model_t2_belong_z$decision_tree\nplots_policy_trees$model_t2_belong_z$policy_tree\nplots_policy_trees$model_t2_belong_z$combined_plot\n\n\n#############################################################################\n# theoretical comparisons ---------------------------------------------------\n# individual theoretical comparisons (if relevant)\n# need to get values for wealth if wealth is compared\n\n# step 1 get information for wealth for conditonal comparisons\nhead(df_grf$t0_log_household_inc_z)\n\n# get mean on original data scale\nlog_mean_inc <- mean(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# get sd on original data scale\nlog_sd_inc <- sd(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# function to get back to data scale\nmargot_back_transform_log_z(\n  log_mean = log_mean_inc, \n  log_sd = log_sd_inc,\n  z_scores = c(-1, 0, 1),\n  label = \"data_scale\"\n)\n\n# define complex conditions for subsetting\ncomplex_condition_political <- X[, \"t0_political_conservative_z\"] > -1 &\n  X[, \"t0_political_conservative_z\"] < 1\n\ncomplex_condition_wealth <- X[, \"t0_log_household_inc_z\"] > -1 & \n  X[, \"t0_log_household_inc_z\"] < 1\n\n# wealth subsets\nsubsets_standard_wealth <- list(\n  Poor = list(\n    var = \"t0_log_household_inc_z\",\n    value = -1,\n    operator = \"<\",\n    description = \"HShold income < -1 SD (NZD ~41k)\"\n    # label = \"Conservative\"  # label remains as is, but could be changed if desired\n  ),\n  MiddleIncome = list(\n    subset_condition = complex_condition_wealth,\n    description = \"HShold income within +/-1SD (> NZD 41k < NZD 191k)\"\n  ),\n  Rich = list(\n    var = \"t0_log_household_inc_z\",\n    value = 1,\n    operator = \">\",\n    description = \"HShold income > +1 SD (NZD 191k)\",\n    label = \"Rich\"\n  )\n)\n\n# political subsets\nsubsets_standard_political <- list(\n  Liberal = list(\n    var = \"t0_political_conservative_z\",\n    value = -1,\n    operator = \"<\",\n    description = \"< -1 SD in political conservativism\"\n  ),\n  Moderates = list(\n    var = \"t0_political_conservative_z\",\n    # operator = \"<\",\n    subset_condition = complex_condition_political,\n    description = \"Effects among those > -1 SD and < +1 in political conservativism\",\n    label = \"Centrist\"\n  ),\n  Conservative = list(\n    var = \"t0_political_conservative_z\",\n    value = 1,\n    operator = \">\",\n    description = \"> +1 SD in political conservativism\",\n    label = \"Conservative\"\n  )\n)\n\n# gender subsets\nsubsets_standard_gender <- list(\n  Female = list(\n    var = \"t0_male_binary\",\n    value = 0,\n    description = \"Females\"\n  ),\n  Male = list(\n    var = \"t0_male_binary\",\n    value = 1,\n    description = \"Males\"\n  )\n) \n\n# ethnicity subsets\nsubsets_standard_ethnicity <- list(\n  Asian = list(\n    var = \"t0_eth_cat_asian_binary\",\n    value = 1,\n    description = \"Asians\"\n  ),\n  Euro = list(\n    var = \"t0_eth_cat_euro_binary\",\n    value = 1,\n    description = \"Europeans (Pakeha)\"\n  ),\n  Pacific = list(\n    var = \"t0_eth_cat_pacific_binary\",\n    value = 1,\n    description = \"Pacific Peoples\"\n  ),\n  Maori = list(\n    var = \"t0_eth_cat_maori_binary\",\n    value = 1,\n    description = \"Māori\"\n  )\n)\n\n# subsets_standard_cohort <- list(\n#   boomers = list(\n#     var = \"t0_gen_cohort_gen_Boomers_binary\",\n#     value = 1,\n#     description = \"Baby Boomers\",\n#     label = \"Boomers\"  # label remains as is, but could be changed if desired\n#   ),\n#   gen_X = list(\n#     var = \"t0_gen_cohort_gen_X_binary\",\n#     value = 1,\n#     description = \"Generation X\",\n#     label = \"Generation_X\"  # label remains as is, but could be changed if desired\n#   ),\n#   gen_Y = list(\n#     var = \"t0_gen_cohort_gen_Y_binary\",\n#     value = 1,\n#     description = \"Generation Y\",\n#     label = \"Generation_Y\"  # label remains as is, but could be changed if desired\n#   ),\n#   gen_Z = list(\n#     var = \"t0_gen_cohort_gen_Z_binary\",\n#     value = 1,\n#     description = \"Generation Z\",\n#     label = \"Generation_Z\"  # label remains as is, but could be changed if desired\n#   )\n# )\n\n# check\nmean(original_df$t0_age) - sd(original_df$t0_age) #\n\n# if we have specific groups to compare\ncomplex_condition_age_under_neg_1_sd  <- X[, \"t0_age_z\"] < -1 \ncomplex_condition_age_gr_eq_neg_1_sd  <- X[, \"t0_age_z\"] >= -1 \n\n# age subsets\nsubsets_standard_cohort <- list(\n  under_37 = list(\n    value = complex_condition_age_under_neg_1_sd,\n    description = \"under_37\"\n  ),\n  over_37 = list(\n    value = complex_condition_age_gr_eq_neg_1_sd,\n    description = \"37 and over\"\n  )\n)\n\n# batch planned subgroup analysis -----------------------------------------\n# set up lists of models, names, and subtitles\ndomain_models <- list(\n  models_binary_health,\n  models_binary_psych,\n  models_binary_present,\n  models_binary_life,\n  models_binary_social\n)\n\n# set up domain names\ndomain_names <- c(\"health\", \"psych\", \"present\", \"life\", \"social\")\n\n# set up subtitles\nsubtitles <- c(\n  subtitle_health,\n  subtitle_psych,\n  subtitle_present,\n  subtitle_life,\n  subtitle_social\n)\n\n# set up subset types in a list\nsubset_types <- list(\n  wealth = subsets_standard_wealth,\n  ethnicity = subsets_standard_ethnicity,\n  political = subsets_standard_political,\n  gender = subsets_standard_gender#,\n  # cohort = subsets_standard_cohort\n)\n\n# run the batch processing for all domains and subsets\nplanned_subset_results <- margot_planned_subgroups_batch(\n  domain_models = domain_models,\n  X = X,\n  base_defaults = base_defaults_binary,\n  subset_types = subset_types,\n  original_df = original_df,\n  domain_names = domain_names,\n  subtitles = subtitles,\n  push_mods = push_mods\n)\n\n# results\n# health subgroup\ncat(planned_subset_results$health$wealth$explanation)\ncat(planned_subset_results$health$ethnicity$explanation) \ncat(planned_subset_results$health$political$explanation) \ncat(planned_subset_results$health$gender$explanation) \ncat(planned_subset_results$health$cohort$explanation) \n\n\ncat(planned_subset_results$psych$wealth$explanation) \ncat(planned_subset_results$psych$ethnicity$explanation) \ncat(planned_subset_results$psych$political$explanation) \ncat(planned_subset_results$psych$gender$explanation) \ncat(planned_subset_results$psych$cohort$explanation) \n\n\ncat(planned_subset_results$present$wealth$explanation) \ncat(planned_subset_results$present$ethnicity$explanation) \ncat(planned_subset_results$present$political$explanation) \ncat(planned_subset_results$present$gender$explanation) \ncat(planned_subset_results$present$cohort$explanation) \n\n\ncat(planned_subset_results$life$wealth$explanation) \ncat(planned_subset_results$life$ethnicity$explanation) \ncat(planned_subset_results$life$political$explanation) \ncat(planned_subset_results$life$gender$explanation) \ncat(planned_subset_results$life$cohort$explanation) \n\ncat(planned_subset_results$social$wealth$explanation) \ncat(planned_subset_results$social$ethnicity$explanation) \ncat(planned_subset_results$social$political$explanation) \ncat(planned_subset_results$social$gender$explanation) \ncat(planned_subset_results$social$cohort$explanation) \n\n\n# combine tables ----------------------------------------------------------\n# wrap each domain's table in a list:\n# wealth subgroups --------------------------------------------------------\ntables_list_poor <- list(\n  Health = planned_subset_results$health$wealth$results$Poor$transformed_table,\n  Psych  = planned_subset_results$psych$wealth$results$Poor$transformed_table,\n  Life   = planned_subset_results$life$wealth$results$Poor$transformed_table,\n  Social = planned_subset_results$social$wealth$results$Poor$transformed_table\n)\n\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_poor,\n  bold = TRUE,\n  kbl_args = list(booktabs = TRUE, caption = \"Wealth Subgroup Analysis: Poor\"),\n  highlight_color = NULL, \n  output_format = \"html\"\n)\n\n# create table list for middle income subgroup\ntables_list_middleincome <- list(\n  Health = planned_subset_results$health$wealth$results$MiddleIncome$transformed_table,\n  Psych  = planned_subset_results$psych$wealth$results$MiddleIncome$transformed_table,\n  Life   = planned_subset_results$life$wealth$results$MiddleIncome$transformed_table,\n  Social = planned_subset_results$social$wealth$results$MiddleIncome$transformed_table\n)\n\n# bind tables and display results\nmargot::margot_bind_tables(\n  tables_list = tables_list_middleincome,\n  bold = TRUE,\n  kbl_args = list(\n    booktabs = TRUE, \n    caption = \"Wealth Subgroup Analysis: Middle Income\"\n  ),\n  highlight_color = NULL, \n  output_format = \"html\"\n)\n\nplanned_subset_results$health$wealth$results$Rich$transformed_table\n\ntables_list_rich <- list(\n  Health = planned_subset_results$health$wealth$results$Rich$transformed_table,\n  Psych  = planned_subset_results$psych$wealth$results$Rich$transformed_table,\n  Life   = planned_subset_results$life$wealth$results$Rich$transformed_table,\n  Social = planned_subset_results$social$wealth$results$Rich$transformed_table\n)\n\n\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_rich,\n  bold = TRUE,\n  kbl_args = list(\n    booktabs = TRUE, \n    caption = \"Wealth Subgroup Analysis: Rich\"\n  ),\n  highlight_color = NULL, \n  output_format = \"html\"\n)\n\n# cohort subgroups --------------------------------------------------------\n\n# wealth subgroups --------------------------------------------------------\nplanned_subset_results$health$cohort$results$Generation_X$transformed_table\n\nplanned_subset_results$health$cohort$results$Boomers$transformed_table\ntables_list_boomers <- list(\n  Health = planned_subset_results$health$cohort$results$Boomers$transformed_table,\n  Psych  = planned_subset_results$psych$cohort$results$Boomers$transformed_table,\n  Life   = planned_subset_results$psych$cohort$results$Boomers$transformed_table,\n  Social = planned_subset_results$psych$cohort$results$Boomers$transformed_table\n)\n\n\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_boomers,\n  bold = TRUE,\n  kbl_args = list(\n    booktabs = TRUE, \n    caption = \"Cohort Subgroup Analysis: Boomers\"\n  ),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\n\n# margot_bind_tables(tables_list = tables_lisblue()# margot_bind_tables(tables_list = tables_list,\n#                    bold = TRUE,\n#                    highlight_color = NULL, output_format = \"latex\")\ntables_list_genZ <- list(\n  Health = planned_subset_results$health$cohort$results$Generation_Z$transformed_table,\n  Psych  = planned_subset_results$psych$cohort$results$Generation_Z$transformed_table,\n  Life   = planned_subset_results$psych$cohort$results$Generation_Z$transformed_table,\n  Social = planned_subset_results$psych$cohort$results$Generation_Z$transformed_table\n)\n\n\n# new function bind tables\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_genZ,\n  bold = TRUE,\n  kbl_args = list(\n    booktabs = TRUE, \n    caption = \"Cohort Subgroup Analysis: Generation Z\"\n  ),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_rich,\n  bold = TRUE,\n  kbl_args = list(\n    booktabs = TRUE, \n    caption = \"Wealth Subgroup Analysis: Rich\"\n  ),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\n\n# plots -------------------------------------------------------------------\n# Results Plots\n# health\nplots_subgroup_wealth_health <- wrap_plots(\n  list(\n    planned_subset_results$health$wealth$results$Poor$plot,\n    planned_subset_results$health$wealth$results$MiddleIncome$plot,\n    planned_subset_results$health$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(\n    title = subtitle_health,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_wealth_health)\n\n# plots\nplots_subgroup_ethnicity_health<- wrap_plots(\n  list(\n    planned_subset_results$health$ethnicity$results$Asian$plot,\n    planned_subset_results$health$ethnicity$results$Euro$plot,\n    planned_subset_results$health$ethnicity$results$Maori$plot,\n    planned_subset_results$health$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_health,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_ethnicity_health)\n\n# plots\nplots_subgroup_political_health <- wrap_plots(\n  list(\n    planned_subset_results$health$political$results$Conservative$plot,\n    planned_subset_results$health$political$results$Centrist$plot,\n    planned_subset_results$health$political$results$Conservative$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_health,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_political_health)\n\n# plots\nplots_subgroup_gender_health<- wrap_plots(\n  list(\n    planned_subset_results$health$gender$results$Female$plot,\n    planned_subset_results$health$gender$results$Male$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_health,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_gender_health)\n\n# plots\nplots_subgroup_cohort_health<- wrap_plots(\n  list(\n    planned_subset_results$health$cohort$results$Boomers$plot,\n    planned_subset_results$health$cohort$results$Generation_X$plot,\n    planned_subset_results$health$cohort$results$Generation_Y$plot,\n    planned_subset_results$health$cohort$results$Generation_Z$plot\n    \n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_health,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_cohort_health)\n\n# psychological well-being\nplots_subgroup_wealth_psych <- wrap_plots(\n  list(\n    planned_subset_results$psych$wealth$results$Poor$plot,\n    planned_subset_results$psych$wealth$results$MiddleIncome$plot,\n    planned_subset_results$psych$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(\n    title = subtitle_psych,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_wealth_psych)\n\n# plots\nplots_subgroup_ethnicity_psych<- wrap_plots(\n  list(\n    planned_subset_results$psych$ethnicity$results$Asian$plot,\n    planned_subset_results$psych$ethnicity$results$Euro$plot,\n    planned_subset_results$psych$ethnicity$results$Maori$plot,\n    planned_subset_results$psych$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_psych,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_ethnicity_psych)\n\n# plots\nplots_subgroup_political_psych <- wrap_plots(\n  list(\n    planned_subset_results$psych$political$results$Conservative$plot,\n    planned_subset_results$psych$political$results$Centrist$plot,\n    planned_subset_results$psych$political$results$Conservative$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_psych,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_political_psych)\n\n# plots\nplots_subgroup_gender_psych<- wrap_plots(\n  list(\n    planned_subset_results$psych$gender$results$Female$plot,\n    planned_subset_results$psych$gender$results$Male$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_psych,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_gender_psych)\n\n# plots\nplots_subgroup_cohort_psych<- wrap_plots(\n  list(\n    planned_subset_results$psych$cohort$results$Boomers$plot,\n    planned_subset_results$psych$cohort$results$Generation_X$plot,\n    planned_subset_results$psych$cohort$results$Generation_Y$plot,\n    planned_subset_results$psych$cohort$results$Generation_Z$plot\n    \n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_psych,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_cohort_psych)\n\n# present focussed well-being\nplots_subgroup_wealth_present <- wrap_plots(\n  list(\n    planned_subset_results$present$wealth$results$Poor$plot,\n    planned_subset_results$present$wealth$results$MiddleIncome$plot,\n    planned_subset_results$present$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(\n    title = subtitle_present,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_wealth_present)\n\n# plots\nplots_subgroup_ethnicity_present<- wrap_plots(\n  list(\n    planned_subset_results$present$ethnicity$results$Asian$plot,\n    planned_subset_results$present$ethnicity$results$Euro$plot,\n    planned_subset_results$present$ethnicity$results$Maori$plot,\n    planned_subset_results$present$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_present,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_ethnicity_present)\n\n# plots\nplots_subgroup_political_present <- wrap_plots(\n  list(\n    planned_subset_results$present$political$results$Conservative$plot,\n    planned_subset_results$present$political$results$Centrist$plot,\n    planned_subset_results$present$political$results$Conservative$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_present,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_political_present)\n\n# plots\nplots_subgroup_gender_present<- wrap_plots(\n  list(\n    planned_subset_results$present$gender$results$Female$plot,\n    planned_subset_results$present$gender$results$Male$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_present,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view  \nprint(plots_subgroup_gender_present)\n\n# plots\nplots_subgroup_cohort_present<- wrap_plots(\n  list(\n    planned_subset_results$present$cohort$results$Boomers$plot,\n    planned_subset_results$present$cohort$results$Generation_X$plot,\n    planned_subset_results$present$cohort$results$Generation_Y$plot,\n    planned_subset_results$present$cohort$results$Generation_Z$plot\n    \n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_present,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_cohort_present)\n\n\n## life focussed well-being\nplots_subgroup_wealth_life <- wrap_plots(\n  list(\n    planned_subset_results$life$wealth$results$Poor$plot,\n    planned_subset_results$life$wealth$results$MiddleIncome$plot,\n    planned_subset_results$life$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(\n    title = subtitle_life,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_wealth_life)\n\n# plots\nplots_subgroup_ethnicity_life<- wrap_plots(\n  list(\n    planned_subset_results$life$ethnicity$results$Asian$plot,\n    planned_subset_results$life$ethnicity$results$Euro$plot,\n    planned_subset_results$life$ethnicity$results$Maori$plot,\n    planned_subset_results$life$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_life,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_ethnicity_life)\n\n# plots\nplots_subgroup_political_life <- wrap_plots(\n  list(\n    planned_subset_results$life$political$results$Conservative$plot,\n    planned_subset_results$life$political$results$Centrist$plot,\n    planned_subset_results$life$political$results$Conservative$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_life,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_political_life)\n\n# plots\nplots_subgroup_gender_life<- wrap_plots(\n  list(\n    planned_subset_results$life$gender$results$Female$plot,\n    planned_subset_results$life$gender$results$Male$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_life,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_gender_life)\n\n# plots\nplots_subgroup_cohort_life<- wrap_plots(\n  list(\n    planned_subset_results$life$cohort$results$Boomers$plot,\n    planned_subset_results$life$cohort$results$Generation_X$plot,\n    planned_subset_results$life$cohort$results$Generation_Y$plot,\n    planned_subset_results$life$cohort$results$Generation_Z$plot\n    \n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_life,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\nprint(plots_subgroup_cohort_life)\n\n# social well-being\nplots_subgroup_wealth_social <- wrap_plots(\n  list(\n    planned_subset_results$social$wealth$results$Poor$plot,\n    planned_subset_results$social$wealth$results$MiddleIncome$plot,\n    planned_subset_results$social$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(\n    title = subtitle_social,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_wealth_social)\n\nplots_subgroup_ethnicity_social<- wrap_plots(\n  list(\n    planned_subset_results$social$ethnicity$results$Asian$plot,\n    planned_subset_results$social$ethnicity$results$Euro$plot,\n    planned_subset_results$social$ethnicity$results$Maori$plot,\n    planned_subset_results$social$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_social,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_ethnicity_social)\n\n\nplots_subgroup_political_social <- wrap_plots(\n  list(\n    planned_subset_results$social$political$results$Conservative$plot,\n    planned_subset_results$social$political$results$Centrist$plot,\n    planned_subset_results$social$political$results$Conservative$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_social,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_political_social)\n\n# plots\nplots_subgroup_gender_social<- wrap_plots(\n  list(\n    planned_subset_results$social$gender$results$Female$plot,\n    planned_subset_results$social$gender$results$Male$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_social,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_gender_social)\n\n# plots\nplots_subgroup_cohort_social<- wrap_plots(\n  list(\n    planned_subset_results$social$cohort$results$Boomers$plot,\n    planned_subset_results$social$cohort$results$Generation_X$plot,\n    planned_subset_results$social$cohort$results$Generation_Y$plot,\n    planned_subset_results$social$cohort$results$Generation_Z$plot\n    \n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_social,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\nprint(plots_subgroup_cohort_social)\n\n\n\n# plot options: showcased ---------------------------------------------\n# default\nmargot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n)\n# tighten branches for easier viewing in single graphs\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  span_ratio = .30,\n  text_size = 3.8,\n  border_size = .1,\n  #  title = \"none\",\n  original_df = original_df\n)\n# colour decision node\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  span_ratio = .3,\n  text_size = 4, \n  title = \"New Title\",\n  non_leaf_fill =  \"violet\",\n  original_df = original_df\n)\n# make new title\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  span_ratio = .2,\n  text_size = 3, \n  title = \"New Title\",\n  non_leaf_fill =  \"white\",\n  original_df = original_df\n)\n\n# remove title\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  text_size = 5, \n  title = 'none', # set title to none\n  original_df = original_df\n)\n\n# policy tree options \n# select only plot 1 change alpha\nmargot::margot_plot_policy_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  point_alpha = .25, \n  plot_selection = \"p1\"\n)\n# select only plot 2 change size of axis_text\n# change colours, modify etc... \nmargot::margot_plot_policy_tree(\n  models_binary,\n  \"model_t2_agreeableness_z\",\n  plot_selection = \"p2\",\n  axis_title_size = 30,\n  split_label_size = 20,\n  split_label_color = \"red\",\n  split_line_color = \"red\",\n)\n\n# adjust only the alpha\nmargot::margot_plot_policy_tree(\n  models_binary,\n  \"model_t2_agreeableness_z\",\n  point_alpha = .1\n)\n```\n:::\n\n\n\n\n\n\n## HOMEWORK: Prepare a fresh set of analysis scripts using a different exposure\n\n- E.g. Ask: what are the effects of a shift in religious service `religion_church` on multi-dimensional well-being. \n- Consider what variables you need for confounding control at baseline. \n- Think about how to make the exposure variable binary.\n- You may consider different outcome(s) as well as a different exposure. \n\n\n\n\n### Packages\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::cite_packages()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Bulbulia J (2024). _boilerplate_. doi:10.5281/zenodo.13370825 <https://doi.org/10.5281/zenodo.13370825>, R package version 1.0.4, <https://go-bayes.github.io/biolerplate/>.\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 <https://doi.org/10.5281/zenodo.10907724>, R package version 1.0.21 Functions to obtain MARGinal Observational Treatment-effects from observational data., <https://go-bayes.github.io/margot/>.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont <https://doi.org/10.32614/CRAN.package.extrafont>, R package version 0.19, <https://CRAN.R-project.org/package=extrafont>.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. <https://www.jstatsoft.org/v40/i03/>.\n  - Müller K (2020). _here: A Simpler Way to Find Your Files_. doi:10.32614/CRAN.package.here <https://doi.org/10.32614/CRAN.package.here>, R package version 1.0.1, <https://CRAN.R-project.org/package=here>.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. doi:10.32614/CRAN.package.tibble <https://doi.org/10.32614/CRAN.package.tibble>, R package version 3.2.1, <https://CRAN.R-project.org/package=tibble>.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. doi:10.32614/CRAN.package.patchwork <https://doi.org/10.32614/CRAN.package.patchwork>, R package version 1.3.0, <https://CRAN.R-project.org/package=patchwork>.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. <https://www.R-project.org/>.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, <https://ggplot2.tidyverse.org>.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. doi:10.32614/CRAN.package.forcats <https://doi.org/10.32614/CRAN.package.forcats>, R package version 1.0.0, <https://CRAN.R-project.org/package=forcats>.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. doi:10.32614/CRAN.package.stringr <https://doi.org/10.32614/CRAN.package.stringr>, R package version 1.5.1, <https://CRAN.R-project.org/package=stringr>.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. doi:10.32614/CRAN.package.dplyr <https://doi.org/10.32614/CRAN.package.dplyr>, R package version 1.1.4, <https://CRAN.R-project.org/package=dplyr>.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. doi:10.32614/CRAN.package.purrr <https://doi.org/10.32614/CRAN.package.purrr>, R package version 1.0.4, <https://CRAN.R-project.org/package=purrr>.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. doi:10.32614/CRAN.package.readr <https://doi.org/10.32614/CRAN.package.readr>, R package version 2.1.5, <https://CRAN.R-project.org/package=readr>.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. doi:10.32614/CRAN.package.tidyr <https://doi.org/10.32614/CRAN.package.tidyr>, R package version 1.3.1, <https://CRAN.R-project.org/package=tidyr>.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, <https://github.com/rstudio/tinytex>. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. <https://tug.org/TUGboat/Contents/contents40-1.html>.\n  - Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. doi:10.32614/CRAN.package.kableExtra <https://doi.org/10.32614/CRAN.package.kableExtra>, R package version 1.4.0, <https://CRAN.R-project.org/package=kableExtra>.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Appendix \n\n## Review: The Fundamental Problem of Causal Inference as a Missing Data Problem\n\nRecall the fundamental problem of causal inference, returning to the question of whether bilingualism improves cognitive abilities:\n\n-   $Y_i^{a = 1}$: The cognitive ability of child $i$ if they were bilingual. This is the counterfactual outcome when A = 1.\n-   $Y_i^{a = 0}$:: The cognitive ability of child $i$ if they were monolingual. This is the counterfactual outcome when A = 0.\n\nThe causal effect of bilingualism on cognitive ability for individual $i$ is then defined as the difference between these potential outcomes:\n\n$$\n\\text{Causal Effect}_i = Y_i^{a=1} - Y_i^{a=0} \n$$\n\nWe say there is a causal effect if:\n\n$$\nY_i^{a=1} - Y_i^{a=0}  \\neq 0\n$$\n\nHowever, we only observe one of the potential outcomes for each child. The other outcome is not observed because physics prevents a child from both receiving and not receiving bilingual exposure.\n\nThe fact that causal contrasts are not observed in individuals is called \"The fundamental problem of causal inference.\"\n\nAlthough we typically cannot observe individual causal effects, we can obtain average causal effects when certain assumptions are satisfied.\n\n\n\n\n```{=tex}\n\\begin{align}\nE(\\delta) = E(Y^{a=1} - Y^{a=0})\\\\\n          ~  = E(Y^{a=1}) - E(Y^{a=0}) \\\\\n          ~  = ATE\n\\end{align}\n```\n\n\n\nWe may identify average causal effects from the data when the following assumptions are met:\n\n-   **Causal Consistency:** The exposure values under comparisons correspond to well-defined interventions that, in turn, correspond to the treatment versions in the data.[]\n-   **Positivity:** The probability of receiving every value of the exposure within all strata of co-variates is greater than zero []\n-   **Exchangeability:** The conditional probability of receiving every value of an exposure level, though not decided by the investigators, depends only on the measured covariates []\n\nFurther assumptions:\n\n-   **No Interference,** also known as the **Stable Unit Treatment Value Assumption** (SUTVA), requires that the treatment given to one unit (e.g., person, group, organization) does not interfere with the potential outcomes of another unit. Put differently, there are no \"spillover\" effects. Note: this assumption may be thought to be part of causal consistency, namely individual has only one potential outcome under each treatment condition.\n-   **Correctly specified model**: the requirement that the underlying statistical model used to estimate causal effects accurately represents the true relationships between the variables of interest. We say the model should be able to capture \"the functional form\" of the relationship between the treatment, the outcome, and any covariates. The model's functional form should be flexible enough to capture the true underlying relationship. The estimated causal effects may be biased if the model's functional form is incorrect. Additionally, the model must handle omitted variable bias by including all relevant confounders and should correctly handle missing data from non-response or loss-to follow up. We will return to the bias arising from missing data in the weeks ahead. For now, it is important to note that causal inference assumes that our model is correctly specified.\n\n\n## Subgroup analysis\n\nRedcall, **Effect Modification** (also known as \"heterogeneity of treatment effects\", and \"Effect-measure modification\") occurs when the causal effect of intervention $A$ varies across different levels of another variable $R$:\n\n$$E(Y^{a=1}|G=g_1, L=l) - E(Y^{a=0}|G=g_1, L=l) \\neq E(Y^{a=1}|G=g_2, L=l) - E(Y^{a=0}|G=g_2, L=l)$$\n\nEffect modification indicates that the magnitude of the causal effect of intervention $A$ is related to the modifier variable $G$ level. As discussed last week, effect modification can be observed even when there is no direct causal interaction between the treatment and the modifier variable. We noted that **interaction in causal inference refers to a situation where the combined effect of two interventions is not equal to the sum of their individual effects**. **Effect modification, on the other hand, occurs when the causal effect of one intervention varies across different levels of another variable.**\n\n\nWe also noted that \n\n> **For comparative research, we are typically interested in effect-modification, which requires subgroup analysis.**\n\n\n### Causal Estimand, Statistical Estimand, Statistical Estimator\n\nLet's set subgroup analysis to the side for a moment and begin focussing on statistical estimation.  \n\nSuppose a researcher wants to understand the causal effect of marriage on individual happiness. Participants in the study are surveyed for their marital status (\"married\" or \"not married\") and their self-reported happiness on a scale from 1 to 10.\n\n#### Causal Estimand\n\n- **Definition**: The causal estimand is the specific quantity or parameter that we aim to estimate to understand the causal effect of an intervention or treatment on an outcome.\n\n- **Example**: Here, the **Causal Estimand** would be the Average Treatment Effect (ATE) of being married on happiness. Specifically, we define the ATE as the difference in the potential outcomes of happiness if all individuals were married versus if no individuals were married:\n\n  $$\n  \\text{ATE} = E[Y^{a=1} - Y^{a=0}]\n  $$\n\n\n  Here, $Y^{a=1}$ represents the potential happiness score if an individual is married, and $Y^{a=0}$ if they are not married.\n\n\n#### Next step: Are Causal Assumptions Met? \n\n- Identification (Exchangeability): balance in the confounders across the treatments to be compared\n\n- Consistency: well-defined interventions\n\n- Positivity: treatments occur within levels of covariates $L$\n\n\n#### Statistical Estimand (next step)\n\n- **The problem**: how do we bridge the gap between potential outcomes and data? \n\n- **Definition**: the statistical estimand is the parameter or function that summarises the relationship between variables as described by a statistical model applied to data. \n\n- **Example**: for our study, the **Statistical Estimand** might be the mean difference in happiness scores between individuals who are married and those who are not, as derived from a linear regression model:\n\n  $$\n  \\text{Happiness} = \\beta_0 + \\beta_1 \\times \\text{Married} + \\epsilon\n  $$\n\n  In this equation, $\\beta_1$ represents the estimated difference in happiness scores between the married and non-married groups.\n\n#### Statistical Estimator\n\n- **Definition**: a statistical estimator is a rule or method by which a numerical estimate of a statistical estimand is calculated from the data.\n\n- **Example**: in our marriage study, the **Statistical Estimator** for $\\beta_1$ is the ordinary least squares (OLS) estimator. This estimator is used to calculate $\\beta_1$ from the sample data provided by the survey. It provides an estimate of the impact of being married on happiness, calculated using:\n  $$\n  \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n  $$\n  where $X_i$ is a binary indicator for being married (1 for married, 0 for not married), $Y_i$ is the observed happiness score, and $\\bar{X}$, $\\bar{Y}$ are the sample means of $X$ and $Y$, respectively.\n\nThe upshot, we anchor our causal inquiries within a multi-step framework of data analysis. This involves: \n\n1. clearly defining our causal estimand within a specified *target population,*\n2. clarifying assumptions, & especially identification assumptions, \n3. describing a statistical strategy for extracting this estimand from the data, and then \n4. applying an algorithm that embodies this statistical method.\n\n\n<!-- ## Methods for Statistical Estimation in Causal Inference: Inverse Probability of Treatment Weights Using Propensity Scores -->\n\n<!-- Last week, we discussed confounding control using regression adjustment. Recall the formula for the average treatment effect (ATE) when conditioning on a set of covariates $L$: -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\text{ATE} = E[Y^{a=1} \\mid L = l] - E[Y^{a=0} \\mid L = l] \\quad \\text{for any value of } l -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- > \"We say that a set $L$ of measured non-descendants of $L$ is a sufficient set for confounding adjustment when conditioning on $L$ blocks all backdoor paths—that is, the treated and the untreated are exchangeable within levels of $L$\" (Hernán & Robins, *Causal Inference*, p. 86). -->\n\n<!-- This formula calculates the expected outcome difference between treated ($a=1$) and untreated ($a=0$) groups, given a specific value of the covariates $l$. -->\n\n<!-- Inverse Probability of Treatment Weighting (IPTW) takes a different approach. We create a pseudo-population where the treatment assignment is independent of the observed covariates by assigning weights to each individual based on their propensity scores. -->\n\n<!-- **We do this by modelling the treatment** -->\n\n<!-- Denote the treatment indicator by $A$, where $A = 1$ if an individual receives treatment and $A = 0$ otherwise. $L$ represents the vector of observed covariates, and $Y^a$ the potential outcomes. The propensity score, $e(L)$, is defined as the probability of receiving the treatment given the observed covariates: -->\n\n<!-- $$ -->\n<!-- \\hat{e}(L) = P(A = 1 \\mid L) -->\n<!-- $$ -->\n\n<!-- To obtain IPTW weights, compute the inverse probability of treatment: -->\n\n<!-- $$ -->\n<!-- v_i = \\frac{A_i}{\\hat{e}(L_i)} + \\frac{1 - A_i}{1 - \\hat{e}(L_i)} -->\n<!-- $$ -->\n\n<!-- Which simplifies to  -->\n\n<!-- $$ -->\n<!-- v_i =  -->\n<!-- \\begin{cases}  -->\n<!-- \\frac{1}{\\hat{e}} & \\text{if } A_i = 1 \\\\ -->\n<!-- \\frac{1}{1-\\hat{e}} & \\text{if } A_i = 0  -->\n<!-- \\end{cases} -->\n<!-- $$ -->\n\n<!-- where $v_i$ is the IPTW weight for individual $i$, $A_i$ is the treatment indicator for individual $i$, and $\\hat{e}(L_i)$ is the estimated propensity score for individual $i$.    -->\n\n<!-- How might we use these weights to obtain causal effect estimates? -->\n\n\n\n<!-- ## Marginal Structural Models (MSMs) -->\n\n<!-- Marginal Structural Models (MSMs) estimate causal effects without requiring an \"outcome model\" that stratifies on covariates. Rather, MSMs employ weights derived from the inverse probability of treatment weighting (IPTW) to create a pseudo-population in which the distribution of covariates is independent of treatment assignment over time. -->\n\n<!-- The general form of an MSM can be expressed as follows: -->\n\n<!-- $$ -->\n<!-- E[Y^a] = \\beta_0 + \\beta_1a -->\n<!-- $$ -->\n\n<!-- where $E[Y^a]$ is the expected outcome under treatment $a$  and $\\beta_0$ and $\\beta_1$ are parameters estimated by fitting the weighted model. Again, the weights used in the MSM, typically derived from the IPTW (or another treatment model), adjust for the confounding, allowing the model to estimate the unbiased effect of the treatment on the outcome without requiring covariates in the model. -->\n\n<!-- Where do weights fit in?   Note, we have $E[Y^a]$ in please of $E[Y|A=a]$.  When applying propensity score weights in the linear regression model $E[Y^a] = \\beta_0 + \\beta_1a$, each observation is weighted by $v_i$, such that $v_i(\\beta_0 + \\beta_1a)$. This changes the estimation process to focus on a weighted sum of outcomes, where each individual's contribution is adjusted to reflect their probability of receiving the treatment, given their covariates. -->\n\n\n<!-- ## Interpretation of $\\beta_0$ and $\\beta_1$  in a Marginal Structural Model -->\n\n<!-- ### Binary Treatment -->\n\n<!-- In models where the treatment $a$ is binary (e.g., $a = 0$ or $a = 1$), such as in many causal inference studies: -->\n\n<!-- - **$\\beta_0$**: the expected value of the outcome $Y$ when the treatment is not applied ($a = 0$). This is the baseline level of the outcome in the absence of treatment. -->\n<!-- - **$\\beta_1$**: the change in the expected outcome when the treatment status changes from 0 to 1. In logistic regression, $\\beta_1$ represents the log-odds ratio of the outcome for the treatment group relative to the control group. In linear regression, $\\beta_1$ quantifies the difference in the average outcome between the treated and untreated groups. -->\n\n<!-- ### Continuous Treatment -->\n\n<!-- When the treatment $a$ is continuous, the interpretation of $\\beta_0$ and $\\beta_1$ adjusts slightly: -->\n\n<!-- - **$\\beta_0$**: represents the expected value of the outcome $Y$ when the treatment $a$ is at its reference value (often zero).  -->\n<!-- - **$\\beta_1$**: represents the expected change in the outcome for each unit increase in the treatment. In this case, $\\beta_1$ measures the gradient or slope of the relationship between the treatment and the outcome. For every one-unit increase in treatment, the outcome changes by $\\beta_1$ units, assuming all other factors remain constant. -->\n\n\n<!-- ###  How can we apply marginal structural models in subgroups?  -->\n\n\n<!-- ### Assumptions -->\n\n<!-- - **Model assumptions**: the treatment model is correctly specified. -->\n<!-- - **Causal assumptions**: all confounders are appropriately controlled, positivity and consistency assumptions hold. -->\n\n\n<!-- ### Calculating Treatment Weights (Propensity Scores) and Confounding Control in Subgroups -->\n\n<!-- We may often achieve greater balance when conducting weighted analyses in subgroups by estimating propensity scores *within* these subgroups. The propensity score $ e(L, G) $ is the conditional probability of receiving the exposure $ A = 1 $, given the covariates $ L $ and subgroup indicator $ G $. This is often modelled using logistic regression or other methods that ensure covariate balance -->\n<!-- We define the estimated propensity score as follows: -->\n\n<!-- $$ -->\n<!-- \\hat{e} = P(A = 1 \\mid L, G) = f_A(L, G; \\theta_A) -->\n<!-- $$ -->\n\n<!-- Here, $ f_A(L, G; \\theta_A) $ is the statistical model estimating the probability of exposure $A = 1$ given covariates $L$ and subgroup $G$. We then calculate the weights for each individual, denoted $v$, using the estimated propensity score: -->\n\n<!-- $\\theta_A$ encapsulates all the coefficients (parameters) in this model, including intercepts, slopes, and potentially other parameters depending on the model complexity (e.g., interaction terms, non-linear effects...etc). -->\n\n<!-- These weights $v$ depend on $A$ and are calculated as the inverse of the propensity score for exposed individuals and as the inverse of $ 1-\\hat{e} $ for unexposed individuals. -->\n\n<!-- Propensity scores are estimated *separately* within strata of the subgroup to control for potential confounding tailored to each subgroup. These weights $v$ are specific to each individual in subgroup $G$. In the lab, we will clarify how to fit models to estimate contrasts for the causal effects within groups $\\hat{\\delta}_{g}, \\hat{\\delta}_{g'}$, etc., and how to obtain estimates for group-wise differences: -->\n\n<!-- $$ -->\n<!-- \\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y^a \\mid G=g] - \\hat{E}[Y^{a'} \\mid G=g] \\big)}^{\\hat{\\delta}_g} - \\overbrace{\\big( \\hat{E}[Y^{a'} \\mid G=g'] - \\hat{E}[Y^a \\mid G=g'] \\big)}^{\\hat{\\delta}_{g'}} -->\n<!-- $$ -->\n\n\n<!-- - **$\\hat{E}[Y^a \\mid G=g]$**: Estimated expected outcome when treatment $a$ is applied to subgroup $G=g$. -->\n<!-- - **$\\hat{E}[Y^{a'} \\mid G=g]$**: Estimated expected outcome when a different treatment or control $a'$ is applied to the same subgroup $G=g$. -->\n<!-- - **$\\hat{\\delta}_g$**: Represents the estimated treatment effect within subgroup $G=g$, computed as the difference in expected outcomes between treatment $a$ and $a'$ within this subgroup. -->\n\n<!-- - **$\\hat{E}[Y^{a'} \\mid G=g']$**: Estimated expected outcome when treatment $a'$ is applied to a different subgroup $G=g'$. -->\n<!-- - **$\\hat{E}[Y^a \\mid G=g']$**: Estimated expected outcome when treatment $a$ is applied to subgroup $G=g'$. -->\n<!-- - **$\\hat{\\delta}_{g'}$**: Represents the estimated treatment effect within subgroup $G=g'$, computed as the difference in expected outcomes between treatment $a'$ and $a$ within this subgroup. -->\n\n<!-- - **$\\hat{\\gamma}$**: The overall measure calculated from your formula represents the difference in treatment effects between two subgroups, $G=g$ and $G=g'$. It quantifies how the effect of switching between treatments $a$ and $a'$ differs across the two subgroups. -->\n\n\n<!-- ### Considerations -->\n\n<!-- - **Estimation**: to estimate the expected outcomes $\\hat{E}[Y^a \\mid G]$ and $\\hat{E}[Y^{a'} \\mid G]$, we require statistical models. If we use regression, we include interaction terms between treatment and subgroup indicators to directly estimate subgroup-specific treatment effects. Our use depends on correct model specification. -->\n<!-- - **Confidence intervals**: we may compute confidence intervals for $\\hat{\\gamma}$ using bootstrap, the delta method, or -- in our excercises -- simulation based methods. -->\n<!-- - **Causal assumptions**: again, a causal interpretation of $\\hat{\\gamma}$ relies on satisfying both causal assumptions and modelling assumptions.  Here, we have described estimation using propensity scores. -->\n\n\n<!-- ## Doubly Robust Estimation -->\n\n<!-- We can combine regression-based estimation with propensity score estimation to obtain *doubly robust* estimation. I will walk you through the steps in the lab. The TL;DR is this: doubly robust estimation reduces reliance on correct model specification. If either the PS model or the regression model is correctly specified, the model will be unbiased -- if the other causal inference assumptions are met. -->\n\n<!-- We cannot know whether these assumptions are met, we will need to do a sensitivity analysis, the topic of next week. -->\n\n<!-- I'll show you in lab how to employ simulation-based inference methods to compute standard errors and confidence intervals, following the approaches suggested by Greifer (2023)[]. -->\n\n<!-- ## Extra Readings n on Propensity Scores: -->\n\n<!-- Noah Griefer's Software and Blogs: [https://ngreifer.github.io/blog/subgroup-analysis-psm/](https://ngreifer.github.io/blog/) -->",
    "supporting": [
      "08-content_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}