{
  "hash": "575f29a41da9f67a24ec6a1c5ac93b93",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Causal inference: a step by step guide\"\ndate: \"2025-MAY-06\"\nbibliography: /Users/joseph/GIT/templates/bib/references.bib\neditor_options: \n  chunk_output_type: console\nformat:\n  html:\n    warnings: FALSE\n    error: FALSE\n    messages: FALSE\n    code-overflow: scroll\n    highlight-style: kate\n    code-tools:\n      source: true\n      toggle: FALSE\nhtml-math-method: katex\nreference-location: margin\ncitation-location: margin\ncap-location: margin\ncode-fold: true\ncode-block-border-left: true\n---\n\n\n\n\n::: {.callout-note}\n**Required**\n- grf package readings [https://grf-labs.github.io/grf/](https://grf-labs.github.io/grf/)\n- Do \"Homework\" form Lecture 8\n\n**Optional**\n- [@Bulbulia2024PracticalGuide] [link](https://osf.io/preprints/psyarxiv/uyg3d)\n- [@hoffman2023] [link](https://arxiv.org/pdf/2304.09460.pdf)\n- [@vanderweele2020] [link](https://www.dropbox.com/scl/fi/srpynr0dvjcndveplcydn/OutcomeWide_StatisticalScience.pdf?rlkey=h4fv32oyjegdfl3jq9u1fifc3&dl=0)\n:::\n\n::: {.callout-important}\n## Key concepts\n\nDoing a causal analysis\n\n:::\n\n::: {.callout-important}\n-  Download the R script\n-  Download the relevant libraries.\n-  Will go through this script step-by-step.\n:::\n\n\n\n\n## LAB\n\n\n### Script 0 Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 00 get data in\n# example script 1: data wrangling\n# spring 2025\n# example estimation of average treatment effect - script 1\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# restart fresh session if needed\n# rstudioapi::restartSession()\n\n# set seed for reproducibility\nset.seed(123)\n\n# save paths -------------------------------------------------------------------\n# specify the path where data will be saved\n# this is the path used by joseph\n# push_mods <- here::here('/Users/joseph/v-project\\ Dropbox/data/courses/25-psych-434')\n# replace with your own path after creating a data file\n\npull_mods_data <- here::here(\"data\")\n\n# load libraries ---------------------------------------------------------\n# install and load 'margot' package if not already installed\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\") # ensure version is at least 1.0.32\n  library(\"margot\")\n}\n\n# load required libraries\nlibrary(margot)\nlibrary(tidyverse)\nlibrary(qs)\nlibrary(here)\n\n# import synthetic data ---------------------------------------------------\n#link to synthetic data\nurl <- \"https://www.dropbox.com/scl/fi/ru0ecayju04ja8ky1mhel/df_nz_long.qs?rlkey=prpk9a5v4vcg1ilhkgf357dhd&dl=1\"\n\n# download data to a temporary file\ntmp <- tempfile(fileext = \".qs\")\ndownload.file(url, tmp, mode = \"wb\")\n\n# read data into R\nlibrary(qs)\ndf_nz_long <- qread(tmp)\n\n# # view first few rows of synthetic data\nhead(df_nz_long)\n\n# # save data to your directory for later use and comment the above\nmargot::here_save_qs(df_nz_long,\"df_nz_long\" ,pull_mods_data)\n```\n:::\n\n\n\n### Script 01 Initial Data Wrangling\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# example 2 script 1: data wrangling\n# may 2025\n# example estimation of average treatment effect - script 1\n# questions: joseph.bulbulia@vuw.ac.nz\n\n# restart fresh session if needed\n# rstudioapi::restartSession()\n\n\n\n# set seed ----------------------------------------------------------------\n\n# set seed for reproducibility\nset.seed(123)\n\n\n# load libraries ---------------------------------------------------------\n# install and load 'margot' package if not already installed\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\") # ensure version is at least 1.0.32\n  library(margot)\n}\n\n# install/  load pacman\nif (!require(pacman, quietly = TRUE)) {\n  install.packages(\"pacman\")\n  library(pacman)\n}\n\n\n\n# load required packages -------------------------------------------------------\npacman::p_load(\n  # causal inference\n  clarify,      # sensitivity analysis\n  cobalt,       # covariate balance tables and plots\n  lmtp,         # longitudinal targeted maximum likelihood estimation\n  margot,       # functions for causal inference\n  MatchIt,      # matching methods\n  MatchThem,    # matching for multiply imputed datasets\n  policytree,   # causal inference with policy trees\n  WeightIt,     # weighting methods for covariate balancing\n  \n  # data processing\n  data.table,   # fast data wrangling\n  fastDummies,  # fast creation of dummy variables\n  fs,           # cross-platform file system operations\n  qs,           # saving\n  here,         # simple and robust file referencing\n  janitor,      # data cleaning and validation\n  naniar,       # handling and visualization of missing data\n  skimr,        # summary statistics for data frames\n  tidyverse,    # collection of \"R\" packages for data science\n  \n  # machine learning\n  glmnet,       # lasso and elastic-net regularized models\n  grf,          # generalized random forests\n  ranger,       # fast implementation of random forests\n  SuperLearner, # ensemble learning\n  xgboost,      # extreme gradient boosting\n  \n  # visualization\n  DiagrammeR,   # graph and network visualization\n  ggbeeswarm,   # data visualization   \n  ggplot2,      # data visualization\n  kableExtra,   # advanced table formatting\n  \n  # parallel processing\n  doParallel,   # parallel processing with foreach\n  progressr,    # progress reporting for \"R\"\n  \n  # analysis\n  parameters,   # parameters and performance metrics\n  EValue        # compute e-values\n)\n\n# check package versions\npackageVersion(pkg = 'margot')    # make sure it is 1.0.19 or greater\npackageVersion(pkg = 'boilerplate')\n\n\n\n# save paths -------------------------------------------------------------------\n\n# data path\npull_data <- here::here(\"data\")\n\n# create a folder called \"models\" in the main directory\ndir.create(\"models_example_2\")\n\n# then create a link to this directory using `here::here()`\npush_mods <- here::here(\"models_example_2\")\n\n# read data from saved file\ndf_nz_long <- margot::here_read_qs(\"df_nz_long\", pull_data)\n\n\n# prepare initial dataframe -----------------------------------------------\ndat_prep <- df_nz_long |>\n  arrange(id, wave) |>\n  as.data.frame() |>\n  margot::remove_numeric_attributes() |>\n  mutate(sample_weights = sample_weights) |> \n  mutate(alcohol_intensity = if_else(alcohol_intensity >=15, 15, alcohol_intensity)) |>\n  mutate(heavy_drinker = ifelse(df_nz_long$alcohol_frequency %in% c(3, 4), 1,\n                                ifelse(df_nz_long$alcohol_frequency %in% c(0, 1, 2), 0, NA))) |> \n  mutate(\n    alcohol_frequency_weekly = case_when(\n      alcohol_frequency == 0 ~ 0,\n      alcohol_frequency == 1 ~ 0.25,\n      alcohol_frequency == 2 ~ 1,\n      alcohol_frequency == 3 ~ 2.5,\n      alcohol_frequency == 4 ~ 4.5,\n      alcohol_frequency == 5 ~ NA_real_, # assign NA for 'Don't know'\n      TRUE ~ NA_real_ # handles any unexpected values as NA\n    )\n  ) |> \n  mutate(religion_church = round( ifelse(religion_church > 8, 8, religion_church)),1) |>  # to simplify\n  arrange(id, wave) |>\n  droplevels()\n\n\n\n# define study variables --------------------------------------------------------\n\n# view variable names in the dataset\nglimpse(dat_prep)\n\n# count the number of unique participants\nlength(unique(dat_prep$id))\n\n# define exposure variable\nname_exposure <- c(\"extraversion\") # ← ** define for your study **\n\nvar_labels_exposure = c(\"extraversion\" = \"Extraversion\",# ← ** define for your study **\n                        \"extraversion_binary\" = \"Extraversion (binary)\") # ← ** define **\n\n# save variable labels for manuscript\nhere_save(var_labels_exposure, \"var_labels_exposure\")\n\n# define variable names for binary exposure\nname_exposure_binary <- paste0(name_exposure, \"_binary\")\nt0_name_exposure_continuous <- paste0(\"t0_\", name_exposure)\nt0_name_exposure_binary <- paste0(\"t0_\", name_exposure, \"_binary\")\n\n# define wide variable names\nt0_name_exposure <- paste0(\"t0_\", name_exposure)\nt0_name_exposure_continuous <- paste0(\"t0_\", name_exposure)\nt0_name_exposure_binary <- paste0(\"t0_\", name_exposure, \"_binary\")\n\n# define variable names for exposures used in models\nt1_name_exposure <- paste0(\"t1_\", name_exposure)\nt1_name_exposure_binary <- paste0(\"t1_\", name_exposure, \"_binary\")\n\n# define study waves -----------------------------------------------------------\nbaseline_wave <- \"2018\"\nexposure_waves <- c(\"2019\")\noutcome_wave <- \"2020\"\n\n# define wave combinations for analysis\nall_waves <- c(baseline_wave, exposure_waves, outcome_wave)\nbaseline_and_exposure_waves <- c(baseline_wave, exposure_waves)\nbaseline_and_outcome_waves <- c(baseline_wave, outcome_wave)\n\n# define scale ranges ----------------------------------------------------------\nscale_range_exposure <- c(1, 7)  # used for descriptive graphs\nscale_ranges_outcomes <- c(1, 7)  # used for descriptive graphs\n\n# data preparation -------------------------------------------------------------\n# import and prepare data\n\n# get total sample size\nn_total <- skimr::n_unique(df_nz_long$id)\nn_total <- margot::pretty_number(n_total)\nmargot::here_save(n_total, \"n_total\")\n\n# view\nn_total\n\n# Define Baseline Variables ----------------------------------------------------\nbaseline_vars <- c(\n  # note all outcomes will be added to baseline vars later. \n  # demographics\n  \"age\",\n  \"born_nz_binary\",\n  \"education_level_coarsen\",\n  \"employed_binary\",\n  \"eth_cat\",\n  \"male_binary\",\n  \"not_heterosexual_binary\",\n  \"parent_binary\",\n  \"partner_binary\",\n  \"rural_gch_2018_l\",\n  \"sample_frame_opt_in_binary\",\n  \n  # personality traits\n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"neuroticism\",\n  \"openness\",\n  \n  # health and lifestyle\n  \"alcohol_frequency\",\n  \"alcohol_intensity\",\n  \"hlth_disability_binary\",\n  \"log_hours_children\",\n  \"log_hours_commute\",\n  \"log_hours_exercise\",\n  \"log_hours_housework\",\n  \"log_household_inc\",\n  # \"log_hours_community\", # commented out because using as exposure\n  \"short_form_health\",\n  \"smoker_binary\",\n  \n  # social and psychological\n  \"belong\",\n  \"nz_dep2018\",\n  \"nzsei_13_l\",\n  \"political_conservative\",\n  \"religion_identification_level\"\n)\n\n# Sort baseline variables\nbaseline_vars <- sort(baseline_vars)\n\n\n# baseline vars no log ----------------------------------------------------\n\n# for individual plots\nall_waves <- c(baseline_wave, exposure_waves, outcome_wave)\nbaseline_and_exposure_waves <- c(baseline_wave, exposure_waves)\nbaseline_and_outcome_waves <- c(baseline_wave, outcome_wave)\n\n# define baseline variables\nbaseline_vars <- c(\n  \"age\",\n  \"agreeableness\",\n  \"alcohol_frequency_weekly\", \n  \"alcohol_intensity\",\n  \"belong\",\n  \"born_nz_binary\",\n  \"conscientiousness\",\n  \"education_level_coarsen\",\n  \"employed_binary\",\n  \"eth_cat\",\n  \"extraversion\",\n  \"hlth_disability_binary\",\n  \"hlth_fatigue\",\n  \"honesty_humility\",\n  \"kessler_latent_anxiety\",  # will be added as outcomes\n  \"kessler_latent_depression\", # will be added as outcomes\n  \"log_hours_children\",\n  \"log_hours_commute\",\n  \"log_hours_exercise\",\n  \"log_hours_housework\",\n  \"log_household_inc\",\n  \"log_hours_community\",\n  \"male_binary\",\n  \"neuroticism\",\n  \"not_heterosexual_binary\",\n  \"nz_dep2018\",\n  \"nzsei_13_l\",\n  \"openness\",\n  \"parent_binary\",\n  \"partner_binary\",\n  \"political_conservative\",\n  \"religion_identification_level\", \n  \"religion_church_binary\", \n  \"rural_gch_2018_l\",\n  \"rwa\",\n  \"sample_frame_opt_in_binary\", \n  \"sdo\", \n  \"short_form_health\", # will be added as but use as standard\n  \"smoker_binary\" #,\n)\n\n# define baseline variables without log transformation\nbaseline_vars_no_log <- c(\n  baseline_vars,\n  c(\n    \"hours_children\",\n    \"hours_commute\",\n    \"hours_exercise\",\n    \"hours_housework\",\n    \"hours_community\",\n    \"household_inc\"\n  )\n)\n\n\n# sort baseline variables\nbaseline_vars <- sort(baseline_vars)\nbaseline_vars_log <- sort(baseline_vars_no_log)\n\n# outcomes\noutcome_vars <- c(\n  \"alcohol_frequency_weekly\", \n  \"alcohol_intensity\",\n  \"belong\",\n  \"bodysat\",\n  \"forgiveness\",\n  \"gratitude\",\n  \"hlth_bmi\",\n  \"hlth_fatigue\",\n  \"hlth_sleep_hours\",\n  \"kessler_latent_anxiety\",\n  \"kessler_latent_depression\",\n  \"lifesat\",\n  \"log_hours_exercise\",\n  \"meaning_purpose\", \n  \"meaning_sense\",\n  \"neighbourhood_community\",\n  # \"perfectionism\", #  *--* excluded (exposure ) *--*\n  \"pwi\", \n  \"rumination\",\n  \"self_control\",\n  \"self_esteem\",\n  \"sexual_satisfaction\",\n  \"short_form_health\",\n  \"support\"\n)\n\n# sort\noutcome_vars <- sort(outcome_vars)\n\n# save outcomes\nhere_save(outcome_vars, \"outcome_vars\")\n\n# define outcome variables without log transformation\noutcome_vars_no_log <- c(outcome_vars,\"hours_exercise\")\n\n# sort\noutcome_vars_no_log <- sort(outcome_vars_no_log)\n\n# save outcomes\nhere_save(outcome_vars_no_log, \"outcome_vars_no_log\")\n\n# raw outcomes \nraw_outcomes_health <- c(\n  \"alcohol_frequency_weekly\", \n  \"alcohol_intensity\",\n  \"hlth_bmi\", \n  \"log_hours_exercise\", \n  \"hlth_sleep_hours\", \n  \"short_form_health\"\n)\n# sort\nraw_outcomes_health <- sort(raw_outcomes_health)\n\n# save \nhere_save(raw_outcomes_health, \"raw_outcomes_health\")\n\n# with no log\nraw_outcomes_health_no_log <- c(raw_outcomes_health, \"hours_exercise\")\n\n# sort\nraw_outcomes_health_no_log <- sort(raw_outcomes_health_no_log)\n\n# save \nhere_save(raw_outcomes_health_no_log, \"raw_outcomes_health_no_log\")\n\n# define psych outcomes\nraw_outcomes_psych <- c( \n  \"hlth_fatigue\", \n  \"kessler_latent_anxiety\", \n  \"kessler_latent_depression\",  \n  \"rumination\"\n)\n\n# sort\nraw_outcomes_psych <- sort(raw_outcomes_psych)\n\n# save\nhere_save(raw_outcomes_psych, \"raw_outcomes_psych\")\n\n# define present outcomes\nraw_outcomes_present <- c(\n  \"bodysat\",\n  \"forgiveness\",\n  # \"perfectionism\",  *--* excluded  *--*\n  \"self_control\" , \n  \"self_esteem\", \n  \"sexual_satisfaction\" )\n\n# sort\nraw_outcomes_present <- sort(raw_outcomes_present)\n\n# save\nhere_save(raw_outcomes_present, \"raw_outcomes_present\")\n\n# define life outcomes\nraw_outcomes_life <- c( \n  \"gratitude\", \n  \"lifesat\", \n  \"meaning_purpose\", \n  \"meaning_sense\",\n  \"pwi\"  # move personal well-being here if not using individual facents\n)\n\n# sort\nraw_outcomes_life <- sort(raw_outcomes_life)\n\n# save\nhere_save(raw_outcomes_life, \"raw_outcomes_life\")\n\n# define social outcomes\nraw_outcomes_social <- c(\n  \"belong\",\n  \"neighbourhood_community\", \n  \"support\" \n)\n\n# sort\nraw_outcomes_social <- sort(raw_outcomes_social)\n\n# save\nhere_save(raw_outcomes_social, \"raw_outcomes_social\")\n\n# create all outcome variable names ---------------------------------------\n# for tables\nraw_outcomes_all = c(\n  baseline_vars_no_log,\n  raw_outcomes_health_no_log,\n  raw_outcomes_psych,\n  raw_outcomes_present,\n  raw_outcomes_life,\n  raw_outcomes_social\n)\n\n# select only unique measures\nraw_outcomes_all <- unique(raw_outcomes_all)\n\n# save\nhere_save(raw_outcomes_all, \"raw_outcomes_all\")\n\n\n# set time varying confounding --------------------------------------------\n\n# those vars that are commented out are included as time-varying confounders\n\nconfounder_vars <- c(\n  \"hlth_disability_binary\"#,\n  # outcome_vars # not for grf\n)\n\n# select only unique\nconfounder_vars <- unique(confounder_vars)\n\n# view\nconfounder_vars\n\n# save\nhere_save(confounder_vars, \"confounder_vars\")\n\n# names for outcomes ------------------------------------------------------\n# define extra variables\nextra_vars <- c(\"id\", \"wave\", \"year_measured\", \"sample_weights\")\n\n# combine all variables\nall_vars_prep <- c(baseline_vars, exposure_var, outcome_vars, extra_vars)\n\n# get unique\nall_vars <- unique(all_vars_prep)\n\n# sort\nall_vars <- sort(all_vars)\n\n# define extra variables for table\nextra_vars_table <- c(\"id\", \"wave\", \"year_measured\")\nnot_all_vars_prep <- c(baseline_vars_no_log, exposure_var, outcome_vars_no_log, extra_vars_table)\n\n# get unique\nnot_all_vars_prep <- unique(not_all_vars_prep)\n\n# sort\nnot_all_vars <- sort(not_all_vars_prep)\n\n# define columns that will later be handled in a special way\n# define continuous columns that we will not standardise\ncontinuous_columns_keep <- c(\"t0_sample_weights\")\n\n# define ordinal columns that we will expand into binary variables\nordinal_columns <- c(\"t0_education_level_coarsen\", \"t0_eth_cat\", \"t0_rural_gch_2018_l\")\n\n# eligibility  ------------------------------------------------------------\n# select baseline and exposure ids based on eligibility criteria\nids_baseline <- dat_prep |>\n  filter(year_measured == 1, wave == baseline_wave) |>\n  filter(!is.na(!!sym(name_exposure))) |>\n  pull(id)\n\n# if we are allowing missing values in the exposure then ids_study are the ids at baseline\nids_study <- ids_baseline\n\n# get n\nn_participants<- length(ids_study)\n\n# save after making a nice number\nn_participants <- margot::pretty_number(n_participants)\n\n# check\nn_participants\n\n# save\nhere_save(n_participants, \"n_participants\")\n\n# filter data\ndat_long_1 <- dat_prep |>\n  filter(id %in% ids_study & wave %in% c(baseline_wave, exposure_waves, outcome_wave)) |>\n  droplevels() # note that we might have more than one exposure wave\n\n# check\nhead(dat_long_1)\n\n# check\ndim(dat_long_1)\n\n# shift intervention graphs -----------------------------------------------\n# # see appendix for more styles\n# max_exposure <- max(dat_long_1[[name_exposure]], na.rm=TRUE)\n# min_exposure <- min(dat_long_1[[name_exposure]], na.rm=TRUE)\n# \n# # get data for baseline wave and exposure waves\n# dat_shift <- dat_long_1 |> \n#   filter((wave %in% c(baseline_wave, exposure_waves))) |> \n#   select(all_of(name_exposure)) |> \n#   drop_na()\n# \n# # shift conditions\n# # up\n# shift_exposure_up <- margot_plot_shift(\n#   dat_shift,\n#   col_name = name_exposure,\n#   label_mapping = var_labels_exposure,\n#   shift = \"down\",\n#   range_highlight = c(3.1, 7),\n#   binwidth = .25,\n#   save_path = here::here(push_mods),\n#   show_avg_line = TRUE\n# )\n# \n# # view\n# shift_exposure_up\n\n# binary graph ------------------------------------------------------------\n\n# get only exposure wave\ndat_long_exposure <- dat_long_1 |> filter(wave %in% exposure_waves ) |> droplevels()\n\n\n# make a plot to evaluate cut points\ngraph_exposure_binary <- margot_plot_categorical(\n  dat_long_exposure,\n  label_mapping = var_labels_exposure,\n  col_name = name_exposure,\n  cutpoint_inclusive = \"upper\",\n  sd_multipliers = c(-1, 1),\n  show_mean = TRUE,\n  show_median = FALSE,\n  show_sd = TRUE,\n  # n_divisions = 2, #  ← ** define **\n  custom_breaks = c(1,4),#  ← ** define **\n  binwidth = .2)\n\n# view\nprint(graph_exposure_binary)\n\n\n\n# end plot ----------------------------------------------------------------\n\n# create categorical variable ---------------------------------------------\ndat_long_2 <- create_ordered_variable(\n  dat_long_1,  # make sure this is correct\n  var_name = name_exposure,\n  cutpoint_inclusive = \"upper\",\n  #n_divisions = 2\n  custom_breaks = c(1,4),\n)\n\n# view\ntable( dat_long_2$religion_church_binary) \n\n\n\n# convert binary factors to 0, 1 -----------------------------------------\n# we do this using a custom function\n\n# religion church binary already named \"binary\"\ndat_long_3 <- margot::margot_process_binary_vars(dat_long_2, exceptions = \"religion_church_binary\")\n\n\ndat_long_4 <- dat_long_3\n\n\n\n\n# log-transform 'hours_' variables ----------------------------------------\n\ndat_long_table <- margot_log_transform_vars(\n  dat_long_4,\n  vars = c(starts_with(\"hours_\"), \"household_inc\"),\n  # consider gen_cohort if used\n  #  exceptions = \"hours_work\", no exceptions\n  prefix = \"log_\",\n  keep_original = TRUE ## Set to FALSE\n) |>\n  select(all_of(not_all_vars)) |>\n  droplevels()\n\n# view\ndat_long_table\n\n\n# baseline outcome waves --------------------------------------------------\n# make tables ----------------------------------------------------------\n\nvar_labels_health <- list(\n  \"alcohol_frequency_weekly\" = \"Alcohol Frequency (weekly)\", \n  \"alcohol_intensity\" = \"Alcohol Intensity\", \n  \"hlth_bmi\" = \"Body Mass Index\", \n  \"hlth_sleep_hours\" = \"Sleep\", \n  \"hours_exercise\" = \"Hours of Exercise\",   #logged in models\n  \"short_form_health\" = \"Short Form Health\" \n)\n\n# define psych outcomes \nvar_labels_psych<- list(\n  \"hlth_fatigue\" = \"Fatigue\", \n  \"kessler_latent_anxiety\" = \"Anxiety\", \n  \"kessler_latent_depression\" = \"Depression\",  \n  \"rumination\" = \"Rumination\"\n)\n\n# define present outcomes\nvar_labels_present<- list(\n  \"bodysat\" = \"Body Satisfaction\",\n  \"foregiveness\" = \"Forgiveness\",\n  # \"perfectionism\" = \"Perfectionism\",  \n  \"self_control\" = \"Self Control\",  \n  \"self_esteem\" = \"Self Esteem\", \n  \"sexual_satisfaction\" = \"Sexual Satisfaction\"  )\n\n# define life outcomes\nvar_labels_life <- list(\n  \"gratitude\" = \"Gratitude\", \n  \"lifesat\" = \"Life Satisfaction\", \n  \"meaning_purpose\" = \"Meaning: Purpose\", # exposure variable\n  \"meaning_sense\" = \"Meaning: Sense\",\n  \"pwi = Personal Well-being Index\"\n)\n\n# define social outcome names\nvar_labels_social <- list(\n  \"belong\" = \"Social Belonging\",\n  \"neighbourhood_community\" = \"Neighbourhood Community\", \n  \"support\" = \"Social Support\" \n)\n\n\n# make labels\nvar_labels_baseline <- list(\n  \"sdo\" = \"Social Dominance Orientation\",\n  \"belong\" = \"Social Belonging\",\n  \"born_nz_binary\" = \"Born in NZ\",\n  \"rural_gch_2018_l\" = \"Rural Gch 2018 Levels\",\n  \"education_level_coarsen\" = \"Education Level\",\n  \"employed_binary\" = \"Employed (binary)\",\n  \"eth_cat\" = \"Ethnicity\",\n  \"household_inc\" = \"Household Income\",\n  \"log_household_inc\" = \"Log Household Income\",\n  \"male_binary\" = \"Male (binary)\",\n  \"nz_dep2018\" = \"NZ Deprevation Index 2018\",\n  \"nzsei_13_l\" = \"NZSEI (Occupational Prestige Index)\",\n  \"parent_binary\" = \"Parent (binary)\",\n  \"rwa\" = \"Right Wing Authoritarianism\",\n  \"sample_frame_opt_in_binary\" = \"Sample Frame Opt-In (binary)\",\n  \"sdo\" = \"Social Dominance Orientation\",\n  \"smoker_binary\" = \"Smoker (binary)\",\n  \"support\" = \"Social Support\" \n)\n\n\n\n\n# labels ---------------------------------------------------------------------\n\nvar_labels_health <- list(\n  \"alcohol_frequency_weekly\" = \"Alcohol Frequency (weekly)\", \n  \"alcohol_intensity\" = \"Alcohol Intensity\", \n  \"hlth_bmi\" = \"Body Mass Index\", \n  \"hlth_sleep_hours\" = \"Sleep\", \n  \"hours_exercise\" = \"Hours of Exercise\",   #logged in models\n  \"short_form_health\" = \"Short Form Health\" \n)\n\n# define psych outcomes \nvar_labels_psych<- list(\n  \"hlth_fatigue\" = \"Fatigue\", \n  \"kessler_latent_anxiety\" = \"Anxiety\", \n  \"kessler_latent_depression\" = \"Depression\",  \n  \"rumination\" = \"Rumination\"\n)\n\n# define present outcomes\nvar_labels_present<- list(\n  \"bodysat\" = \"Body Satisfaction\",\n  \"foregiveness\" = \"Forgiveness\",\n  \"perfectionism\" = \"Perfectionism\",  \n  \"self_control\" = \"Self Control\",  \n  \"self_esteem\" = \"Self Esteem\", \n  \"sexual_satisfaction\" = \"Sexual Satisfaction\"  \n)\n\n# define life outcomes\nvar_labels_life <- list(\n  \"gratitude\" = \"Gratitude\", \n  \"lifesat\" = \"Life Satisfaction\", \n  \"meaning_purpose\" = \"Meaning: Purpose\", # exposure variable\n  \"meaning_sense\" = \"Meaning: Sense\",\n  \"pwi = Personal Well-being Index\"\n)\n\n# define social outcome names\nvar_labels_social <- list(\n  \"belong\" = \"Social Belonging\",\n  \"neighbourhood_community\" = \"Neighbourhood Community\", \n  \"support\" = \"Social Support\" \n)\n\n\n# make labels\nvar_labels_baseline <- list(\n  \"sdo\" = \"Social Dominance Orientation\",\n  \"belong\" = \"Social Belonging\",\n  \"born_nz_binary\" = \"Born in NZ\",\n  \"rural_gch_2018_l\" = \"Rural Gch 2018 Levels\",\n  \"education_level_coarsen\" = \"Education Level\",\n  \"employed_binary\" = \"Employed (binary)\",\n  \"eth_cat\" = \"Ethnicity\",\n  \"household_inc\" = \"Household Income\",\n  \"log_household_inc\" = \"Log Household Income\",\n  \"male_binary\" = \"Male (binary)\",\n  \"nz_dep2018\" = \"NZ Deprevation Index 2018\",\n  \"nzsei_13_l\" = \"NZSEI (Occupational Prestige Index)\",\n  \"parent_binary\" = \"Parent (binary)\",\n  \"rwa\" = \"Right Wing Authoritarianism\",\n  \"sample_frame_opt_in_binary\" = \"Sample Frame Opt-In (binary)\",\n  \"sdo\" = \"Social Dominance Orientation\",\n  \"smoker_binary\" = \"Smoker (binary)\",\n  \"support\" = \"Social Support\" \n)\n\n# make variable labels for all measures -----------------------------------\nvar_labels_all = c(\n  var_labels_baseline,\n  var_labels_exposure,\n  var_labels_health,\n  var_labels_psych,\n  var_labels_present,\n  var_labels_life,\n  var_labels_social\n)\nvar_labels_all\n\n# save for manuscript\nhere_save(var_labels_all, \"var_labels_all\")\n\n\n# make table --------------------------------------------------------------\n# labels for factors\nrural_labels <- c(\n  \"High Urban Accessibility\",\n  \"Medium Urban Accessibility\",\n  \"Low Urban Accessibility\",\n  \"Remote\",\n  \"Very Remote\"\n)\n\n# new df for table\ndat_long_table_x <- dat_long_table\ndat_long_table_x$rural_gch_2018_l <- factor(\n  dat_long_table_x$rural_gch_2018_l,\n  levels = 1:5,\n  labels = rural_labels,\n  ordered = TRUE  # Optional: if the levels have an inherent order\n)\n\n# only use if using the frequency church variable\n# dat_long_table_x$religion_church <- factor(\n#   dat_long_table_x$religion_church,\n#   levels = 0:8,\n#   ordered = TRUE  # Optional: if the levels have an inherent order\n# ) \n\n\ndat_long_table_baseline = dat_long_table_x |>\n  filter(wave %in% c(baseline_wave)) |>\n  mutate(\n    male_binary = factor(male_binary),\n    parent_binary = factor(parent_binary),\n    smoker_binary = factor(smoker_binary),\n    born_nz_binary = factor(born_nz_binary),\n    employed_binary = factor(employed_binary),\n    not_heterosexual_binary = factor(not_heterosexual_binary),\n    sample_frame_opt_in_binary = factor(sample_frame_opt_in_binary)\n  )\n\n\ndat_long_table_exposure_waves = dat_long_table_x |> \n  filter(wave %in% c(baseline_wave, exposure_waves))\n\ndat_long_table_outcome_waves = dat_long_table_x |> \n  filter(wave %in% c(baseline_wave, outcome_wave))\n\n# make tables\nmarkdown_table_baseline <- margot::margot_make_tables(\n  data = dat_long_table_baseline,\n  vars = baseline_vars_no_log,\n  by = \"wave\",\n  labels = var_labels_baseline,\n  factor_vars = c(\"rural_gch_2018_l\", \"eth_cat\"),\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"#,\n  # kable_opts = list(\n  #   booktabs = TRUE,\n  #   longtable = TRUE,\n  #   font_size = 6,\n  #   latex_options = c(\"hold_position\", \"repeat_header\", \"striped\", \"longtable\")\n  # )\n)\n\n# view\nmarkdown_table_baseline\n\n# save\nmargot::here_save(markdown_table_baseline, \"markdown_table_baseline\")\n\n# make tables\nmarkdown_table_exposures <- margot::margot_make_tables(\n  data = dat_long_table_exposure_waves,\n  vars = exposure_var,\n  by = \"wave\",\n  labels = var_labels_exposure,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"#,\n  # kable_opts = list(\n  #   booktabs = TRUE,\n  #   longtable = TRUE,\n  #   font_size = 6,\n  #   latex_options = c(\"hold_position\", \"repeat_header\", \"striped\", \"longtable\")\n  # )\n)\n\n\n# view\nmarkdown_table_exposures\n\n# save\nhere_save(markdown_table_exposures, \"markdown_table_exposures\")\n\n\ndat_long_table_outcome_waves = dat_long_table_x |> \n  filter(wave %in% c(baseline_wave, outcome_wave))\n\n# make tables\nmarkdown_table_outcomes_all <- margot::margot_make_tables(\n  data = dat_long_table_outcome_waves,\n  vars = outcome_vars_no_log,\n  by = \"wave\",\n  labels = var_labels_all,\n  table1_opts = list(overall = FALSE, transpose = FALSE),\n  format = \"markdown\"#,\n  # kable_opts = list(\n  #   booktabs = TRUE,\n  #   longtable = TRUE,\n  #   font_size = 6,\n  #   latex_options = c(\"hold_position\", \"repeat_header\", \"striped\", \"longtable\")\n  # )\n)\n# view\nmarkdown_table_outcomes_all\n\n# save\nhere_save(markdown_table_outcomes_all, \"markdown_table_outcomes_all\")\n\n\n# data for study, remove originals ----------------------------------------\n## create gender weights if needed\ndat_long_selected <- dat_long_4\n\n# not needed in this study\n# dat_long_selected$gender_weights <- margot_compute_gender_weights_by_wave(\n#   dat_long_selected,\n#   male_col = \"male_binary\",\n#   wave_col = \"wave\",\n#   target_male_prop = 0.48\n# )\n\n# set logs\ndat_long_prepare <- margot::margot_log_transform_vars(\n  dat_long_selected,\n  vars = c(starts_with(\"hours_\"), \"household_inc\"),\n  #  exceptions = \"hours_work\", no exceptions\n  prefix = \"log_\",\n  keep_original = TRUE ## Set to FALSE\n) |>\n  select(all_of(all_vars)) |>\n  droplevels()\n\n# check\nmargot::here_save(dat_long_prepare, \"dat_long_prepare\")\n\n# check\ncolnames(dat_long_prepare)\n\n\n\n# get baseline missingness ------------------------------------------------\n\n# get baseline data\ndat_baseline <- dat_long_prepare |>\n  filter(wave == baseline_wave)\n\n# sample weights\nt0_sample_weights <- dat_baseline$sample_weights\n\nmargot::here_save(t0_sample_weights, \"t0_sample_weights\")\n\n\n# transition matrices -----------------------------------------------------\n# used to verify positivity\ndt_positivity <- dat_long_table_exposure_waves |>\n  select(!!sym(name_exposure), id, wave, year_measured) |>\n  mutate(exposure= round( as.numeric(!!sym(name_exposure))),0) |>\n  # for this study\n  mutate(exposure_binary = ifelse(exposure>=4, 1, 0)) |>\n  droplevels()\ndt_positivity$wave <- as.numeric((dt_positivity$wave))\n\n# view\ntransition_table <- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure\",\n  id_var = \"id\",\n  observed_var = \"year_measured\",\n  observed_val = 1,\n  waves = c(1:2),\n  wave_var = \"wave\",\n  table_name = \"transition_table\"\n)\n# explanation\ncat(transition_table$explanation)\n\n# tables\ntransition_table$tables[[1]]\n\n\n# for publication\ntransition_table$quarto_code()\n\n# save\nhere_save(transition_tables, \"transition_tables\")\n\n\n# binary table\n# view\ntransition_tables_binary <- margot::margot_transition_table(\n  dt_positivity,\n  state_var = \"exposure_binary\",\n  id_var = \"id\",\n  observed_var = \"year_measured\",\n  observed_val = 1,\n  waves = c(1:2),\n  wave_var = \"wave\",\n  table_name = \"transition_table_binary\"\n)\n# explanation\ncat(transition_tables_binary$explanation)\n\n# tables\ntransition_tables_binary$tables[[1]]\n\n# for publication\ntransition_tables_binary$quarto_code()\n\n\n# check missing values ---------------------------------------------------\n# and look for other problems\nnaniar::miss_var_summary(dat_long_prepare) |> print(n = 100)\nnaniar::gg_miss_var(dat_long_prepare)\nnaniar::vis_miss(dat_long_prepare, warn_large_data = FALSE)\n\ndat_baseline <- dat_long_prepare |> filter(wave == baseline_wave)\n\n# make percentage missing at baseline\npercent_missing_baseline <- naniar::pct_miss(dat_baseline)\n\n# save\nhere_save(percent_missing_baseline, \"percent_missing_baseline\")\n\n# view\npercent_missing_baseline\n\n\n# If everything looks OK, save the data and indicators --------------------\n# save the data and indicators\nmargot::here_save(name_exposure, \"name_exposure\")\nmargot::here_save(baseline_vars, \"baseline_vars\")\nmargot::here_save(exposure_var, \"exposure_var\")\nmargot::here_save(outcome_vars, \"outcome_vars\")\nmargot::here_save(baseline_wave, \"baseline_wave\")\nmargot::here_save(exposure_waves, \"exposure_waves\")\nmargot::here_save(outcome_wave, \"outcome_wave\")\nmargot::here_save(extra_vars, \"extra_vars\")\nmargot::here_save(all_vars, \"all_vars\")\nmargot::here_save(continuous_columns_keep, \"continuous_columns_keep\")\nmargot::here_save(ordinal_columns, \"ordinal_columns\")\n\n# save names and labels\nmargot::here_save(baseline_wave, \"baseline_wave\")\nmargot::here_save(exposure_waves, \"exposure_waves\")\nmargot::here_save(outcome_wave, \"outcome_wave\")\nmargot::here_save(all_waves, \"all_waves\")\nmargot::here_save(baseline_and_exposure_waves, \"baseline_and_exposure_waves\")\nmargot::here_save(baseline_and_outcome_waves, \"baseline_and_outcome_waves\")\n```\n:::\n\n\n\n### Script 2 Secondary Data Wrangling\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# # example 2 02-data-wrangling-grf-model\n# get data into wide format and ready for modelling using grf\n# joseph.bulbulia@gmail.com\n# may 2025\n\n# restart fresh session\n# rstudioapi::restartSession()\n# set reproducibility\nset.seed(123)\n\n# load libraries ---------------------------------------------------------\n# install and load 'margot' package\n# make sure you have at least margot 1.0.31\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n# save paths -------------------------------------------------------------------\n# create a save path to your on computer\n# this is mine\n# yours might be (after creating a data file)\npush_mods <- here::here(\"models_example_2\")\n\n# load necessary libraries\npacman::p_load(\n  clarify,     # sensitivity analysis for causal inference\n  cobalt,      # covariate balance tables and plots\n  DiagrammeR,  # graph and network visualization\n  doParallel,  # parallel processing with foreach\n  fastDummies, # fast creation of dummy variables\n  fs,          # cross-platform file system operations\n  ggplot2,     # data visualisation\n  glmnet,      # lasso and elastic-net regularized models\n  grf,         # generalized random forests\n  here,        # simple and robust file referencing\n  janitor,     # data cleaning and validation\n  kableExtra,  # advanced table formatting\n  lmtp,        # longitudinal targeted maximum likelihood estimation\n  MatchIt,     # matching methods for causal inference\n  MatchThem,   # matching methods for multiply imputed datasets\n  naniar,      # handling and visualization of missing data\n  parameters,  # parameters and performance metrics\n  policytree,  # causal inference with policy trees\n  progressr,   # progress reporting for R\n  ranger,      # fast implementation of random forests\n  skimr,       # summary statistics for data frames\n  SuperLearner,# ensemble learning\n  tidyverse,   # collection of R packages for data science\n  WeightIt,    # weighting methods for covariate balancing\n  xgboost,     # extreme gradient boosting\n  EValue,      # compute Evalues\n  data.table,  # fast data wrangling\n  maq,         # qini curves\n  purrr,       # data wrangling\n  patchwork,   # multiple plots\n  labelled,\n  purrr\n  )\n\n# read variables\nbaseline_vars <- margot::here_read(\"baseline_vars\")\nexposure_var <- margot::here_read(\"exposure_var\")\noutcome_vars <- margot::here_read(\"outcome_vars\")\nt0_sample_weights <- margot::here_read(\"t0_sample_weights\")\nbaseline_wave <- margot::here_read(\"baseline_wave\")\nexposure_waves <- margot::here_read(\"exposure_waves\")\noutcome_wave <- margot::here_read(\"outcome_wave\")\ncontinuous_columns_keep <- margot::here_read(\"continuous_columns_keep\")\nordinal_columns <- margot::here_read(\"ordinal_columns\")\n\n# define continuous columns to keep\ncontinuous_columns_keep <- c(\"t0_sample_weights\")  \n\n# read data\ndat_long_prepare <- margot::here_read(\"dat_long_prepare\")\n\n# read exposure\nname_exposure <- margot::here_read(\"name_exposure\")\n\n# raw outcomes\n# read health outcomes \nraw_outcomes_health <- here_read(\"raw_outcomes_health\")\nt2_outcome_health_z <- paste0(\"t2_\", raw_outcomes_health, \"_z\")\nt2_outcome_health_z <- sort(t2_outcome_health_z)\nt2_outcome_health_z\n\n# read raw outcomes \nraw_outcomes_psych <- here_read(\"raw_outcomes_psych\")\nt2_outcome_psych_z <- paste0(\"t2_\", raw_outcomes_psych, \"_z\")\nt2_outcome_psych_z <- sort(t2_outcome_psych_z)\nt2_outcome_psych_z\n\n# read raw outcomes\nraw_outcomes_present <- here_read(\"raw_outcomes_present\")\nt2_outcome_present_z <- paste0(\"t2_\", raw_outcomes_present, \"_z\")\nt2_outcome_present <- sort(t2_outcome_present_z)\nt2_outcome_present_z\n\n# read raw outcomes\nraw_outcomes_life <- here_read(\"raw_outcomes_life\")\nt2_outcome_life_z <- paste0(\"t2_\", raw_outcomes_life, \"_z\")\nt2_outcome_life_z <- sort(t2_outcome_life_z)\nt2_outcome_life_z\n\n# read raw outcomes\nraw_outcomes_social <- here_read(\"raw_outcomes_social\")\nt2_outcome_social_z <- paste0(\"t2_\", raw_outcomes_social, \"_z\")\nt2_outcome_social_z <- sort(t2_outcome_social_z)\nt2_outcome_social_z\n\n# treatments\nt0_name_exposure <- paste0(\"t0_\", name_exposure)\nt1_name_exposure <- paste0(\"t1_\", name_exposure)\n\n# sort\noutcome_vars <- sort(outcome_vars)\noutcome_vars\n\n# time-varying confounders\nconfounder_vars <- here_read(\"confounder_vars\")\n# ensure unique\nconfounder_vars <- unique(confounder_vars)\n# check\nconfounder_vars\n\n# sort\noutcome_vars <- sort(outcome_vars)\noutcome_vars\n\n# view\nstr(dat_long_prepare)\n\n# check\nnaniar::gg_miss_var(dat_long_prepare)\n\n# impute data --------------------------------------------------------------\n# read data in\ndat_long_prepare <- margot::here_read(\"dat_long_prepare\")\nname_exposure <- margot::here_read(\"name_exposure\") # USE THE CATEGORICAL EXPOSURE\n\n# check\nname_exposure_binary = paste0(name_exposure, \"_binary\")\nname_exposure_continuous = name_exposure\n\n#check\nname_exposure_binary\nname_exposure_continuous\n\n# get vars\nbaseline_vars <- margot::here_read(\"baseline_vars\")\noutcome_vars <- margot::here_read(\"outcome_vars\")\n\n# make both\nname_exposure_both <- c(name_exposure_binary,name_exposure_continuous)\n\n#check\nname_exposure_both\n\n# baseline_exposure_cat <- margot::here_read(\"baseline_exposure_cat\")\nbaseline_wave <- margot::here_read(\"baseline_wave\")\nexposure_waves <- margot::here_read(\"exposure_waves\")\noutcome_wave <- margot::here_read(\"outcome_wave\")\nextra_vars <- margot::here_read(\"extra_vars\")\nall_vars <- margot::here_read(\"all_vars\")\nt0_sample_weights <- margot::here_read(\"t0_sample_weights\")\n\n\n# check\n# define wide variable names\nt0_name_exposure_binary <- paste0(\"t0_\", name_exposure_binary)\nt0_name_exposure_binary\n\n\n# make exposure names (continuous not genreally used)\nt1_name_exposure_binary <- paste0(\"t1_\",name_exposure_binary)\nt1_name_exposure_binary\n\n# for predictive models for censoring/ use continuous variable for better \n# predictions\nt1_name_exposure_continuous <- paste0(\"t1_\", name_exposure)\n\n# ordinal use\nordinal_columns <- c(\n  \"t0_education_level_coarsen\",\n  \"t0_eth_cat\",\n  \"t0_rural_gch_2018_l\",\n  \"t0_gen_cohort\"\n)\n\n# for\ncontinuous_columns_keep <- c(\"t0_sample_weights\")\n\n# prepare data for analysis ----------------------\ndat_long_prepare <- margot::remove_numeric_attributes(dat_long_prepare)\n\n# wide data\ndf_wide <- margot_wide_machine(dat_long_prepare,\n                               id = \"id\",\n                               wave = \"wave\",\n                               baseline_vars,\n                               exposure_var = name_exposure_both,\n                               outcome_vars,\n                               confounder_vars = NULL,\n                               imputation_method = \"none\",\n                               include_exposure_var_baseline = TRUE,\n                               include_outcome_vars_baseline = TRUE,\n                               extend_baseline = FALSE,\n                               include_na_indicators = FALSE)\n\n# check\ncolnames(df_wide)\n\n# add weights back to data\ndf_wide$t0_sample_weights <- t0_sample_weights\n\n# make sure that rural is a factor\ndf_wide$t0_rural_gch_2018_l <- as.factor(df_wide$t0_rural_gch_2018_l)\n\n# save the wide data\nmargot::here_save(df_wide, \"df_wide\")\n\n#df_wide <- margot::here_read(\"df_wide\")\nnaniar::vis_miss(df_wide, warn_large_data = FALSE)\n\n# order data with missingness assigned to work with grf and lmtp\n# if any outcome is censored all are censored\n# create version for model reports\n\n# check\ncolnames(df_wide)\n\n\n# made data wide in correct format\n# ignore warning\ndf_wide_encoded  <- margot::margot_process_longitudinal_data_wider(\n  df_wide,\n  ordinal_columns = ordinal_columns,\n  continuous_columns_keep = continuous_columns_keep,\n  not_lost_in_following_wave = \"not_lost_following_wave\",\n  lost_in_following_wave = \"lost_following_wave\",\n  remove_selected_columns = TRUE,\n  exposure_var = name_exposure_both,\n  scale_continuous = TRUE,\n  censored_if_any_lost = FALSE\n)\n\n# check\ncolnames(df_wide_encoded)\n\n# check\ntable(df_wide_encoded$t0_not_lost_following_wave)\n\n# make the binary variable numeric\ndf_wide_encoded[[t0_name_exposure_binary]] <- \n  as.numeric(df_wide_encoded[[t0_name_exposure_binary]]) - 1\ndf_wide_encoded[[t1_name_exposure_binary]] <- \n  as.numeric(df_wide_encoded[[t1_name_exposure_binary]]) - 1\n\n# view\ndf_wide_encoded[[t0_name_exposure_binary]]\ndf_wide_encoded[[t1_name_exposure_binary]] \n\n# 1. ensure both binaries only take values 0 or 1 (ignore NA)\nstopifnot(\n  all(\n    df_wide_encoded[[t0_name_exposure_binary]][\n      !is.na(df_wide_encoded[[t0_name_exposure_binary]])\n    ] %in% 0:1\n  ),\n  all(\n    df_wide_encoded[[t1_name_exposure_binary]][\n      !is.na(df_wide_encoded[[t1_name_exposure_binary]])\n    ] %in% 0:1\n  )\n)\n\n# 2. ensure NA‐patterns match between t1_exposure and t0_lost flag\n# count n-as in t1 exposure\nn_na_t1 <- sum(is.na(df_wide_encoded[[t1_name_exposure_binary]]))\n\n# count how many were lost at t0\nn_lost_t0 <- sum(df_wide_encoded$t0_lost_following_wave == 1, na.rm = TRUE)\n\n# print them for inspection\nmessage(\"NAs in \", t1_name_exposure_binary, \": \", n_na_t1)\nmessage(\"t0_lost_following_wave == 1: \", n_lost_t0)\n\n# stop if they don’t match\nstopifnot(n_na_t1 == n_lost_t0)\n\n# 3. ensure if t1 is non‐NA then subject was not lost at t0\nstopifnot(\n  all(\n    is.na(df_wide_encoded[[t1_name_exposure_binary]]) |\n      df_wide_encoded[[\"t0_not_lost_following_wave\"]] == 1\n  )\n)\n\n# now it’s safe to save\nhere_save_qs(df_wide_encoded, \"df_wide_encoded\", push_mods)\n\n#naniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\nnaniar::gg_miss_var(df_wide_encoded)\n\ndf_wide_encoded$t0_sample_weights\n\n# predict attrition and create censoring weights --------------------------\n# step 1: prepare baseline covariates\nE <- df_wide_encoded %>%\n  select(\n    - all_of(t0_name_exposure_binary), # inserted by function but irrelevant\n    -\"t0_sample_weights\") %>%\n  select(starts_with(\"t0_\"),\n         -ends_with(\"_lost\"),\n         -ends_with(\"lost_following_wave\")) %>%\n  colnames() %>%\n  sort()\n\n# get unique values (to be safe)\nE <- unique(E) \n\n# view\nprint(E)\n\n# save baseline covariates\nmargot::here_save(E, \"E\")\n\n# view\nprint(E)\n\n# step 2: calculate weights for t0\nD_0 <- as.factor(df_wide_encoded$t0_lost_following_wave)\n\n# get co-variates\ncen_0 <- df_wide_encoded[, E]\n\n# probability forest for censoring\n# this will take time\ncen_forest_0 <- probability_forest(cen_0, D_0)\n\n# get predictions\npredictions_grf_0 <- predict(cen_forest_0, newdata = cen_0, type = \"response\")\n\n# get propensity scores\npscore_0 <- predictions_grf_0$pred[, 2]\n\n# view\nhist(pscore_0)\n\n# check corrrect sample weights\nhist(t0_sample_weights)\n\n# use margot_adjust_weights for t0\nt0_weights <- margot_adjust_weights(\n  pscore = pscore_0,\n  trim = TRUE,\n  normalize = TRUE,\n  # lower trimming\n  lower_percentile = 0.00,\n  upper_percentile = 0.99,\n  # upper trimming\n  censoring_indicator = df_wide_encoded$t0_lost_following_wave,\n  sample_weights = df_wide_encoded$t0_sample_weights \n)\nlength(t0_weights$adjusted_weights)\nlength(df_wide_encoded$t0_sample_weights)\n\n# veiw\nnrow(df_wide_encoded)\nhist(t0_weights$adjusted_weights)\n\n# give weights\ndf_wide_encoded$t0_adjusted_weights <- t0_weights$adjusted_weights\n\n#check\nnaniar::vis_miss(df_wide_encoded, warn_large_data = FALSE)\n\n# view\nhead(df_wide_encoded)\ncolnames(df_wide_encoded)\n\n# remove lost next wave (censored)\ndf_wide_encoded_1 <- df_wide_encoded %>%\n  filter(t0_lost_following_wave == 0) %>%\n  droplevels()\n\n# step 4: calculate weights for t1\nE_and_exposure <- c(E, t1_name_exposure_continuous)\nD_1 <- as.factor(df_wide_encoded_1$t1_lost_following_wave)\ncen_1 <- df_wide_encoded_1[, E_and_exposure]\n\n# probability forest for censoring\n#  *** this will take time *** \ncen_forest_1 <- probability_forest(cen_1, D_1, \n                                   sample.weights = df_wide_encoded_1$t0_adjusted_weights)\n\n# predict forest\npredictions_grf_1 <- predict(cen_forest_1, \n                             newdata = cen_1, type = \"response\")\n# get propensity score\npscore_1 <- predictions_grf_1$pred[, 2]\n\n# check\nhist(pscore_1)\n\n# use margot_adjust_weights for t1\nt1_weights <- margot_adjust_weights(\n  pscore = pscore_1,\n  trim = TRUE,\n  normalize = TRUE,\n  lower_percentile = 0.00,\n  upper_percentile = 0.99,\n  # upper trimming\n  censoring_indicator = df_wide_encoded_1$t1_lost_following_wave,\n  sample_weights = df_wide_encoded_1$t0_adjusted_weights # combine with weights\n)\n\n# check\nhist(t1_weights$adjusted_weights)\n\n# add weights -- these will be the weights we use\ndf_wide_encoded_1$t1_adjusted_weights <- t1_weights$adjusted_weights\n\n\n# calculate summary statistics\nt0_adjusted_weight_summary <- summary(df_wide_encoded$t0_adjusted_weights)\nt1_adjusted_weight_summary <- summary(df_wide_encoded_1$t1_adjusted_weights)\n\n# view summaries\nt0_adjusted_weight_summary\nt1_adjusted_weight_summary\n\n#check\nnaniar::vis_miss(df_wide_encoded_1, warn_large_data = FALSE)\n\n# save\nhere_save(df_wide_encoded_1, \"df_wide_encoded_1\")\n\n# read if needed\n# df_wide_encoded_1 <- here_read(\"df_wide_encoded_1\")\n\n# check names\ncolnames(df_wide_encoded_1)\n\n# check\ndf_wide_encoded_1[[t1_name_exposure_binary]]\n\n# step 5: prepare final dataset\nnrow(df_wide_encoded_1)\ntable(df_wide_encoded_1$t1_lost_following_wave)\n\n# arrange\ndf_grf <- df_wide_encoded_1 |>\n  filter(t1_lost_following_wave == 0) |>\n  # rename(t0_adjusted_weights = t1_adjusted_weights) |> # do this later\n  # if you have not previously made a binary variable, make it here\n  # mutate(\n  #   # use the dynamically created binary variable name\n  #   !!t1_name_exposure_binary := ifelse(\n  #     get(t1_name_exposure_continuous) > 0, ### THINK ABOUT THIS\n  #     1,\n  #     0\n  #   )\n  # ) |>\n  # mutate(across(\n  #   where(is.factor),\n  #   ~ factor(as.character(.), levels = levels(.), ordered = is.ordered(.))\n  # )) |>\n  select(\n    where(is.factor),\n    ends_with(\"_binary\"),\n    ends_with(\"_lost_following_wave\"),\n    ends_with(\"_z\"),\n    ends_with(\"_weights\"),\n    starts_with(\"t0_\"),\n    starts_with(\"t1_\"),\n    starts_with(\"t2_\"),\n  ) |>\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\")) |>\n  relocate(starts_with(\"t1_\"), .before = starts_with(\"t2_\")) |>\n  relocate(\"t0_not_lost_following_wave\", .before = starts_with(\"t1_\")) |>\n  relocate(all_of(t1_name_exposure_binary), .before = starts_with(\"t2_\")) |>\n  droplevels()\n\n# save final data\nmargot::here_save(df_grf, \"df_grf\")\n\n# check final dataset\ncolnames(df_grf)\n\n# visualise missing\nnaniar::vis_miss(df_grf, warn_large_data = FALSE)\n\n#checks\ncolnames(df_grf)\nstr(df_grf)\n\n# check exposures\ntable(df_grf[[t1_name_exposure_binary]])\n\n# check\nhist(df_grf$t1_adjusted_weights)\n\n# calculate summary statistics\nt0_weight_summary <- summary(df_wide_encoded)\n\n# check\nglimpse(df_grf$t1_adjusted_weights)\n\n# visualise weight distributions\nhist(df_grf$t0_adjusted_weights, main = \"t0_stabalised weights\", xlab = \"Weight\")\n\n# visualise and check missing values\nnaniar::gg_miss_var(df_grf)\n\n# check n\nn_observed_grf <- nrow(df_grf)\n\n# view\nn_observed_grf\n\n# save\nmargot::here_save(n_observed_grf, \"n_observed_grf\")\n\n# inspect propensity scores -----------------------------------------------\n# get data\ndf_grf <- here_read('df_grf')\n\n# assign weights var name\nweights_var_name = \"t0_adjusted_weights\"\n\n# baseline covariates  # E already exists and is defined\nE\n\n# must be a data frame, no NA in exposure\n\n# df_grf is a data frame - we must process this data frame in several steps\n# user to specify which columns are outcomes, default to 'starts_with(\"t2_\")'\ndf_propensity_org <- df_grf |> select(!starts_with(\"t2_\"))\n\n# Remove NAs and print message that this has been done\ndf_propensity <- df_propensity_org |> drop_na() |> droplevels()\n\n# E_propensity_names\n# first run model for baseline propensity if this is selected.  The default should be to not select it.\npropensity_model_and_plots <- margot_propensity_model_and_plots(\n  df_propensity = df_propensity,\n  exposure_variable = t1_name_exposure_binary,\n  baseline_vars = E,\n  weights_var_name = weights_var_name,\n  estimand = \"ATE\",\n  method = \"ebal\",\n  focal = NULL\n)\n\n# visualise\nsummary(propensity_model_and_plots$match_propensity)\n\n# key plot\npropensity_model_and_plots$love_plot\n\n# other plots\npropensity_model_and_plots$summary_plot\npropensity_model_and_plots$balance_table\npropensity_model_and_plots$diagnostics\n\n\n# check size\nsize_bytes <- object.size(propensity_model_and_plots)\nprint(size_bytes, units = \"auto\") # Mb\n\n# use qs to save only if you have space\nhere_save_qs(propensity_model_and_plots,\n             \"propensity_model_and_plots\",\n             push_mods)\n```\n:::\n\n\n\n### Script 3 Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# # example 2 02-data-wrangling-grf-model\n# get data into wide format and ready for modelling using grf\n# joseph.bulbulia@gmail.com\ndevtools::load_all(\"/Users/joseph/GIT/margot/\")\n\n# may 2025\n\n# restart fresh session\n# rstudioapi::restartSession()\n# set reproducibility\nset.seed(123)\n\n# essential libraries ---------------------------------------------------------\n\nif (!require(devtools, quietly = TRUE)) {\n  install.packages(\"devtools\")\n  library(devtools)\n}\n\nif (!require(margot, quietly = TRUE)) {\n  devtools::install_github(\"go-bayes/margot\")\n  library(margot)\n}\n\n# check if pacman is installed; if not, install it\nif (!require(pacman, quietly = TRUE)) {\n  install.packages(\"pacman\")\n  library(pacman)\n}\n\n\n# check package version\npackageVersion(pkg = \"margot\")\n\n\n# set seed ----------------------------------------------------------------\n\n\n# reproducibility\nset.seed(123)\n\n\n# directory path configuration -----------------------------------------------\n# save path (customise for your own computer) ----------------------------\npush_mods <- here::here(\"models_example_2\")\n\n# read original data (for plots) ------------------------------------------\noriginal_df <- margot::here_read(\"df_wide\", push_mods)\n\n# plot title --------------------------------------------------------------\n\ntitle_binary = \"Extraversion (binary)\"\nfilename_prefix = \"grf_extraversion_wb\"\n\n\n# import names ------------------------------------------------------------\nname_exposure <- margot::here_read(\"name_exposure\")\nname_exposure\n\n# make exposure names\nt1_name_exposure_binary <- paste0(\"t1_\", name_exposure, \"_binary\")\n\n# check exposure name\nt1_name_exposure_binary\n\n# read health outcomes \nraw_outcomes_health <- here_read(\"raw_outcomes_health\")\nt2_outcome_health_z <- paste0(\"t2_\", raw_outcomes_health, \"_z\")\nt2_outcome_health_z <- sort(t2_outcome_health_z)\n\n# read raw outcomes \nraw_outcomes_psych <- here_read(\"raw_outcomes_psych\")\nt2_outcome_psych_z <- paste0(\"t2_\", raw_outcomes_psych, \"_z\")\nt2_outcome_psych_z <- sort(t2_outcome_psych_z)\n\n# read raw outcomes\nraw_outcomes_present <- here_read(\"raw_outcomes_present\")\nt2_outcome_present_z <- paste0(\"t2_\", raw_outcomes_present, \"_z\")\nt2_outcome_present_z <- sort(t2_outcome_present_z)\n\n# read raw outcomes\nraw_outcomes_life <- here_read(\"raw_outcomes_life\")\nt2_outcome_life_z <- paste0(\"t2_\", raw_outcomes_life, \"_z\")\nt2_outcome_life_z <- sort(t2_outcome_life_z)\n\n# read raw outcomes\nraw_outcomes_social <- here_read(\"raw_outcomes_social\")\nt2_outcome_social_z <- paste0(\"t2_\", raw_outcomes_social, \"_z\")\nt2_outcome_social_z <- sort(t2_outcome_social_z)\n\n# bind all outcomes\nraw_outcomes_all <- c(raw_outcomes_health, \n                      raw_outcomes_psych,\n                      raw_outcomes_present, \n                      raw_outcomes_life, \n                      raw_outcomes_social)\n\n# all outcomes\nt2_outcomes_all <- c(t2_outcome_health_z, t2_outcome_psych_z, t2_outcome_present_z, t2_outcome_life_z, t2_outcome_social_z)\n\n# save for pub\nhere_save(t2_outcomes_all, \"t2_outcomes_all\")\n\n# save all outcomes\nhere_save(raw_outcomes_all, \"raw_outcomes_all\")\n\n# label mappings for health outcomes\nlabel_mapping_health <- list(\n  \"t2_alcohol_frequency_weekly_z\" = \"Alcohol Frequency\",\n  \"t2_alcohol_intensity_weekly_z\" = \"Alcohol Intensity\",\n  \"t2_hlth_bmi_z\" = \"BMI\", \n  \"t2_hlth_sleep_hours_z\" = \"Sleep\", \n  \"t2_log_hours_exercise_z\" = \"Hours of Exercise (log)\",\n  \"t2_short_form_health_z\" = \"Short Form Health\" \n)\n\n# label mappings for psychological well-being outcomes\nlabel_mapping_psych <- list(\n  \"t2_hlth_fatigue_z\" = \"Fatigue\", \n  \"t2_kessler_latent_anxiety_z\" = \"Anxiety\", \n  \"t2_kessler_latent_depression_z\" = \"Depression\",  \n  \"t2_rumination_z\" = \"Rumination\"\n)\n\n# label mappings for present reflective outcomes\nlabel_mapping_present <- list(\n  \"t2_bodysat_z\" = \"Body Satisfaction\",\n  \"t2_foregiveness_z\" = \"Forgiveness\",  \n  # \"t2_perfectionism_z\" = \"Perfectionism\",  ** --- EXPOSURE ---**\n  \"t2_self_control_z\" = \"Self Control\",  \n  \"t2_sexual_satisfaction_z\" = \"Sexual Satisfaction\"\n)\n\n# label mappings for life reflective outcomes\nlabel_mapping_life <- list(\n  \"t2_gratitude_z\" = \"Gratitude\", \n  \"t2_lifesat_z\" = \"Life Satisfaction\", \n  \"t2_meaning_purpose_z\" = \"Meaning: Purpose\",  \n  \"t2_meaning_sense_z\" = \"Meaning: Sense\", \n  \"t2_pwi_z\" = \"Personal Well-being Index\"\n)\n\n# label mappings for social outcomes\nlabel_mapping_social <- list(\n  \"t2_belong_z\" = \"Social Belonging\",\n  \"t2_neighbourhood_community_z\" = \"Neighbourhood Community\", \n  \"t2_support_z\" = \"Social Support\" \n)\n\n# label mapping all -------------------------------------------------------\nlabel_mapping_all <- c(\n  label_mapping_health,\n  label_mapping_psych,\n  label_mapping_present,\n  label_mapping_life,\n  label_mapping_social\n)\n\n# save\nhere_save(label_mapping_all, \"label_mapping_all\")\n\n# load libraries ----------------------------------------------------------\n# load necessary libraries\npacman::p_load(\n  clarify,      # sensitivity analysis for causal inference\n  cobalt,       # covariate balance tables and plots\n  DiagrammeR,   # graph and network visualization\n  doParallel,   # parallel processing with foreach\n  fastDummies,  # fast creation of dummy variables\n  fs,           # cross-platform file system operations\n  ggplot2,      # data visualisation\n  glmnet,       # lasso and elastic-net regularized models\n  grf,          # generalized random forests\n  gtsummary,    # summary tables for regression models\n  here,         # simple and robust file referencing\n  janitor,      # data cleaning and validation\n  kableExtra,   # advanced table formatting\n  lmtp,         # longitudinal targeted maximum likelihood estimation\n  # margot,       # functions for casual inference\n  naniar,       # handling and visualization of missing data\n  parameters,   # parameters and performance metrics\n  policytree,   # causal inference with policy trees\n  progressr,    # progress reporting for R\n  ranger,       # fast implementation of random forests\n  skimr,        # summary statistics for data frames\n  SuperLearner, # ensemble learning\n  tidyverse,    # collection of R packages for data science\n  WeightIt,     # weighting methods for covariate balancing\n  xgboost,      # extreme gradient boosting\n  EValue,       # compute Evalues\n  data.table,   # fast data wrangling\n  maq,          # qini curves\n  purrr,        # data wrangling\n  patchwork,     # multiple plots\n  labelled,\n  cli,\n  rlang\n)\n\n\n# start analysis here ----------------------------------------------------\n# import data\ndf_grf <- margot::here_read('df_grf')\n\n# check\ncolnames(df_grf)\n\n# check missing values (baseline missingness is handled by grf)\n# takes a long time to render so commented out\n# naniar::vis_miss(df_grf, warn_large_data = FALSE)\n\n# check another way\nnaniar::gg_miss_var(df_grf)\n\n# import names of baseline covariates\nE <- margot::here_read(\"E\")\n\n# check\nprint(E)\n\n\n# get exposure variable, call it W ----------------------------------------\n# check that variables are 0 or 1\ndf_grf[[t1_name_exposure_binary]]\n\n# check: ensure both binaries only take values 0 or 1 (ignore NA)\nstopifnot(\n  all(\n    df_grf[[t1_name_exposure_binary]][\n      !is.na(df_grf[[t1_name_exposure_binary]])\n    ] %in% 0:1\n  )\n)\n\n# needs to be a matrix\nW <- as.vector(df_grf[[t1_name_exposure_binary]])\n\n# set weights\nweights <- df_grf$t1_adjusted_weights \n\n# view/ check none too extreme\nhist(weights)\n\n# remove attributes of baseline co-variaties\nX <-  margot::remove_numeric_attributes(df_grf[E]) \n\n# set model defaults -----------------------------------------------------\ngrf_defaults <- list(\n  seed = 123, # reproduce results\n  stabilize.splits = TRUE, # robustness\n  # min.node.size = 5,  # default is five/ requires at least 5 observed in control and treated\n  # set higher for greater smoothing\n  num.trees = 2000 # grf default = 2000 # set lower or higher depending on storage\n)\n\n# set defaults for graphs (see bottom of script for options)\n# see margot package for options\ndecision_tree_defaults <- list(\n  span_ratio = .3,\n  text_size = 3.8,\n  y_padding = 0.25  # use full parameter name\n  # edge_label_offset = .002, # options\n  # border_size = .05 # options\n)\n\n# set defaults for graphs (see bottom of script for options)\n# set policy tree defaults\npolicy_tree_defaults <- list(\n  point_alpha = .5,\n  title_size = 12,\n  subtitle_size = 14,\n  axis_title_size = 14,\n  legend_title_size = 14,\n  split_line_color = \"red\",\n  split_line_alpha = 0.8,\n  split_label_color = \"red\"\n)\n\n\n# Uncomment and Run Tests Once ----------------------------------------------------------\n\n\n# \n# # test --------------------------------------------------------------------\nn <- nrow(X) # n in sample\n\n# define training sample\ntoy <- sample(1:n, n / 4) # get half sample\n\n# test set data\ntoy_data = df_grf[toy, ]\n\n# check size\nnrow(toy_data)\n\n# covariates\nX_toy = X[toy, ]\n\n# test set covariates check\nstr(X_toy)\n\n# test set exposure\nW_toy = W[toy]\n\n# test weights\nweights_toy = weights[toy]\n\n# 1. fit model and save everything\ncf_out <- margot_causal_forest(\n  data        = toy_data,\n  outcome_vars=  c(\"t2_kessler_latent_depression_z\"),\n  covariates  = X_toy,\n  W           = W_toy,\n  weights     = weights_toy,\n  save_data   = TRUE,   #  ← needed for flipping models\n  save_models = TRUE, # ←  save models\n)\n\n\n# inspect qini data\n# # where there are very low or high propensity scores (prob of exposure) we might consider trimming\n# table_inspect_qini <- margot::margot_inspect_qini(cf_out, propensity_bounds = c(0.01, 0.97))\n# table_inspect_qini\n# \n# # test model: 1L tree\n# policy_tree_test_1L <- margot_plot_policy_tree(\n#   mc_test    = cf_out,\n#   model_name = \"model_t2_kessler_latent_depression_z\",\n#   max_depth  = 1L, # ← new argument\n#   original_df = original_df,\n#   label_mapping = label_mapping_all\n# )\n# \n# # view - notice this plot is **not persuasive**\n# policy_tree_test_1L\n# \n# \n# # test level 2 - we will default to a policy tree of two levels\n# policy_tree_test <- margot_plot_policy_tree(\n#   mc_test    = cf_out,\n#   model_name = \"model_t2_kessler_latent_depression_z\",\n#   max_depth  = 2L, # ← new argument\n#   original_df = original_df,\n#   label_mapping = label_mapping_all\n# )\n# \n# # view - a little better but as we'll see there's not much evidence for HTE\n# plot(policy_tree_test)\n# \n# \n# # decision tree 1L test level 1\n# decision_tree_test_1L <- margot_plot_decision_tree(\n#   cf_out,\n#   model_name = \"model_t2_kessler_latent_depression_z\",\n#   max_depth  = 1L, # ← new argument\n#   original_df = original_df,\n#   label_mapping = label_mapping_all\n# )\n# \n# decision_tree_test_1L\n# \n# # result was\n# cf_out$results$model_t2_kessler_latent_depression_z$policy_tree_depth_1\n# \n# # plotting using policy tree gives correct split\n# plot(cf_out$results$model_t2_kessler_latent_depression_z$policy_tree_depth_1)\n# \n# # plotting using margot function does not give correct label colour\n# decision_tree_test_1L\n# \n# \n# # decision tree 2L\n# decision_tree_test_2L <- margot_plot_decision_tree(\n#   cf_out,\n#   model_name = \"model_t2_kessler_latent_depression_z\",\n#   max_depth  = 2L, # ← new argument\n#   original_df = original_df,\n#   label_mapping = label_mapping_all\n# )\n# \n# # view\n# decision_tree_test_2L\n# cf_out$results$model_t2_kessler_latent_depression_z$policy_tree_depth_2\n# \n# # check again policy tree graph\n# plot(cf_out$results$model_t2_kessler_latent_depression_z$policy_tree_depth_2)\n# \n# # more tests\n# combo1 <- margot_plot_policy_combo(\n#   result_object       = cf_out,\n#   model_name          = \"model_t2_kessler_latent_depression_z\",\n#   max_depth           = 1L,       # <— depth‐1 decision tree\n#   policy_tree_args    = list(point_alpha = 0.7),\n#   decision_tree_args  = list(text_size   = 4)\n# )\n# \n# # L1 combo tree ** (we'll generally plot combo trees) **\n# combo1$combined_plot\n# \n# # test L2 combo tree\n# combo2 <- margot_plot_policy_combo(\n#   result_object       = cf_out,\n#   model_name          = \"model_t2_kessler_latent_depression_z\",\n#   max_depth           = 2L,       # <— depth‐2 decision tree\n#   policy_tree_args    = list(point_alpha = 0.7),\n#   decision_tree_args  = list(text_size   = 4)\n# )\n# # view\n# combo2$combined_plot\n# \n# # test batch function 1 L (ignore warnings)\n# models_binary_batch_test <-margot_policy(\n#   cf_out,\n#   save_plots = FALSE,\n#   output_dir = here::here(push_mods),\n#   decision_tree_args = decision_tree_defaults,\n#   policy_tree_args = policy_tree_defaults,\n#   model_names = c(\"model_t2_kessler_latent_depression_z\"),\n#   original_df = original_df,\n#   label_mapping = label_mapping_psych,\n#   max_depth     = 1L # test with depth 1\n# )\n# \n# \n# models_binary_batch_test$model_t2_kessler_latent_depression_z$qini_plot\n# models_binary_batch_test$model_t2_kessler_latent_depression_z$combined_plot\n# \n# \n# # interpretation of qini curves\n# interpretation_qini_curves_test_1L <- margot_interpret_policy_tree(\n#   model       = cf_out,\n#   model_name  = \"model_t2_kessler_latent_depression_z\",\n#   max_depth   = 1L,\n#   label_mapping = label_mapping_all,\n#   original_df   = original_df\n# )\n# \n# # read interpretation 1 L\n# cat(interpretation_qini_curves_test_1L)\n# \n# # interpretation for 2L policy tree\n# interpretation_qini_curves_test <- margot_interpret_policy_batch(\n#   models       = cf_out,\n#   model_name  = \"model_t2_kessler_latent_depression_z\",\n#   max_depth   = 2L,\n#   label_mapping = label_mapping_all,\n#   original_df   = original_df\n# )\n# \n# cat(interpretation_qini_curves_test)\n# \n# \n# # 2. flip the selected outcomes (and regen trees)\n# # often we will reverse outcomes that we want to minimise\n# cf_out_f <- margot_flip_forests(\n#   model_results = cf_out,\n#   flip_outcomes = c(\"t2_kessler_latent_depression_z\"),\n#   recalc_policy = TRUE\n# )\n# \n# # check\n# cf_out_f$flip_outcomes_postprocessed\n# \n# policy_tree_test_1 <- margot_interpret_policy_tree(\n#   model    = cf_out_f,\n#   model_name = \"model_t2_kessler_latent_depression_z\",\n#   max_depth  = 1L, # ← new argument\n#   original_df = original_df,\n#   label_mapping = label_mapping_all\n# )\n# \n# cat(policy_tree_test_1)\n# \n# # check  we should find that treatments are rare\n# policy_tree_test_2 <- margot_plot_policy_tree(\n#   cf_out_f,\n#   model_name = \"model_t2_kessler_latent_depression_z\",\n#   max_depth  = 2L, # ← new argument\n#   original_df = original_df,\n#   label_mapping = label_mapping_all\n# )\n# \n# # note\n# policy_tree_test_2\n# \n# # depth-1 (ignore warnings)\n# tree1 <- margot_plot_decision_tree(\n#   result_object = cf_out_f,\n#   model_name    = \"model_t2_kessler_latent_depression_z\",\n#   max_depth     = 1L,\n#   original_df   = original_df,\n#   label_mapping = label_mapping_all\n# )\n# \n# # depth-2 scatter panels\n# tree2 <- margot_plot_decision_tree(\n#   result_object = cf_out_f,\n#   model_name    = \"model_t2_kessler_latent_depression_z\",\n#   max_depth     = 2L,\n#   original_df   = original_df,\n#   label_mapping = label_mapping_all\n# )\n# \n# # 3. rescue any empty‐gain Qini curves via propensity trimming\n# cf_out_flipped <- margot_rescue_qini(\n#   model_results      = cf_out_f,\n#   propensity_bounds  = c(0.05, 0.95)\n# )\n# \n# # test plots\n# models_binary_batch_test <-margot_policy(\n#   cf_out_f,\n#   save_plots = FALSE,\n#   output_dir = here::here(push_mods),\n#   decision_tree_args = decision_tree_defaults,\n#   policy_tree_args = policy_tree_defaults,\n#   model_names = c(\"model_t2_kessler_latent_depression_z\"),\n#   original_df = original_df,\n#   label_mapping = label_mapping_psych,\n#   max_depth     = 2L\n# )\n# \n# # plots\n# models_binary_batch_test[[1]][[1]]\n# models_binary_batch_test[[1]][[2]]\n# models_binary_batch_test[[1]][[3]]\n# \n\n\n# full models -------------------------------------------------------------\n\n# ** uncomment to run full models** \n\n# # health models -----------------------------------------------------------\nmodels_binary_health <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_health_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n# check size if needed\nmargot::margot_size(models_binary_health)\n\n# save model\nmargot::here_save_qs(models_binary_health, \"models_binary_health\", push_mods)\n\n\n# psych models ------------------------------------------------------------\nmodels_binary_psych <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_psych_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n# save model\nmargot::here_save_qs(models_binary_psych, \"models_binary_psych\", push_mods)\n\n\n# present models ----------------------------------------------------------\nmodels_binary_present <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_present_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n# save model\nmargot::here_save_qs(models_binary_present, \"models_binary_present\", push_mods)\n\n\n# life models -------------------------------------------------------------\nmodels_binary_life <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_life_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n# save model\nmargot::here_save_qs(models_binary_life, \"models_binary_life\", push_mods)\n\n\n# social models -----------------------------------------------------------\nmodels_binary_social <- margot::margot_causal_forest(\n  data = df_grf,\n  outcome_vars = t2_outcome_social_z,\n  covariates = X,\n  W = W,\n  weights = weights,\n  grf_defaults = grf_defaults,\n  top_n_vars = 15,\n  save_models = TRUE,\n  save_data = TRUE,\n  train_proportion = 0.7\n)\n\n\n# save model\nmargot::here_save_qs(models_binary_social, \"models_binary_social\", push_mods)\n\n\n\n# read results ------------------------------------------------------------ \n\n# if you save models you do not need to re-run them\nmodels_binary_health <- margot::here_read_qs(\"models_binary_health\", push_mods)\nmodels_binary_psych <- margot::here_read_qs(\"models_binary_psych\", push_mods)\nmodels_binary_present <- margot::here_read_qs(\"models_binary_present\", push_mods)\nmodels_binary_life <- margot::here_read_qs(\"models_binary_life\", push_mods)\nmodels_binary_social <- margot::here_read_qs(\"models_binary_social\", push_mods)\n\n# make graphs -------------------------------------------------------------\n# titles\nsubtitle_health = \"Health\"\nsubtitle_psych = \"Psychological Well-being\"\nsubtitle_present = \"Present-Focussed Well-being\"\nsubtitle_life = \"Life-Focussed Well-being\"\nsubtitle_social = \"Social Well-being\"\n\n\n# settings\nx_offset = -.5\nx_lim_lo = -.5\nx_lim_hi = .5\n\n\n# defaults for ATE plots\nbase_defaults_binary <- list(\n  type = \"RD\",\n  title = title_binary,\n  e_val_bound_threshold = 1.2,\n  colors = c(\n    \"positive\" = \"#E69F00\",\n    \"not reliable\" = \"grey50\",\n    \"negative\" = \"#56B4E9\"\n  ),\n  x_offset = x_offset,\n  # will be set based on type\n  x_lim_lo = x_lim_lo,\n  # will be set based on type\n  x_lim_hi = x_lim_hi,\n  text_size = 4,\n  linewidth = 0.5,\n  estimate_scale = 1,\n  base_size = 18,\n  point_size = 2,\n  title_size = 19,\n  subtitle_size = 16,\n  legend_text_size = 10,\n  legend_title_size = 10,\n  include_coefficients = FALSE\n)\n\n# health graph options\noutcomes_options_health <- margot_plot_create_options(\n  title = subtitle_health,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n# psych graph options\noutcomes_options_psych <- margot_plot_create_options(\n  title = subtitle_psych,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n# present graph options ---------------------------------------------------\noutcomes_options_present <- margot_plot_create_options(\n  title = subtitle_present,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n\n# life graph options ------------------------------------------------------\noutcomes_options_life <- margot_plot_create_options(\n  title = subtitle_life,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n\n# social graph options ----------------------------------------------------\noutcomes_options_social <- margot_plot_create_options(\n  title = subtitle_social,\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n# all graph options ------------------------------------------------------\noptions_all_models <- margot_plot_create_options(\n  title = \"Outcomewide Wellbeing\",\n  base_defaults = base_defaults_binary,\n  subtitle = \"\",\n  filename_prefix = filename_prefix)\n\n\n# make graphs -------------------------------------------------------------\n\n# make ATE plots ----------------------------------------------------------\n# health plots ------------------------------------------------------------\nbinary_results_health <- margot_plot(\n  models_binary_health$combined_table,\n  options = outcomes_options_health,\n  label_mapping = label_mapping_health,\n  include_coefficients = FALSE,\n  save_output = FALSE, \n  order = \"evaluebound_asc\",\n  original_df = original_df,\n  e_val_bound_threshold = 1.2\n)\n\n# view\nbinary_results_health$transformed_table |> rename(\n  \"E-Value\" = \"E_Value\",\n  \"E-Value bound\" = \"E_Val_bound\"\n) |>\n  kbl(format = 'markdown')\n\n# check\ncat(binary_results_health$interpretation)\n\n# interpretation\ncat(binary_results_health$interpretation)\n\n# plot psych\nbinary_results_psych_asc <- margot_plot(\n  models_binary_psych$combined_table,\n  options = outcomes_options_psych,\n  label_mapping = label_mapping_psych,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  original_df = original_df,\n  e_val_bound_threshold = 1.2,\n  order = \"evaluebound_asc\")\n\n\n# order\nbinary_results_psych_asc$plot\n\n# reorder for descriptions\nbinary_results_psych <- margot_plot(\n  models_binary_psych$combined_table,\n  options = outcomes_options_psych,\n  label_mapping = label_mapping_psych,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  e_val_bound_threshold = 1.2,\n  original_df = original_df,\n  order = \"evaluebound_asc\"\n)\n\n# table\nbinary_results_psych$transformed_table |> rename(\n  \"E-Value\" = \"E_Value\",\n  \"E-Value bound\" = \"E_Val_bound\"\n) |>\n  kbl(format = 'markdown')\n\n# interpretation\ncat(binary_results_psych$interpretation)\n\n# plot present\n# order\nbinary_results_present <- margot_plot(\n  models_binary_present$combined_table,\n  options = outcomes_options_present,\n  label_mapping = label_mapping_present,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  original_df = original_df,\n  order = \"evaluebound_asc\"\n)\n\n# plot\nbinary_results_present$plot\n\n# interpretation\ncat(binary_results_present$interpretation)\n(binary_results_present$transformed_table)\n\n# plot life\nbinary_results_life <- margot_plot(\n  models_binary_life$combined_table,\n  options = outcomes_options_life,\n  label_mapping = label_mapping_life,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  order = \"evaluebound_asc\",\n  original_df = original_df\n)\n\n# table\nbinary_results_life$transformed_table |> rename(\n  \"E-Value\" = \"E_Value\",\n  \"E-Value bound\" = \"E_Val_bound\"\n) |>\n  kbl(format = 'markdown')\n\n# plot\nbinary_results_life$transformed_table\n\n# interpretation\ncat(binary_results_life$interpretation)\n\n# plot social\nbinary_results_social <- margot_plot(\n  models_binary_social$combined_table,\n  options = outcomes_options_social,\n  label_mapping = label_mapping_social,\n  include_coefficients = FALSE,\n  save_output = FALSE,\n  original_df = original_df,\n  order = \"evaluebound_asc\"\n)\n\n# table\nbinary_results_social$transformed_table |> rename(\n  \"E-Value\" = \"E_Value\",\n  \"E-Value bound\" = \"E_Val_bound\"\n) |>\n  kbl(format = 'markdown')\n\n# interpretation\ncat(binary_results_social$interpretation)\n\n\n# combine ate plots ------------------------------------------------------\n# plot_ate_health <- binary_results_health_asc$plot\n# plot_ate_psych <- binary_results_psych_asc$plot\n# plot_ate_present <- binary_results_present_asc$plot\n# plot_ate_life <- binary_results_life_asc$plot\n# plot_ate_social <- binary_results_social_asc$plot\n# \n# \n# \n# # create combined plot with annotations\n# ate_plots_combined <- plot_ate_health + \n#   plot_ate_psych + \n#   plot_ate_present + \n#   plot_ate_life + \n#   plot_ate_social +\n#   plot_annotation(\n#     title = title_binary,\n#     tag_levels = \"A\",\n#     theme = theme(\n#       plot.title = element_text(size = 20),\n#       legend.position = \"top\"\n#     )\n#   ) +\n#   plot_layout(guides = \"collect\")\n# \n# # view combined plot\n# ate_plots_combined\n\n# combine all models -----------------------------------------------------\n# merge all domain models into single object\n\n\nall_models <- margot_bind_models(\n  models_binary_health,\n  models_binary_psych,\n  models_binary_present,\n  models_binary_life,\n  models_binary_social\n)\n\n# graph\nplot_all_models <- margot_plot(\n  all_models$combined_table,\n  options = options_all_models,\n  save_output = FALSE,\n  e_val_bound_threshold = 1.2,\n  label_mapping = label_mapping_all,\n  save_path = here::here(push_mods),\n  original_df = original_df,\n  include_coefficients = FALSE,\n  order = \"evaluebound_asc\"\n)\n\n# view plot\nplot_all_models$plot\n\n# interpretation\ncat(plot_all_models$interpretation)\n\n# table\nplot_all_models$transformed_table\n\n# nice table\ntables_list <- list(\n  Health = binary_results_health$transformed_table,\n  Psych = binary_results_psych$transformed_table,\n  Present = binary_results_present$transformed_table,\n  Life = binary_results_life$transformed_table,\n  Social = binary_results_social$transformed_table\n)\n\n# make markdown tables (to be imported into the manuscript)\nmargot_bind_tables_markdown <- margot_bind_tables(\n  tables_list = tables_list, #list(all_models$combined_table),\n  sort_E_val_bound = \"desc\",\n  e_val_bound_threshold = 1.2, # ← choose threshold\n  highlight_color = NULL,\n  bold = TRUE,\n  rename_cols = TRUE,\n  col_renames = list(\n    \"E-Value\" = \"E_Value\",\n    \"E-Value bound\" = \"E_Val_bound\"\n  ),\n  rename_ate = TRUE,\n  threshold_col = \"E_Val_bound\",\n  output_format = \"markdown\",\n  kbl_args = list(\n    booktabs = TRUE,\n    caption = NULL,\n    align = NULL\n  )\n)\n\n# view markdown table\nmargot_bind_tables_markdown\n\n# save for publication\nhere_save(margot_bind_tables_markdown, \"margot_bind_tables_markdown\")\n\n\n# count models by category\ncat(\"Number of original models:\\n\")\ncat(\"Social models:\", length(models_binary_social$results), \"\\n\")\ncat(\"Psych models:\", length(models_binary_psych$results), \"\\n\")\ncat(\"Health models:\", length(models_binary_health$results), \"\\n\")\ncat(\"Present models:\", length(models_binary_present$results), \"\\n\")\ncat(\"Life models:\", length(models_binary_life$results), \"\\n\")\ncat(\"\\nTotal models in combined object:\", length(all_models$results), \"\\n\")\n\n\n# evaluate models ---------------------------------------------------------\n# trim models if extreme propensity scores dominate\n# diag_tbl_health_trim_98 <- margot_inspect_qini(models_binary_health,\n#                                        propensity_bounds = c(0.01, 0.99))\n# diag_tbl_health_trim_98\n# \n# diag_tbl_health_trim_95 <- margot_inspect_qini(models_binary_health,\n#                                       propensity_bounds = c(0.03, 0.97))\n# diag_tbl_health_trim_95\n\n# rescue qini if needed ---------------------------------------------------\n# test\n# diag_tbl_trim_all <- margot_inspect_qini(all_models,\n#                                        propensity_bounds = c(0.03, 0.97))\n# diag_tbl_trim_all\n\n\n\n\n# flipping models: outcomes we want to minimise given the exposure --------\n# standard negative outcomes/  not used in this study\nflip_outcomes_standard = c(\n  \"t2_alcohol_frequency_weekly_z\", \n  \"t2_alcohol_intensity_z\",\n  \"t2_hlth_bmi_z\",\n  \"t2_hlth_fatigue_z\",\n  \"t2_kessler_latent_anxiety_z\",\n  \"t2_kessler_latent_depression_z\",\n  \"t2_rumination_z\",\n  \"t2_perfectionism_z\" # the exposure variable was not investigated\n)\n\n# we will investigate losses to these outcomes\n# usual flipped names for positive interventions\n# commented out for this study\nflipped_names <- c(\n  \"Alcohol Frequency\",\n  \"Alcohol Intensity\",\n  \"BMI\",\n  \"Fatigue\",\n  \"Anxiety\",\n  \"Depression\",\n  \"Rumination\",\n  \"Perfectionism\"\n)\n\n# set diff for all outcomes to obtain vector of postive outcomes to reverse\nflip_outcomes <- flip_outcomes_standard #c( setdiff(t2_outcomes_all, flip_outcomes_standard) ) \n\n# check\nflip_outcomes\n\n# checks\n# neg_check <- vapply(all_models$results[ paste0(\"model_\", flip_outcomes) ],\n#                     \\(x) mean(x$tau_hat, na.rm = TRUE) < 0, logical(1))\n# stopifnot(all(neg_check))   # every chosen outcome has a negative mean CATE\n\n# get labels\nflipped_names <- margot_get_labels(flip_outcomes, label_mapping_all)\n\n# check\nflipped_names\n\n# save for publication\nhere_save(flipped_names, \"flipped_names\")\n\n\n\n# flip negatively oriented outcomes --------------------------------------\n\n# flip models using margot's function\n\n#  *** this will take some time ***\n\n# ** give it time ** \nmodels_binary_flipped_all <- margot_flip_forests(\n  all_models,\n  flip_outcomes = flip_outcomes, #  ← select \n  recalc_policy = TRUE\n)\n\n# save\nhere_save_qs(models_binary_flipped_all, \"models_binary_flipped_all\", push_mods)\n\n# read if needed\nmodels_binary_flipped_all <- here_read_qs(\"models_binary_flipped_all\", push_mods)\n\n# test\nmodels_binary_flipped_all$results$model_t2_kessler_latent_depression_z$policy_tree_depth_1\nmodels_binary_flipped_all$results$model_t2_kessler_latent_depression_z$policy_tree_depth_2\n\n\n\n# omnibus heterogeneity tests --------------------------------------------\n# test for treatment effect heterogeneity across all outcomes\nresult_ominbus_hetero_all <- margot::margot_omnibus_hetero_test(models_binary_flipped_all, label_mapping = label_mapping_all)\n\n# view results table\nresult_ominbus_hetero_all$summary_table |> kbl(\"markdown\")\n\n# view test interpretation\ncat(result_ominbus_hetero_all$brief_interpretation)\n\n# rate test analysis -----------------------------------------------------\n# define flipped outcome names for interpretation\n\n# create rate analysis table\nrate_table_all <- margot_rate(\n  models = models_binary_flipped_all,  # <- the big results list\n  policy        = \"treat_best\",            # or \"withold_best\"/ although do not attempt fitting curves or policytrees\n  label_mapping = label_mapping_all\n)\n# view rate tables\nrate_table_all$rate_autoc |> kbl(\"markdown\")\nrate_table_all$rate_qini |> kbl(\"markdown\")\n\n# rate_table_all_previous <- margot_rate(\n#   models = models_binary_flipped_all,  \n#   label_mapping = label_mapping_all\n# )\n# # view rate tables\n# rate_table_all_previous$rate_autoc |> kbl(\"markdown\")\n# rate_table_all_previous$rate_qini |> kbl(\"markdown\")\n\n\n# generate interpretation\nrate_interpretation_all <- margot_interpret_rate(\n  rate_table_all, \n  flipped_outcomes = flipped_names\n)\n\n\n# view interpretations\ncat(rate_interpretation_all$autoc_results)\ncat(rate_interpretation_all$qini_results)\ncat(rate_interpretation_all$comparison)\n\n# check out model names\nrate_interpretation_all$either_model_names\nrate_interpretation_all$qini_model_names\nrate_interpretation_all$both_model_names\nrate_interpretation_all$autoc_model_names\n\n# autoc plots ------------------------------------------------------------\n# generate batch rate plots for models with significant heterogeneity\n# autoc plots ------------------------------------------------------------\n# generate batch rate plots for models with significant heterogeneity\nbatch_rate_autoc_plots <- margot_plot_rate_batch(\n  models_binary_flipped_all, \n  save_plots = FALSE,\n  # just use rate autoc\n  model_names = rate_interpretation_all$autoc_model_names\n)\n\n# extract individual plots from the batch result\nautoc_plots <- batch_rate_autoc_plots\n# determine number of columns based on number of plots\nnum_cols <- ifelse(length(autoc_plots) > 3, 2, 1) # ← choose \n\n# combine plots using patchwork\nlibrary(patchwork)\n\n# only proceed if there are plots to combine\nif (length(autoc_plots) > 0) {\n  # initialize with first plot\n  combined_autoc_plot <- autoc_plots[[1]]\n  \n  # add remaining plots if any\n  if (length(autoc_plots) > 1) {\n    for (i in 2:length(autoc_plots)) {\n      combined_autoc_plot <- combined_autoc_plot + autoc_plots[[i]]\n    }\n  }\n  \n  # apply the dynamic layout\n  combined_autoc_plot <- combined_autoc_plot + \n    plot_layout(ncol = num_cols) &\n    plot_annotation(\n      title = \"AUTOC Model Plots\",\n      subtitle = paste0(length(autoc_plots), \" models with significant heterogeneity\"),\n      tag_levels = \"A\"\n    )\n  \n  # view the combined plot\n  print(combined_autoc_plot)\n  \n  # save the combined plot if needed\n  width <- ifelse(num_cols == 1, 8, 12)\n  height <- 6 * ceiling(length(autoc_plots)/num_cols)\n  \n  ggsave(here::here(push_mods, \"combined_autoc_plots.pdf\"), \n         combined_autoc_plot, \n         width = width, height = height)\n} else {\n  # handle case with no plots\n  message(\"No AUTOC plots available\")\n}\n\n\n\n# QINI --------------------------------------------------------------------\nplots_qini <- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$qini_model_names,\n  max_depth  = 2L, # ← new argument\n  original_df = original_df,\n  label_mapping = label_mapping_all\n)\n\n# view\nplots_qini[[1]][[4]]\nplots_qini[[2]][[4]]\nplots_qini[[3]][[4]]\n\n# get all plots\nplots <- lapply(seq_along(plots_qini), function(i) {\n  models_binary_batch_qini[[i]][[4]]  # extract the 4th element (plot) from each model\n})\n\n# give the plots meaningful names\nnames(plots) <- rate_interpretation_all$qini_model_names\n\n# determine number of columns based on number of plots# determine number of columns based on number of plots\nnum_cols <- ifelse(length(plots) > 3, 2, 1)\n\n# combine plots using patchwork\nlibrary(patchwork)\n\n\n# create combined plot\ncombined_plot <- plots[[1]]\nfor (i in 2:length(plots)) {\n  combined_plot <- combined_plot + plots[[i]]\n}\n\n# apply the dynamic layout\ncombined_plot <- combined_plot + plot_layout(ncol = num_cols)\n\n# add titles and annotations\ncombined_plot <- combined_plot & \n  plot_annotation(\n    title = \"Qini Model Plots\",\n    subtitle = paste0(length(plots), \" models arranged in \", num_cols, \" column(s)\"),\n    tag_levels = \"A\"  # Adds A, B, C, etc. to the plots\n  )\n\n# view\ncombined_plot\n\n# save the combined plot with appropriate dimensions\n# adjust width and height based on the layout\n# width <- ifelse(num_cols == 1, 8, 12)\n# height <- 6 * ceiling(length(plots)/num_cols)  # height per row * number of rows\n# \n# ggsave(here::here(push_mods, \"combined_qini_plots.pdf\"), \n#        combined_plot, \n#        width = width, height = height)\n\n\n# interpretation ----------------------------------------------------------\n# interpret qini curves\ninterpretation_qini_curves <- margot_interpret_qini(\n  models_binary_batch_qini,\n  model_names = rate_interpretation_all$qini_model_names,\n  label_mapping = label_mapping_all\n)\n\n# view qini interpretation\ncat(interpretation_qini_curves$qini_explanation)\n\n# view summary table\ninterpretation_qini_curves$summary_table |> kbl(\"markdown\")\n\n\n\n# policy tree analysis ---------------------------------------------------\n# make policy trees\nplots_policy_trees_1L <- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names, # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all, \n  max_depth = 1L\n)\n\n# model 1\nplots_policy_trees_1L[[1]][[3]]\n\n# model 2\nplots_policy_trees_1L[[2]][[3]]\n\n# model 3\nplots_policy_trees_1L[[2]][[3]]\n\ninterpret_plots_policy_trees_1L <- margot_interpret_policy_batch(models_binary_flipped_all,  \n                              model_names = rate_interpretation_all$either_model_names)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_1L)\n\n# policy tree analysis ---------------------------------------------------\n# make policy trees\n# *** 2L is MUCH MORE PERSUASIVE *** \nplots_policy_trees_2L <- margot_policy(\n  models_binary_flipped_all,\n  save_plots = FALSE,\n  output_dir = here::here(push_mods),\n  decision_tree_args = decision_tree_defaults,\n  policy_tree_args = policy_tree_defaults,\n  model_names = rate_interpretation_all$either_model_names, # defined above\n  original_df = original_df,\n  label_mapping = label_mapping_all, \n  max_depth = 2L\n)\n\n\n# model 1\nplots_policy_trees_2L[[1]][[3]]\n\n# model 2\nplots_policy_trees_2L[[2]][[3]]\n\n# model 3\nplots_policy_trees_1L[[2]][[3]]\n\ninterpret_plots_policy_trees_2L <- margot_interpret_policy_batch(models_binary_flipped_all, model_names = rate_interpretation_all$either_model_names)\n\n\n# view interpretation\ncat(interpret_plots_policy_trees_2L)\n\n\n\n#############################################################################\n# theoretical comparisons ---------------------------------------------------\n# individual theoretical comparisons (if relevant)\n# need to get values for wealth if wealth is compared\n\n# step 1 get information for wealth for conditonal comparisons\nhead(df_grf$t0_log_household_inc_z)\n\n# get mean on original data scale\nlog_mean_inc <- mean(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# get sd on original data scale\nlog_sd_inc <- sd(original_df$t0_log_household_inc, na.rm = TRUE)\n\n# function to get back to data scale\nmargot_back_transform_log_z(\n  log_mean = log_mean_inc, \n  log_sd = log_sd_inc,\n  z_scores = c(-1, 0, 1),\n  label = \"data_scale\"\n)\n\n# define complex conditions for subsetting\ncomplex_condition_political <- X[, \"t0_political_conservative_z\"] > -1 &\n  X[, \"t0_political_conservative_z\"] < 1\n\ncomplex_condition_wealth <- X[, \"t0_log_household_inc_z\"] > -1 & \n  X[, \"t0_log_household_inc_z\"] < 1\n\ncomplex_condition_age <- X[, \"t0_age_z\"] > -1 &\n  X[, \"t0_age_z\"] < 1\n\n# # if we have specific groups to compare\n# complex_condition_age_under_neg_1_sd  <- X[, \"t0_age_z\"] < -1 \n# complex_condition_age_gr_eq_neg_1_sd  <- X[, \"t0_age_z\"] > -1 \n\n# check ages to get number\nmean(original_df$t0_age) - sd(original_df$t0_age) \nmean(original_df$t0_age) + sd(original_df$t0_age) \n\n\n# wealth subsets\nsubsets_standard_wealth <- list(\n  Poor = list(\n    var = \"t0_log_household_inc_z\",\n    value = -1,\n    operator = \"<\",\n    description = \"Effects among those HShold income < -1 SD (NZD ~41k)\",\n    label = \"Poor\"  # label remains as is, but could be changed if desired\n  ),\n  MiddleIncome = list(\n    subset_condition = complex_condition_wealth,\n    description = \"Effects among those HS_hold income within +/-1SD (> NZD 41k < NZD 191k)\"\n  ),\n  Rich = list(\n    var = \"t0_log_household_inc_z\",\n    value = 1,\n    operator = \">\",\n    description = \"Effects among those HS_hold income > +1 SD (NZD 191k)\",\n    label = \"Rich\"\n  )\n)\n\n# political subsets\nsubsets_standard_political <- list(\n  Liberal = list(\n    var = \"t0_political_conservative_z\",\n    value = -1,\n    operator = \"<\",\n    description = \"Effects among those < -1 SD in political conservativism\",\n    label = \"Liberal\"\n  ),\n  Centrist = list(\n    var = \"t0_political_conservative_z\",\n    # operator = \"<\",\n    subset_condition = complex_condition_political,\n    description = \"Effects among those > -1 SD and < +1 in political conservativism\",\n    label = \"Centrist\"\n  ),\n  Conservative = list(\n    var = \"t0_political_conservative_z\",\n    value = 1,\n    operator = \">\",\n    description = \"Effects among those > +1 SD in political conservativism\",\n    label = \"Conservative\"\n  )\n)\n\n\n# political subsets\nsubsets_standard_age <- list(\n  Younger = list(\n    var = \"t0_age_z\",\n    value = -1,\n    operator = \"<\",\n    description = \"Effects among those < under 35 years old\",\n    label = \"Age < 35\"\n  ),\n  Middle = list(\n    var = \"t0_age_z\",\n    # operator = \"<\",\n    subset_condition = complex_condition_age,\n    description = \"Effects among those 35-62\",\n    label = \"Age 35-62\"\n  ),\n  Older = list(\n    var = \"t0_political_conservative_z\",\n    value = 1,\n    operator = \">\",\n    description = \"Effects among those > 62\",\n    label = \"Age > 62\"\n  )\n)\n\n\n# gender subsets\nsubsets_standard_gender <- list(\n  Female = list(\n    var = \"t0_male_binary\",\n    value = 0,\n    description = \"Females\"\n  ),\n  Male = list(\n    var = \"t0_male_binary\",\n    value = 1,\n    description = \"Males\"\n  )\n) \n\n# ethnicity subsets\nsubsets_standard_ethnicity <- list(\n  Asian = list(\n    var = \"t0_eth_cat_asian_binary\",\n    value = 1,\n    description = \"Asians\"\n  ),\n  Euro = list(\n    var = \"t0_eth_cat_euro_binary\",\n    value = 1,\n    description = \"Europeans (Pakeha)\"\n  ),\n  Pacific = list(\n    var = \"t0_eth_cat_pacific_binary\",\n    value = 1,\n    description = \"Pacific Peoples\"\n  ),\n  Maori = list(\n    var = \"t0_eth_cat_maori_binary\",\n    value = 1,\n    description = \"Māori\"\n  )\n)\n\n\n# batch planned subgroup analysis -----------------------------------------\n# set up lists of models, names, and subtitles\ndomain_models <- list(\n  models_binary_health,\n  models_binary_psych,\n  models_binary_present,\n  models_binary_life,\n  models_binary_social\n)\n\n\n# set up domain names\ndomain_names <- c(\"health\", \"psych\", \"present\", \"life\", \"social\")\n\n# set up subtitles\nsubtitles <- c(\n  subtitle_health,\n  subtitle_psych,\n  subtitle_present,\n  subtitle_life,\n  subtitle_social\n)\n\n# set up subset types in a list\nsubset_types <- list(\n  wealth = subsets_standard_wealth,\n  ethnicity = subsets_standard_ethnicity,\n  political = subsets_standard_political,\n  gender = subsets_standard_gender,\n  cohort = subsets_standard_age\n)\n\n\n# run model\nplanned_subset_results <- margot_planned_subgroups_batch(\n  domain_models = domain_models,\n  X = X,\n  base_defaults = base_defaults_binary,\n  subset_types = subset_types,\n  original_df = original_df,\n  domain_names = domain_names,\n  subtitles = subtitles\n)\n\n\n# results\n# health subgroup\ncat(planned_subset_results$health$wealth$explanation)\ncat(planned_subset_results$health$ethnicity$explanation)\ncat(planned_subset_results$health$political$explanation)\ncat(planned_subset_results$health$gender$explanation)\ncat(planned_subset_results$health$cohort$explanation) \n\n\ncat(planned_subset_results$psych$wealth$explanation) \ncat(planned_subset_results$psych$ethnicity$explanation)\ncat(planned_subset_results$psych$political$explanation)\ncat(planned_subset_results$psych$gender$explanation)\ncat(planned_subset_results$psych$cohort$explanation) \n\n\ncat(planned_subset_results$present$wealth$explanation) \ncat(planned_subset_results$present$ethnicity$explanation)\ncat(planned_subset_results$present$political$explanation)\ncat(planned_subset_results$present$gender$explanation)\ncat(planned_subset_results$present$cohort$explanation) \n\n\ncat(planned_subset_results$life$wealth$explanation) \ncat(planned_subset_results$life$ethnicity$explanation) \ncat(planned_subset_results$life$political$explanation) \ncat(planned_subset_results$life$gender$explanation) \ncat(planned_subset_results$life$cohort$explanation) \n\ncat(planned_subset_results$social$wealth$explanation) \ncat(planned_subset_results$social$ethnicity$explanation) \ncat(planned_subset_results$social$political$explanation) \ncat(planned_subset_results$social$gender$explanation) \ncat(planned_subset_results$social$cohort$explanation) \n\nplanned_subset_results$health$wealth$results$Poor$transformed_table\n# combine tables ----------------------------------------------------------\n# wrap each domain's table in a list:\n# wealth subgroups --------------------------------------------------------\ntables_list_poor <- list(\n  Health = planned_subset_results$health$wealth$results$Poor$transformed_table,\n  Psych  = planned_subset_results$psych$wealth$results$Poor$transformed_table,\n  Life   = planned_subset_results$life$wealth$results$Poor$transformed_table,\n  Social = planned_subset_results$social$wealth$results$Poor$transformed_table\n)\n\n# function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_poor,\n  bold = TRUE,\n  kbl_args = list(booktabs = TRUE, caption = \"Wealth Subgroup Analysis: Poor\"),\n  highlight_color = NULL, \n  output_format = \"html\" # could be \"markdown\"\n)\n\n# create table list for middle income subgroup\ntables_list_middleincome <- list(\n  Health = planned_subset_results$health$wealth$results$MiddleIncome$transformed_table,\n  Psych  = planned_subset_results$psych$wealth$results$MiddleIncome$transformed_table,\n  Life   = planned_subset_results$life$wealth$results$MiddleIncome$transformed_table,\n  Social = planned_subset_results$social$wealth$results$MiddleIncome$transformed_table\n)\n\n# bind tables and display results\nmargot::margot_bind_tables(\n  tables_list = tables_list_middleincome,\n  bold = TRUE,\n  kbl_args = list(\n    booktabs = TRUE, \n    caption = \"Wealth Subgroup Analysis: Middle Income\"\n  ),\n  highlight_color = NULL, \n  output_format = \"html\"\n)\n\ntables_list_rich <- list(\n  Health = planned_subset_results$health$wealth$results$Rich$transformed_table,\n  Psych  = planned_subset_results$psych$wealth$results$Rich$transformed_table,\n  Life   = planned_subset_results$life$wealth$results$Rich$transformed_table,\n  Social = planned_subset_results$social$wealth$results$Rich$transformed_table\n)\n\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_rich,\n  bold = TRUE,\n  kbl_args = list(\n    booktabs = TRUE, \n    caption = \"Wealth Subgroup Analysis: Rich\"\n  ),\n  highlight_color = NULL, \n  output_format = \"html\"\n)\n\n# cohort subgroups --------------------------------------------------------\n\n# wealth subgroups --------------------------------------------------------\nplanned_subset_results$health$\n  planned_subset_results$health$cohort$results$Boomers$transformed_table\ntables_list_younger <- list(\n  Health = planned_subset_results$health$cohort$results$`Age < 35`$transformed_table,\n  Psych  = planned_subset_results$psych$cohort$results$`Age < 35`$transformed_table,\n  Life   = planned_subset_results$psych$cohort$results$`Age < 35`$transformed_table,\n  Social = planned_subset_results$psych$cohort$results$`Age < 35`$transformed_table\n)\n\n\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_younger,\n  bold = TRUE,\n  kbl_args = list(\n    booktabs = TRUE,\n    caption = \"Cohort Subgroup Analysis: Age under 35\"\n  ),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\n\ntables_list_middle <- list(\n  Health = planned_subset_results$health$cohort$results$`Age 35-62`$transformed_table,\n  Psych  = planned_subset_results$psych$cohort$results$`Age 35-62`$transformed_table,\n  Life   = planned_subset_results$psych$cohort$results$`Age 35-62`$transformed_table,\n  Social = planned_subset_results$psych$cohort$results$`Age 35-62`$transformed_table\n)\n\n\n#  function bind tables\n#  function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_middle,\n  bold = TRUE,\n  kbl_args = list(\n    booktabs = TRUE, \n    caption = \"Cohort Subgroup Analysis: Ages 35-62\"\n  ),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\ntables_list_older <- list(\n  Health = planned_subset_results$health$cohort$results$`Age > 62`$transformed_table,\n  Psych  = planned_subset_results$psych$cohort$results$`Age > 62`$transformed_table,\n  Life   = planned_subset_results$psych$cohort$results$`Age > 62`$transformed_table,\n  Social = planned_subset_results$psych$cohort$results$`Age > 62`$transformed_table\n)\n\n# new function bind tables\nmargot::margot_bind_tables(\n  tables_list = tables_list_older,\n  bold = TRUE,\n  kbl_args = list(\n    booktabs = TRUE, \n    caption = \"Cohort Subgroup Analysis: Age > 62\"\n  ),\n  highlight_color = NULL,\n  output_format = \"html\"\n)\n\n\n# plots -------------------------------------------------------------------\n# Results Plots\n# health\nplots_subgroup_wealth_health <- wrap_plots(\n  list(\n    planned_subset_results$health$wealth$results$Poor$plot,\n    planned_subset_results$health$wealth$results$MiddleIncome$plot,\n    planned_subset_results$health$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(\n    title = subtitle_health,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_wealth_health)\n\n# plots\nplots_subgroup_ethnicity_health<- wrap_plots(\n  list(\n    planned_subset_results$health$ethnicity$results$Asian$plot,\n    planned_subset_results$health$ethnicity$results$Euro$plot,\n    planned_subset_results$health$ethnicity$results$Maori$plot,\n    planned_subset_results$health$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_health,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_ethnicity_health)\n\n# plots\nplots_subgroup_political_health <- wrap_plots(\n  list(\n    planned_subset_results$health$political$results$Conservative$plot,\n    planned_subset_results$health$political$results$Centrist$plot,\n    planned_subset_results$health$political$results$Conservative$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_health,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_political_health)\n\n# plots\nplots_subgroup_gender_health<- wrap_plots(\n  list(\n    planned_subset_results$health$gender$results$Female$plot,\n    planned_subset_results$health$gender$results$Male$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_health,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_gender_health)\n\n# plots\nplots_subgroup_cohort_health<- wrap_plots(\n  list(\n    planned_subset_results$health$cohort$results$`Age < 35`$plot,\n    planned_subset_results$health$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$health$cohort$results$`Age > 62`$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_health,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_cohort_health)\n\n# psychological well-being\nplots_subgroup_wealth_psych <- wrap_plots(\n  list(\n    planned_subset_results$psych$wealth$results$Poor$plot,\n    planned_subset_results$psych$wealth$results$MiddleIncome$plot,\n    planned_subset_results$psych$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(\n    title = subtitle_psych,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_wealth_psych)\n\n# plots\nplots_subgroup_ethnicity_psych<- wrap_plots(\n  list(\n    planned_subset_results$psych$ethnicity$results$Asian$plot,\n    planned_subset_results$psych$ethnicity$results$Euro$plot,\n    planned_subset_results$psych$ethnicity$results$Maori$plot,\n    planned_subset_results$psych$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_psych,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_ethnicity_psych)\n\n# plots\nplots_subgroup_political_psych <- wrap_plots(\n  list(\n    planned_subset_results$psych$political$results$Conservative$plot,\n    planned_subset_results$psych$political$results$Centrist$plot,\n    planned_subset_results$psych$political$results$Conservative$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_psych,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_political_psych)\n\n# plots\nplots_subgroup_gender_psych<- wrap_plots(\n  list(\n    planned_subset_results$psych$gender$results$Female$plot,\n    planned_subset_results$psych$gender$results$Male$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_psych,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_gender_psych)\n\n# plots\nplots_subgroup_cohort_psych<- wrap_plots(\n  list(\n    planned_subset_results$health$psych$results$`Age < 35`$plot,\n    planned_subset_results$health$psych$results$`Age 35-62`$plot,\n    planned_subset_results$health$psych$results$`Age > 62`$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_psych,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_cohort_psych)\n\n# present focussed well-being\nplots_subgroup_wealth_present <- wrap_plots(\n  list(\n    planned_subset_results$present$wealth$results$Poor$plot,\n    planned_subset_results$present$wealth$results$MiddleIncome$plot,\n    planned_subset_results$present$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(\n    title = subtitle_present,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_wealth_present)\n\n# plots\nplots_subgroup_ethnicity_present<- wrap_plots(\n  list(\n    planned_subset_results$present$ethnicity$results$Asian$plot,\n    planned_subset_results$present$ethnicity$results$Euro$plot,\n    planned_subset_results$present$ethnicity$results$Maori$plot,\n    planned_subset_results$present$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_present,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_ethnicity_present)\n\n# plots\nplots_subgroup_political_present <- wrap_plots(\n  list(\n    planned_subset_results$present$political$results$Conservative$plot,\n    planned_subset_results$present$political$results$Centrist$plot,\n    planned_subset_results$present$political$results$Conservative$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_present,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_political_present)\n\n# plots\nplots_subgroup_gender_present<- wrap_plots(\n  list(\n    planned_subset_results$present$gender$results$Female$plot,\n    planned_subset_results$present$gender$results$Male$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_present,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view  \nprint(plots_subgroup_gender_present)\n\n# plots\nplots_subgroup_cohort_present<- wrap_plots(\n  list(\n    planned_subset_results$present$cohort$results$`Age < 35`$plot,\n    planned_subset_results$present$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$present$cohort$results$`Age > 62`$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_present,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_cohort_present)\n\n\n## life focussed well-being\nplots_subgroup_wealth_life <- wrap_plots(\n  list(\n    planned_subset_results$life$wealth$results$Poor$plot,\n    planned_subset_results$life$wealth$results$MiddleIncome$plot,\n    planned_subset_results$life$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(\n    title = subtitle_life,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_wealth_life)\n\n# plots\nplots_subgroup_ethnicity_life<- wrap_plots(\n  list(\n    planned_subset_results$life$ethnicity$results$Asian$plot,\n    planned_subset_results$life$ethnicity$results$Euro$plot,\n    planned_subset_results$life$ethnicity$results$Maori$plot,\n    planned_subset_results$life$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_life,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_ethnicity_life)\n\n# plots\nplots_subgroup_political_life <- wrap_plots(\n  list(\n    planned_subset_results$life$political$results$Conservative$plot,\n    planned_subset_results$life$political$results$Centrist$plot,\n    planned_subset_results$life$political$results$Conservative$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_life,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_political_life)\n\n# plots\nplots_subgroup_gender_life<- wrap_plots(\n  list(\n    planned_subset_results$life$gender$results$Female$plot,\n    planned_subset_results$life$gender$results$Male$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_life,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_gender_life)\n\n# plots\nplots_subgroup_cohort_life<- wrap_plots(\n  list(\n    planned_subset_results$life$cohort$results$`Age < 35`$plot,\n    planned_subset_results$life$cohort$results$`Age 35-62`$plot,\n    planned_subset_results$life$cohort$results$`Age > 62`$plot\n    \n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_life,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\nprint(plots_subgroup_cohort_life)\n\n# social well-being\nplots_subgroup_wealth_social <- wrap_plots(\n  list(\n    planned_subset_results$social$wealth$results$Poor$plot,\n    planned_subset_results$social$wealth$results$MiddleIncome$plot,\n    planned_subset_results$social$wealth$results$Rich$plot\n  ),\n  ncol = 1\n) +\n  patchwork::plot_annotation(\n    title = subtitle_social,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_wealth_social)\n\nplots_subgroup_ethnicity_social<- wrap_plots(\n  list(\n    planned_subset_results$social$ethnicity$results$Asian$plot,\n    planned_subset_results$social$ethnicity$results$Euro$plot,\n    planned_subset_results$social$ethnicity$results$Maori$plot,\n    planned_subset_results$social$ethnicity$results$Pacific$plot\n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_social,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_ethnicity_social)\n\n\nplots_subgroup_political_social <- wrap_plots(\n  list(\n    planned_subset_results$social$political$results$Conservative$plot,\n    planned_subset_results$social$political$results$Centrist$plot,\n    planned_subset_results$social$political$results$Conservative$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_social,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_political_social)\n\n# plots\nplots_subgroup_gender_social<- wrap_plots(\n  list(\n    planned_subset_results$social$gender$results$Female$plot,\n    planned_subset_results$social$gender$results$Male$plot\n  ),\n  ncol = 1\n)+\n  patchwork::plot_annotation(\n    title = subtitle_social,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\n# view\nprint(plots_subgroup_gender_social)\n\n# plots\nplots_subgroup_cohort_social<- wrap_plots(\n  list(\n    planned_subset_results$social$cohort$results$Boomers$plot,\n    planned_subset_results$social$cohort$results$Generation_X$plot,\n    planned_subset_results$social$cohort$results$Generation_Y$plot,\n    planned_subset_results$social$cohort$results$Generation_Z$plot\n    \n  ),\n  ncol = 2\n)+\n  patchwork::plot_annotation(\n    title = subtitle_social,\n    theme = theme(plot.title = element_text(size = 18,  face = \"bold\"))\n  )\n\nprint(plots_subgroup_cohort_social)\n\n\n\n# plot options: showcased ---------------------------------------------\n# default\nmargot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n)\n# tighten branches for easier viewing in single graphs\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  span_ratio = .30,\n  text_size = 3.8,\n  border_size = .1,\n  #  title = \"none\",\n  original_df = original_df\n)\n# colour decision node\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  span_ratio = .3,\n  text_size = 4, \n  title = \"New Title\",\n  non_leaf_fill =  \"violet\",\n  original_df = original_df\n)\n# make new title\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  span_ratio = .2,\n  text_size = 3, \n  title = \"New Title\",\n  non_leaf_fill =  \"white\",\n  original_df = original_df\n)\n\n# remove title\nmargot::margot_plot_decision_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  text_size = 5, \n  title = 'none', # set title to none\n  original_df = original_df\n)\n\n# policy tree options \n# select only plot 1 change alpha\nmargot::margot_plot_policy_tree(\n  models_binary_social,\n  \"model_t2_support_z\",\n  point_alpha = .25, \n  plot_selection = \"p1\"\n)\n# select only plot 2 change size of axis_text\n# change colours, modify etc... \nmargot::margot_plot_policy_tree(\n  models_binary,\n  \"model_t2_agreeableness_z\",\n  plot_selection = \"p2\",\n  axis_title_size = 30,\n  split_label_size = 20,\n  split_label_color = \"red\",\n  split_line_color = \"red\",\n)\n\n# adjust only the alpha\nmargot::margot_plot_policy_tree(\n  models_binary,\n  \"model_t2_agreeableness_z\",\n  point_alpha = .1\n)\n```\n:::\n\n\n\n\n\n\n## What You Will Learn\n\n- **Sensitivity Analysis: E-values**\n- **Workflow**\n\n<!-- ### Modified Treatment Policies and E-values -->\n\n\n<!-- A modified treatment policy is a *flexible* intervention of the following form:  -->\n\n\n<!-- $$ -->\n<!-- \\mathbf{d}^\\lambda (a_1) = \\begin{cases} 4 & \\text{if } a_1 < 4 \\\\  -->\n<!-- a_1 & \\text{otherwise} \\end{cases} -->\n<!-- $$ -->\n<!-- $$ -->\n<!-- \\mathbf{d}^\\phi (a_1) = \\begin{cases} 0 & \\text{if } a_1 > 0 \\\\  -->\n<!-- a_1 & \\text{otherwise} \\end{cases} -->\n<!-- $$ -->\n\n<!-- $$ g' = \\text{Intervention 1 - Intervention 2} = E[Y(\\mathbf{d}^\\lambda) - Y(\\mathbf{d}^\\phi)] $$ -->\n\n<!-- $$ g'' = \\text{Intervention 1 - Intervention 2} = E[Y(\\mathbf{d}^\\lambda) - Y(\\mathbf{d}^\\phi)] $$ -->\n\n<!-- $$ -->\n<!-- {\\delta}(g) ={g'} - {g''} -->\n<!-- $$ -->\n\n\n<!-- Rather than shifting an entire population into one of two states, the estimand flexibly shifts a population according to a pre-specified function.  -->\n\n<!-- see: @hoffman2023 -->\n\n\n### Sensitivity Analysis using E-values\n\n\n> The minimum strength of association on the risk ratio scale that an unmeasured confounder would need to have with both the exposure and the outcome, conditional on the measured covariates, to fully explain away a specific exposure-outcome association\n\nSee: @mathur2018; @linden2020EVALUE; @vanderweele2017.\n\nFor example, suppose that the lower bound of the the E-value was 1.3 with the lower bound of the confidence interval = 1.12, we might then write:\n\n> With an observed risk ratio of RR=1.3, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 1.3-fold each (or 30%), above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 1.12-fold (or 12%) each could do so, but weaker joint confounder associations could not.\n\nThe equations are as follows (for risk ratios)\n\n$$\nE-value_{RR} = RR + \\sqrt{RR \\times (RR - 1)}\n$$ \n\n$$\nE-value_{LCL} = LCL + \\sqrt{LCL \\times (LCL - 1)}\n$$\n\nHere is an R function that will calculate E-values\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# evalue for risk ratio\ncalculate_e_value <- function(rr, lcl) {\n  e_value_rr = rr + sqrt(rr*(rr - 1))\n  e_value_lcl = lcl + sqrt(lcl*(lcl - 1))\n  \n  list(e_value_rr = e_value_rr, e_value_lcl = e_value_lcl)\n}\n\n# e.g. smoking causes cancer\n# finding \tRR = 10.73 (95% CI: 8.02, 14.36)\nevalue_computed <- calculate_e_value(10.73, 8.02)\n\n#print\nevalue_computed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$e_value_rr\n[1] 20.94777\n\n$e_value_lcl\n[1] 15.52336\n```\n\n\n:::\n:::\n\n\nWe write:\n\n> With an observed risk ratio of RR=10.7, an unmeasured confounder that was associated 20.9477737-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of `e_value_rr$e_value_lcl`-fold each could do so, but weaker joint confounder associations could not.\n\nNote that in this class, most of the outcomes will be (standardised) continuous outcomes. Here's a function and LaTeX code to describe the approximation.\n\nThis function takes a linear regression coefficient estimate (`est`), its standard error (`se`), the standard deviation of the outcome (`sd`), a contrast of interest in the exposure (`delta`, which defaults to 1), and a \"true\" standardized mean difference (true, which defaults to 0). It calculates the odds ratio using the formula from Chinn (2000) and VanderWeele (2017), and then uses this to calculate the E-value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#evalue for ols\ncompute_evalue_ols <- function(est, se, delta = 1, true = 0) {\n  # rescale estimate and SE to get a contrast of size delta\n  est <- est / delta\n  se <- se / delta\n\n  # compute transformed odds ratio and ci's\n  odds_ratio <- exp(0.91 * est)\n  lo <- exp(0.91 * est - 1.78 * se)\n  hi <- exp(0.91 * est + 1.78 * se)\n\n  # compute E-Values based on the RR values\n  evalue_point_estimate <- odds_ratio * sqrt(odds_ratio + 1)\n  evalue_lower_ci <- lo * sqrt(lo + 1)\n\n  # return the e-values\n  return(list(EValue_PointEstimate = evalue_point_estimate,\n              EValue_LowerCI = evalue_lower_ci))\n}\n\n\n# example:\n# suppose we have an estimate of 0.5, a standard error of 0.1, and a standard deviation of 1.\n# this would correspond to a half a standard deviation increase in the outcome per unit increase in the exposure.\nresults <- compute_evalue_ols(est = 0.5, se = 0.1, delta = 1)\npoint_round <- round(results$EValue_PointEstimate, 3)\nci_round <- round(results$EValue_LowerCI, 3)\n\n# print results\nprint(point_round)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.53\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(ci_round)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.009\n```\n\n\n:::\n:::\n\n\nWe write:\n\n> With an observed risk ratio of 2.53, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 2.53-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 2.009-fold each could do so, but weaker joint confounder associations could not.\n\nNote the E-values package will do the computational work for us and this is what we use in the `margot` package to obtain E-values for sensitivity analysis.\n\n\n\n\n\n## Part 2 Guide For Preparing Your Study\n\n\nRecall that psychology begins with two questions. \n\n> What do I want to know about thought and behaviour? \n> What is the target population? \n\nIn cross-cultural psychology, these questions relate to differences, and similarities, between groups.\n\nSuppose we have asked a question. How can we address it using observational data?\n\nToo fast.\n\nOur question must be made precise. \n\nToday we will consider how to make psychological questions precise, and how to answer them, using 3-wave panel designs [@vanderweele2020].\n\n\n## Comprehensive Checklist for Detailed Reporting of a Causal Inferenctial Study.\n\n## STEP 1 Formulate the Research Question\n\n-   **State your question:** is my question clearly stated? If not, state it.\n-   **Relevance:** have I explained its importance? If not, explain.\n-   **Ethics** how might this question affect people? How might not investigating this question affect people? \n-   **Causality:** Is my question causal? If not, refine your question.\n-   **Heterogeneous Treatment Effects**:  Do I want to examine who responds differently (CATE)?\n-   **Subgroup analysis:** does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\n-   **Explain the Framework:** can I explain the causal inference framework and convey the gist to non-specialists? If not, review course materials.\n\n#### Determine Data Requirements\n\n-   **Data types:** are my data experimental? If yes, your project may not fit this course.\n-   **Time-series data:** are my data time-series? If not, reconsider your causal question.\n-   **Data waves:** do I have at least three waves of data? If not, beware of confounding control issues.\n-   **Data source:** are my data from the NZAVS simulated data set? If not, consult with me.\n\n#### Determine the Outcome\n\n-   **Outcome variable:** is the outcome variable *Y* defined? If not, define it.\n-   **Multiple outcomes:** are there multiple outcomes? If yes, explain and define them.\n-   **Outcome relevance:** can I explain how the outcome variable/s relate to my question? If not, clarify.\n-   **Outcome type:** is my outcome binary and rare? If yes, consider logistic regression. If my outcome is continuous, consider z-transforming it or categorising it (consult an expert).\n-   **Outcome timing:** does the outcome appear after the exposure? It should.\n\n#### Determine the Exposure\n\n-   **Exposure variable:** is the exposure variable *A* defined? If not, define it.\n-   **Multiple exposures:** are there multiple exposures? If yes, reassess; if only one exposure, proceed.\n-   **Exposure relevance:** can I explain how the exposure variable relates to my question? If not, clarify.\n-   **Positivity:** can we intervene on the exposure at all levels of the covariates? We should be able to.\n-   **Consistency:** can I interpret what it means to intervene on the exposure? I should be able to.\n-   **Exchangeability:** are different versions of the exposure conditionally exchangeable given measured baseline confounders? They should be.\n-   **Exposure type:** is the exposure binary or continuous?\n-   **Shift intervention**: Am I contrasting static interventions or modified treatment policies? \n-   **Exposure timing:** Does the exposure appear before the outcome? It should.\n\n#### Account for Confounders\n\n-   **Baseline confounders:** Have I defined my baseline confounders *L*? I should have.\n-   **Justification:** Can I explain how the baseline confounders could affect both *A* and *Y*? I should be able to.\n-   **Timing:** Are the baseline confounders measured before the exposure? They should be.\n-   **Inclusion:** Is the baseline measure of the exposure and the baseline outcome included in the set of baseline confounders? They should be.\n-   **Sufficiency:** Are the baseline confounders sufficient to ensure balance on the exposure, such that *A* is independent of *Y* given *L*? If not, plan a sensitivity analysis.\n-   **Confounder type:** Are the confounders continuous or binary? If so, consider converting them to z-scores. If they are categorical with three or more levels, do not convert them to z-scores.\n\n#### Draw a Causal Diagram with Unmeasured Confounders\n\n-   **Unmeasured confounders:** Does previous science suggest the presence of unmeasured confounders? If not, expand your understanding.\n-   **Causal diagram:** Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding? I should have.\n-   **Measurement error:** Have I described potential biases from measurement errors? If not, we'll discuss later.\n-   **Temporal order:** Does my DAG have time indicators to ensure correct temporal order? It should.\n-   **Time consistency:** Is my DAG organized so that time follows in a consistent direction? It should.\n\n#### Identify the Estimand\n\n- ATE or CATE or both? \n\n#### Understanding Source and Target Populations\n\n-   **Populations identified:** Have I explained how my sample relates to my target populations? I should have.\n-   **Generalisability and transportability:** Have I considered whether my results generalise different populations?  I should have.\n\n#### Set Eligibility Criteria\n\n-   **Criteria stated:** Have I stated the eligibility criteria for the study? I should have.\n\n#### Describe Sample Characteristics\n\n-   **Descriptive statistics:** have I provided descriptive statistics for demographic information taken at baseline? I should have.\n-   **Exposure change:** Have I demonstrated the magnitudes of change in the exposure from baseline to the exposure interval? I should have.\n-   **References:** Have I included references for more information about the sample? I should have.\n\n#### Addressing Missing Data\n\n-   **Missing data checks:** Have I checked for missing data? I should have.\n-   **Missing data plan:** If there is missing data, have I described how I will address it? I should have.\n\n#### Selecting the Model Approach: If Not Using Machine Learning (Lecture 7)\n\n-   **Approach decision:** Have I decided on using G-computation, IPTW, or Doubly-Robust Estimation? I should have.\n-   **Interactions:** If not using machine learning, have I included the interaction of the exposure and baseline covariates? I should have.\n-   **Big data:** If I have a large data set, should I include the interaction of the exposure, group, and baseline confounders? I should consider it.\n-   **Model specification:** have I double-checked the model specification? I should.\n-   **Outcome assessment:** If the outcome is rare and binary, have I specified logistic regression? If it's continuous, have I considered converting it to z-scores?\n-   **Sensitivity analysis:** am I planning a sensitivity analysis using simulation? If yes, describe it (e.g. E-values.)\n\n#### If Machine Learing\n\n-  **Machine Learning**: have I explained how causal forests work (next week's lecture).\n\n### d. Highlight unmeasured pre-treatment covariates\n\nLet **U** denoted unmeasured pre-treatment covariates that may potentially bias the statistical association between *A* and *Y* independently of the measured covariates.\n\n#### Consider:\n\n-   To affect *Y* and *A*, *U* must occur before *A*.\n-   It is useful to draw a causal diagramme to illustrate all potential sources of bias.\n-   Causal diagrammes are qualitative tools that require specialist expertise. We cannot typically obtain a causal graph from the data.\n-   A causal diagramme should include only as much information as is required to assess confounding. See @fig-dag-outcomewide for an example.\n-   Because we cannot ensure the absence of unmeasured confounders in observational settings, it is vital to conduct sensitivity analyses for the results. For sensitivity analyeses, we use E-values.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Causal graph: three-wave panel design.](09-content_files/figure-html/fig-dag-outcomewide-1.png){#fig-dag-outcomewide width=100%}\n:::\n:::\n\n\n### e. Choose the scale for a causal contrast\n\nDifference/ Risk ratio? \n\n#### Consider:\n\n-   *In this course, we are interested in stratum specific comparisons*\n-   In the causal inference literature, the concept we use to make sense of stratum specific comparisons is called \"effect modification.\"\n-   By inferring effects within strata, we may evaluate whether the effects of different exposures or treatments on some well-defined outcome (measured in some well-defined time-period after the exposure) differ depending on group measurement.\n-   The logic of effect modification differs slightly from that of interaction.\n\n#### Aside: extensions\n\nFor continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):\n\n$$ATE_{A,A'} = E[Y(A) - Y(A')| L]$$\n\nThis essentially denotes an average treatment effect comparing the outcome under treatment level $A$ to the outcome under treatment level $A'$.\n\nLikewise:\n\n$$ATE_{A/A'} = \\frac{E[Y(A)| L]}{E[Y(A')| L]}$$\n\nThis defines the contrast of $A$ and $A'$ on a ratio scale.\n\n#### f. Describe the population(s) for whom the intended study is meant to generalise by distinguishing between source and target populations.\n\nConsider the following concepts:\n\n-   **Source population**: a source population is where we gather our data for a study. We pull our specific sample from this group. It needs to mirror the broader group for our conclusions to be valid and widely applicable.\n\n-   **Target population**: the target population is the larger group we aim to apply our study's results to. It could be defined by location, demographics, or specific conditions. The closer the source matches the target in ways that are relevant to our causal questions, the stronger our causal inferences about the target population will be.\n\n    -   **Generalisability** refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called \"external validity\".\n\n$$\\text{Generalisability} = PATE \\approx ATE_{\\text{sample}}$$\n\n-   **Transportability** refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.\n\n$$\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, T)$$\n\nwhere $f$ is a function and $T$ is a function that maps the results from our source population to another population. To achieve transportability, we need information about the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates differ between the populations. Assessing transportability requires scientific knowledge.\n\n### Summary Step 1: Consider how much we need to do when asking a causal question!\n\nWe discover that asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: \"How may we answer this causal question?\"\n\n## STEP 2: ANSWER A CAUSAL QUESTION\n\n#### Obtain longitudinal data\n\nNote that causal inference from observational data turns on the appropriate temporal ordering of the key variables involved in the study.\n\nRecall we have defined.\n\n-   **A**: Our exposure or treatment variable, denoted as **A**. Here we consider the example of 'Church attendance'.\n\n-   **Y**: The outcome variable we are interested in, represented by **Y**, is psychological distress. We operationalise this variable through the 'Kessler-6' distress scale.\n\n-   **L**: The confounding variables, collectively referred to as **L**, represent factors that can independently influence both **A** and **Y**. For example, socio-economic status could be a confounder that impacts both the likelihood of church attendance and the levels of psychological distress.\n\nGiven the importance of temporal ordering, we must now define time:\n\n-   **t** $\\in$ T: Let $t$ denote within a multiwave panel study with **T** measurement intervals.\n\nWhere $t/\\text{{exposure}}$ denotes the measurement interval for the exposure. Longitudinal data collection provides us the ability to establish a causal model such that:\n\n$$t_{confounders} < t_{exposure}< t_{outcome}$$\n\nTo minimise the posibility of time-varying confounding and obtain the clearest effect estimates, we should acquire the most recent values of $\\mathbf{L}$ preceding $A$ and the latest values of $A$ before $Y$.\n\nNote in @fig-dag-outcomewide, We use the prefixes \"t0, t1, and t2\" to denote temporal ordering. We include in the set of baseline confounders the pre-exposure measurement of *A* and *Y*. This allows for more substantial confounding control. For unmeasured confounder to affect both the exposure and the outcome, it would need to do so independently of the pre-exposure confounders. Additionally, including the baseline exposure gives us an effect estimate for the incidence exposure, rather than the prevelance of the exposure. This helps us to assess the expected change in the outcome were we to initate a change in the exposure.\n\n### Include the measured exposure with baseline covariates\n\nControlling for prior exposure enables the interpretation of the effect estimate as a change in the exposure in a manner akin to a randomised trial. We propose that the effect estimate with prior control for the exposure estimates the \"incidence exposure\" rather than the \"prevalence exposure\" [@danaei2012]. It is crucial to estimate the incidence exposure because if the effects of an exposure are harmful in the short term such that these effects are not subsequently measured, a failure to adjust for prior exposure will yield the illusion that the exposure is beneficial. Furthermore, this approach aids in controlling for unmeasured confounding. For such a confounder to explain away the observed exposure-outcome association, it would need to do so independently of the prior level of the exposure and outcome.\n\n### State the eligibility criteria for participation\n\nThis step is invaluable for assessing whether we are answering the causal question that we have asked.\n\n#### Consider:\n\n-   Generalisability: we cannot evaluate inferences to a target group from the source population if we do not describe the source population\n-   Eligibility criteria will help us to ensure whether we have correctly evaluated potential measurement bias/error in our instruments.\n\nFor example, the New Zealand Attitudes and Values Study is a National Probability study of New Zealanders. The details provided in the supplementary materials describe how individuals were randomly selected from the country's electoral roll. From these invitations there was typically less than 15% response rate. How might this process of recruitment affect generalisability and transportability of our results?\n\n-   Aside: discuss per protocol effects/ intention to treat effects\n\n### Determine how missing data will be handled\n\n-   As we will consider in the upcoming weeks, loss to follow up and non-response opens sources for bias. We must develop a strategy for handling missing data.\n\n### State a statistical model\n\nThe models we have considered in this course are G-computation, Inverse Probability of Treatement Weighting, and Doubly-Robust estimation.\n\n### Reporting\n\nConsider the following ideas about how to report one's model:\n\n-   **Estimator**: Doubly robust where possible.\n-   **Propensity Score Reporting:** Detail the process of propensity score derivation, including the model used and any variable transformations.\n-   **WeightIt Package:** Explicitly mention the use of the 'WeightIt' package in R, including any specific options or parameters used in the propensity score estimation process.\n-   **Method Variations:** Report if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as 'ebal', 'energy', and 'ps'.\n-   **Continuous Exposures:** Highlight that for continuous exposures, only the 'energy' option was used for propensity score estimation.\n-   **Subgroup Estimation:** Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\n-   **Covariate Balance:** Include a Love plot to visually represent covariate balance on the exposure both before and after weighting.\n-   **Weighting Algorithm Statistics:** Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit.\n-   **Outcome Regression Model:** Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation.\n-   **Subgroup Interaction:** Address whether the subgroup was included separately as an interaction in the outcome model, and if the model successfully converged.\n-   **Machine Learning Using `lmtp`** If using the `lmtp` package, do a stratified analysis. (see today's lab)\n-   **Model coefficients:** note that the model coefficients should not be interpreted, as they are not meaningful in this context.\n-   **Confidence intervals and standard errors:** Describe the methods used to derive confidence intervals and standard errors, noting the use of the 'clarify' package in R for simulation based inference.\n\n### Example of how to report a doubly robust method in your report\n\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\n\n**Step 1** involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\n\n**Step 2** focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\n\n**Step 3** entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\n\n**Step 4** is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup. \n\n**Step 5** involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods [@greifer2023]. This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\n### Inference\n\nConsider the following ideas about what to discuss in one's findings:\nConsider the following ideas about what to discuss in one's findings. The order of exposition might be different.\n\n1.  **Summary of results**:  What did you find?\n\n2. **Interpretation of E-values:** Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\n\n3.  **Causal Effect Interpretation:** What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question. \n\n4.  **Comparison of Subgroups:** Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\n\n5.  **Uncertainty and Confidence Intervals:** Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\n\n6.  **Generalisability and Transportability:** Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\n\n7.  **Assumptions and Limitations:** Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed. \n\n8.  **Theoretical Relevance**: How are these findings relevant to existing theories. \n\n9.  **Replication and Future Research:** Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\n\n10. **Real-world Implications:** Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research.\n\n\n## Appendix A: Details of Estimation Approaches\n\n### G-computation for Subgroup Analysis Estimator\n\n**Step 1:** Estimate the outcome model. Fit a model for the outcome $Y$, conditional on the exposure $A$, the covariates $L$, and subgroup indicator $G$. This model can be a linear regression, logistic regression, or another statistical model. The goal is to capture the relationship between the outcome, exposure, confounders, and subgroups.\n\n$$ \\hat{E}(Y|A,L,G) = f_Y(A,L,G; \\theta_Y) $$\n\nThis equation represents the expected value of the outcome $Y$ given the exposure $A$, covariates $L$, and subgroup $G$, as modelled by the function $f_Y$ with parameters $\\theta_Y$. This formulation allows for the prediction of the average outcome $Y$ given certain values of $A$, $L$, and $G$.\n\n**Step 2:** Simulate potential outcomes. For each individual in each subgroup, predict their potential outcome under the intervention $A=a$ using the estimated outcome model:\n\n$$\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,L,G=g; \\hat{\\theta}_Y]$$\n\nWe also predict the potential outcome for everyone in each subgroup under the causal contrast, setting the intervention for everyone in that group to $A=a'$:\n\n$$\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',L,G=g; \\hat{\\theta}_Y]$$\n\nIn these equations, $Y$ represents the potential outcome, $A$ is the intervention, $L$ are the covariates, $G=g$ represents the subgroup, and $\\theta_Y$ are the parameters of the outcome model.\n\n**Step 3:** Calculate the estimated difference for each subgroup $g$:\n\n$$\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]$$\n\nThis difference $\\hat{\\delta}_g$ represents the average causal effect of changing the exposure from level $a'$ to level $a$ within each subgroup $g$.\n\nWe use simulation-based inference methods to compute standard errors and confidence intervals [@greifer2023].\n\n**Step 4:** Compare differences in causal effects by subgroups:\n\n$$\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}$$\n\nwhere,\n\n$$\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a^{\\prime})|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a^{\\prime})|G=g^{\\prime}]- \\hat{E}[Y(a)|G=g^{\\prime}]\\big)}^{\\hat{\\delta_{g^{\\prime}}}}$$\n\nThis difference $\\hat{\\gamma}$ represents the difference in the average causal effects between the subgroups $g$ and $g'$. It measures the difference in effect of the exposure $A$ within subgroup $G$ on the outcome $Y$.\n\n\n[^note_care] \n\n[^note_care]: $A$ and $G$ on $Y$ might not be additive. We assume that the potential confounders $L$ are sufficient to control for confounding. See Appendix\n\n\nWe again use simulation-based inference methods to compute standard errors and confidence intervals [@greifer2023].\n\n\n\n\n### Inverse Probability of Treatment Weighting (IPTW) for Subgroup Analysis Estimator\n\n\n**Step 1:** Estimate the propensity score. The propensity score $e(L, G)$ is the conditional probability of the exposure $A = 1$, given the covariates $L$ and subgroup indicator $G$. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\n\n$$\\hat{e} = P(A = 1 | L, G) = f_A(L, G; \\theta_A)$$\n\nHere, $f_A(L, G; \\theta_A)$ is a function (statistical model) that estimates the probability of the exposure $A = 1$ given covariates $L$ and subgroup $G$. Then, we calculate the weights for each individual, denoted as $v$, using the estimated propensity score:\n\n$$\nv = \n\\begin{cases} \n\\frac{1}{\\hat{e}} & \\text{if } A = 1 \\\\\n\\frac{1}{1-\\hat{e}} & \\text{if } A = 0 \n\\end{cases}\n$$\n\n**Step 2:** Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome $Y$, conditional on the exposure $A$ and subgroup $G$. This can be represented as:\n\n$$ \\hat{E}(Y|A, G; V) = f_Y(A, G ; \\theta_Y, V) $$\n\nIn this model, $f_Y$ is a function (such as a weighted regression model) with parameters $θ_Y$.\n\n**Step 3:** Simulate potential outcomes. For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention $A=a$ regardless of their actual exposure level:\n\n$$\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,G=g; \\hat{\\theta}_Y, v]$$\n\nAnd also under the hypothetical scenario where everyone is exposed to intervention $A=a'$:\n\n$$\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',G=g; \\hat{\\theta}_Y, v]$$\n\n**Step 4:** Estimate the average causal effect for each subgroup as the difference in the predicted outcomes:\n\n$$\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]$$\n\nThe estimated difference $\\hat{\\delta}_g$ represents the average causal effect within group $g$.\n\n**Step 5:** Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\n$$\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}$$\n\nwhere,\n\n$$\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a')|G=g']- \\hat{E}[Y(a)|G=g']\\big)}^{\\hat{\\delta_{g'}}}$$\n\nThis $\\hat{\\gamma}$ represents the difference in the average causal effects between the subgroups $g$ and $g'$.\n\n\nWe again use simulation-based inference methods to compute standard errors and confidence intervals [@greifer2023].\n\n\n### Doubly Robust Estimation for Subgroup Analysis Estimator\nIt appears that the Doubly Robust Estimation explanation for subgroup analysis is already clear and correct, covering all the necessary steps in the process. Nevertheless, there's a slight confusion in step 4. The difference $\\delta_g$ is not defined within the document. I assume that you intended to write $\\hat{\\delta}_g$. Here's the corrected version:\n\n### Doubly Robust Estimation for Subgroup Analysis Estimator\n\nDoubly Robust Estimation is a powerful technique that combines the strengths of both the IPTW and G-computation methods. It uses both the propensity score model and the outcome model, which makes it doubly robust: it produces unbiased estimates if either one of the models is correctly specified.\n\n**Step 1** Estimate the propensity score. The propensity score $\\hat{e}(L, G)$ is the conditional probability of the exposure $A = 1$, given the covariates $L$ and subgroup indicator $G$. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\n\n$$\\hat{e} = P(A = 1 | L, G) = f_A(L, G; \\theta_A)$$\n\nHere, $f_A(L, G; \\theta_A)$ is a function (statistical model) that estimates the probability of the exposure $A = 1$ given covariates $L$ and subgroup $G$. Then, we calculate the weights for each individual, denoted as $v$, using the estimated propensity score:\n\n$$\nv = \n\\begin{cases} \n\\frac{1}{\\hat{e}} & \\text{if } A = 1 \\\\\n\\frac{1}{1-\\hat{e}} & \\text{if } A = 0 \n\\end{cases}\n$$\n\n**Step 2** Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome $Y$, conditional on the exposure $A$, covariates $L$, and subgroup $G$.\n\n$$ \\hat{E}(Y|A, L, G; V) = f_Y(A, L, G ; \\theta_Y, V) $$\n\n**Step 3** For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention $A=a$ regardless of their actual exposure level:\n\n$$\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,G=g; L,\\hat{\\theta}_Y, v]$$\n\nAnd also under the hypothetical scenario where everyone in each subgroup is exposed to intervention $A=a'$:\n\n$$\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',G=g; L; \\hat{\\theta}_Y, v]$$\n\n**Step 4** Estimate the average causal effect for each subgroup. Compute the estimated expected value of the potential outcomes under each intervention level for each subgroup:\n\n$$\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]$$\n\nThe estimated difference $\\hat{\\delta}_g$ represents the average causal effect of changing the exposure from level $a'$ to level $a$ within each subgroup.\n\n**Step 5** Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\n$$\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}$$\n\nwhere,\n\n$$\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a')|G=g']- \\hat{E}[Y(a)|G=g']\\big)}^{\\hat{\\delta_{g'}}}$$\n\n\nWe again use simulation-based inference methods to compute standard errors and confidence intervals [@greifer2023].\n\n\n## Appendix B: G-computation for Subgroup Analysis Estimator with Non-Additive Effects\n\n**Step 1:** Estimate the outcome model. Fit a model for the outcome $Y$, conditional on the exposure $A$, the covariates $L$, subgroup indicator $G$, and interactions between $A$ and $G$. This model can be a linear regression, logistic regression, or another statistical model. The goal is to capture the relationship between the outcome, exposure, confounders, subgroups, and their interactions.\n\n$$ \\hat{E}(Y|A,L,G,AG) = f_Y(A,L,G,AG; \\theta_Y) $$\n\nThis equation represents the expected value of the outcome $Y$ given the exposure $A$, covariates $L$, subgroup $G$, and interaction term $AG$, as modeled by the function $f_Y$ with parameters $\\theta_Y$. \n\n**Step 2:** Simulate potential outcomes. For each individual in each subgroup, predict their potential outcome under the intervention $A=a$ using the estimated outcome model:\n\n$$\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,L,G=g,AG=ag; \\hat{\\theta}_Y]$$\n\nWe also predict the potential outcome for everyone in each subgroup under the causal contrast, setting the intervention for everyone in that group to $A=a'$:\n\n$$\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',L,G=g,AG=a'g; \\hat{\\theta}_Y]$$\n\n**Step 3:** Calculate the estimated difference for each subgroup $g$:\n\n$$\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]$$\n\n**Step 4:** Compare differences in causal effects by subgroups:\n\n$$\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}$$\n\nwhere,\n\n$$\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a^{\\prime})|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a^{\\prime})|G=g^{\\prime}]- \\hat{E}[Y(a)|G=g^{\\prime}]\\big)}^{\\hat{\\delta_{g^{\\prime}}}}$$\n\nThis difference $\\hat{\\gamma}$ represents the difference in the average causal effects between the subgroups $g$ and $g'$, taking into account the interaction effect of the exposure $A$ and the subgroup $G$ on the outcome $Y$.\n\nNote that the interaction term $AG$ (or $ag$ and $a'g$ in the potential outcomes) stands for the interaction between the exposure level and the subgroup. This term is necessary to accommodate the non-additive effects in the model. As before, we must ensure that potential confounders $L$ are sufficient to control for confounding. \n\n## Appendix C: Doubly Robust Estimation for Subgroup Analysis Estimator with Interaction\n\nAgain, Doubly Robust Estimation combines the strengths of both the IPTW and G-computation methods. It uses both the propensity score model and the outcome model, which makes it doubly robust: it produces unbiased estimates if either one of the models is correctly specified.\n\n**Step 1** Estimate the propensity score. The propensity score $e(L, G)$ is the conditional probability of the exposure $A = 1$, given the covariates $L$ and subgroup indicator $G$. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\n\n$$e = P(A = 1 | L, G) = f_A(L, G; \\theta_A)$$\n\nHere, $f_A(L, G; \\theta_A)$ is a function (statistical model) that estimates the probability of the exposure $A = 1$ given covariates $L$ and subgroup $G$. Then, we calculate the weights for each individual, denoted as $v$, using the estimated propensity score:\n\n$$\nv = \n\\begin{cases} \n\\frac{1}{\\hat{e}} & \\text{if } A = 1 \\\\\n\\frac{1}{1-\\hat{e}} & \\text{if } A = 0 \n\\end{cases}\n$$\n\n**Step 2** Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome $Y$, conditional on the exposure $A$, covariates $L$, subgroup $G$ and the interaction between $A$ and $G$.\n\n$$ \\hat{E}(Y|A, L, G, AG; V) = f_Y(A, L, G, AG ; \\theta_Y, V) $$\n\n**Step 3** For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention $A=a$ regardless of their actual exposure level:\n\n$$\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,G=g, AG=ag; L,\\hat{\\theta}_Y, v]$$\n\nAnd also under the hypothetical scenario where everyone in each subgroup is exposed to intervention $A=a'$:\n\n$$\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',G=g, AG=a'g; L; \\hat{\\theta}_Y, v]$$\n\n**Step 4** Estimate the average causal effect for each subgroup. Compute the estimated expected value of the potential outcomes under each intervention level for each subgroup:\n\n$$\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]$$\n\nThe estimated difference $\\hat{\\delta}_g$ represents the average causal effect of changing the exposure from level $a'$ to level $a$ within each subgroup.\n\n**Step 5** Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\n$$\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}$$\n\nwhere,\n\n$$\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a')|G=g']- \\hat{E}[Y(a)|G=g']\\big)}^{\\hat{\\delta_{g'}}}$$\n\nWe again use simulation-based inference methods to compute standard errors and confidence intervals [@greifer2023].\n\n\n\n## Appendix D: Marginal Structural Models for Estimating Population Average Treatment Effect with Interaction (Doubly Robust)\n\n\nSometimes we will only wish to estimate a marginal effect. In that case.\n\n**Step 1** Estimate the propensity score. The propensity score $e(L)$ is the conditional probability of the exposure $A = 1$, given the covariates $L$ which contains the subgroup $G$. This can be modelled using logistic regression or other functions as described in @greifer2023\n\n$$\\hat{e} = P(A = 1 | L) = f_A(L; \\theta_A)$$\n\nHere, $f_A(L; \\theta_A)$ is a function (a statistical model) that estimates the probability of the exposure $A = 1$ given covariates $L$. Then, we calculate the weights for each individual, denoted as $v$, using the estimated propensity score:\n\n$$\nv = \n\\begin{cases} \n\\frac{1}{e} & \\text{if } A = 1 \\\\\n\\frac{1}{1-e} & \\text{if } A = 0 \n\\end{cases}\n$$\n\n**Step 2** Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome $Y$, conditional on the exposure $A$ and covariates $L$.\n\n$$ \\hat{E}(Y|A, L; V) = f_Y(A, L; \\theta_Y, V) $$\n\nThis model should include terms for both the main effects of $A$ and $L$ and their interaction $AL$.\n\n**Step 3** For the entire population, simulate the potential outcome under the hypothetical scenario where everyone is exposed to the intervention $A=a$ regardless of their actual exposure level:\n\n$$\\hat{E}(Y(a)) = \\hat{E}[Y|A=a; L,\\hat{\\theta}_Y, v]$$\n\nAnd also under the hypothetical scenario where everyone is exposed to intervention $A=a'$:\n\n$$\\hat{E}(Y(a')) = \\hat{E}[Y|A=a'; L; \\hat{\\theta}_Y, v]$$\n\n**Step 4** Estimate the average causal effect for the entire population. Compute the estimated expected value of the potential outcomes under each intervention level for the entire population:\n\n$$\\hat{\\delta} = \\hat{E}[Y(a)] - \\hat{E}[Y(a')]$$\n\nThe estimated difference $\\hat{\\delta}$ represents the average causal effect of changing the exposure from level $a'$ to level $a$ in the entire population.\n\nWe again use simulation-based inference methods to compute standard errors and confidence intervals [@greifer2023].\n\n\n### Machine Learning\n\nExample from https://osf.io/cnphs\n\n\n> We perform statistical estimation using semi-parametric Targeted\nLearning, specifically a Targeted Minimum Loss-based Estimation (TMLE)\nestimator. TMLE is a robust method that combines machine learning\ntechniques with traditional statistical models to estimate causal\neffects while providing valid statistical uncertainty measures for these\nestimates [@van2012targeted; @van2014targeted].\n\n> TMLE operates through a two-step process that involves modelling both\nthe outcome and treatment (exposure). Initially, TMLE employs machine\nlearning algorithms to flexibly model the relationship between\ntreatments, covariates, and outcomes. This flexibility allows TMLE to\naccount for complex, high-dimensional covariate spaces\n\\emph{efficiently} without imposing restrictive model assumptions;\n[@vanderlaan2011; @vanderlaan2018]. The outcome of this step is a set\nof initial estimates for these relationships.\n\n> The second step of TMLE involves ``targeting'' these initial estimates\nby incorporating information about the observed data distribution to\nimprove the accuracy of the causal effect estimate. TMLE achieves this\nprecision through an iterative updating process, which adjusts the\ninitial estimates towards the true causal effect. This updating process\nis guided by the efficient influence function, ensuring that the final\nTMLE estimate is as close as possible, given the measures and data, to\nthe targeted causal effect while still being robust to\nmodel-misspecification in either the outcome or the treatment model\n[@van2014discussion].\n\n> Again, a central feature of TMLE is its double-robustness property. If\neither the treatment model or the outcome model is correctly specified,\nthe TMLE estimator will consistently estimate the causal effect.\nAdditionally, we used cross-validation to avoid over-fitting, following\nthe pre-stated protocols in Bulbulia [@bulbulia2024PRACTICAL]. The integration of TMLE\nand machine learning technologies reduces the dependence on restrictive\nmodelling assumptions and introduces an additional layer of robustness.\nFor further details of the specific targeted learning strategy we\nfavour, see [@duxedaz2021; @hoffman2022, @hoffman2023]. We perform estimation using the\n\\texttt{lmtp} package [@williams2021]. We used the \\texttt{superlearner} library for semi-parametric estimation with the predefined libraries \\texttt{SL.ranger},\n\\texttt{SL.glmnet}, and \\texttt{SL.xgboost} [@xgboost2023; @polley2023; @Ranger2017]. We created graphs, tables and output reports using the \\texttt{margot} package\n[@margot2024].\n\n#### Sensitivity Analysis Using the E-value\n\n> To assess the sensitivity of results to unmeasured confounding, we\nreport VanderWeele and Ding's ``E-value'' in all analyses\n[@vanderweele2017]. The E-value quantifies the minimum strength of association (on the risk ratio scale) that an unmeasured confounder would need to have with both the exposure\nand the outcome (after considering the measured covariates) to explain\naway the observed exposure-outcome association\n[@linden2020EVALUE; @vanderweele2020]. To\nevaluate the strength of evidence, we use the bound of the E-value 95\\%\nconfidence interval closest to 1.\n\n\n\n\n### Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport::cite_packages()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Barrett M (2021). _ggokabeito: 'Okabe-Ito' Scales for 'ggplot2' and 'ggraph'_. doi:10.32614/CRAN.package.ggokabeito <https://doi.org/10.32614/CRAN.package.ggokabeito>, R package version 0.1.0, <https://CRAN.R-project.org/package=ggokabeito>.\n  - Bulbulia J (2024). _margot: MARGinal Observational Treatment-effects_. doi:10.5281/zenodo.10907724 <https://doi.org/10.5281/zenodo.10907724>, R package version 1.0.33 Functions to obtain MARGinal Observational Treatment-effects from observational data., <https://go-bayes.github.io/margot/>.\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. doi:10.32614/CRAN.package.extrafont <https://doi.org/10.32614/CRAN.package.extrafont>, R package version 0.19, <https://CRAN.R-project.org/package=extrafont>.\n  - Firke S (2024). _janitor: Simple Tools for Examining and Cleaning Dirty Data_. doi:10.32614/CRAN.package.janitor <https://doi.org/10.32614/CRAN.package.janitor>, R package version 2.2.1, <https://CRAN.R-project.org/package=janitor>.\n  - Greifer N (2025). _WeightIt: Weighting for Covariate Balance in Observational Studies_. doi:10.32614/CRAN.package.WeightIt <https://doi.org/10.32614/CRAN.package.WeightIt>, R package version 1.4.0, <https://CRAN.R-project.org/package=WeightIt>.\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. <https://www.jstatsoft.org/v40/i03/>.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. doi:10.32614/CRAN.package.tibble <https://doi.org/10.32614/CRAN.package.tibble>, R package version 3.2.1, <https://CRAN.R-project.org/package=tibble>.\n  - Pedersen T (2024). _patchwork: The Composer of Plots_. doi:10.32614/CRAN.package.patchwork <https://doi.org/10.32614/CRAN.package.patchwork>, R package version 1.3.0, <https://CRAN.R-project.org/package=patchwork>.\n  - R Core Team (2025). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. <https://www.R-project.org/>.\n  - VanderWeele TJ, Ding P (2011). \"Sensitivity analysis in observational research: introducing the E-value.\" _Annals of Internal Medicine_, *167*(4), 268-274. Mathur MB, VanderWeele TJ (2019). \"Sensitivity analysis for unmeasured confounding in meta-analyses.\" _Journal of the American Statistical Association>_. Smith LH, VanderWeele TJ (2019). \"Bounding bias due to selection.\" _Epidemiology_.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, <https://ggplot2.tidyverse.org>.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. doi:10.32614/CRAN.package.forcats <https://doi.org/10.32614/CRAN.package.forcats>, R package version 1.0.0, <https://CRAN.R-project.org/package=forcats>.\n  - Wickham H (2023). _stringr: Simple, Consistent Wrappers for Common String Operations_. doi:10.32614/CRAN.package.stringr <https://doi.org/10.32614/CRAN.package.stringr>, R package version 1.5.1, <https://CRAN.R-project.org/package=stringr>.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 <https://doi.org/10.21105/joss.01686>.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. doi:10.32614/CRAN.package.dplyr <https://doi.org/10.32614/CRAN.package.dplyr>, R package version 1.1.4, <https://CRAN.R-project.org/package=dplyr>.\n  - Wickham H, Henry L (2025). _purrr: Functional Programming Tools_. doi:10.32614/CRAN.package.purrr <https://doi.org/10.32614/CRAN.package.purrr>, R package version 1.0.4, <https://CRAN.R-project.org/package=purrr>.\n  - Wickham H, Hester J, Bryan J (2024). _readr: Read Rectangular Text Data_. doi:10.32614/CRAN.package.readr <https://doi.org/10.32614/CRAN.package.readr>, R package version 2.1.5, <https://CRAN.R-project.org/package=readr>.\n  - Wickham H, Vaughan D, Girlich M (2024). _tidyr: Tidy Messy Data_. doi:10.32614/CRAN.package.tidyr <https://doi.org/10.32614/CRAN.package.tidyr>, R package version 1.3.1, <https://CRAN.R-project.org/package=tidyr>.\n  - Xie Y (2025). _knitr: A General-Purpose Package for Dynamic Report Generation in R_. R package version 1.50, <https://yihui.org/knitr/>. Xie Y (2015). _Dynamic Documents with R and knitr_, 2nd edition. Chapman and Hall/CRC, Boca Raton, Florida. ISBN 978-1498716963, <https://yihui.org/knitr/>. Xie Y (2014). \"knitr: A Comprehensive Tool for Reproducible Research in R.\" In Stodden V, Leisch F, Peng RD (eds.), _Implementing Reproducible Computational Research_. Chapman and Hall/CRC. ISBN 978-1466561595.\n  - Xie Y (2025). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.57, <https://github.com/rstudio/tinytex>. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. <https://tug.org/TUGboat/Contents/contents40-1.html>.\n  - Zhu H (2024). _kableExtra: Construct Complex Table with 'kable' and Pipe Syntax_. doi:10.32614/CRAN.package.kableExtra <https://doi.org/10.32614/CRAN.package.kableExtra>, R package version 1.4.0, <https://CRAN.R-project.org/package=kableExtra>.\n```\n\n\n:::\n:::\n\n",
    "supporting": [
      "09-content_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}