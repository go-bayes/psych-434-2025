---
title: "Estimation of ATE and CATE Using Machine Learning"
date: "2025-APR-29"
format:
  html:
    warnings: false
    error: false
    messages: false
    code-overflow: scroll
    highlight-style: kate
    code-line-numbers: true
    code-fold: false
    code-tools:
      source: true
      toggle: false
html-math-method: katex
reference-location: margin
citation-location: margin
cap-location: margin
code-block-border-left: true
bibliography: /Users/joseph/GIT/templates/bib/references.bib
editor_options: 
  chunk_output_type: console
---

```{r}
#| eval: true
#| include: false
#| echo: false
# libraries
if (!require(margot, quietly = TRUE)) {
  devtools::install_github("go-bayes/margot") # ensure version is at least 1.0.21
}

if (!require(boilerplate, quietly = TRUE)) {
  devtools::install_github("go-bayes/boilerplate") # 
}

library("tinytex")
library(extrafont)
loadfonts(device = "win")
library(margot)
library(here)
# library(boilerplate)
library(patchwork)
library(kableExtra)
library(tidyverse)

# set save path
push_mods <- here::here('/Users/joseph/v-project\ Dropbox/data/courses/25-psych-434')

# read (large file)
# models_binary_flipped_all <- margot::here_read_qs("models_binary_flipped_all", push_mods)

models_binary_flipped_all <- here_read_qs("models_binary_flipped_all", here::here("data"))

# names
flipped_names <- here_read("flipped_names", push_mods)

# read labels
label_mapping_all <- here_read("label_mapping_all")

# set boilerplate path (set to your own machine)
master_path <- "/Users/joseph/GIT/templates/boilerplate_data"

# set directory database path
here_data_path = here::here("data")

# data
original_df <- margot::here_read("df_wide", push_mods)

# # import
# master_unified_db <- boilerplate_import(data_path = master_path)

# boilerplate_save(master_unified_db, output_file = "unified_db", data_path  = here_data_path, create_backup = FALSE)

# unified_db <- boilerplate_import(data_path = here_data_path)

# cat(unified_db$appendix$explain$grf)
# set model defaults -----------------------------------------------------
grf_defaults <- list(seed = 123,
                     # reproduce results
                     stabilize.splits = TRUE,
                     # robustness
                     # min.node.size = 5,  # default is five/ requires at least 5 observed in control and treated
                     # set higher for greater smoothing
                     num.trees = 2000 )
                     
                     # grf default = 2000 # set lower or higher depending on storage)

# set defaults for graphs (see bottom of script for options)
decision_tree_defaults <- list(span_ratio = .3,
                               text_size = 3.8,
                               y_padding = 0.25)  # Use full parameter name
                               # edge_label_offset = .002,
                               # border_size = .05)

# set defaults for graphs (see bottom of script for options)
# set policy tree defaults
policy_tree_defaults <- list(
  point_alpha = .5,
  title_size = 12,
  subtitle_size = 14,
  axis_title_size = 14,
  legend_title_size = 14,
  split_line_color = "red",
  split_line_alpha = 0.8,
  split_label_color = "red"
)

# rate table
rate_table_all <- margot_rate(
  models_binary_flipped_all, 
  label_mapping = label_mapping_all, 
  highlight_significant = TRUE
)

# view
# rate_table_all$rate_autoc |> kbl("markdown")
# rate_table_all$rate_qini |> kbl("markdown")

# generate interpretation
rate_interpretation_all <- margot_interpret_rate(
  rate_table_all, 
  flipped_outcomes = flipped_names
)

# view interpretations
cat(rate_interpretation_all$autoc_results)
cat(rate_interpretation_all$qini_results)
cat(rate_interpretation_all$comparison)

# # check out model names
# rate_interpretation_all$either_model_names
# rate_interpretation_all$qini_model_names
# rate_interpretation_all$both_model_names
# rate_interpretation_all$autoc_model_names

# define autoc model names for batch processing
# autoc plots ------------------------------------------------------------
# generate batch rate plots
batch_rate_autoc_plots <- margot_plot_rate_batch(
  models_binary_flipped_all, 
  save_plots = FALSE,
  # just use rate autoc
  model_names = rate_interpretation_all$autoc_model_names  
)

# batch_rate_autoc_plots$model_t2_hlth_fatigue_z
# view selected autoc plots
# batch_rate_autoc_plots$model_t2_log_hours_exercise_z
# batch_rate_autoc_plots$model_t2_hlth_fatigue_z
# batch_rate_autoc_plots$model_t2_self_control_z

# QINI --------------------------------------------------------------------
# batch process heterogeneity results for qini
models_binary_batch_qini <- margot::margot_policy(
  models_binary_flipped_all,
  save_plots = FALSE,
  decision_tree_args = decision_tree_defaults,
  policy_tree_args = policy_tree_defaults,
  model_names = rate_interpretation_all$qini_model_names,
  original_df = original_df,
  label_mapping = label_mapping_all
)

# view models
# first make graphs
plot_qini_exercise <-models_binary_batch_qini$model_t2_log_hours_exercise_z$qini_plot
plot_qini_fatigue <- models_binary_batch_qini$model_t2_hlth_fatigue_z$qini_plot
plot_qini_bodysat <- models_binary_batch_qini$model_t2_bodysat_z$qini_plot
plot_qini_self_control <- models_binary_batch_qini$model_t2_self_control_z$qini_plot
plot_qini_self_esteem <- models_binary_batch_qini$model_t2_self_esteem_z$qini_plot
plot_qini_belong <- models_binary_batch_qini$model_t2_belong_z$qini_plot

# recall the ate
# plot_all_models$plot

# patchwork allows us to group graphs together

# view plots
# (plot_qini_exercise / plot_qini_fatigue / plot_qini_bodysat) 
# 
# (plot_qini_self_control /plot_qini_self_esteem / plot_qini_t2_belong)
# (plot_qini_exercise / plot_qini_fatigue / plot_qini_bodysat) 

# (plot_qini_self_control /plot_qini_self_esteem / plot_qini_t2_belong)

# extract plots for models without reliable heterogeneity

# interpret qini curves
interpretation_qini_curves <- margot::margot_interpret_qini(
  models_binary_batch_qini,
  model_names = rate_interpretation_all$qini_model_names,
  label_mapping = label_mapping_all
)

# view qini interpretation
cat(interpretation_qini_curves$qini_explanation)

# view summary table
interpretation_qini_curves$summary_table |> kbl("markdown")

# others
# combine qini plots
# qini_plots_combined <- 
#   (plot_qini_exercise + plot_qini_fatigue +  plot_qini_bodysat) / (plot_qini_self_control + plot_qini_self_esteem + plot_qini_t2_belong) + 
#   plot_annotation(
#     title = "Qini Plots: Reliable Priority 'Spending' at Fractional Budgets",
#     tag_levels = "A",
#     theme = theme(legend.position = "top")
#   ) +
#   plot_layout(guides = "collect")
# 
# # view combined qini plots
# qini_plots_combined

# again compare with ate
# plot_all_models$plot 


# policy tree analysis ---------------------------------------------------
# model_names_subset <- c(
#   "model_t2_log_hours_exercise_z",
#   "model_t2_self_control_z", 
#   "model_t2_sexual_satisfaction_z",
#   "model_t2_belong_z",
#   "model_t2_support_z"
# )


plots_policy_trees <- margot::margot_policy(
  models_binary_flipped_all,
  save_plots = FALSE,
  output_dir = here::here(push_mods),
  decision_tree_args = decision_tree_defaults,
  policy_tree_args = policy_tree_defaults,
  model_names = rate_interpretation_all$either_model_names, # defined above
  original_df = original_df,
  label_mapping = label_mapping_all
)

# generate policy tree interpretations
interpretation_policy_trees <- margot::margot_interpret_policy_batch(
  models_binary_flipped_all,
  # use eithre model
  model_names = rate_interpretation_all$either_model_names, # defined above
  train_proportion = 0.8,
  original_df = original_df,
  label_mapping = label_mapping_all
)

# this will give you results
# cat(interpretation_policy_trees)

# # plots
# plots_policy_trees$model_t2_log_hours_exercise_z$decision_tree
# plots_policy_trees$model_t2_log_hours_exercise_z$policy_tree
# plots_policy_trees$model_t2_log_hours_exercise_z$combined_plot
# # 
# plots_policy_trees$model_t2_hlth_fatigue_z$decision_tree
# plots_policy_trees$model_t2_hlth_fatigue_z$policy_tree
# plots_policy_trees$model_t2_hlth_fatigue_z$combined_plot
# # 
# plots_policy_trees$model_t2_self_control_z$decision_tree
# plots_policy_trees$model_t2_self_control_z$policy_tree
# plots_policy_trees$model_t2_self_control_z$combined_plot

# plots_policy_trees$model_t2_meaning_sense_z$decision_tree
# plots_policy_trees$model_t2_meaning_sense_z$policy_tree
# plots_policy_trees$model_t2_meaning_sense_z$combined_plot
# 
# plots_policy_trees$model_t2_bodysat_z$decision_tree
# plots_policy_trees$model_t2_bodysat_z$policy_tree
# plots_policy_trees$model_t2_bodysat_z$combined_plot
# # 
# 
# plots_policy_trees$model_t2_self_esteem_z$decision_tree
# plots_policy_trees$model_t2_self_esteem_z$policy_tree
# plots_policy_trees$model_t2_self_esteem_z$combined_plot

# plots_policy_trees$model_t2_belong_z$decision_tree
# plots_policy_trees$model_t2_belong_z$policy_tree
# plots_policy_trees$model_t2_belong_z$combined_plot

# batch_rate_autoc_plots$model_t2_meaning_sense_z
# models_binary_batch_qini$model_t2_belong_z$qini_plot

```


::: {.callout-note}
**Required**
- [https://grf-labs.github.io/grf/](https://grf-labs.github.io/grf/)



**Optional**
- [@vanderweele2020] [link](https://www.dropbox.com/scl/fi/srpynr0dvjcndveplcydn/OutcomeWide_StatisticalScience.pdf?rlkey=h4fv32oyjegdfl3jq9u1fifc3&dl=0)
- [@suzuki2020] [link](https://www.dropbox.com/scl/fi/4midxwr9ltg9oce02e0ss/suzuki-causal-diagrams.pdf?rlkey=uktzf3nurtgpbj8m4h0xz82dn&dl=0)
- [@Bulbulia2024PracticalGuide] [link](https://osf.io/preprints/psyarxiv/uyg3d)
- [@hoffman2023] [link](https://arxiv.org/pdf/2304.09460.pdf)
:::


::: {.callout-important}
### Key concepts  
The workflow below introduces **heterogeneous-treatment-effect (HTE) analysis** with *causal forests*. By the end of the lecture you should recognise five technical ideas - ATE, CATE, the estimator $\widehat{\tau}(x)$, the RATE statistics drawn from a **Targeting-Operator Characteristic** (TOC) curve, and **policy trees**—and know how each fits into an applied research pipeline.
:::

::: {.callout-important}
### For the lab  

There are **Three** R scripts we will be using over the next few weeks. 

First:

[Script 1](https://raw.githubusercontent.com/go-bayes/psych-434-2025/refs/heads/main/laboratory/01-example-script-data-wrangling.R)

[Script 2](https://raw.githubusercontent.com/go-bayes/psych-434-2025/refs/heads/main/laboratory/02-example-script-data-wrangling-2.R)

[Script 3](https://raw.githubusercontent.com/go-bayes/psych-434-2025/refs/heads/main/laboratory/03-example-script-estimation-results.R)

:::



## Heterogeneous-Treatment-Effect Analysis with **causal forests**

### Why worry about heterogeneity?  


Relying on the average treatment effect (ATE) is a bit like handing out size-nine shoes to an entire student body: on *average* they might fit, but watch the tall students hobble and the small ones trip.  In the same way, a one-hour boost in weekly community socialising could send some students' sense of belonging soaring while leaving others cold—or even wishing they'd stayed home with the cat.  Spotting that spread, measuring how big it really is, and deciding whether it is worth tailoring the 'shoe size' are the three practical goals of HTE analysis.


---

### 1 Start with the average treatment effect (ATE)  

We begin with the most straightforward (and secretly impossible) counterfactual: *run two parallel universes—one where **everyone** gets the treatment, another where **no-one** does—and compare the final scores.  The resulting difference is the **average treatment effect**:  


$$
\text{ATE}=E\!\bigl[Y(1)-Y(0)\bigr].
$$

This gives us the average response -- the shoe size...


---

### 2 Do effects differ across people?  

Variation is captured by the **conditional average treatment effect (CATE)**,  

$$
\tau(x)=E\!\bigl[Y(1)-Y(0)\mid X=x\bigr],
$$

where $X$ gathers pre-treatment covariates -- age, baseline wellbeing, personality, etc... Normally these will be our baseline confounders. 

If $\tau(x)$ turns out to be flat, there is no heterogeneity worth targeting. 

People differ in countless, overlapping ways—think of age, baseline wellbeing, personality traits, study habits, and more. A linear interaction model tests whether the treatment works differently along one straight dimension, such as gender, by fitting a straight line. But real‐world data often twist and turn. If the true relationship bends like a garden hose, a straight line will miss the curve. Causal forests fix this by letting the data place splits wherever the shape changes, so they can follow any bends that appear [@wager2018].  Straight-line models are fine for simple patterns, but causal forests can trace the curves that simple lines overlook.


### 3. From straight lines to trees  

Traditional 'parametric' models (like simple regression) guess a single functional shape -- often a straight line -- before seeing the data.  A **non-parametric** model, by contrast, lets the data decide the shape.  A *regression tree* is the simplest non-parametric learner we will use.  

1. **Regression tree**  

*Idea*: split the covariate space by asking yes/no questions— 'Age ≤ 20?', 'Baseline wellbeing > 0.3?' — until each terminal **leaf** is fairly homogeneous.  Inside a leaf the predicted outcome is just the sample mean, so the tree builds a *piece-wise constant* surface instead of a global line.  

*Analogy*: think of tiling a garden with stepping-stones: each stone is flat, but taken together they follow the ground’s contours.

2. **Regression forest**  
   A single tree is quick and interpretable but unstable: small changes in the data can move the splits and shift predictions.  A **random forest** grows many trees on bootstrap samples and averages their outputs.  Averaging cancels much of the noise [@breiman2001random].  

3. **Causal Forests**  
   To estimate treatment effects rather than outcomes, each tree plays a two-step 'honest' game [@wager2018]:  
   - use one half of its sample to choose splits that separate treated from control units;  
   - use the other half to compute treatment-control differences within every leaf.  

   For a new individual with covariates $x_i$ each tree supplies a noisy leaf-level effect; the forest reports the **average**, written  

$$
  \widehat{\tau}(x)=E[Y(1)-Y(0)\mid X=x].
$$

Because the noisy estimates point in many directions, their average is markedly less variable -- *the wisdom of trees is a wisdom of crowds*.

In sum, a regression tree chops the data into locally flat chunks; a regression forest averages many such trees to smooth away chance idiosyncrasies; a causal forest adds honesty so that its averaged differences, though never directly observable for any one person, give our best data-driven forecast of individual treatment effects.
---

### 4 Built-in protection against over-fitting  

Honesty already separates model selection from estimation, but the forest adds a second safeguard: **out-of-bag (OOB) prediction**. Each $\widehat{\tau}(x_i)$ is averaged only over trees that never used $i$ in their split phase.  Together, honesty and OOB prediction deliver reliable uncertainty estimates even in high-dimensional settings.

---

### 5 Handling missing data  

The `grf` package adopts **Missing Incorporated in Attributes (MIA)** splitting.  'Missing' can itself become a branch, so cases are neither discarded nor randomly imputed.  This pragmatic approach keeps all observations in play while preserving the forest’s interpretability.

---

### 6 Is the heterogeneity *actionable*? — RATE statistics  


Once we have a personalised score $\widehat{\tau}(x)$ for every unit, the practical question is whether *targeting* high scorers delivers a benefit large enough to justify the extra effort.  The tool of choice is the **Targeting-Operator Characteristic (TOC)** curve:

$$
G(q)=\frac{1}{n}\sum_{i=1}^{\lfloor qn\rfloor}\widehat{\tau}_{(i)}, \qquad 0\le q\le1,
$$

where $\widehat{\tau}_{(1)}\ge\widehat{\tau}_{(2)}\ge\cdots$ are the estimated effects sorted from largest to smallest.  The horizontal axis $q$ is the fraction of the population we would treat; the vertical axis $G(q)$ is the cumulative gain we expect from treating that top slice.

Two integrals of the TOC curve summarise how lucrative targeting could be:

* **RATE AUTOC** (Area *Under* the TOC) puts equal weight on every $q$.  This answers: *If benefits are concentrated among the very best prospects, how much can we harvest by cherry-picking them?*  

* **RATE Qini** applies heavier weight to the mid-range of $q$.  This is the go-to metric when investigators face a fixed, moderate-sized budget—say, "we can afford to treat 40 % of individuals; will targeting help?"  [@yadlowsky2021evaluating]. We will evaluate the curve at treatment of 20% and 50% of the population.


To quantify the economic or policy value of heterogeneity, rank units by $\widehat{\tau}(x)$ and draw a **Targeting-Operator Characteristic (TOC)** curve that plots cumulative gain against the fraction $q$ of the population treated.  

---

### 7 RATE AUTOC EXAMPLE

Although OOB predictions are 'out-of-sample' for individual trees, the full forest still reuses information.  A simple remedy is to cut the data in half: **train** the forest on one fold and **test** RATE/Qini on the other.  This explicit split blocks optimistic bias and yields honest test statistics ($p$-values) [@grf2024].

```{r, results='asis'}
#| label: fig-rate-example
#| fig-cap: "RATE AUTOC: Hours Socialising → Sense of Meaning"
#| echo: false
#| column: screen
batch_rate_autoc_plots$model_t2_meaning_sense_z
```

@fig-rate-example depicts a typical RATE AUTOC curve with sample splitting.  A steep initial rise indicates that a small, correctly targeted programme could deliver large gains. Note that the curve begins dipping below zero past about 30%. At that point we might be doing worse than the ATE by targeting the CATE.

---

### 8 Visualising policy value: the Qini curve  

A **Qini curve** displays cumulative benefit on the vertical axis and treatment coverage on the horizontal.  As with the AUTOC curve we are using a held-out test fold to validate the reponse curve.

```{r, results='asis'}
#| label: fig-qini-example
#| fig-cap: "Qini Curve: Hours Socialising → Social Belonging"
#| echo: false
#| column: screen
plot_qini_belong
```




@fig-qini-example: we find that focussing on the top 20 % of individuals nets a gain of 0.08 units (95 % CI 0.04–0.12).  Widening the net to 50 % bumps the haul to 0.13 units (95 % CI 0.07–0.19).  After that the curve flattens -- once you’ve treated everyone who offers a decent return, there are no more 'big fish' left to catch.

---


### 9 From 'a black box' to simple rules: policy trees  

The causal forest hands us a personalised CATE for every individual, mapping a **high-dimensional** covariate vector $X$ to a number $\widehat{\tau}(X)$.  Helpful as that forecast is, it stops short of telling us *what to do*: the function itself is too tangled --- thousands of overlapping splits -- to translate directly into a policy.  

The **policytree** algorithm bridges that gap by collapsing the forest's many $\widehat{\tau}(X)$ values into a single, shallow decision tree whose depth you choose; each split is chosen to maximise expected benefit [@policytree_package_2024].  In this course we cap the depth at **2** for a practical balance:

- at most two yes/no questions per rule, so the logic fits on a slide you can present to policy-makers;
- each leaf still contains enough observations to yield a stable effect estimate;  
- deeper trees increase complexity faster than they improve payoff.

```{r, results='asis'}
#| label: fig-decision-tree
#| fig-cap: "Decision tree for Social Belonging"
#| echo: false
#| column: screen
plots_policy_trees$model_t2_belong_z$decision_tree
```

 
**Policy Tree Findings for Effect of Hour Socialising on Social Belonging:**

Participants are first split by Self Esteem at -0.925 (original scale: 3.958). For those with Self Esteem <= this threshold, the next split is by Neuroticism at 0.642 (original scale: 4.228). Within that subgroup, individuals with Neuroticism <= the threshold are recommended **control**, while those with Neuroticism > the threshold are recommended **treated**.

For participants with Self Esteem > -0.925 (original scale: 3.958), the second split is by Social Belonging at 0.776 (original scale: 5.972). In this subgroup, individuals with Social Belonging <= the threshold are recommended **treated**, while those with Social Belonging > the threshold are recommended **control**.

**Policy Rule**

> If self-esteem ≤ −0.93 and neuroticism ≤ 0.64, do **not** recommend extra socialising; otherwise, recommend it unless current belonging > 0.78.*


```{r, results='asis'}
#| label: fig-policy-map
#| fig-cap: "Predicted treatment assignment (predictions out of training sample)"
#| fig-height: 14   # adjust
#| fig-width: 16  # adjust
#| echo: false
plots_policy_trees$model_t2_belong_z$policy_tree
```



---

### 10 Ethical and practical considerations  

Statistical optimality rarely lines up with social acceptability.  A rule that maximises expected health gains might still be **unaffordable** for a public agency, **unfair** to a protected group, or **opaque** to those asked to trust it.  Typically these trade-offs lie beyond the statistician’s remit (see the caveats in [Lecture 6](https://go-bayes.github.io/psych-434-2025/content/06-content.html#appendix-b-evidence-for-effect-modification-is-relative-to-inclusion-of-other-variables-in-the-model)).  

Yet the very same CATE machinery that powers targeting also helps science move past a *one-size-fits-all* mindset.  By mapping treatment effects across a high-dimensional covariate space, we can test whether our favourite categories -- gender, age group, clinical severity -- actually capture the differences that matter.  Sometimes they do; often they don't, revealing that nature is not carved at the joints of our folk classifications.  Discovering *where* the forest sees meaningful splits can generate fresh psychological hypotheses about who responds, why, and under what circumstances, even when no policy decision is on the table.


<!-- Before deployment we therefore need three extra layers of scrutiny: -->

<!-- 1. **Cost realism**   Will the programme's administrative and opportunity costs erase the forecast benefit?  A cost–effectiveness analysis can reveal when a simpler, less 'optimal' rule actually delivers better value.   -->
<!-- 2. **Fairness auditing**  check whether error rates or treatment probabilities differ by gender, ethnicity, or socioeconomic status.  If gaps appear, consider adjusting the loss function or adding fairness constraints [@mitchell2021algorithmic].   -->
<!-- 3. **Transparency and consent**   Publish the rule, document data sources, and secure stakeholder buy-in.  Transparent governance limits the risk of political backlash and encourages external replication. -->

---

### Summary/next steps  

Our workflow answers three questions in sequence:

1. **Is there substantial heterogeneity?**  Reject $H_0{:}\tau(x)$ constant if RATE AUTOC or RATE Qini is positive and statistically reliable  
2. **Does targeting pay at realistic budgets?**  Inspect the slope of the Qini curve around plausible coverage levels.
3. **Can we express the targeting rule in a few defensible steps?**  fit and validate a shallow policy tree.

In the lab section you will reproduce each stage on a simulated dataset.

---


## Lab: Data Preparation and Analysis Scripts


### Link to data dictionary

::: {.callout-note}
For information about the variables in the synthetic data, download the New Zealand Attitudes and Values Data Dictionary here under "Primary Resources"

[https://osf.io/75snb/](https://osf.io/75snb/)

:::


### Script 1:



```{r}
#| include: true
#| eval: false
#| file: ../laboratory/01-example-script-data-wrangling.R


```


### Script 2

```{r}
#| include: true
#| eval: false
#| file: ../laboratory/02-example-script-data-wrangling-2.R

```


### Script 3

```{r}
#| include: true
#| eval: false
#| file: ../laboratory/03-example-script-estimation-results.R

```



## HOMEWORK: Prepare a fresh set of analysis scripts using a different exposure

- E.g. Ask: what are the effects of a shift in religious service `religion_church` on multi-dimensional well-being. 
- Consider what variables you need for confounding control at baseline. 
- Think about how to make the exposure variable binary.
- You may consider different outcome(s) as well as a different exposure. 




### Packages

```{r}
report::cite_packages()
```



## Appendix 

## Review: The Fundamental Problem of Causal Inference as a Missing Data Problem

Recall the fundamental problem of causal inference, returning to the question of whether bilingualism improves cognitive abilities:

-   $Y_i^{a = 1}$: The cognitive ability of child $i$ if they were bilingual. This is the counterfactual outcome when A = 1.
-   $Y_i^{a = 0}$:: The cognitive ability of child $i$ if they were monolingual. This is the counterfactual outcome when A = 0.

The causal effect of bilingualism on cognitive ability for individual $i$ is then defined as the difference between these potential outcomes:

$$
\text{Causal Effect}_i = Y_i^{a=1} - Y_i^{a=0} 
$$

We say there is a causal effect if:

$$
Y_i^{a=1} - Y_i^{a=0}  \neq 0
$$

However, we only observe one of the potential outcomes for each child. The other outcome is not observed because physics prevents a child from both receiving and not receiving bilingual exposure.

The fact that causal contrasts are not observed in individuals is called "The fundamental problem of causal inference."

Although we typically cannot observe individual causal effects, we can obtain average causal effects when certain assumptions are satisfied.

```{=tex}
\begin{align}
E(\delta) = E(Y^{a=1} - Y^{a=0})\\
          ~  = E(Y^{a=1}) - E(Y^{a=0}) \\
          ~  = ATE
\end{align}
```
We may identify average causal effects from the data when the following assumptions are met:

-   **Causal Consistency:** The exposure values under comparisons correspond to well-defined interventions that, in turn, correspond to the treatment versions in the data.[]
-   **Positivity:** The probability of receiving every value of the exposure within all strata of co-variates is greater than zero []
-   **Exchangeability:** The conditional probability of receiving every value of an exposure level, though not decided by the investigators, depends only on the measured covariates []

Further assumptions:

-   **No Interference,** also known as the **Stable Unit Treatment Value Assumption** (SUTVA), requires that the treatment given to one unit (e.g., person, group, organization) does not interfere with the potential outcomes of another unit. Put differently, there are no "spillover" effects. Note: this assumption may be thought to be part of causal consistency, namely individual has only one potential outcome under each treatment condition.
-   **Correctly specified model**: the requirement that the underlying statistical model used to estimate causal effects accurately represents the true relationships between the variables of interest. We say the model should be able to capture "the functional form" of the relationship between the treatment, the outcome, and any covariates. The model's functional form should be flexible enough to capture the true underlying relationship. The estimated causal effects may be biased if the model's functional form is incorrect. Additionally, the model must handle omitted variable bias by including all relevant confounders and should correctly handle missing data from non-response or loss-to follow up. We will return to the bias arising from missing data in the weeks ahead. For now, it is important to note that causal inference assumes that our model is correctly specified.


## Subgroup analysis

Redcall, **Effect Modification** (also known as "heterogeneity of treatment effects", and "Effect-measure modification") occurs when the causal effect of intervention $A$ varies across different levels of another variable $R$:

$$E(Y^{a=1}|G=g_1, L=l) - E(Y^{a=0}|G=g_1, L=l) \neq E(Y^{a=1}|G=g_2, L=l) - E(Y^{a=0}|G=g_2, L=l)$$

Effect modification indicates that the magnitude of the causal effect of intervention $A$ is related to the modifier variable $G$ level. As discussed last week, effect modification can be observed even when there is no direct causal interaction between the treatment and the modifier variable. We noted that **interaction in causal inference refers to a situation where the combined effect of two interventions is not equal to the sum of their individual effects**. **Effect modification, on the other hand, occurs when the causal effect of one intervention varies across different levels of another variable.**


We also noted that 

> **For comparative research, we are typically interested in effect-modification, which requires subgroup analysis.**


### Causal Estimand, Statistical Estimand, Statistical Estimator

Let's set subgroup analysis to the side for a moment and begin focussing on statistical estimation.  

Suppose a researcher wants to understand the causal effect of marriage on individual happiness. Participants in the study are surveyed for their marital status ("married" or "not married") and their self-reported happiness on a scale from 1 to 10.

#### Causal Estimand

- **Definition**: The causal estimand is the specific quantity or parameter that we aim to estimate to understand the causal effect of an intervention or treatment on an outcome.

- **Example**: Here, the **Causal Estimand** would be the Average Treatment Effect (ATE) of being married on happiness. Specifically, we define the ATE as the difference in the potential outcomes of happiness if all individuals were married versus if no individuals were married:

  $$
  \text{ATE} = E[Y^{a=1} - Y^{a=0}]
  $$


  Here, $Y^{a=1}$ represents the potential happiness score if an individual is married, and $Y^{a=0}$ if they are not married.


#### Next step: Are Causal Assumptions Met? 

- Identification (Exchangeability): balance in the confounders across the treatments to be compared

- Consistency: well-defined interventions

- Positivity: treatments occur within levels of covariates $L$


#### Statistical Estimand (next step)

- **The problem**: how do we bridge the gap between potential outcomes and data? 

- **Definition**: the statistical estimand is the parameter or function that summarises the relationship between variables as described by a statistical model applied to data. 

- **Example**: for our study, the **Statistical Estimand** might be the mean difference in happiness scores between individuals who are married and those who are not, as derived from a linear regression model:

  $$
  \text{Happiness} = \beta_0 + \beta_1 \times \text{Married} + \epsilon
  $$

  In this equation, $\beta_1$ represents the estimated difference in happiness scores between the married and non-married groups.

#### Statistical Estimator

- **Definition**: a statistical estimator is a rule or method by which a numerical estimate of a statistical estimand is calculated from the data.

- **Example**: in our marriage study, the **Statistical Estimator** for $\beta_1$ is the ordinary least squares (OLS) estimator. This estimator is used to calculate $\beta_1$ from the sample data provided by the survey. It provides an estimate of the impact of being married on happiness, calculated using:
  $$
  \hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}
  $$
  where $X_i$ is a binary indicator for being married (1 for married, 0 for not married), $Y_i$ is the observed happiness score, and $\bar{X}$, $\bar{Y}$ are the sample means of $X$ and $Y$, respectively.

The upshot, we anchor our causal inquiries within a multi-step framework of data analysis. This involves: 

1. clearly defining our causal estimand within a specified *target population,*
2. clarifying assumptions, & especially identification assumptions, 
3. describing a statistical strategy for extracting this estimand from the data, and then 
4. applying an algorithm that embodies this statistical method.


<!-- ## Methods for Statistical Estimation in Causal Inference: Inverse Probability of Treatment Weights Using Propensity Scores -->

<!-- Last week, we discussed confounding control using regression adjustment. Recall the formula for the average treatment effect (ATE) when conditioning on a set of covariates $L$: -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \text{ATE} = E[Y^{a=1} \mid L = l] - E[Y^{a=0} \mid L = l] \quad \text{for any value of } l -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- > "We say that a set $L$ of measured non-descendants of $L$ is a sufficient set for confounding adjustment when conditioning on $L$ blocks all backdoor paths—that is, the treated and the untreated are exchangeable within levels of $L$" (Hernán & Robins, *Causal Inference*, p. 86). -->

<!-- This formula calculates the expected outcome difference between treated ($a=1$) and untreated ($a=0$) groups, given a specific value of the covariates $l$. -->

<!-- Inverse Probability of Treatment Weighting (IPTW) takes a different approach. We create a pseudo-population where the treatment assignment is independent of the observed covariates by assigning weights to each individual based on their propensity scores. -->

<!-- **We do this by modelling the treatment** -->

<!-- Denote the treatment indicator by $A$, where $A = 1$ if an individual receives treatment and $A = 0$ otherwise. $L$ represents the vector of observed covariates, and $Y^a$ the potential outcomes. The propensity score, $e(L)$, is defined as the probability of receiving the treatment given the observed covariates: -->

<!-- $$ -->
<!-- \hat{e}(L) = P(A = 1 \mid L) -->
<!-- $$ -->

<!-- To obtain IPTW weights, compute the inverse probability of treatment: -->

<!-- $$ -->
<!-- v_i = \frac{A_i}{\hat{e}(L_i)} + \frac{1 - A_i}{1 - \hat{e}(L_i)} -->
<!-- $$ -->

<!-- Which simplifies to  -->

<!-- $$ -->
<!-- v_i =  -->
<!-- \begin{cases}  -->
<!-- \frac{1}{\hat{e}} & \text{if } A_i = 1 \\ -->
<!-- \frac{1}{1-\hat{e}} & \text{if } A_i = 0  -->
<!-- \end{cases} -->
<!-- $$ -->

<!-- where $v_i$ is the IPTW weight for individual $i$, $A_i$ is the treatment indicator for individual $i$, and $\hat{e}(L_i)$ is the estimated propensity score for individual $i$.    -->

<!-- How might we use these weights to obtain causal effect estimates? -->



<!-- ## Marginal Structural Models (MSMs) -->

<!-- Marginal Structural Models (MSMs) estimate causal effects without requiring an "outcome model" that stratifies on covariates. Rather, MSMs employ weights derived from the inverse probability of treatment weighting (IPTW) to create a pseudo-population in which the distribution of covariates is independent of treatment assignment over time. -->

<!-- The general form of an MSM can be expressed as follows: -->

<!-- $$ -->
<!-- E[Y^a] = \beta_0 + \beta_1a -->
<!-- $$ -->

<!-- where $E[Y^a]$ is the expected outcome under treatment $a$  and $\beta_0$ and $\beta_1$ are parameters estimated by fitting the weighted model. Again, the weights used in the MSM, typically derived from the IPTW (or another treatment model), adjust for the confounding, allowing the model to estimate the unbiased effect of the treatment on the outcome without requiring covariates in the model. -->

<!-- Where do weights fit in?   Note, we have $E[Y^a]$ in please of $E[Y|A=a]$.  When applying propensity score weights in the linear regression model $E[Y^a] = \beta_0 + \beta_1a$, each observation is weighted by $v_i$, such that $v_i(\beta_0 + \beta_1a)$. This changes the estimation process to focus on a weighted sum of outcomes, where each individual's contribution is adjusted to reflect their probability of receiving the treatment, given their covariates. -->


<!-- ## Interpretation of $\beta_0$ and $\beta_1$  in a Marginal Structural Model -->

<!-- ### Binary Treatment -->

<!-- In models where the treatment $a$ is binary (e.g., $a = 0$ or $a = 1$), such as in many causal inference studies: -->

<!-- - **$\beta_0$**: the expected value of the outcome $Y$ when the treatment is not applied ($a = 0$). This is the baseline level of the outcome in the absence of treatment. -->
<!-- - **$\beta_1$**: the change in the expected outcome when the treatment status changes from 0 to 1. In logistic regression, $\beta_1$ represents the log-odds ratio of the outcome for the treatment group relative to the control group. In linear regression, $\beta_1$ quantifies the difference in the average outcome between the treated and untreated groups. -->

<!-- ### Continuous Treatment -->

<!-- When the treatment $a$ is continuous, the interpretation of $\beta_0$ and $\beta_1$ adjusts slightly: -->

<!-- - **$\beta_0$**: represents the expected value of the outcome $Y$ when the treatment $a$ is at its reference value (often zero).  -->
<!-- - **$\beta_1$**: represents the expected change in the outcome for each unit increase in the treatment. In this case, $\beta_1$ measures the gradient or slope of the relationship between the treatment and the outcome. For every one-unit increase in treatment, the outcome changes by $\beta_1$ units, assuming all other factors remain constant. -->


<!-- ###  How can we apply marginal structural models in subgroups?  -->


<!-- ### Assumptions -->

<!-- - **Model assumptions**: the treatment model is correctly specified. -->
<!-- - **Causal assumptions**: all confounders are appropriately controlled, positivity and consistency assumptions hold. -->


<!-- ### Calculating Treatment Weights (Propensity Scores) and Confounding Control in Subgroups -->

<!-- We may often achieve greater balance when conducting weighted analyses in subgroups by estimating propensity scores *within* these subgroups. The propensity score $ e(L, G) $ is the conditional probability of receiving the exposure $ A = 1 $, given the covariates $ L $ and subgroup indicator $ G $. This is often modelled using logistic regression or other methods that ensure covariate balance -->
<!-- We define the estimated propensity score as follows: -->

<!-- $$ -->
<!-- \hat{e} = P(A = 1 \mid L, G) = f_A(L, G; \theta_A) -->
<!-- $$ -->

<!-- Here, $ f_A(L, G; \theta_A) $ is the statistical model estimating the probability of exposure $A = 1$ given covariates $L$ and subgroup $G$. We then calculate the weights for each individual, denoted $v$, using the estimated propensity score: -->

<!-- $\theta_A$ encapsulates all the coefficients (parameters) in this model, including intercepts, slopes, and potentially other parameters depending on the model complexity (e.g., interaction terms, non-linear effects...etc). -->

<!-- These weights $v$ depend on $A$ and are calculated as the inverse of the propensity score for exposed individuals and as the inverse of $ 1-\hat{e} $ for unexposed individuals. -->

<!-- Propensity scores are estimated *separately* within strata of the subgroup to control for potential confounding tailored to each subgroup. These weights $v$ are specific to each individual in subgroup $G$. In the lab, we will clarify how to fit models to estimate contrasts for the causal effects within groups $\hat{\delta}_{g}, \hat{\delta}_{g'}$, etc., and how to obtain estimates for group-wise differences: -->

<!-- $$ -->
<!-- \hat{\gamma} = \overbrace{\big( \hat{E}[Y^a \mid G=g] - \hat{E}[Y^{a'} \mid G=g] \big)}^{\hat{\delta}_g} - \overbrace{\big( \hat{E}[Y^{a'} \mid G=g'] - \hat{E}[Y^a \mid G=g'] \big)}^{\hat{\delta}_{g'}} -->
<!-- $$ -->


<!-- - **$\hat{E}[Y^a \mid G=g]$**: Estimated expected outcome when treatment $a$ is applied to subgroup $G=g$. -->
<!-- - **$\hat{E}[Y^{a'} \mid G=g]$**: Estimated expected outcome when a different treatment or control $a'$ is applied to the same subgroup $G=g$. -->
<!-- - **$\hat{\delta}_g$**: Represents the estimated treatment effect within subgroup $G=g$, computed as the difference in expected outcomes between treatment $a$ and $a'$ within this subgroup. -->

<!-- - **$\hat{E}[Y^{a'} \mid G=g']$**: Estimated expected outcome when treatment $a'$ is applied to a different subgroup $G=g'$. -->
<!-- - **$\hat{E}[Y^a \mid G=g']$**: Estimated expected outcome when treatment $a$ is applied to subgroup $G=g'$. -->
<!-- - **$\hat{\delta}_{g'}$**: Represents the estimated treatment effect within subgroup $G=g'$, computed as the difference in expected outcomes between treatment $a'$ and $a$ within this subgroup. -->

<!-- - **$\hat{\gamma}$**: The overall measure calculated from your formula represents the difference in treatment effects between two subgroups, $G=g$ and $G=g'$. It quantifies how the effect of switching between treatments $a$ and $a'$ differs across the two subgroups. -->


<!-- ### Considerations -->

<!-- - **Estimation**: to estimate the expected outcomes $\hat{E}[Y^a \mid G]$ and $\hat{E}[Y^{a'} \mid G]$, we require statistical models. If we use regression, we include interaction terms between treatment and subgroup indicators to directly estimate subgroup-specific treatment effects. Our use depends on correct model specification. -->
<!-- - **Confidence intervals**: we may compute confidence intervals for $\hat{\gamma}$ using bootstrap, the delta method, or -- in our excercises -- simulation based methods. -->
<!-- - **Causal assumptions**: again, a causal interpretation of $\hat{\gamma}$ relies on satisfying both causal assumptions and modelling assumptions.  Here, we have described estimation using propensity scores. -->


<!-- ## Doubly Robust Estimation -->

<!-- We can combine regression-based estimation with propensity score estimation to obtain *doubly robust* estimation. I will walk you through the steps in the lab. The TL;DR is this: doubly robust estimation reduces reliance on correct model specification. If either the PS model or the regression model is correctly specified, the model will be unbiased -- if the other causal inference assumptions are met. -->

<!-- We cannot know whether these assumptions are met, we will need to do a sensitivity analysis, the topic of next week. -->

<!-- I'll show you in lab how to employ simulation-based inference methods to compute standard errors and confidence intervals, following the approaches suggested by Greifer (2023)[]. -->

<!-- ## Extra Readings n on Propensity Scores: -->

<!-- Noah Griefer's Software and Blogs: [https://ngreifer.github.io/blog/subgroup-analysis-psm/](https://ngreifer.github.io/blog/) -->